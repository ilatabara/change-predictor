id,project,branch,change_id,subject,status,created,updated,submitted,reviewers,revisions,total_comment_count,number,current_revision,discussion_messages_count,reviewers_count,revisions_count,owner_account_id,owner_name,owner_username,is_owner_bot,commit_message,git_command,changed_files,files_count,commit_id,topic,added_lines,deleted_lines,insertions,deletions
openstack%2Fnova~master~I7ba648a37306ce97cd8ae20e7d2a44c0cf7fc479,openstack/nova,master,I7ba648a37306ce97cd8ae20e7d2a44c0cf7fc479,VMware: support passing flavor object in spawn,MERGED,2014-11-20 13:05:09.000000000,2014-12-08 15:51:28.000000000,2014-12-08 15:51:25.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-20 13:05:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b9af68bab796d6ecceed130ed77300d7f2b25289', 'message': 'VMware: support passing flavor object in spawn\n\nMakes use of the flavor, if defined, passed by the spawn method. This\nsaves a database call.\n\nChange-Id: I7ba648a37306ce97cd8ae20e7d2a44c0cf7fc479\n'}, {'number': 2, 'created': '2014-11-25 12:02:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/750fd94d0599679f482edd6438ec01bd478b80ff', 'message': 'VMware: support passing flavor object in spawn\n\nMakes use of the flavor, if defined, passed by the spawn method. This\nsaves a database call.\n\nChange-Id: I7ba648a37306ce97cd8ae20e7d2a44c0cf7fc479\n'}, {'number': 3, 'created': '2014-11-30 15:55:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/2a38f022a30e68abf76da54571bbbec9e9a89adf', 'message': 'VMware: support passing flavor object in spawn\n\nMakes use of the flavor, if defined, passed by the spawn method. This\nsaves a database call.\n\nChange-Id: I7ba648a37306ce97cd8ae20e7d2a44c0cf7fc479\n'}, {'number': 4, 'created': '2014-12-06 17:13:55.000000000', 'files': ['nova/virt/vmwareapi/vmops.py', 'nova/virt/vmwareapi/driver.py', 'nova/tests/unit/virt/vmwareapi/test_driver_api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/d126457386694cc961833f1bef66167e8c3ecfc3', 'message': 'VMware: support passing flavor object in spawn\n\nMakes use of the flavor, if defined, passed by the spawn method. This\nsaves a database call.\n\nChange-Id: I7ba648a37306ce97cd8ae20e7d2a44c0cf7fc479\n'}]",0,135938,d126457386694cc961833f1bef66167e8c3ecfc3,39,10,4,1653,,,0,"VMware: support passing flavor object in spawn

Makes use of the flavor, if defined, passed by the spawn method. This
saves a database call.

Change-Id: I7ba648a37306ce97cd8ae20e7d2a44c0cf7fc479
",git fetch https://review.opendev.org/openstack/nova refs/changes/38/135938/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/vmwareapi/vmops.py', 'nova/virt/vmwareapi/driver.py', 'nova/tests/unit/virt/vmwareapi/test_driver_api.py']",3,b9af68bab796d6ecceed130ed77300d7f2b25289,vmware-use-instance-type," instance_name=None, power_on=True, flavor=None): power_on=self._power_on, flavor=flavor)"," instance_name=None, power_on=True): power_on=self._power_on)",11,7
openstack%2Fnova~master~Ia2692651dfa6420eec3395b0363833aeadd2c0a5,openstack/nova,master,Ia2692651dfa6420eec3395b0363833aeadd2c0a5,VMware: Refactor common root disk path calculation,ABANDONED,2014-08-27 15:55:18.000000000,2014-12-08 15:51:24.000000000,,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 5170}, {'_account_id': 8412}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9172}, {'_account_id': 9550}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-27 15:55:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/14bcefc62469b46d61f4a0d53ea04a22e6821a73', 'message': ""VMware: Add VirtualMachineInstanceConfigInfo.root_disk_path\n\nRefactor the calculation of an instance's root disk path into\nVirtualMachineInstanceConfigInfo as it's used in 2 places.\n\nChange-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5\n""}, {'number': 2, 'created': '2014-08-28 11:00:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/367d14182ba0638256de3adb93d8bc01f4b10891', 'message': ""VMware: Add VirtualMachineInstanceConfigInfo.root_disk_path\n\nRefactor the calculation of an instance's root disk path into\nVirtualMachineInstanceConfigInfo as it's used in 2 places.\n\nChange-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5\n""}, {'number': 3, 'created': '2014-09-02 08:42:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/5dee7dc15c760af66a788ec6da190e47d217727e', 'message': ""VMware: Add VirtualMachineInstanceConfigInfo.root_disk_path\n\nRefactor the calculation of an instance's root disk path into\nVirtualMachineInstanceConfigInfo as it's used in 2 places.\n\nChange-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5\n""}, {'number': 4, 'created': '2014-09-03 15:58:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/6fe2d7bcf6a8b4212c21f838e64ce714b40a30b8', 'message': ""VMware: Add VirtualMachineInstanceConfigInfo.root_disk_path\n\nRefactor the calculation of an instance's root disk path into\nVirtualMachineInstanceConfigInfo as it's used in 2 places.\n\nChange-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5\n""}, {'number': 5, 'created': '2014-09-16 16:03:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/aac6931e2bb32ce8e933f3bb87094549b1d9c7f8', 'message': ""VMware: Add VirtualMachineInstanceConfigInfo.root_disk_path\n\nRefactor the calculation of an instance's root disk path into\nVirtualMachineInstanceConfigInfo as it's used in 2 places.\n\nChange-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5\n""}, {'number': 6, 'created': '2014-09-24 14:22:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9a49702d07ce247f6252e84fb20062321aa3b170', 'message': 'VMware: Pass root disk location to vmops._use_*\n\nRoot disk path was being calculated in 2 places. This change factors\nit out and passes it to the functions which were using it.\n\nChange-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5\n'}, {'number': 7, 'created': '2014-09-25 15:37:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1f86910ffe0c5e0d5b7cb674a4e2529c40b6935d', 'message': 'VMware: Refactor common root disk path calculation\n\nRoot disk path was being calculated in 2 places. This change makes it\na property of VirtualMachineInstanceConfigInfo, whose properties it is\nwholly dependent on.\n\nChange-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5\n'}, {'number': 8, 'created': '2014-09-29 14:31:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/672dc5a11f599dd7518bd2a7fac91b5ca33b4912', 'message': 'VMware: Refactor common root disk path calculation\n\nRoot disk path was being calculated in 2 places. This change makes it\na property of VirtualMachineInstanceConfigInfo, whose properties it is\nwholly dependent on.\n\nChange-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5\n'}, {'number': 9, 'created': '2014-10-02 15:56:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/6e4e2b1e75f4b1efb5d199c97a8015e469b1b189', 'message': 'VMware: Refactor common root disk path calculation\n\nRoot disk path was being calculated in 2 places. This change makes it\na property of VirtualMachineInstanceConfigInfo, whose properties it is\nwholly dependent on.\n\nChange-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5\n'}, {'number': 10, 'created': '2014-10-08 09:33:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/87d54a8219b5dd3675c44ea198d184caef964e76', 'message': 'VMware: Refactor common root disk path calculation\n\nRoot disk path was being calculated in 2 places. This change makes it\na property of VirtualMachineInstanceConfigInfo, whose properties it is\nwholly dependent on.\n\nChange-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5\n'}, {'number': 11, 'created': '2014-10-15 15:08:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d723eaf000581ac0537156a67a5fdd7638e37854', 'message': 'VMware: Refactor common root disk path calculation\n\nRoot disk path was being calculated in 2 places. This change makes it\na property of VirtualMachineInstanceConfigInfo, whose properties it is\nwholly dependent on.\n\nChange-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5\n'}, {'number': 12, 'created': '2014-10-31 16:19:03.000000000', 'files': ['nova/virt/vmwareapi/vmops.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/aed11a3b1fc153d9f3a9c91052b615a71dbc1bee', 'message': 'VMware: Refactor common root disk path calculation\n\nRoot disk path was being calculated in 2 places. This change makes it\na property of VirtualMachineInstanceConfigInfo, whose properties it is\nwholly dependent on.\n\nChange-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5\n'}]",5,117284,aed11a3b1fc153d9f3a9c91052b615a71dbc1bee,87,12,12,9555,,,0,"VMware: Refactor common root disk path calculation

Root disk path was being calculated in 2 places. This change makes it
a property of VirtualMachineInstanceConfigInfo, whose properties it is
wholly dependent on.

Change-Id: Ia2692651dfa6420eec3395b0363833aeadd2c0a5
",git fetch https://review.opendev.org/openstack/nova refs/changes/84/117284/12 && git format-patch -1 --stdout FETCH_HEAD,['nova/virt/vmwareapi/vmops.py'],1,14bcefc62469b46d61f4a0d53ea04a22e6821a73,vmware_refactor_cleanup," def root_disk_path(self): instance_folder = self.instance_name root_disk_name = ""%s.vmdk"" % self.instance_name return self.datastore.build_path(instance_folder, root_disk_name) @property root_disk_ds_loc = vi.root_disk_path root_disk_ds_loc = vi.root_disk_path"," instance_folder = vi.instance_name root_disk_name = ""%s.vmdk"" % vi.instance_name root_disk_ds_loc = vi.datastore.build_path(instance_folder, root_disk_name) instance_folder = vi.instance_name root_disk_name = ""%s.vmdk"" % vi.instance_name root_disk_ds_loc = vi.datastore.build_path(instance_folder, root_disk_name)",8,8
openstack%2Fnova~master~I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2,openstack/nova,master,I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2,VMware: Rewrite test_fetch_image_if_missing to test outcome,ABANDONED,2014-09-19 14:59:17.000000000,2014-12-08 15:51:17.000000000,,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 4393}, {'_account_id': 5170}, {'_account_id': 7629}, {'_account_id': 9008}, {'_account_id': 9172}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-19 14:59:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/dd02309718eb9d22b7164ef618f923600b8d4947', 'message': ""VMware: Rewrite test_fetch_image_if_missing to test outcome\n\nThis change updates the following tests:\n\ntest_fetch_image_if_missing\ntest_fetch_image_if_missing_with_sparse\ntest_fetch_image_if_missing_with_iso\n\nThese tests were all calls to _test_fetch_image_if_missing with\ndifferent arguments. _test_fetch_image_if_missing made extensive use\nof mock, and executed very little Nova code. It was simply testing\nthat certain internal functions were called for images of a particular\ntype.\n\nThe problem with this is that it assumes every internal detail of the\nimplementation of vmops._fetch_image_if_missing, which means that you\ncan't change any of those details without breaking this test.\nConsequently, this test cannot be used to validate changes made to\nvmops._fetch_image_if_missing.\n\nThis change assumes that our unit boundary for unit testing is\nvmops._fetch_image_if_missing(). We define its correctness by the\neffects it has on a vSphere server. We make no assumptions about the\nimplementation of vmops._fetch_image_if_missing(), only test that it\nworks as expected.\n\nConsequently, we remove almost all use of mock, and allow Fake VIM to\nhandle server calls. On completion, we check that the correct files\nhave been created. We also log server calls so that we can check a\nsparse input was converted during the process.\n\nWe also remove all tests of internal functions which are\nimplementation details of vmops._fetch_image_if_missing(), and\ntherefore inside the unit boundary. The functionality of all of these\ntests is either not relevant due to testing an implementation detail,\nor covered by the new _test_fetch_image_if_missing:\n\ntest_prepare_iso_image\ntest_prepare_sparse_image\n  Tested the returned temp directory and write location. The precise\n  temp directory path is not relevant, and we test the write location\n  when we test the filename of the resulting image.\n\ntest_prepare_flat_image\n  Tested the returned temp directory and write location, as above.\n  Tested that the parent directory of the write location is created\n  explicitly, which is now tested by Fake VIM during disk creation.\n  Tested that create_virtual_disk was called, which we now test by\n  ensuring the disk is present when the function returns. Tested that\n  the -flat.vmdk was deleted, which is not necessary in any case as it\n  is overwritten by the file upload.\n\ntest_cache_iso_image\ntest_cache_flat_image\n  Tested that the function calls ds_util.file_move. This is now tested\n  when we test the resulting file locations.\n\ntest_cache_sparse_image\n  Tested that the function calls vm_util.copy_virtual_disk. This is\n  now tested by logging VIM calls and ensuring that an additional\n  CopyVirtualDisk_Task occurs for a sparse disk.\n\nThe effect of this change is that the tests have equal or greater\ncoverage, and can be used to validate changes to the code under test\nwithout requiring modification.\n\nChange-Id: I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2\n""}, {'number': 2, 'created': '2014-09-24 10:44:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1af35e9417fcaa63cc4a963bd7a6acf41dffc420', 'message': ""VMware: Rewrite test_fetch_image_if_missing to test outcome\n\nThis change updates the following tests:\n\ntest_fetch_image_if_missing\ntest_fetch_image_if_missing_with_sparse\ntest_fetch_image_if_missing_with_iso\n\nThese tests were all calls to _test_fetch_image_if_missing with\ndifferent arguments. _test_fetch_image_if_missing made extensive use\nof mock, and executed very little Nova code. It was simply testing\nthat certain internal functions were called for images of a particular\ntype.\n\nThe problem with this is that it assumes every internal detail of the\nimplementation of vmops._fetch_image_if_missing, which means that you\ncan't change any of those details without breaking this test.\nConsequently, this test cannot be used to validate changes made to\nvmops._fetch_image_if_missing.\n\nThis change assumes that our unit boundary for unit testing is\nvmops._fetch_image_if_missing(). We define its correctness by the\neffects it has on a vSphere server. We make no assumptions about the\nimplementation of vmops._fetch_image_if_missing(), only test that it\nworks as expected.\n\nConsequently, we remove almost all use of mock, and allow Fake VIM to\nhandle server calls. On completion, we check that the correct files\nhave been created. We also log server calls so that we can check a\nsparse input was converted during the process.\n\nWe also remove all tests of internal functions which are\nimplementation details of vmops._fetch_image_if_missing(), and\ntherefore inside the unit boundary. The functionality of all of these\ntests is either not relevant due to testing an implementation detail,\nor covered by the new _test_fetch_image_if_missing:\n\ntest_prepare_iso_image\ntest_prepare_sparse_image\n  Tested the returned temp directory and write location. The precise\n  temp directory path is not relevant, and we test the write location\n  when we test the filename of the resulting image.\n\ntest_prepare_flat_image\n  Tested the returned temp directory and write location, as above.\n  Tested that the parent directory of the write location is created\n  explicitly, which is now tested by Fake VIM during disk creation.\n  Tested that create_virtual_disk was called, which we now test by\n  ensuring the disk is present when the function returns. Tested that\n  the -flat.vmdk was deleted, which is not necessary in any case as it\n  is overwritten by the file upload.\n\ntest_cache_iso_image\ntest_cache_flat_image\n  Tested that the function calls ds_util.file_move. This is now tested\n  when we test the resulting file locations.\n\ntest_cache_sparse_image\n  Tested that the function calls vm_util.copy_virtual_disk. This is\n  now tested by logging VIM calls and ensuring that an additional\n  CopyVirtualDisk_Task occurs for a sparse disk.\n\nThe effect of this change is that the tests have equal or greater\ncoverage, and can be used to validate changes to the code under test\nwithout requiring modification.\n\nChange-Id: I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2\n""}, {'number': 3, 'created': '2014-09-24 13:19:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/02ea196c9c19a7195b120e26a5ceedd04c531667', 'message': ""VMware: Rewrite test_fetch_image_if_missing to test outcome\n\nThis change updates the following tests:\n\ntest_fetch_image_if_missing\ntest_fetch_image_if_missing_with_sparse\ntest_fetch_image_if_missing_with_iso\n\nThese tests were all calls to _test_fetch_image_if_missing with\ndifferent arguments. _test_fetch_image_if_missing made extensive use\nof mock, and executed very little Nova code. It was simply testing\nthat certain internal functions were called for images of a particular\ntype.\n\nThe problem with this is that it assumes every internal detail of the\nimplementation of vmops._fetch_image_if_missing, which means that you\ncan't change any of those details without breaking this test.\nConsequently, this test cannot be used to validate changes made to\nvmops._fetch_image_if_missing.\n\nThis change assumes that our unit boundary for unit testing is\nvmops._fetch_image_if_missing(). We define its correctness by the\neffects it has on a vSphere server. We make no assumptions about the\nimplementation of vmops._fetch_image_if_missing(), only test that it\nworks as expected.\n\nConsequently, we remove almost all use of mock, and allow Fake VIM to\nhandle server calls. On completion, we check that the correct files\nhave been created. We also log server calls so that we can check a\nsparse input was converted during the process.\n\nWe also remove all tests of internal functions which are\nimplementation details of vmops._fetch_image_if_missing(), and\ntherefore inside the unit boundary. The functionality of all of these\ntests is either not relevant due to testing an implementation detail,\nor covered by the new _test_fetch_image_if_missing:\n\ntest_prepare_iso_image\ntest_prepare_sparse_image\n  Tested the returned temp directory and write location. The precise\n  temp directory path is not relevant, and we test the write location\n  when we test the filename of the resulting image.\n\ntest_prepare_flat_image\n  Tested the returned temp directory and write location, as above.\n  Tested that the parent directory of the write location is created\n  explicitly, which is now tested by Fake VIM during disk creation.\n  Tested that create_virtual_disk was called, which we now test by\n  ensuring the disk is present when the function returns. Tested that\n  the -flat.vmdk was deleted, which is not necessary in any case as it\n  is overwritten by the file upload.\n\ntest_cache_iso_image\ntest_cache_flat_image\n  Tested that the function calls ds_util.file_move. This is now tested\n  when we test the resulting file locations.\n\ntest_cache_sparse_image\n  Tested that the function calls vm_util.copy_virtual_disk. This is\n  now tested by logging VIM calls and ensuring that an additional\n  CopyVirtualDisk_Task occurs for a sparse disk.\n\nThe effect of this change is that the tests have equal or greater\ncoverage, and can be used to validate changes to the code under test\nwithout requiring modification.\n\nChange-Id: I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2\n""}, {'number': 4, 'created': '2014-09-25 15:15:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8105571fb16e23efe033f9d8be13e52eb574c779', 'message': ""VMware: Rewrite test_fetch_image_if_missing to test outcome\n\nThis change updates the following tests:\n\ntest_fetch_image_if_missing\ntest_fetch_image_if_missing_with_sparse\ntest_fetch_image_if_missing_with_iso\n\nThese tests were all calls to _test_fetch_image_if_missing with\ndifferent arguments. _test_fetch_image_if_missing made extensive use\nof mock, and executed very little Nova code. It was simply testing\nthat certain internal functions were called for images of a particular\ntype.\n\nThe problem with this is that it assumes every internal detail of the\nimplementation of vmops._fetch_image_if_missing, which means that you\ncan't change any of those details without breaking this test.\nConsequently, this test cannot be used to validate changes made to\nvmops._fetch_image_if_missing.\n\nThis change assumes that our unit boundary for unit testing is\nvmops._fetch_image_if_missing(). We define its correctness by the\neffects it has on a vSphere server. We make no assumptions about the\nimplementation of vmops._fetch_image_if_missing(), only test that it\nworks as expected.\n\nConsequently, we remove almost all use of mock, and allow Fake VIM to\nhandle server calls. On completion, we check that the correct files\nhave been created. We also log server calls so that we can check a\nsparse input was converted during the process.\n\nWe also remove all tests of internal functions which are\nimplementation details of vmops._fetch_image_if_missing(), and\ntherefore inside the unit boundary. The functionality of all of these\ntests is either not relevant due to testing an implementation detail,\nor covered by the new _test_fetch_image_if_missing:\n\ntest_prepare_iso_image\ntest_prepare_sparse_image\n  Tested the returned temp directory and write location. The precise\n  temp directory path is not relevant, and we test the write location\n  when we test the filename of the resulting image.\n\ntest_prepare_flat_image\n  Tested the returned temp directory and write location, as above.\n  Tested that the parent directory of the write location is created\n  explicitly, which is now tested by Fake VIM during disk creation.\n  Tested that create_virtual_disk was called, which we now test by\n  ensuring the disk is present when the function returns. Tested that\n  the -flat.vmdk was deleted, which is not necessary in any case as it\n  is overwritten by the file upload.\n\ntest_cache_iso_image\ntest_cache_flat_image\n  Tested that the function calls ds_util.file_move. This is now tested\n  when we test the resulting file locations.\n\ntest_cache_sparse_image\n  Tested that the function calls vm_util.copy_virtual_disk. This is\n  now tested by logging VIM calls and ensuring that an additional\n  CopyVirtualDisk_Task occurs for a sparse disk.\n\nThe effect of this change is that the tests have equal or greater\ncoverage, and can be used to validate changes to the code under test\nwithout requiring modification.\n\nChange-Id: I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2\n""}, {'number': 5, 'created': '2014-09-29 14:09:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1fb6e79cf8a16635a73586f82e5e06c0c881e340', 'message': ""VMware: Rewrite test_fetch_image_if_missing to test outcome\n\nThis change updates the following tests:\n\ntest_fetch_image_if_missing\ntest_fetch_image_if_missing_with_sparse\ntest_fetch_image_if_missing_with_iso\n\nThese tests were all calls to _test_fetch_image_if_missing with\ndifferent arguments. _test_fetch_image_if_missing made extensive use\nof mock, and executed very little Nova code. It was simply testing\nthat certain internal functions were called for images of a particular\ntype.\n\nThe problem with this is that it assumes every internal detail of the\nimplementation of vmops._fetch_image_if_missing, which means that you\ncan't change any of those details without breaking this test.\nConsequently, this test cannot be used to validate changes made to\nvmops._fetch_image_if_missing.\n\nThis change assumes that our unit boundary for unit testing is\nvmops._fetch_image_if_missing(). We define its correctness by the\neffects it has on a vSphere server. We make no assumptions about the\nimplementation of vmops._fetch_image_if_missing(), only test that it\nworks as expected.\n\nConsequently, we remove almost all use of mock, and allow Fake VIM to\nhandle server calls. On completion, we check that the correct files\nhave been created. We also log server calls so that we can check a\nsparse input was converted during the process.\n\nWe also remove all tests of internal functions which are\nimplementation details of vmops._fetch_image_if_missing(), and\ntherefore inside the unit boundary. The functionality of all of these\ntests is either not relevant due to testing an implementation detail,\nor covered by the new _test_fetch_image_if_missing:\n\ntest_prepare_iso_image\ntest_prepare_sparse_image\n  Tested the returned temp directory and write location. The precise\n  temp directory path is not relevant, and we test the write location\n  when we test the filename of the resulting image.\n\ntest_prepare_flat_image\n  Tested the returned temp directory and write location, as above.\n  Tested that the parent directory of the write location is created\n  explicitly, which is now tested by Fake VIM during disk creation.\n  Tested that create_virtual_disk was called, which we now test by\n  ensuring the disk is present when the function returns. Tested that\n  the -flat.vmdk was deleted, which is not necessary in any case as it\n  is overwritten by the file upload.\n\ntest_cache_iso_image\ntest_cache_flat_image\n  Tested that the function calls ds_util.file_move. This is now tested\n  when we test the resulting file locations.\n\ntest_cache_sparse_image\n  Tested that the function calls vm_util.copy_virtual_disk. This is\n  now tested by logging VIM calls and ensuring that an additional\n  CopyVirtualDisk_Task occurs for a sparse disk.\n\nThe effect of this change is that the tests have equal or greater\ncoverage, and can be used to validate changes to the code under test\nwithout requiring modification.\n\nChange-Id: I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2\n""}, {'number': 6, 'created': '2014-10-02 15:52:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/bd735d26f92c70fbf6e84300973b245317a24df2', 'message': ""VMware: Rewrite test_fetch_image_if_missing to test outcome\n\nThis change updates the following tests:\n\ntest_fetch_image_if_missing\ntest_fetch_image_if_missing_with_sparse\ntest_fetch_image_if_missing_with_iso\n\nThese tests were all calls to _test_fetch_image_if_missing with\ndifferent arguments. _test_fetch_image_if_missing made extensive use\nof mock, and executed very little Nova code. It was simply testing\nthat certain internal functions were called for images of a particular\ntype.\n\nThe problem with this is that it assumes every internal detail of the\nimplementation of vmops._fetch_image_if_missing, which means that you\ncan't change any of those details without breaking this test.\nConsequently, this test cannot be used to validate changes made to\nvmops._fetch_image_if_missing.\n\nThis change assumes that our unit boundary for unit testing is\nvmops._fetch_image_if_missing(). We define its correctness by the\neffects it has on a vSphere server. We make no assumptions about the\nimplementation of vmops._fetch_image_if_missing(), only test that it\nworks as expected.\n\nConsequently, we remove almost all use of mock, and allow Fake VIM to\nhandle server calls. On completion, we check that the correct files\nhave been created. We also log server calls so that we can check a\nsparse input was converted during the process.\n\nWe also remove all tests of internal functions which are\nimplementation details of vmops._fetch_image_if_missing(), and\ntherefore inside the unit boundary. The functionality of all of these\ntests is either not relevant due to testing an implementation detail,\nor covered by the new _test_fetch_image_if_missing:\n\ntest_prepare_iso_image\ntest_prepare_sparse_image\n  Tested the returned temp directory and write location. The precise\n  temp directory path is not relevant, and we test the write location\n  when we test the filename of the resulting image.\n\ntest_prepare_flat_image\n  Tested the returned temp directory and write location, as above.\n  Tested that the parent directory of the write location is created\n  explicitly, which is now tested by Fake VIM during disk creation.\n  Tested that create_virtual_disk was called, which we now test by\n  ensuring the disk is present when the function returns. Tested that\n  the -flat.vmdk was deleted, which is not necessary in any case as it\n  is overwritten by the file upload.\n\ntest_cache_iso_image\ntest_cache_flat_image\n  Tested that the function calls ds_util.file_move. This is now tested\n  when we test the resulting file locations.\n\ntest_cache_sparse_image\n  Tested that the function calls vm_util.copy_virtual_disk. This is\n  now tested by logging VIM calls and ensuring that an additional\n  CopyVirtualDisk_Task occurs for a sparse disk.\n\nThe effect of this change is that the tests have equal or greater\ncoverage, and can be used to validate changes to the code under test\nwithout requiring modification.\n\nChange-Id: I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2\n""}, {'number': 7, 'created': '2014-10-08 09:29:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d8020ffd973f12252f1304e453c679eaecd5d6b9', 'message': ""VMware: Rewrite test_fetch_image_if_missing to test outcome\n\nThis change updates the following tests:\n\ntest_fetch_image_if_missing\ntest_fetch_image_if_missing_with_sparse\ntest_fetch_image_if_missing_with_iso\n\nThese tests were all calls to _test_fetch_image_if_missing with\ndifferent arguments. _test_fetch_image_if_missing made extensive use\nof mock, and executed very little Nova code. It was simply testing\nthat certain internal functions were called for images of a particular\ntype.\n\nThe problem with this is that it assumes every internal detail of the\nimplementation of vmops._fetch_image_if_missing, which means that you\ncan't change any of those details without breaking this test.\nConsequently, this test cannot be used to validate changes made to\nvmops._fetch_image_if_missing.\n\nThis change assumes that our unit boundary for unit testing is\nvmops._fetch_image_if_missing(). We define its correctness by the\neffects it has on a vSphere server. We make no assumptions about the\nimplementation of vmops._fetch_image_if_missing(), only test that it\nworks as expected.\n\nConsequently, we remove almost all use of mock, and allow Fake VIM to\nhandle server calls. On completion, we check that the correct files\nhave been created. We also log server calls so that we can check a\nsparse input was converted during the process.\n\nWe also remove all tests of internal functions which are\nimplementation details of vmops._fetch_image_if_missing(), and\ntherefore inside the unit boundary. The functionality of all of these\ntests is either not relevant due to testing an implementation detail,\nor covered by the new _test_fetch_image_if_missing:\n\ntest_prepare_iso_image\ntest_prepare_sparse_image\n  Tested the returned temp directory and write location. The precise\n  temp directory path is not relevant, and we test the write location\n  when we test the filename of the resulting image.\n\ntest_prepare_flat_image\n  Tested the returned temp directory and write location, as above.\n  Tested that the parent directory of the write location is created\n  explicitly, which is now tested by Fake VIM during disk creation.\n  Tested that create_virtual_disk was called, which we now test by\n  ensuring the disk is present when the function returns. Tested that\n  the -flat.vmdk was deleted, which is not necessary in any case as it\n  is overwritten by the file upload.\n\ntest_cache_iso_image\ntest_cache_flat_image\n  Tested that the function calls ds_util.file_move. This is now tested\n  when we test the resulting file locations.\n\ntest_cache_sparse_image\n  Tested that the function calls vm_util.copy_virtual_disk. This is\n  now tested by logging VIM calls and ensuring that an additional\n  CopyVirtualDisk_Task occurs for a sparse disk.\n\nThe effect of this change is that the tests have equal or greater\ncoverage, and can be used to validate changes to the code under test\nwithout requiring modification.\n\nChange-Id: I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2\n""}, {'number': 8, 'created': '2014-10-15 14:50:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/fa7437ab61f0857b8814f82fc9e749e8f6909ec5', 'message': ""VMware: Rewrite test_fetch_image_if_missing to test outcome\n\nThis change updates the following tests:\n\ntest_fetch_image_if_missing\ntest_fetch_image_if_missing_with_sparse\ntest_fetch_image_if_missing_with_iso\n\nThese tests were all calls to _test_fetch_image_if_missing with\ndifferent arguments. _test_fetch_image_if_missing made extensive use\nof mock, and executed very little Nova code. It was simply testing\nthat certain internal functions were called for images of a particular\ntype.\n\nThe problem with this is that it assumes every internal detail of the\nimplementation of vmops._fetch_image_if_missing, which means that you\ncan't change any of those details without breaking this test.\nConsequently, this test cannot be used to validate changes made to\nvmops._fetch_image_if_missing.\n\nThis change assumes that our unit boundary for unit testing is\nvmops._fetch_image_if_missing(). We define its correctness by the\neffects it has on a vSphere server. We make no assumptions about the\nimplementation of vmops._fetch_image_if_missing(), only test that it\nworks as expected.\n\nConsequently, we remove almost all use of mock, and allow Fake VIM to\nhandle server calls. On completion, we check that the correct files\nhave been created. We also log server calls so that we can check a\nsparse input was converted during the process.\n\nWe also remove all tests of internal functions which are\nimplementation details of vmops._fetch_image_if_missing(), and\ntherefore inside the unit boundary. The functionality of all of these\ntests is either not relevant due to testing an implementation detail,\nor covered by the new _test_fetch_image_if_missing:\n\ntest_prepare_iso_image\ntest_prepare_sparse_image\n  Tested the returned temp directory and write location. The precise\n  temp directory path is not relevant, and we test the write location\n  when we test the filename of the resulting image.\n\ntest_prepare_flat_image\n  Tested the returned temp directory and write location, as above.\n  Tested that the parent directory of the write location is created\n  explicitly, which is now tested by Fake VIM during disk creation.\n  Tested that create_virtual_disk was called, which we now test by\n  ensuring the disk is present when the function returns. Tested that\n  the -flat.vmdk was deleted, which is not necessary in any case as it\n  is overwritten by the file upload.\n\ntest_cache_iso_image\ntest_cache_flat_image\n  Tested that the function calls ds_util.file_move. This is now tested\n  when we test the resulting file locations.\n\ntest_cache_sparse_image\n  Tested that the function calls vm_util.copy_virtual_disk. This is\n  now tested by logging VIM calls and ensuring that an additional\n  CopyVirtualDisk_Task occurs for a sparse disk.\n\nThe effect of this change is that the tests have equal or greater\ncoverage, and can be used to validate changes to the code under test\nwithout requiring modification.\n\nChange-Id: I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2\n""}, {'number': 9, 'created': '2014-10-30 17:47:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1c884f55ff2e1b73753c26728db114c1dc5d36c7', 'message': ""VMware: Rewrite test_fetch_image_if_missing to test outcome\n\nThis change updates the following tests:\n\ntest_fetch_image_if_missing\ntest_fetch_image_if_missing_with_sparse\ntest_fetch_image_if_missing_with_iso\n\nThese tests were all calls to _test_fetch_image_if_missing with\ndifferent arguments. _test_fetch_image_if_missing made extensive use\nof mock, and executed very little Nova code. It was simply testing\nthat certain internal functions were called for images of a particular\ntype. The problem with this is that it assumes every internal detail\nof the implementation of vmops._fetch_image_if_missing, which means\nthat you can't change any of those details without breaking this test.\nConsequently, this test cannot be used to validate changes made to\nvmops._fetch_image_if_missing.\n\nThis change assumes that our unit boundary for unit testing is\nvmops._fetch_image_if_missing(). We define its correctness by the\neffects it has on a vSphere server. We make no assumptions about the\nimplementation of vmops._fetch_image_if_missing(), only test that it\nworks as expected.\n\nConsequently, we remove almost all use of mock, and allow Fake VIM to\nhandle server calls. On completion, we check that the correct files\nhave been created. We also log server calls so that we can check a\nsparse input was converted during the process.\n\nWe also remove all tests of internal functions which are\nimplementation details of vmops._fetch_image_if_missing(), and\ntherefore inside the unit boundary. The functionality of all of these\ntests is either not relevant due to testing an implementation detail,\nor covered by the new test functions:\n\ntest_prepare_iso_image\ntest_prepare_sparse_image\n  Tested the returned temp directory and write location. The precise\n  temp directory path is not relevant, and we test the write location\n  when we test the filename of the resulting image.\n\ntest_prepare_flat_image\n  Tested the returned temp directory and write location, as above.\n  Tested that the parent directory of the write location is created\n  explicitly, which is now tested by Fake VIM during disk creation.\n  Tested that create_virtual_disk was called, which we now test by\n  ensuring the disk is present when the function returns. Tested that\n  the -flat.vmdk was deleted, which is not necessary in any case as it\n  is overwritten by the file upload.\n\ntest_cache_iso_image\ntest_cache_flat_image\n  Tested that the function calls ds_util.file_move. This is now tested\n  when we test the resulting file locations.\n\ntest_cache_sparse_image\n  Tested that the function calls vm_util.copy_virtual_disk. This is\n  now tested by logging VIM calls and ensuring that an additional\n  CopyVirtualDisk_Task occurs for a sparse disk.\n\nThe effect of this change is that the tests have equal or greater\ncoverage, and can be used to validate changes to the code under test\nwithout requiring modification.\n\nChange-Id: I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2\n""}, {'number': 10, 'created': '2014-10-30 17:50:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b20e916a96888524f3330a04ddf9125665fbd13e', 'message': ""VMware: Rewrite test_fetch_image_if_missing to test outcome\n\nThis change updates the following tests:\n\ntest_fetch_image_if_missing\ntest_fetch_image_if_missing_with_sparse\ntest_fetch_image_if_missing_with_iso\n\nThese tests were all calls to _test_fetch_image_if_missing with\ndifferent arguments. _test_fetch_image_if_missing made extensive use\nof mock, and executed very little Nova code. It was simply testing\nthat certain internal functions were called for images of a particular\ntype. The problem with this is that it assumes every internal detail\nof the implementation of vmops._fetch_image_if_missing, which means\nthat you can't change any of those details without breaking this test.\nConsequently, this test cannot be used to validate changes made to\nvmops._fetch_image_if_missing.\n\nThis change assumes that our unit boundary for unit testing is\nvmops._fetch_image_if_missing(). We define its correctness by the\neffects it has on a vSphere server. We make no assumptions about the\nimplementation of vmops._fetch_image_if_missing(), only test that it\nworks as expected.\n\nConsequently, we remove almost all use of mock, and allow Fake VIM to\nhandle server calls. On completion, we check that the correct files\nhave been created. We also log server calls so that we can check a\nsparse input was converted during the process.\n\nWe also remove all tests of internal functions which are\nimplementation details of vmops._fetch_image_if_missing(), and\ntherefore inside the unit boundary. The functionality of all of these\ntests is either not relevant due to testing an implementation detail,\nor covered by the new test functions:\n\ntest_prepare_iso_image\ntest_prepare_sparse_image\n  Tested the returned temp directory and write location. The precise\n  temp directory path is not relevant, and we test the write location\n  when we test the filename of the resulting image.\n\ntest_prepare_flat_image\n  Tested the returned temp directory and write location, as above.\n  Tested that the parent directory of the write location is created\n  explicitly, which is now tested by Fake VIM during disk creation.\n  Tested that create_virtual_disk was called, which we now test by\n  ensuring the disk is present when the function returns. Tested that\n  the -flat.vmdk was deleted, which is not necessary in any case as it\n  is overwritten by the file upload.\n\ntest_cache_iso_image\ntest_cache_flat_image\n  Tested that the function calls ds_util.file_move. This is now tested\n  when we test the resulting file locations.\n\ntest_cache_sparse_image\n  Tested that the function calls vm_util.copy_virtual_disk. This is\n  now tested by logging VIM calls and ensuring that an additional\n  CopyVirtualDisk_Task occurs for a sparse disk.\n\nThe effect of this change is that the tests have equal or greater\ncoverage, and can be used to validate changes to the code under test\nwithout requiring modification.\n\nChange-Id: I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2\n""}, {'number': 11, 'created': '2014-10-31 14:17:50.000000000', 'files': ['nova/tests/virt/vmwareapi/test_vmops.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/eb7cafd0245ec2d4218d8ca1d1ffba71aea8c006', 'message': 'VMware: Rewrite test_fetch_image_if_missing to test outcome\n\nThis change updates the following tests:\n\ntest_fetch_image_if_missing\ntest_fetch_image_if_missing_with_sparse\ntest_fetch_image_if_missing_with_iso\n\nIn addition to the coverage provided by the previous tests, we also\nadd tests for calling _fetch_image_if_missing in the presence of\nexisting files, including a test that an existing image is not\noverwritten.\n\nThe new tests are also considerably simpler to understand, and\ntherefore verify.\n\nThis change assumes that our unit boundary for unit testing is\nvmops._fetch_image_if_missing(). We define its correctness by the\neffects it has on a vSphere server. We make no assumptions about the\nimplementation of vmops._fetch_image_if_missing(), only test that it\nworks as expected.\n\nConsequently, we remove almost all use of mock, and allow FakeVim to\nhandle server calls. On completion, we check that the correct files\nhave been created. We also log server calls so that we can check a\nsparse input was converted during the process.\n\nWe also remove all tests of internal functions which are\nimplementation details of vmops._fetch_image_if_missing(), and\ntherefore inside the unit boundary. The functionality of all of these\ntests is either not relevant due to testing an implementation detail,\nor covered by the new test functions:\n\ntest_prepare_iso_image\ntest_prepare_sparse_image\n  Tested the returned temp directory and write location. The precise\n  temp directory path is not relevant, and we test the write location\n  when we test the filename of the resulting image.\n\ntest_prepare_flat_image\n  Tested the returned temp directory and write location, as above.\n  Tested that the parent directory of the write location is created\n  explicitly, which is now tested by Fake VIM during disk creation.\n  Tested that create_virtual_disk was called, which we now test by\n  ensuring the disk is present when the function returns. Tested that\n  the -flat.vmdk was deleted, which is not necessary in any case as it\n  is overwritten by the file upload.\n\ntest_cache_iso_image\ntest_cache_flat_image\n  Tested that the function calls ds_util.file_move. This is now tested\n  when we test the resulting file locations.\n\ntest_cache_sparse_image\n  Tested that the function calls vm_util.copy_virtual_disk. This is\n  now tested by logging VIM calls and ensuring that an additional\n  CopyVirtualDisk_Task occurs for a sparse disk.\n\nThe effect of this change is that the tests have equal or greater\ncoverage, and can be used to validate changes to the code under test\nwithout requiring modification.\n\nChange-Id: I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2\n'}]",34,122760,eb7cafd0245ec2d4218d8ca1d1ffba71aea8c006,76,13,11,9555,,,0,"VMware: Rewrite test_fetch_image_if_missing to test outcome

This change updates the following tests:

test_fetch_image_if_missing
test_fetch_image_if_missing_with_sparse
test_fetch_image_if_missing_with_iso

In addition to the coverage provided by the previous tests, we also
add tests for calling _fetch_image_if_missing in the presence of
existing files, including a test that an existing image is not
overwritten.

The new tests are also considerably simpler to understand, and
therefore verify.

This change assumes that our unit boundary for unit testing is
vmops._fetch_image_if_missing(). We define its correctness by the
effects it has on a vSphere server. We make no assumptions about the
implementation of vmops._fetch_image_if_missing(), only test that it
works as expected.

Consequently, we remove almost all use of mock, and allow FakeVim to
handle server calls. On completion, we check that the correct files
have been created. We also log server calls so that we can check a
sparse input was converted during the process.

We also remove all tests of internal functions which are
implementation details of vmops._fetch_image_if_missing(), and
therefore inside the unit boundary. The functionality of all of these
tests is either not relevant due to testing an implementation detail,
or covered by the new test functions:

test_prepare_iso_image
test_prepare_sparse_image
  Tested the returned temp directory and write location. The precise
  temp directory path is not relevant, and we test the write location
  when we test the filename of the resulting image.

test_prepare_flat_image
  Tested the returned temp directory and write location, as above.
  Tested that the parent directory of the write location is created
  explicitly, which is now tested by Fake VIM during disk creation.
  Tested that create_virtual_disk was called, which we now test by
  ensuring the disk is present when the function returns. Tested that
  the -flat.vmdk was deleted, which is not necessary in any case as it
  is overwritten by the file upload.

test_cache_iso_image
test_cache_flat_image
  Tested that the function calls ds_util.file_move. This is now tested
  when we test the resulting file locations.

test_cache_sparse_image
  Tested that the function calls vm_util.copy_virtual_disk. This is
  now tested by logging VIM calls and ensuring that an additional
  CopyVirtualDisk_Task occurs for a sparse disk.

The effect of this change is that the tests have equal or greater
coverage, and can be used to validate changes to the code under test
without requiring modification.

Change-Id: I5df772ed3e39964d026b19f8d7fa34caf7f0eeb2
",git fetch https://review.opendev.org/openstack/nova refs/changes/60/122760/10 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/virt/vmwareapi/test_vmops.py'],1,dd02309718eb9d22b7164ef618f923600b8d4947,vmware_better_tests," def _make_vm_config_info(self, image_type='flat'): disk_type = (constants.DISK_TYPE_SPARSE if image_type == 'sparse' file_type = (constants.DISK_FORMAT_ISO if image_type == 'iso' def _test_fetch_image_if_missing(self, image_type='flat'): # Create a wrapper for vim's __getattr__ which will log vim calls # without modifying them or their effect. vim_calls = {} vim_funcs = {} orig_getattr = self._session.vim.__getattr__ def new_getattr(vim, attr_name): if attr_name in vim_funcs: return vim_funcs[attr_name] def wrapped(*args, **kwargs): vim_calls[attr_name].append((args, kwargs)) return orig_getattr(attr_name)(*args, **kwargs) vim_funcs[attr_name] = wrapped vim_calls[attr_name] = [] return wrapped vi = self._make_vm_config_info(image_type) vmwareapi_fake.create_copy_requires_parent_dir = True with mock.patch.object(self._session.vim.__class__, '__getattr__', new=new_getattr): self._vmops._fetch_image_if_missing(self._context, vi) expected = [] # The temp directory will be created implicitly expected.append(str(self._ds.build_path('vmware_temp/'))) # Check the cache directory has been created basedir = self._ds.build_path('vmware_base') expected.append(str(basedir) + '/') # Check the image directory has been created imagedir = basedir.join(self._image_id) expected.append(str(imagedir) + '/') # Check the vmdk exists vmdk_basename = '%s.%s' % (self._image_id, vi.ii.file_type) expected.append(str(imagedir.join(vmdk_basename))) flat_vmdk = str(imagedir.join(self._image_id + '-flat.vmdk')) if image_type == 'flat': expected.append(flat_vmdk) elif image_type == 'sparse': expected.append(flat_vmdk) # A sparse file should have been written to a temporary location, # copied into place with a CopyVirtualDisk_Task, and the original # location cleaned up. The end result of this dance is # indistinguishable from that of a flat image, but we need to # ensure it happened. Here we check the vim call log for a copy # with the expected target. self.assertIn('CopyVirtualDisk_Task', vim_calls.keys()) for call in vim_calls['CopyVirtualDisk_Task']: dest = call[1]['destName'] destPath = ds_util.DatastorePath.parse(dest) if destPath.basename == vmdk_basename: break else: self.fail('Sparse file was not copied') # Check that the expected files exist, and no others self.assertSetEqual(set(expected), set(vmwareapi_fake._db_content['files'])) self._test_fetch_image_if_missing(image_type='sparse') self._test_fetch_image_if_missing(image_type='iso')"," def _make_vm_config_info(self, is_iso=False, is_sparse_disk=False): disk_type = (constants.DISK_TYPE_SPARSE if is_sparse_disk file_type = (constants.DISK_FORMAT_ISO if is_iso @mock.patch.object(vmops.VMwareVMOps, 'check_cache_folder') @mock.patch.object(vmops.VMwareVMOps, '_fetch_image_as_file') @mock.patch.object(vmops.VMwareVMOps, '_prepare_iso_image') @mock.patch.object(vmops.VMwareVMOps, '_prepare_sparse_image') @mock.patch.object(vmops.VMwareVMOps, '_prepare_flat_image') @mock.patch.object(vmops.VMwareVMOps, '_cache_iso_image') @mock.patch.object(vmops.VMwareVMOps, '_cache_sparse_image') @mock.patch.object(vmops.VMwareVMOps, '_cache_flat_image') @mock.patch.object(vmops.VMwareVMOps, '_delete_datastore_file') def _test_fetch_image_if_missing(self, mock_delete_datastore_file, mock_cache_flat_image, mock_cache_sparse_image, mock_cache_iso_image, mock_prepare_flat_image, mock_prepare_sparse_image, mock_prepare_iso_image, mock_fetch_image_as_file, mock_check_cache_folder, is_iso=False, is_sparse_disk=False): tmp_dir_path = mock.Mock() tmp_image_path = mock.Mock() if is_iso: mock_prepare = mock_prepare_iso_image mock_cache = mock_cache_iso_image elif is_sparse_disk: mock_prepare = mock_prepare_sparse_image mock_cache = mock_cache_sparse_image else: mock_prepare = mock_prepare_flat_image mock_cache = mock_cache_flat_image mock_prepare.return_value = tmp_dir_path, tmp_image_path vi = self._make_vm_config_info(is_iso, is_sparse_disk) self._vmops._fetch_image_if_missing(self._context, vi) mock_check_cache_folder.assert_called_once_with( self._ds.name, self._ds.ref) mock_prepare.assert_called_once_with(vi) mock_fetch_image_as_file.assert_called_once_with( self._context, vi, tmp_image_path) mock_cache.assert_called_once_with(vi, tmp_image_path) mock_delete_datastore_file.assert_called_once_with( vi.instance, str(tmp_dir_path), self._dc_info.ref) self._test_fetch_image_if_missing( is_sparse_disk=True) self._test_fetch_image_if_missing( is_iso=True) @mock.patch.object(uuidutils, 'generate_uuid', return_value='tmp-uuid') def test_prepare_iso_image(self, mock_generate_uuid): vi = self._make_vm_config_info(is_iso=True) tmp_dir_loc, tmp_image_ds_loc = self._vmops._prepare_iso_image(vi) expected_tmp_dir_path = '[%s] vmware_temp/tmp-uuid' % (self._ds.name) expected_image_path = '[%s] vmware_temp/tmp-uuid/%s/%s.iso' % ( self._ds.name, self._image_id, self._image_id) self.assertEqual(str(tmp_dir_loc), expected_tmp_dir_path) self.assertEqual(str(tmp_image_ds_loc), expected_image_path) @mock.patch.object(uuidutils, 'generate_uuid', return_value='tmp-uuid') def test_prepare_sparse_image(self, mock_generate_uuid): vi = self._make_vm_config_info(is_sparse_disk=True) tmp_dir_loc, tmp_image_ds_loc = self._vmops._prepare_sparse_image(vi) expected_tmp_dir_path = '[%s] vmware_temp/tmp-uuid' % (self._ds.name) expected_image_path = '[%s] vmware_temp/tmp-uuid/%s/%s' % ( self._ds.name, self._image_id, ""tmp-sparse.vmdk"") self.assertEqual(str(tmp_dir_loc), expected_tmp_dir_path) self.assertEqual(str(tmp_image_ds_loc), expected_image_path) @mock.patch.object(ds_util, 'mkdir') @mock.patch.object(vm_util, 'create_virtual_disk') @mock.patch.object(vmops.VMwareVMOps, '_delete_datastore_file') @mock.patch.object(uuidutils, 'generate_uuid', return_value='tmp-uuid') def test_prepare_flat_image(self, mock_generate_uuid, mock_delete_datastore_file, mock_create_virtual_disk, mock_mkdir): vi = self._make_vm_config_info() tmp_dir_loc, tmp_image_ds_loc = self._vmops._prepare_flat_image(vi) expected_tmp_dir_path = '[%s] vmware_temp/tmp-uuid' % (self._ds.name) expected_image_path = '[%s] vmware_temp/tmp-uuid/%s/%s-flat.vmdk' % ( self._ds.name, self._image_id, self._image_id) expected_image_path_parent = '[%s] vmware_temp/tmp-uuid/%s' % ( self._ds.name, self._image_id) expected_path_to_create = '[%s] vmware_temp/tmp-uuid/%s/%s.vmdk' % ( self._ds.name, self._image_id, self._image_id) mock_mkdir.assert_called_once_with( self._session, DsPathMatcher(expected_image_path_parent), self._dc_info.ref) self.assertEqual(str(tmp_dir_loc), expected_tmp_dir_path) self.assertEqual(str(tmp_image_ds_loc), expected_image_path) image_info = vi.ii mock_create_virtual_disk.assert_called_once_with( self._session, self._dc_info.ref, image_info.adapter_type, image_info.disk_type, DsPathMatcher(expected_path_to_create), image_info.file_size_in_kb) mock_delete_datastore_file.assert_called_once_with( vi.instance, DsPathMatcher(expected_image_path), self._dc_info.ref) @mock.patch.object(ds_util, 'file_move') def test_cache_iso_image(self, mock_file_move): vi = self._make_vm_config_info(is_iso=True) tmp_image_ds_loc = mock.Mock() self._vmops._cache_iso_image(vi, tmp_image_ds_loc) mock_file_move.assert_called_once_with( self._session, self._dc_info.ref, tmp_image_ds_loc.parent, DsPathMatcher(str(self._ds.build_path('vmware_base', self._image_id)))) @mock.patch.object(ds_util, 'file_move') def test_cache_flat_image(self, mock_file_move): vi = self._make_vm_config_info() tmp_image_ds_loc = mock.Mock() self._vmops._cache_flat_image(vi, tmp_image_ds_loc) mock_file_move.assert_called_once_with( self._session, self._dc_info.ref, tmp_image_ds_loc.parent, DsPathMatcher(str(self._ds.build_path('vmware_base', self._image_id)))) @mock.patch.object(ds_util, 'file_move') @mock.patch.object(vm_util, 'copy_virtual_disk') @mock.patch.object(vmops.VMwareVMOps, '_delete_datastore_file') def test_cache_sparse_image(self, mock_delete_datastore_file, mock_copy_virtual_disk, mock_file_move): vi = self._make_vm_config_info(is_sparse_disk=True) sparse_disk_path = ""[%s] vmware_temp/tmp-uuid/%s/tmp-sparse.vmdk"" % ( self._ds.name, self._image_id) tmp_image_ds_loc = ds_util.DatastorePath.parse(sparse_disk_path) self._vmops._cache_sparse_image(vi, tmp_image_ds_loc) target_disk_path = ""[%s] vmware_temp/tmp-uuid/%s/%s.vmdk"" % ( self._ds.name, self._image_id, self._image_id) mock_copy_virtual_disk.assert_called_once_with( self._session, self._dc_info.ref, sparse_disk_path, DsPathMatcher(target_disk_path))",70,161
openstack%2Fnova~master~I6285f1a0e3ed8e4856a3189c854d784072f95a9e,openstack/nova,master,I6285f1a0e3ed8e4856a3189c854d784072f95a9e,VMware: Use existing fake objects in test_vmops,ABANDONED,2014-09-19 14:59:17.000000000,2014-12-08 15:51:11.000000000,,"[{'_account_id': 3}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-19 14:59:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e5db61b81bef7b45f731425ca9e23926e853164c', 'message': ""VMware: Use existing fake objects in test_vmops\n\nMany tests in test_vmops use test values which don't correspond to any\nobject created by vmwareapi_fake. This change updates them to use\nobjects automatically created by vmwareapi_fake in reset(), which in\nturn means that tests need not use mock to get predictable returned\ndata.\n\nChange-Id: I6285f1a0e3ed8e4856a3189c854d784072f95a9e\n""}, {'number': 2, 'created': '2014-09-24 10:44:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/68d953424cafffad858085d18b44b98243770b78', 'message': ""VMware: Use existing fake objects in test_vmops\n\nMany tests in test_vmops use test values which don't correspond to any\nobject created by vmwareapi_fake. This change updates them to use\nobjects automatically created by vmwareapi_fake in reset(), which in\nturn means that tests need not use mock to get predictable returned\ndata.\n\nChange-Id: I6285f1a0e3ed8e4856a3189c854d784072f95a9e\n""}, {'number': 3, 'created': '2014-09-25 15:15:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c325b9648762d75b12c66dad1a72381aa1442976', 'message': ""VMware: Use existing fake objects in test_vmops\n\nMany tests in test_vmops use test values which don't correspond to any\nobject created by vmwareapi_fake. This change updates them to use\nobjects automatically created by vmwareapi_fake in reset(), which in\nturn means that tests need not use mock to get predictable returned\ndata.\n\nChange-Id: I6285f1a0e3ed8e4856a3189c854d784072f95a9e\n""}, {'number': 4, 'created': '2014-09-29 14:09:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/0cb266eeba05a4bad3ed2e52680aaf36804f4621', 'message': ""VMware: Use existing fake objects in test_vmops\n\nMany tests in test_vmops use test values which don't correspond to any\nobject created by vmwareapi_fake. This change updates them to use\nobjects automatically created by vmwareapi_fake in reset(), which in\nturn means that tests need not use mock to get predictable returned\ndata.\n\nChange-Id: I6285f1a0e3ed8e4856a3189c854d784072f95a9e\n""}, {'number': 5, 'created': '2014-10-02 15:52:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/4ddccdf89109f75f9a9c9b77a8b2f7ca425192ef', 'message': ""VMware: Use existing fake objects in test_vmops\n\nMany tests in test_vmops use test values which don't correspond to any\nobject created by vmwareapi_fake. This change updates them to use\nobjects automatically created by vmwareapi_fake in reset(), which in\nturn means that tests need not use mock to get predictable returned\ndata.\n\nChange-Id: I6285f1a0e3ed8e4856a3189c854d784072f95a9e\n""}, {'number': 6, 'created': '2014-10-08 09:29:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/5a5f7febac15205aac21e246cda1c53a2c1e063e', 'message': ""VMware: Use existing fake objects in test_vmops\n\nMany tests in test_vmops use test values which don't correspond to any\nobject created by vmwareapi_fake. This change updates them to use\nobjects automatically created by vmwareapi_fake in reset(), which in\nturn means that tests need not use mock to get predictable returned\ndata.\n\nChange-Id: I6285f1a0e3ed8e4856a3189c854d784072f95a9e\n""}, {'number': 7, 'created': '2014-10-15 14:50:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e63570c6743fec981a5b2968f45e7947c2dc04d6', 'message': ""VMware: Use existing fake objects in test_vmops\n\nMany tests in test_vmops use test values which don't correspond to any\nobject created by vmwareapi_fake. This change updates them to use\nobjects automatically created by vmwareapi_fake in reset(), which in\nturn means that tests need not use mock to get predictable returned\ndata.\n\nChange-Id: I6285f1a0e3ed8e4856a3189c854d784072f95a9e\n""}, {'number': 8, 'created': '2014-10-30 17:47:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/939bc056ca4c344770d69ac198622dd1d1dcc777', 'message': ""VMware: Use existing fake objects in test_vmops\n\nMany tests in test_vmops use test values which don't correspond to any\nobject created by vmwareapi_fake. This change updates them to use\nobjects automatically created by vmwareapi_fake in reset(), which in\nturn means that tests need not use mock to get predictable returned\ndata.\n\nChange-Id: I6285f1a0e3ed8e4856a3189c854d784072f95a9e\n""}, {'number': 9, 'created': '2014-10-31 14:17:50.000000000', 'files': ['nova/tests/virt/vmwareapi/test_vmops.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/a9dac1176686360808a8d1f119809cce194f5003', 'message': ""VMware: Use existing fake objects in test_vmops\n\nMany tests in test_vmops use test values which don't correspond to any\nobject created by vmwareapi_fake. This change updates them to use\nobjects automatically created by vmwareapi_fake in reset(), which in\nturn means that tests need not use mock to get predictable returned\ndata.\n\nChange-Id: I6285f1a0e3ed8e4856a3189c854d784072f95a9e\n""}]",0,122758,a9dac1176686360808a8d1f119809cce194f5003,50,6,9,9555,,,0,"VMware: Use existing fake objects in test_vmops

Many tests in test_vmops use test values which don't correspond to any
object created by vmwareapi_fake. This change updates them to use
objects automatically created by vmwareapi_fake in reset(), which in
turn means that tests need not use mock to get predictable returned
data.

Change-Id: I6285f1a0e3ed8e4856a3189c854d784072f95a9e
",git fetch https://review.opendev.org/openstack/nova refs/changes/58/122758/8 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/virt/vmwareapi/test_vmops.py'],1,e5db61b81bef7b45f731425ca9e23926e853164c,vmware_better_tests," # Fake automatically creates several fake objects, including 2 # datacenters. Pull out the first datacenter, and its associated # datastore. dc_ref = vmwareapi_fake._db_content['Datacenter'].keys()[0] dc = vmwareapi_fake._get_object(dc_ref) ds_ref = dc.datastore.ManagedObjectReference[0] ds = vmwareapi_fake._get_object(ds_ref) ref=ds_ref, name=ds.get('summary.name'), capacity=ds.get('summary.capacity'), freespace=ds.get('summary.freeSpace')) ref=dc_ref.value, name=dc.get('name'), vmFolder=dc.get('vmFolder')) str(self._ds.build_path('fake_uuid/fake_uuid.vmdk'))) str(self._ds.build_path('fake_uuid/fake_uuid.vmdk')), self._dc_info.ref) vi.ii.disk_type, str(self._ds.build_path('fake_uuid/fake_uuid.vmdk')), str(self._ds.build_path('fake_uuid/fake_uuid.vmdk')), str(self._ds.build_path('fake_uuid/fake_uuid.vmdk')), self._ds.name, self._dc_info.vmFolder, source_file = str(self._ds.build_path( 'vmware_base', self._image_id, '%s.vmdk' % self._image_id)) dest_file = str(self._ds.build_path( 'vmware_base', self._image_id, '%s.%d.vmdk' % (self._image_id, self._instance['root_gb']))) self._dc_info.ref, DsPathMatcher(str(self._ds.build_path('vmware_base', self._image_id)))) DsPathMatcher(str(self._ds.build_path('vmware_base', self._image_id))))"," fake_ds_ref = vmwareapi_fake.ManagedObjectReference('fake-ds') ref=fake_ds_ref, name='fake_ds', capacity=10 * units.Gi, freespace=10 * units.Gi) ref='fake_dc_ref', name='fake_dc', vmFolder='fake_vm_folder') '[fake_ds] fake_uuid/fake_uuid.vmdk') '[fake_ds] fake_uuid/fake_uuid.vmdk', self._dc_info.ref) vi.ii.disk_type, '[fake_ds] fake_uuid/fake_uuid.vmdk', '[fake_ds] fake_uuid/fake_uuid.vmdk', '[fake_ds] fake_uuid/fake_uuid.vmdk', 'fake_ds', 'fake_vm_folder', dc_ref = 'fake_dc_ref' source_file = unicode('[fake_ds] vmware_base/%s/%s.vmdk' % (self._image_id, self._image_id)) dest_file = unicode('[fake_ds] vmware_base/%s/%s.%d.vmdk' % (self._image_id, self._image_id, self._instance['root_gb'])) dc_ref, DsPathMatcher('[fake_ds] vmware_base/%s' % self._image_id)) DsPathMatcher('[fake_ds] vmware_base/%s' % self._image_id))",36,22
openstack%2Fnova~master~Ieb3e2395b0b29fe59b9766cae369e68cd24ba39b,openstack/nova,master,Ieb3e2395b0b29fe59b9766cae369e68cd24ba39b,VMware: Check source and destination during move in fake,ABANDONED,2014-09-24 10:24:34.000000000,2014-12-08 15:51:06.000000000,,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 5170}, {'_account_id': 5367}, {'_account_id': 8412}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-24 10:24:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/4b3dc5a5870d11956dc9ac40545c99e7ec06aa19', 'message': ""VMware: Check source and destination during move in fake\n\nCheck:\n\n* Source file exists\n* Destination file doesn't exist, or\n  * destination file isn't a directory, and\n  * force option was given\n\nChange-Id: Ieb3e2395b0b29fe59b9766cae369e68cd24ba39b\n""}, {'number': 2, 'created': '2014-09-25 15:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e4d649fdbb3428295bd79880a79543bf049356f2', 'message': ""VMware: Check source and destination during move in fake\n\nCheck:\n\n* Source file exists\n* Destination file doesn't exist, or\n  * destination file isn't a directory, and\n  * force option was given\n\nChange-Id: Ieb3e2395b0b29fe59b9766cae369e68cd24ba39b\n""}, {'number': 3, 'created': '2014-09-25 15:11:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ef1b24f90b6822d54dc075632f16f4f5f6641bb7', 'message': ""VMware: Check source and destination during move in fake\n\nCheck:\n\n* Source file exists\n* Destination file doesn't exist, or\n  * destination file isn't a directory, and\n  * force option was given\n\nChange-Id: Ieb3e2395b0b29fe59b9766cae369e68cd24ba39b\n""}, {'number': 4, 'created': '2014-09-29 13:45:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e31b0aabf5252df9b55521f9bfc048e7d77397e4', 'message': ""VMware: Check source and destination during move in fake\n\nCheck:\n\n* Source file exists\n* Destination file doesn't exist, or\n  * destination file isn't a directory, and\n  * force option was given\n\nChange-Id: Ieb3e2395b0b29fe59b9766cae369e68cd24ba39b\n""}, {'number': 5, 'created': '2014-10-02 15:49:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c0d47a5ab591f5b996052eb0b6a08ab96f881393', 'message': ""VMware: Check source and destination during move in fake\n\nCheck:\n\n* Source file exists\n* Destination file doesn't exist, or\n  * destination file isn't a directory, and\n  * force option was given\n\nChange-Id: Ieb3e2395b0b29fe59b9766cae369e68cd24ba39b\n""}, {'number': 6, 'created': '2014-10-08 09:23:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/03d707f33722d47dc76b52f53dfe82d6f7f3f085', 'message': ""VMware: Check source and destination during move in fake\n\nCheck:\n\n* Source file exists\n* Destination file doesn't exist, or\n  * destination file isn't a directory, and\n  * force option was given\n\nChange-Id: Ieb3e2395b0b29fe59b9766cae369e68cd24ba39b\n""}, {'number': 7, 'created': '2014-10-15 14:50:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1cf95bc505cf9f74002cce360d74322921bfa96f', 'message': ""VMware: Check source and destination during move in fake\n\nCheck:\n\n* Source file exists\n* Destination file doesn't exist, or\n  * destination file isn't a directory, and\n  * force option was given\n\nChange-Id: Ieb3e2395b0b29fe59b9766cae369e68cd24ba39b\n""}, {'number': 8, 'created': '2014-10-30 17:47:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/efc00285b6ed6c755d495b3773e33428dfe2d097', 'message': ""VMware: Check source and destination during move in fake\n\nCheck:\n\n* Source file exists\n* Destination file doesn't exist, or\n  * destination file isn't a directory, and\n  * force option was given\n\nChange-Id: Ieb3e2395b0b29fe59b9766cae369e68cd24ba39b\n""}, {'number': 9, 'created': '2014-10-31 14:17:50.000000000', 'files': ['nova/tests/virt/vmwareapi/fake.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/76d17962373f59bc8340591aaf105e5fcafaa12b', 'message': ""VMware: Check source and destination during move in fake\n\nCheck:\n\n* Source file exists\n* Destination file doesn't exist, or\n  * destination file isn't a directory, and\n  * force option was given\n\nChange-Id: Ieb3e2395b0b29fe59b9766cae369e68cd24ba39b\n""}]",0,123688,76d17962373f59bc8340591aaf105e5fcafaa12b,61,11,9,9555,,,0,"VMware: Check source and destination during move in fake

Check:

* Source file exists
* Destination file doesn't exist, or
  * destination file isn't a directory, and
  * force option was given

Change-Id: Ieb3e2395b0b29fe59b9766cae369e68cd24ba39b
",git fetch https://review.opendev.org/openstack/nova refs/changes/88/123688/8 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/virt/vmwareapi/fake.py'],1,4b3dc5a5870d11956dc9ac40545c99e7ec06aa19,vmware_better_tests," if source not in paths: raise vexc.FileNotFoundException(source) if destination in paths: if path_isdir[destination] or not kwargs.get('force', False): raise vexc.FileAlreadyExistsException(destination) ",,7,0
openstack%2Fnova~master~I12164ff64b94eccf71323ccf837b8a9301c0ade9,openstack/nova,master,I12164ff64b94eccf71323ccf837b8a9301c0ade9,VMware: Check parent directory during create/copy disk in fake,ABANDONED,2014-09-24 10:24:34.000000000,2014-12-08 15:50:59.000000000,,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 5170}, {'_account_id': 8412}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-24 10:24:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e8ab67b584d875b921a319f4e1bc3d595246b893', 'message': 'VMware: Assert parent directory exists during create/copy disk in fake\n\nRaise FileNotFoundException in the fake driver when attempting to\ncreate a disk in a non-existent directory, or copy a disk into a\nnon-existent directory.\n\nChange-Id: I12164ff64b94eccf71323ccf837b8a9301c0ade9\n'}, {'number': 2, 'created': '2014-09-25 15:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a9f409eec0ab4aaf466c6639afd20432f97eb37c', 'message': 'VMware: Assert parent directory exists during create/copy disk in fake\n\nRaise FileNotFoundException in the fake driver when attempting to\ncreate a disk in a non-existent directory, or copy a disk into a\nnon-existent directory.\n\nChange-Id: I12164ff64b94eccf71323ccf837b8a9301c0ade9\n'}, {'number': 3, 'created': '2014-09-25 15:11:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a665a427fa6170bba454158f0085f9404f1fc1b6', 'message': 'VMware: Check parent directory during create/copy disk in fake\n\nRaise FileNotFoundException in the fake driver when attempting to\ncreate a disk in a non-existent directory, or copy a disk into a\nnon-existent directory.\n\nChange-Id: I12164ff64b94eccf71323ccf837b8a9301c0ade9\n'}, {'number': 4, 'created': '2014-09-29 13:45:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/6947a028748e6aed918e009153770d115cba415a', 'message': 'VMware: Check parent directory during create/copy disk in fake\n\nRaise FileNotFoundException in the fake driver when attempting to\ncreate a disk in a non-existent directory, or copy a disk into a\nnon-existent directory.\n\nChange-Id: I12164ff64b94eccf71323ccf837b8a9301c0ade9\n'}, {'number': 5, 'created': '2014-10-02 15:49:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/480d27e4075ebae8a0bb17e8d123f128359157c8', 'message': 'VMware: Check parent directory during create/copy disk in fake\n\nRaise FileNotFoundException in the fake driver when attempting to\ncreate a disk in a non-existent directory, or copy a disk into a\nnon-existent directory.\n\nChange-Id: I12164ff64b94eccf71323ccf837b8a9301c0ade9\n'}, {'number': 6, 'created': '2014-10-08 09:23:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3fa58be5a8269218b700e822ee5bfa573712da8d', 'message': 'VMware: Check parent directory during create/copy disk in fake\n\nRaise FileNotFoundException in the fake driver when attempting to\ncreate a disk in a non-existent directory, or copy a disk into a\nnon-existent directory.\n\nChange-Id: I12164ff64b94eccf71323ccf837b8a9301c0ade9\n'}, {'number': 7, 'created': '2014-10-15 14:50:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/369f504636bef0b51b4fa963085337fbbf57fee6', 'message': 'VMware: Check parent directory during create/copy disk in fake\n\nRaise FileNotFoundException in the fake driver when attempting to\ncreate a disk in a non-existent directory, or copy a disk into a\nnon-existent directory.\n\nChange-Id: I12164ff64b94eccf71323ccf837b8a9301c0ade9\n'}, {'number': 8, 'created': '2014-10-30 17:47:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3b0bbb76650a2dcb011c181b56de90d114932fab', 'message': 'VMware: Check parent directory during create/copy disk in fake\n\nRaise FileNotFoundException in the fake driver when attempting to\ncreate a disk in a non-existent directory, or copy a disk into a\nnon-existent directory.\n\nChange-Id: I12164ff64b94eccf71323ccf837b8a9301c0ade9\n'}, {'number': 9, 'created': '2014-10-31 14:17:50.000000000', 'files': ['nova/tests/virt/vmwareapi/fake.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/4dc95bf36b3bc4e23acd8147cb70420ac6e76086', 'message': 'VMware: Check parent directory during create/copy disk in fake\n\nRaise FileNotFoundException in the fake driver when attempting to\ncreate a disk in a non-existent directory, or copy a disk into a\nnon-existent directory.\n\nChange-Id: I12164ff64b94eccf71323ccf837b8a9301c0ade9\n'}]",0,123687,4dc95bf36b3bc4e23acd8147cb70420ac6e76086,67,10,9,9555,,,0,"VMware: Check parent directory during create/copy disk in fake

Raise FileNotFoundException in the fake driver when attempting to
create a disk in a non-existent directory, or copy a disk into a
non-existent directory.

Change-Id: I12164ff64b94eccf71323ccf837b8a9301c0ade9
",git fetch https://review.opendev.org/openstack/nova refs/changes/87/123687/2 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/virt/vmwareapi/fake.py'],1,e8ab67b584d875b921a319f4e1bc3d595246b893,vmware_better_tests, if diskdir not in _db_content['paths']: raise vexc.FileNotFoundException(vmdk) ,,3,0
openstack%2Fnova~master~I9f0cb0ca4d076be3713862a890b36bae356d4a6e,openstack/nova,master,I9f0cb0ca4d076be3713862a890b36bae356d4a6e,VMware: Remove fake.get_file(),ABANDONED,2014-10-02 15:49:12.000000000,2014-12-08 15:50:52.000000000,,"[{'_account_id': 3}, {'_account_id': 5170}, {'_account_id': 8412}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-10-02 15:49:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8387144a16806c06855d8365e4e1ac6b521be63b', 'message': 'VMware: Remove fake.get_file()\n\nUpdate all tests which assert get_file() to instead assert membership\nof fake.paths(). This greatly improves the usefulness of generated\nerror messages, and explicitly identifies directories as such.\n\nChange-Id: I9f0cb0ca4d076be3713862a890b36bae356d4a6e\n'}, {'number': 2, 'created': '2014-10-08 09:23:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d7f6cb0d8ea97575d5f6cd9de63938e9dc7903bd', 'message': 'VMware: Remove fake.get_file()\n\nUpdate all tests which assert get_file() to instead assert membership\nof fake.paths(). This greatly improves the usefulness of generated\nerror messages, and explicitly identifies directories as such.\n\nChange-Id: I9f0cb0ca4d076be3713862a890b36bae356d4a6e\n'}, {'number': 3, 'created': '2014-10-15 14:50:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8fe27f4374cbafaf233dae47d9754b91c0161123', 'message': 'VMware: Remove fake.get_file()\n\nUpdate all tests which assert get_file() to instead assert membership\nof fake.paths(). This greatly improves the usefulness of generated\nerror messages, and explicitly identifies directories as such.\n\nChange-Id: I9f0cb0ca4d076be3713862a890b36bae356d4a6e\n'}, {'number': 4, 'created': '2014-10-30 17:47:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/7290bd4cfb3aa48989179cc1f864206a82e68ec4', 'message': 'VMware: Remove fake.get_file()\n\nUpdate all tests which assert get_file() to instead assert membership\nof fake.paths(). This greatly improves the usefulness of generated\nerror messages, and explicitly identifies directories as such.\n\nChange-Id: I9f0cb0ca4d076be3713862a890b36bae356d4a6e\n'}, {'number': 5, 'created': '2014-10-31 14:17:50.000000000', 'files': ['nova/tests/virt/vmwareapi/fake.py', 'nova/tests/virt/vmwareapi/test_driver_api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/b685c4b17df5c7a0b8cf339ff6449926906a8e84', 'message': 'VMware: Remove fake.get_file()\n\nUpdate all tests which assert get_file() to instead assert membership\nof fake.paths(). This greatly improves the usefulness of generated\nerror messages, and explicitly identifies directories as such.\n\nChange-Id: I9f0cb0ca4d076be3713862a890b36bae356d4a6e\n'}]",0,125692,b685c4b17df5c7a0b8cf339ff6449926906a8e84,39,9,5,9555,,,0,"VMware: Remove fake.get_file()

Update all tests which assert get_file() to instead assert membership
of fake.paths(). This greatly improves the usefulness of generated
error messages, and explicitly identifies directories as such.

Change-Id: I9f0cb0ca4d076be3713862a890b36bae356d4a6e
",git fetch https://review.opendev.org/openstack/nova refs/changes/92/125692/5 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/vmwareapi/fake.py', 'nova/tests/virt/vmwareapi/test_driver_api.py']",2,8387144a16806c06855d8365e4e1ac6b521be63b,vmware_better_tests," self.assertIn(cache, vmwareapi_fake.paths()) else: self.assertNotIn(cache, vmwareapi_fake.paths()) self.assertIn(path, vmwareapi_fake.paths()) self.assertIn(path, vmwareapi_fake.paths()) self.assertIn(root, vmwareapi_fake.paths()) self.assertIn(path, vmwareapi_fake.paths()) self.assertIn(path, vmwareapi_fake.paths()) self.assertNotIn(path, vmwareapi_fake.paths()) cache = ds_util.DatastorePath(self.ds, 'vmware_base', self.fake_image_uuid, self.fake_image_uuid + '.vmdk') gb_cache = ds_util.DatastorePath(self.ds, 'vmware_base', self.fake_image_uuid, self.fake_image_uuid + '.0.vmdk') self.assertIn(cache, vmwareapi_fake.paths()) self.assertNotIn(gb_cache, vmwareapi_fake.paths()) self.assertIn(root, vmwareapi_fake.paths()) cached_image = ds_util.DatastorePath(self.ds, 'vmware_base', uuid, uuid + '.80.vmdk') self.assertIn(cached_image, vmwareapi_fake.paths()) self.assertNotIn(cached_image, vmwareapi_fake.paths()) cached_image = ds_util.DatastorePath(self.ds, 'vmware_base', uuid, uuid + '.80.vmdk') self.assertIn(cached_image, vmwareapi_fake.paths()) self.assertIn(cached_image, vmwareapi_fake.paths()) self.assertNotIn(cached_image, vmwareapi_fake.paths()) self.assertNotIn(tmp_file, vmwareapi_fake.paths()) self.assertNotIn(inst_path, vmwareapi_fake.paths()) self.assertNotIn(rescue_file_path, vmwareapi_fake.paths()) self.assertIn(inst_file_path, vmwareapi_fake.paths()) self.assertIn(rescue_file_path, vmwareapi_fake.paths()) self._get_timestamp_filename()) if exists: self.assertIn(timestamp, vmwareapi_fake.paths()) else: self.assertNotIn(timestamp, vmwareapi_fake.paths())"," self.assertTrue(vmwareapi_fake.get_file(str(cache))) else: self.assertFalse(vmwareapi_fake.get_file(str(cache))) self.assertTrue(vmwareapi_fake.get_file(str(path))) self.assertTrue(vmwareapi_fake.get_file(str(path))) self.assertTrue(vmwareapi_fake.get_file(str(root))) self.assertTrue(vmwareapi_fake.get_file(str(path))) self.assertTrue(vmwareapi_fake.get_file(str(path))) self.assertFalse(vmwareapi_fake.get_file(str(path))) cache = ('[%s] vmware_base/%s/%s.vmdk' % (self.ds, self.fake_image_uuid, self.fake_image_uuid)) gb_cache = ('[%s] vmware_base/%s/%s.0.vmdk' % (self.ds, self.fake_image_uuid, self.fake_image_uuid)) self.assertTrue(vmwareapi_fake.get_file(cache)) self.assertFalse(vmwareapi_fake.get_file(gb_cache)) self.assertTrue(vmwareapi_fake.get_file(str(root))) cached_image = '[%s] vmware_base/%s/%s.80.vmdk' % (self.ds, uuid, uuid) self.assertTrue(vmwareapi_fake.get_file(cached_image)) self.assertFalse(vmwareapi_fake.get_file(cached_image)) cached_image = '[%s] vmware_base/%s/%s.80.vmdk' % (self.ds, uuid, uuid) self.assertTrue(vmwareapi_fake.get_file(cached_image)) self.assertTrue(vmwareapi_fake.get_file(cached_image)) self.assertFalse(vmwareapi_fake.get_file(str(cached_image))) self.assertFalse(vmwareapi_fake.get_file(str(tmp_file))) self.assertFalse(vmwareapi_fake.get_file(str(inst_path))) self.assertFalse(vmwareapi_fake.get_file(str(rescue_file_path))) self.assertTrue(vmwareapi_fake.get_file(str(inst_file_path))) self.assertTrue(vmwareapi_fake.get_file(str(rescue_file_path))) self._get_timestamp_filename() + '/') if exists: self.assertTrue(vmwareapi_fake.get_file(str(timestamp))) else: self.assertFalse(vmwareapi_fake.get_file(str(timestamp)))",36,45
openstack%2Fnova~master~Id74856c384d45280bde3ca3fe315f8121a980d54,openstack/nova,master,Id74856c384d45280bde3ca3fe315f8121a980d54,VMware: Remove fake._add_file,ABANDONED,2014-10-02 15:49:12.000000000,2014-12-08 15:50:48.000000000,,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 5170}, {'_account_id': 8412}, {'_account_id': 9008}, {'_account_id': 9550}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-10-02 15:49:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/56690966dc50e5dcc9fd3e820a50797146ed7d2b', 'message': ""VMware: Remove fake._add_file\n\nfake._add_file didn't explicitly identify directories. Replace all\nusers of fake._add_file with fake.add_path, which explicitly\nidentifies directories.\n\nChange-Id: Id74856c384d45280bde3ca3fe315f8121a980d54\n""}, {'number': 2, 'created': '2014-10-08 09:23:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b74afac2d881e0eed227db08ddfbaa9b7c7d8804', 'message': ""VMware: Remove fake._add_file\n\nfake._add_file didn't explicitly identify directories. Replace all\nusers of fake._add_file with fake.add_path, which explicitly\nidentifies directories.\n\nChange-Id: Id74856c384d45280bde3ca3fe315f8121a980d54\n""}, {'number': 3, 'created': '2014-10-15 14:50:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/0e438970980f02e64e8aaa8a2be39483d31a3310', 'message': ""VMware: Remove fake._add_file\n\nfake._add_file didn't explicitly identify directories. Replace all\nusers of fake._add_file with fake.add_path, which explicitly\nidentifies directories.\n\nChange-Id: Id74856c384d45280bde3ca3fe315f8121a980d54\n""}, {'number': 4, 'created': '2014-10-30 17:47:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/db8cc211dc8931bb86a484f08f8b24a0e41bde9b', 'message': ""VMware: Remove fake._add_file\n\nfake._add_file didn't explicitly identify directories. Replace all\nusers of fake._add_file with fake.add_path, which explicitly\nidentifies directories.\n\nChange-Id: Id74856c384d45280bde3ca3fe315f8121a980d54\n""}, {'number': 5, 'created': '2014-10-31 14:17:51.000000000', 'files': ['nova/tests/virt/vmwareapi/fake.py', 'nova/tests/virt/vmwareapi/test_driver_api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/8b9a2fbabf1a40403b352ffd4f1bf90ee264ddf2', 'message': ""VMware: Remove fake._add_file\n\nfake._add_file didn't explicitly identify directories. Replace all\nusers of fake._add_file with fake.add_path, which explicitly\nidentifies directories.\n\nChange-Id: Id74856c384d45280bde3ca3fe315f8121a980d54\n""}]",0,125691,8b9a2fbabf1a40403b352ffd4f1bf90ee264ddf2,40,10,5,9555,,,0,"VMware: Remove fake._add_file

fake._add_file didn't explicitly identify directories. Replace all
users of fake._add_file with fake.add_path, which explicitly
identifies directories.

Change-Id: Id74856c384d45280bde3ca3fe315f8121a980d54
",git fetch https://review.opendev.org/openstack/nova refs/changes/91/125691/3 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/vmwareapi/fake.py', 'nova/tests/virt/vmwareapi/test_driver_api.py']",2,56690966dc50e5dcc9fd3e820a50797146ed7d2b,vmware_better_tests," vmwareapi_fake.add_path(root, False) vmwareapi_fake.add_path(ts_path, True)", vmwareapi_fake._add_file(str(root)) vmwareapi_fake._add_file(str(ts_path)),2,12
openstack%2Fnova~master~I668d301823bcb796424df71792cfe542cbd51b68,openstack/nova,master,I668d301823bcb796424df71792cfe542cbd51b68,VMware: Create vmx and config directory in fake driver,ABANDONED,2014-09-24 10:24:34.000000000,2014-12-08 15:50:43.000000000,,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 5170}, {'_account_id': 8412}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-24 10:24:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/911db90dc45666a41b8fe94beed4021d00d4cc07', 'message': 'VMware: Create vmx and config directory in fake driver\n\nUpdate fake driver to add a vmx file and config directory when\ncreating a VM. This also allows us to remove the weird heuristic from\nremove_path.\n\nChange-Id: I668d301823bcb796424df71792cfe542cbd51b68\n'}, {'number': 2, 'created': '2014-09-25 15:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f54fb846b07fcecef76980a1949170474aa9e81e', 'message': 'VMware: Create vmx and config directory in fake driver\n\nUpdate fake driver to add a vmx file and config directory when\ncreating a VM. This also allows us to remove the weird heuristic from\nremove_path.\n\nChange-Id: I668d301823bcb796424df71792cfe542cbd51b68\n'}, {'number': 3, 'created': '2014-09-29 13:45:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/cf2ef6115c879c577e48ca75c9b8b4ea9e6115ec', 'message': 'VMware: Create vmx and config directory in fake driver\n\nUpdate fake driver to add a vmx file and config directory when\ncreating a VM. This also allows us to remove the weird heuristic from\nremove_path.\n\nChange-Id: I668d301823bcb796424df71792cfe542cbd51b68\n'}, {'number': 4, 'created': '2014-10-02 15:49:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1e612315e9a196c83ec57376fea922d4e5433f66', 'message': 'VMware: Create vmx and config directory in fake driver\n\nUpdate fake driver to add a vmx file and, implicitly, the config\ndirectory when creating a VM.\n\nThis has the side effect of fixing tests that tried to remove the\nconfig directory and expected it not to fail. As the config directory\nnow exists in the db we no longer have to guard against this. This\nallows us to fix remove_path to raise an exception for a missing\ndirectory.\n\nChange-Id: I668d301823bcb796424df71792cfe542cbd51b68\n'}, {'number': 5, 'created': '2014-10-08 09:23:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d863afd2ca7ec461f0818902c94fd6a3b4af5f5a', 'message': 'VMware: Create vmx and config directory in fake driver\n\nUpdate fake driver to add a vmx file and, implicitly, the config\ndirectory when creating a VM.\n\nThis has the side effect of fixing tests that tried to remove the\nconfig directory and expected it not to fail. As the config directory\nnow exists in the db we no longer have to guard against this. This\nallows us to fix remove_path to raise an exception for a missing\ndirectory.\n\nChange-Id: I668d301823bcb796424df71792cfe542cbd51b68\n'}, {'number': 6, 'created': '2014-10-15 14:50:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/224352d140073b21be5cf9182ec89ed07a65cec2', 'message': 'VMware: Create vmx and config directory in fake driver\n\nUpdate fake driver to add a vmx file and, implicitly, the config\ndirectory when creating a VM.\n\nThis has the side effect of fixing tests that tried to remove the\nconfig directory and expected it not to fail. As the config directory\nnow exists in the db we no longer have to guard against this. This\nallows us to fix remove_path to raise an exception for a missing\ndirectory.\n\nChange-Id: I668d301823bcb796424df71792cfe542cbd51b68\n'}, {'number': 7, 'created': '2014-10-30 17:47:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b68a10bf94cd52dab1a4194c77ac99736ed67eef', 'message': 'VMware: Create vmx and config directory in fake driver\n\nUpdate fake driver to add a vmx file and, implicitly, the config\ndirectory when creating a VM.\n\nThis has the side effect of fixing tests that tried to remove the\nconfig directory and expected it not to fail. As the config directory\nnow exists in the db we no longer have to guard against this. This\nallows us to fix remove_path to raise an exception for a missing\ndirectory.\n\nChange-Id: I668d301823bcb796424df71792cfe542cbd51b68\n'}, {'number': 8, 'created': '2014-10-31 14:17:50.000000000', 'files': ['nova/tests/virt/vmwareapi/fake.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/1818327e0d870c3c383ac3991216100d4ba283f1', 'message': 'VMware: Create vmx and config directory in fake driver\n\nUpdate fake driver to add a vmx file and, implicitly, the config\ndirectory when creating a VM.\n\nThis has the side effect of fixing tests that tried to remove the\nconfig directory and expected it not to fail. As the config directory\nnow exists in the db we no longer have to guard against this. This\nallows us to fix remove_path to raise an exception for a missing\ndirectory.\n\nChange-Id: I668d301823bcb796424df71792cfe542cbd51b68\n'}]",2,123686,1818327e0d870c3c383ac3991216100d4ba283f1,59,10,8,9555,,,0,"VMware: Create vmx and config directory in fake driver

Update fake driver to add a vmx file and, implicitly, the config
directory when creating a VM.

This has the side effect of fixing tests that tried to remove the
config directory and expected it not to fail. As the config directory
now exists in the db we no longer have to guard against this. This
allows us to fix remove_path to raise an exception for a missing
directory.

Change-Id: I668d301823bcb796424df71792cfe542cbd51b68
",git fetch https://review.opendev.org/openstack/nova refs/changes/86/123686/8 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/virt/vmwareapi/fake.py'],1,911db90dc45666a41b8fe94beed4021d00d4cc07,vmware_better_tests," if path not in paths: raise vexc.FileNotFoundException(path) if path_isdir[path]: # Create the vmx file and, implicitly, the VM directory add_path(vm_path, False, create_parents=True) "," might_be_dir = False if path not in paths: # This weird heuristic mirrors the previous behaviour, and is currently # required for some existing tests to pass if path.rel_path.endswith('.vmdk'): raise vexc.FileNotFoundException(path) might_be_dir = True if might_be_dir or path_isdir[path]:",6,8
openstack%2Fnova~master~I5e4fa7bf9ba111fadc19b484dd4c91e55fa99820,openstack/nova,master,I5e4fa7bf9ba111fadc19b484dd4c91e55fa99820,VMware: Create directories implicitly in fake driver,ABANDONED,2014-10-02 15:49:12.000000000,2014-12-08 15:50:39.000000000,,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 5170}, {'_account_id': 8412}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-10-02 15:49:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e388f6ac82201e9e9e534382e8151fa28739db90', 'message': 'VMware: Create directories implicitly in fake driver\n\nCertain operations implicitly create directories on a vSphere server.\nThis change models this behaviour in the fake driver.\n\nChange-Id: I5e4fa7bf9ba111fadc19b484dd4c91e55fa99820\n'}, {'number': 2, 'created': '2014-10-08 09:23:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a3c3f3928e4c3f03106782d584bce656d89dca54', 'message': 'VMware: Create directories implicitly in fake driver\n\nCertain operations implicitly create directories on a vSphere server.\nThis change models this behaviour in the fake driver.\n\nChange-Id: I5e4fa7bf9ba111fadc19b484dd4c91e55fa99820\n'}, {'number': 3, 'created': '2014-10-15 14:50:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/2606e131d78649d31a54e38f60e8e403a0a4e756', 'message': 'VMware: Create directories implicitly in fake driver\n\nCertain operations implicitly create directories on a vSphere server.\nThis change models this behaviour in the fake driver.\n\nChange-Id: I5e4fa7bf9ba111fadc19b484dd4c91e55fa99820\n'}, {'number': 4, 'created': '2014-10-30 17:47:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f6a12b86940686fcf413d1c51c0ac1883e499b16', 'message': 'VMware: Create directories implicitly in fake driver\n\nCertain operations implicitly create directories on a vSphere server.\nThis change models this behaviour in the fake driver.\n\nChange-Id: I5e4fa7bf9ba111fadc19b484dd4c91e55fa99820\n'}, {'number': 5, 'created': '2014-10-31 14:17:51.000000000', 'files': ['nova/tests/virt/vmwareapi/fake.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/9e83cc5596e561e03f3d571c20bdf5bb499b5641', 'message': 'VMware: Create directories implicitly in fake driver\n\nCertain operations implicitly create directories on a vSphere server.\nThis change models this behaviour in the fake driver.\n\nChange-Id: I5e4fa7bf9ba111fadc19b484dd4c91e55fa99820\n'}]",0,125690,9e83cc5596e561e03f3d571c20bdf5bb499b5641,40,10,5,9555,,,0,"VMware: Create directories implicitly in fake driver

Certain operations implicitly create directories on a vSphere server.
This change models this behaviour in the fake driver.

Change-Id: I5e4fa7bf9ba111fadc19b484dd4c91e55fa99820
",git fetch https://review.opendev.org/openstack/nova refs/changes/90/125690/4 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/virt/vmwareapi/fake.py'],1,e388f6ac82201e9e9e534382e8151fa28739db90,vmware_better_tests,"def add_path(path, is_dir, create_parents=False): parent = path.parent if create_parents: while parent.rel_path != '/' and parent.rel_path != '': paths.add(parent) path_isdir[parent] = True parent = parent.parent might_be_dir = False if path not in paths: # Existing tests assume that anything ending in .vmdk is a file, and # only files generate FileNotFoundException if they don't exist. if path.rel_path.endswith('.vmdk'): might_be_dir = True # Remove folder contents if required if might_be_dir or path_isdir[path]: paths.remove(path) del(path_isdir[path]) add_path(ds_util.DatastorePath(ds_name, file_path), False, create_parents=True) if source != destination: to_move = set([source]) if path_isdir[source]: prefix = source.rel_path + '/' for i in paths: if (i.datastore == source.datastore and i.rel_path.startswith(prefix)): to_move.add(i) for i in to_move: new_path = ds_util.DatastorePath( destination.datastore, i.rel_path.replace(source.rel_path, destination.rel_path)) paths.add(new_path) path_isdir[new_path] = path_isdir[i] paths.remove(i) del(path_isdir[i]) add_path(ds_util.DatastorePath(ds_name, file_path), False, create_parents=True) create_parents = kwargs.get('createParentDirectories', False) add_path(ds_path, True, create_parents=create_parents)","def add_path(path, is_dir): # Existing tests assume that anything ending in .vmdk is a file, and # only files generate FileNotFoundException if they don't exist. if path.rel_path.endswith('.vmdk'): if path not in paths: paths.remove(path) del(path_isdir[path]) else: # Remove folder contents if required if path in paths: paths.remove(path) del(path_isdir[path]) add_path(ds_util.DatastorePath(ds_name, file_path), False) to_move = set() prefix = source.rel_path + '/' for i in paths: if (i.datastore == source.datastore and ( i.rel_path == source.rel_path or i.rel_path.startswith(prefix))): to_move.add(i) for i in to_move: new_path = ds_util.DatastorePath( destination.datastore, i.rel_path.replace(source.rel_path, destination.rel_path)) paths.add(new_path) path_isdir[new_path] = path_isdir[i] paths.remove(i) del(path_isdir[i]) add_path(ds_util.DatastorePath(ds_name, file_path), False) add_path(ds_path, True)",43,31
openstack%2Fnova~master~I3888a35933cd041571512bc36dc75614ed49dc90,openstack/nova,master,I3888a35933cd041571512bc36dc75614ed49dc90,VMware: Improve storage of files in fake driver,ABANDONED,2014-09-24 10:24:34.000000000,2014-12-08 15:50:34.000000000,,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 5170}, {'_account_id': 8412}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-24 10:24:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f08f728562f845f93b49b63f2bf76add6ee61c2e', 'message': 'VMware: Improve storage of files in fake driver\n\nThe principal driver of this change is to improve handling of\ndirectories.  Instead of having a heuristic, we now store directory\ninformation explicitly in a separate entry in _db_content.\n\nWe also:\n\n* store DatastorePath objects rather than strings.\n* change _add_file to add_path\n* change _remove_file to remove_path\n* remove get_file in favour of exposing paths through fake.paths()\n* update tests which assert get_file() to assert set (in/ex)clusion\n\nThe latter update greatly improves the usefulness of the error\nmessages produced by assertion failures.\n\nChange-Id: I3888a35933cd041571512bc36dc75614ed49dc90\n'}, {'number': 2, 'created': '2014-09-25 15:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d386a62da18a0dfc8ce7e566ca91c9c99f9d5613', 'message': 'VMware: Improve storage of files in fake driver\n\nThe principal driver of this change is to improve handling of\ndirectories.  Instead of having a heuristic, we now store directory\ninformation explicitly in a separate entry in _db_content.\n\nWe also:\n\n* store DatastorePath objects rather than strings.\n* change _add_file to add_path\n* change _remove_file to remove_path\n* remove get_file in favour of exposing paths through fake.paths()\n* update tests which assert get_file() to assert set (in/ex)clusion\n\nThe latter update greatly improves the usefulness of the error\nmessages produced by assertion failures.\n\nChange-Id: I3888a35933cd041571512bc36dc75614ed49dc90\n'}, {'number': 3, 'created': '2014-09-29 13:45:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/406791f1ac81e78e0f251c3f27597eb6a551225f', 'message': 'VMware: Improve storage of files in fake driver\n\nThe principal driver of this change is to improve handling of\ndirectories.  Instead of having a heuristic, we now store directory\ninformation explicitly in a separate entry in _db_content.\n\nWe also:\n\n* store DatastorePath objects rather than strings.\n* change _add_file to add_path\n* change _remove_file to remove_path\n* remove get_file in favour of exposing paths through fake.paths()\n* update tests which assert get_file() to assert set (in/ex)clusion\n\nThe latter update greatly improves the usefulness of the error\nmessages produced by assertion failures.\n\nChange-Id: I3888a35933cd041571512bc36dc75614ed49dc90\n'}, {'number': 4, 'created': '2014-10-02 15:49:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c64239a8bb92dfd6eb590933a3f91b5d24e61342', 'message': ""VMware: Improve storage of files in fake driver\n\nThe principal driver of this change is to improve handling of\ndirectories. Instead of using the heuristic than anything ending in /\nis a directory, we instead store:\n\n* DatastorePath objects in _db_content['paths']\n* directory metadata separately in _db_content['path_isdir']\n\nChange-Id: I3888a35933cd041571512bc36dc75614ed49dc90\n""}, {'number': 5, 'created': '2014-10-08 09:23:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/bf834cfe2b0f090a4f0cfdaa8240d8577625933a', 'message': ""VMware: Improve storage of files in fake driver\n\nThe principal driver of this change is to improve handling of\ndirectories. Instead of using the heuristic than anything ending in /\nis a directory, we instead store:\n\n* DatastorePath objects in _db_content['paths']\n* directory metadata separately in _db_content['path_isdir']\n\nChange-Id: I3888a35933cd041571512bc36dc75614ed49dc90\n""}, {'number': 6, 'created': '2014-10-15 14:50:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/4d3105546045d2779f534479f45756643c4b2a67', 'message': ""VMware: Improve storage of files in fake driver\n\nThe principal driver of this change is to improve handling of\ndirectories. Instead of using the heuristic than anything ending in /\nis a directory, we instead store:\n\n* DatastorePath objects in _db_content['paths']\n* directory metadata separately in _db_content['path_isdir']\n\nChange-Id: I3888a35933cd041571512bc36dc75614ed49dc90\n""}, {'number': 7, 'created': '2014-10-30 17:47:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ffe20c3e32c1688ea2b1efb61c86da438cd866c5', 'message': ""VMware: Improve storage of files in fake driver\n\nThe principal driver of this change is to improve handling of\ndirectories. Instead of using the heuristic than anything ending in /\nis a directory, we instead store:\n\n* DatastorePath objects in _db_content['paths']\n* directory metadata separately in _db_content['path_isdir']\n\nChange-Id: I3888a35933cd041571512bc36dc75614ed49dc90\n""}, {'number': 8, 'created': '2014-10-31 14:17:51.000000000', 'files': ['nova/tests/virt/vmwareapi/fake.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/29b0ad190fdae2d63266642edf7a4007d6aad028', 'message': ""VMware: Improve storage of files in fake driver\n\nThe principal driver of this change is to improve handling of\ndirectories. Instead of using the heuristic than anything ending in /\nis a directory, we instead store:\n\n* DatastorePath objects in _db_content['paths']\n* directory metadata separately in _db_content['path_isdir']\n\nIn updating _search_ds for this change, we also fix a bug in its\noutput due to misuse of lstrip().\n\nCloses-Bug: #1388095\n\nChange-Id: I3888a35933cd041571512bc36dc75614ed49dc90\n""}]",3,123685,29b0ad190fdae2d63266642edf7a4007d6aad028,69,10,8,9555,,,0,"VMware: Improve storage of files in fake driver

The principal driver of this change is to improve handling of
directories. Instead of using the heuristic than anything ending in /
is a directory, we instead store:

* DatastorePath objects in _db_content['paths']
* directory metadata separately in _db_content['path_isdir']

In updating _search_ds for this change, we also fix a bug in its
output due to misuse of lstrip().

Closes-Bug: #1388095

Change-Id: I3888a35933cd041571512bc36dc75614ed49dc90
",git fetch https://review.opendev.org/openstack/nova refs/changes/85/123685/7 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/vmwareapi/fake.py', 'nova/tests/virt/vmwareapi/test_driver_api.py']",2,f08f728562f845f93b49b63f2bf76add6ee61c2e,vmware_better_tests," self.assertIn(cache, vmwareapi_fake.paths()) else: self.assertNotIn(cache, vmwareapi_fake.paths()) self.assertIn(path, vmwareapi_fake.paths()) self.assertIn(path, vmwareapi_fake.paths()) self.assertIn(root, vmwareapi_fake.paths()) self.assertIn(path, vmwareapi_fake.paths()) self.assertIn(path, vmwareapi_fake.paths()) self.assertNotIn(path, vmwareapi_fake.paths()) cache = ds_util.DatastorePath(self.ds, 'vmware_base', self.fake_image_uuid, self.fake_image_uuid + '.vmdk') gb_cache = ds_util.DatastorePath(self.ds, 'vmware_base', self.fake_image_uuid, self.fake_image_uuid + '.0.vmdk') self.assertIn(cache, vmwareapi_fake.paths()) self.assertNotIn(gb_cache, vmwareapi_fake.paths()) vmwareapi_fake.add_path(root, False) self.assertIn(root, vmwareapi_fake.paths()) cached_image = ds_util.DatastorePath(self.ds, 'vmware_base', uuid, uuid + '.80.vmdk') self.assertIn(cached_image, vmwareapi_fake.paths()) self.assertNotIn(cached_image, vmwareapi_fake.paths()) cached_image = ds_util.DatastorePath(self.ds, 'vmware_base', uuid, uuid + '.80.vmdk') self.assertIn(cached_image, vmwareapi_fake.paths()) self.assertIn(cached_image, vmwareapi_fake.paths()) self.assertNotIn(cached_image, vmwareapi_fake.paths()) self.assertNotIn(tmp_file, vmwareapi_fake.paths()) self.assertNotIn(inst_path, vmwareapi_fake.paths()) self.assertNotIn(rescue_file_path, vmwareapi_fake.paths()) self.assertIn(inst_file_path, vmwareapi_fake.paths()) self.assertIn(rescue_file_path, vmwareapi_fake.paths()) self._get_timestamp_filename()) if exists: self.assertIn(timestamp, vmwareapi_fake.paths()) else: self.assertNotIn(timestamp, vmwareapi_fake.paths()) vmwareapi_fake.add_path(ts_path, True)"," self.assertTrue(vmwareapi_fake.get_file(str(cache))) else: self.assertFalse(vmwareapi_fake.get_file(str(cache))) self.assertTrue(vmwareapi_fake.get_file(str(path))) self.assertTrue(vmwareapi_fake.get_file(str(path))) self.assertTrue(vmwareapi_fake.get_file(str(root))) self.assertTrue(vmwareapi_fake.get_file(str(path))) self.assertTrue(vmwareapi_fake.get_file(str(path))) self.assertFalse(vmwareapi_fake.get_file(str(path))) cache = ('[%s] vmware_base/%s/%s.vmdk' % (self.ds, self.fake_image_uuid, self.fake_image_uuid)) gb_cache = ('[%s] vmware_base/%s/%s.0.vmdk' % (self.ds, self.fake_image_uuid, self.fake_image_uuid)) self.assertTrue(vmwareapi_fake.get_file(cache)) self.assertFalse(vmwareapi_fake.get_file(gb_cache)) vmwareapi_fake._add_file(str(root)) self.assertTrue(vmwareapi_fake.get_file(str(root))) cached_image = '[%s] vmware_base/%s/%s.80.vmdk' % (self.ds, uuid, uuid) self.assertTrue(vmwareapi_fake.get_file(cached_image)) self.assertFalse(vmwareapi_fake.get_file(cached_image)) cached_image = '[%s] vmware_base/%s/%s.80.vmdk' % (self.ds, uuid, uuid) self.assertTrue(vmwareapi_fake.get_file(cached_image)) self.assertTrue(vmwareapi_fake.get_file(cached_image)) self.assertFalse(vmwareapi_fake.get_file(str(cached_image))) self.assertFalse(vmwareapi_fake.get_file(str(tmp_file))) self.assertFalse(vmwareapi_fake.get_file(str(inst_path))) self.assertFalse(vmwareapi_fake.get_file(str(rescue_file_path))) self.assertTrue(vmwareapi_fake.get_file(str(inst_file_path))) self.assertTrue(vmwareapi_fake.get_file(str(rescue_file_path))) self._get_timestamp_filename() + '/') if exists: self.assertTrue(vmwareapi_fake.get_file(str(timestamp))) else: self.assertFalse(vmwareapi_fake.get_file(str(timestamp))) vmwareapi_fake._add_file(str(ts_path))",164,98
openstack%2Fnova~master~I9b565670f5b0ad24c7a5ff1853648594643b4c7a,openstack/nova,master,I9b565670f5b0ad24c7a5ff1853648594643b4c7a,Libvirt normalize numa cell ids,MERGED,2014-12-01 17:17:43.000000000,2014-12-08 15:50:29.000000000,2014-12-08 15:50:26.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 5511}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 11647}]","[{'number': 1, 'created': '2014-12-01 17:17:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/2a433337e27ef5e43b9bcf60ad4c9d171ddcb239', 'message': 'Normalize numa cell ids\n\nThis patch normalizes the cell ids before writing xml.\nAs cell id are used to store host cell association,\nthey need to be normalized before writing xml, or otherwise\nthe xml will be invalid.\n\nChange-Id: I9b565670f5b0ad24c7a5ff1853648594643b4c7a\nCloses-bug: #1397381\n'}, {'number': 2, 'created': '2014-12-02 16:00:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d803c4cab422e5780d5911faea9efe305ac75d97', 'message': 'Libvirt normalize numa cell ids\n\nThis patch normalizes the cell ids before writing xml.\nAs cell ids are used to store host cell association,\nthey need to be normalized before writing xml, or otherwise\nthe xml will be invalid.\n\nChange-Id: I9b565670f5b0ad24c7a5ff1853648594643b4c7a\nCloses-bug: #1397381\n'}, {'number': 3, 'created': '2014-12-05 16:37:44.000000000', 'files': ['nova/virt/libvirt/driver.py', 'nova/tests/unit/virt/libvirt/test_driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/cb98f1e2f288de0b3d5f85c84717540b9290f8a8', 'message': 'Libvirt normalize numa cell ids\n\nThis patch normalizes the cell ids before writing xml.\nAs cell ids are used to store host cell association,\nthey need to be normalized before writing xml, or otherwise\nthe xml will be invalid.\n\nChange-Id: I9b565670f5b0ad24c7a5ff1853648594643b4c7a\nCloses-bug: #1397381\n'}]",16,138128,cb98f1e2f288de0b3d5f85c84717540b9290f8a8,51,12,3,11647,,,0,"Libvirt normalize numa cell ids

This patch normalizes the cell ids before writing xml.
As cell ids are used to store host cell association,
they need to be normalized before writing xml, or otherwise
the xml will be invalid.

Change-Id: I9b565670f5b0ad24c7a5ff1853648594643b4c7a
Closes-bug: #1397381
",git fetch https://review.opendev.org/openstack/nova refs/changes/28/138128/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/libvirt/driver.py', 'nova/tests/unit/virt/libvirt/test_driver.py']",2,2a433337e27ef5e43b9bcf60ad4c9d171ddcb239,bug-1397381," id=1, cpuset=set([0, 1]), memory=1024), id=2, cpuset=set([2, 3]), memory=1024)]) flavor = objects.Flavor(memory_mb=2048, vcpus=4, root_gb=496, hardware, 'get_vcpu_pin_set', return_value=set([2, 3, 4, 5])) self.assertEqual(set([2, 3]), cfg.cputune.vcpupin[0].cpuset) self.assertEqual(set([2, 3]), cfg.cputune.vcpupin[1].cpuset) self.assertEqual(set([4, 5]), cfg.cputune.vcpupin[2].cpuset) self.assertEqual(set([4, 5]), cfg.cputune.vcpupin[3].cpuset) self.assertEqual(set([2, 3, 4, 5]), cfg.cputune.emulatorpin.cpuset) for instance_cell, numa_cfg_cell, index in zip( instance_topology.cells, cfg.cpu.numa.cells, range(len(instance_topology.cells))): self.assertEqual(index, numa_cfg_cell.id) for instance_cell, memnode, index in zip( instance_topology.cells, cfg.numatune.memnodes, range(len(instance_topology.cells))): self.assertEqual(index, memnode.cellid) for i in range(4): cell = vconfig.LibvirtConfigCapsNUMACell() cell.id = i cell.memory = 1024 * units.Ki cpu_0 = vconfig.LibvirtConfigCapsNUMACPU() cpu_0.id = 2 * i cpu_0.socket_id = i cpu_0.core_id = 0 cpu_0.sibling = 2 * i cpu_1 = vconfig.LibvirtConfigCapsNUMACPU() cpu_1.id = 2 * i + 1 cpu_1.socket_id = i cpu_1.core_id = 1 cpu_1.sibling = 2 * i + 1 cell.cpus = [cpu_0, cpu_1] topology.cells.append(cell) 'id': 1}, {'cpus': '', 'cpu_usage': 0, 'mem': {'total': 1024, 'used': 0}, 'id': 2}, {'cpus': '', 'cpu_usage': 0, 'mem': {'total': 1024, 'used': 0}, 'id': 3}]}"," id=0, cpuset=set([0, 1]), memory=1024), id=1, cpuset=set([2, 3]), memory=1024)]) flavor = objects.Flavor(memory_mb=2048, vcpus=2, root_gb=496, hardware, 'get_vcpu_pin_set', return_value=set([0, 1, 2])) self.assertEqual(set([0, 1]), cfg.cputune.vcpupin[0].cpuset) self.assertEqual(set([0, 1]), cfg.cputune.vcpupin[1].cpuset) self.assertEqual(set([2]), cfg.cputune.vcpupin[2].cpuset) self.assertEqual(set([2]), cfg.cputune.vcpupin[3].cpuset) self.assertEqual(set([0, 1, 2]), cfg.cputune.emulatorpin.cpuset) for instance_cell, numa_cfg_cell in zip( instance_topology.cells, cfg.cpu.numa.cells): self.assertEqual(instance_cell.id, numa_cfg_cell.id) for instance_cell, memnode in zip( instance_topology.cells, cfg.numatune.memnodes): self.assertEqual(instance_cell.id, memnode.cellid) cell_0 = vconfig.LibvirtConfigCapsNUMACell() cell_0.id = 0 cell_0.memory = 1024 * units.Ki cpu_0_0 = vconfig.LibvirtConfigCapsNUMACPU() cpu_0_0.id = 0 cpu_0_0.socket_id = 0 cpu_0_0.core_id = 0 cpu_0_0.sibling = 0 cpu_0_1 = vconfig.LibvirtConfigCapsNUMACPU() cpu_0_1.id = 1 cpu_0_1.socket_id = 0 cpu_0_1.core_id = 1 cpu_0_1.sibling = 1 cell_0.cpus = [cpu_0_0, cpu_0_1] cell_1 = vconfig.LibvirtConfigCapsNUMACell() cell_1.id = 1 cell_1.memory = 1024 * units.Ki cpu_1_0 = vconfig.LibvirtConfigCapsNUMACPU() cpu_1_0.id = 2 cpu_1_0.socket_id = 1 cpu_1_0.core_id = 0 cpu_1_0.sibling = 2 cpu_1_1 = vconfig.LibvirtConfigCapsNUMACPU() cpu_1_1.id = 3 cpu_1_1.socket_id = 1 cpu_1_1.core_id = 1 cpu_1_1.sibling = 3 cell_1.cpus = [cpu_1_0, cpu_1_1] topology.cells = [cell_0, cell_1] 'id': 1}]}",50,46
openstack%2Fnova~master~I671315122438022b13d6b315b577ef2a5cf42545,openstack/nova,master,I671315122438022b13d6b315b577ef2a5cf42545,Rename private functions in db.sqla.api,MERGED,2014-11-13 16:01:12.000000000,2014-12-08 15:50:08.000000000,2014-12-08 15:50:05.000000000,"[{'_account_id': 3}, {'_account_id': 1812}, {'_account_id': 1849}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-13 16:01:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f35854acaa60872aed07ff4751e1e16c89a2260c', 'message': 'Rename private functions in db.sqla.api\n\nAdd a _ prefix to identify private functions.\n\nChange-Id: I671315122438022b13d6b315b577ef2a5cf42545\n'}, {'number': 2, 'created': '2014-11-14 16:38:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/12566e1d0070b62867c82a678422df9d71b486aa', 'message': 'Rename private functions used by db.sqla.api.instance_get_all_by_filters\n\ninstance_get_all_by_filters() called 3 helper functions: exact_filter,\nregex_filter and tag_filter. None of these functions had any other\ncallers.\n\nThis change prefixes them with _ to identify them as private.\nAdditionally, it removes the model arguments to the functions as these\nalways relate to an Instance and/or its InstanceMetadata.\nConsequently, the functions are renamed to identify them as specific\nto an Instance query. exact_filter is moved from the top of the module\nto be next to the other 2.\n\nChange-Id: I671315122438022b13d6b315b577ef2a5cf42545\n'}, {'number': 3, 'created': '2014-11-14 16:50:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9b8e5f62bf79b0f32373863e42ff9b6266459db1', 'message': 'Rename private functions used by db.sqla.api.instance_get_all_by_filters\n\ninstance_get_all_by_filters() called 3 helper functions: exact_filter,\nregex_filter and tag_filter. None of these functions had any other\ncallers.\n\nThis change prefixes them with _ to identify them as private.\nAdditionally, it removes the model arguments to the functions as these\nalways relate to an Instance and/or its InstanceMetadata.\nConsequently, the functions are renamed to identify them as specific\nto an Instance query. exact_filter is moved from the top of the module\nto be next to the other 2.\n\nChange-Id: I671315122438022b13d6b315b577ef2a5cf42545\n'}, {'number': 4, 'created': '2014-11-17 09:35:23.000000000', 'files': ['nova/db/sqlalchemy/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/36d51245416b3fb18a2bc7dbae7b54214f1c5928', 'message': 'Rename private functions in db.sqla.api\n\ninstance_get_all_by_filters() called 3 helper functions: exact_filter,\nregex_filter and tag_filter. None of these functions had any other\ncallers.\n\nThis change prefixes them with _ to identify them as private.\nAdditionally, it removes the model arguments to the functions as these\nalways relate to an Instance and/or its InstanceMetadata.\nConsequently, the functions are renamed to identify them as specific\nto an Instance query. exact_filter is moved from the top of the module\nto be next to the other 2.\n\nChange-Id: I671315122438022b13d6b315b577ef2a5cf42545\n'}]",0,134270,36d51245416b3fb18a2bc7dbae7b54214f1c5928,35,11,4,9555,,,0,"Rename private functions in db.sqla.api

instance_get_all_by_filters() called 3 helper functions: exact_filter,
regex_filter and tag_filter. None of these functions had any other
callers.

This change prefixes them with _ to identify them as private.
Additionally, it removes the model arguments to the functions as these
always relate to an Instance and/or its InstanceMetadata.
Consequently, the functions are renamed to identify them as specific
to an Instance query. exact_filter is moved from the top of the module
to be next to the other 2.

Change-Id: I671315122438022b13d6b315b577ef2a5cf42545
",git fetch https://review.opendev.org/openstack/nova refs/changes/70/134270/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/db/sqlalchemy/api.py'],1,f35854acaa60872aed07ff4751e1e16c89a2260c,db/prune_api,"def _exact_filter(query, model, filters, legal_keys): query_prefix = _exact_filter(query_prefix, models.Instance, query_prefix = _regex_filter(query_prefix, models.Instance, filters) query_prefix = _tag_filter(context, query_prefix, models.Instance,def _tag_filter(context, query, model, model_metadata,def _regex_filter(query, model, filters):","def exact_filter(query, model, filters, legal_keys): query_prefix = exact_filter(query_prefix, models.Instance, query_prefix = regex_filter(query_prefix, models.Instance, filters) query_prefix = tag_filter(context, query_prefix, models.Instance,def tag_filter(context, query, model, model_metadata,def regex_filter(query, model, filters):",6,6
openstack%2Fglance~master~I55c40ed735781862f8352afb917058bf475d16fe,openstack/glance,master,I55c40ed735781862f8352afb917058bf475d16fe,Fix and add a test case for IPv6,MERGED,2014-12-06 07:15:53.000000000,2014-12-08 15:49:21.000000000,2014-12-08 15:49:21.000000000,"[{'_account_id': 3}, {'_account_id': 6484}, {'_account_id': 6549}, {'_account_id': 7823}, {'_account_id': 8158}, {'_account_id': 8871}]","[{'number': 1, 'created': '2014-12-06 07:15:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/bd648b4f1877c106c499f1947775b550cbbd3a88', 'message': 'Correct test case for IPv6.\n\nCorrect test case comment by Hemanth in patch\nhttps://review.openstack.org/#/c/101079/.\n\nChange-Id: I55c40ed735781862f8352afb917058bf475d16fe\n'}, {'number': 2, 'created': '2014-12-06 12:22:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/2df90bf267bcc8577107c76de8e26b5180fed1cc', 'message': 'Correct test case for IPv6\n\nCorrect test case commented by Hemanth in commit\nd2a8422d3e8c975d023982c038a59ca0257c6dd.\n\nChange-Id: I55c40ed735781862f8352afb917058bf475d16fe\n'}, {'number': 3, 'created': '2014-12-08 01:17:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/4a1dda3f678aa446aa69abf8f8dcdf922e54f1e5', 'message': 'Fix and add a test case for IPv6\n\nFixing and adding a test case in relation to the eventlet ipv6 fix\nin commit I795e004eac3f032217ff9cb0047f1976306fbb71.\n\nChange-Id: I55c40ed735781862f8352afb917058bf475d16fe\n'}, {'number': 4, 'created': '2014-12-08 02:02:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/ed4d0f8f06d0694c534544b0dd875c90b2b7135f', 'message': 'Fix and add a test case for IPv6\n\nFixing and adding a test case in relation to the eventlet ipv6 fix\nin patch set 23 (https://review.openstack.org/#/c/101079/).\n\n  * pre-import eventlet for test_evnetlet_no_dnspython()\n  * add case test_no_evnetlet_no_dnspython()\n\nChange-Id: I55c40ed735781862f8352afb917058bf475d16fe\n'}, {'number': 5, 'created': '2014-12-08 02:16:14.000000000', 'files': ['glance/tests/unit/common/test_wsgi_ipv6.py'], 'web_link': 'https://opendev.org/openstack/glance/commit/9447d37d653d87c89a1daf8f5affe41269beffbd', 'message': ""Fix and add a test case for IPv6\n\nFixing and adding a test case in relation to the eventlet ipv6 fix\nin patch set 23 (https://review.openstack.org/#/c/101079/).\n\n  * pre-import eventlet for test case test_evnetlet_no_dnspython,\n    which wasn't actually importing eventlet like it described.\n  * add case test_no_evnetlet_no_dnspython\n\nChange-Id: I55c40ed735781862f8352afb917058bf475d16fe\n""}]",2,139788,9447d37d653d87c89a1daf8f5affe41269beffbd,26,6,5,7823,,,0,"Fix and add a test case for IPv6

Fixing and adding a test case in relation to the eventlet ipv6 fix
in patch set 23 (https://review.openstack.org/#/c/101079/).

  * pre-import eventlet for test case test_evnetlet_no_dnspython,
    which wasn't actually importing eventlet like it described.
  * add case test_no_evnetlet_no_dnspython

Change-Id: I55c40ed735781862f8352afb917058bf475d16fe
",git fetch https://review.opendev.org/openstack/glance refs/changes/88/139788/4 && git format-patch -1 --stdout FETCH_HEAD,['glance/tests/unit/common/test_wsgi_ipv6.py'],1,bd648b4f1877c106c499f1947775b550cbbd3a88,fix-minor-tests,"eventlet_no_dns = script % ('fake', 'import eventlet', 'foo', 'not', 1, 0) no_eventlet_no_dns = script % ('fake', '', 'foo', '', 1, 0) def test_no_evnetlet_no_dnspython(self): """"""Test eventlet not imported and dnspython not installed"""""" rc = subprocess.call(['python', '-c', no_eventlet_no_dns]) self.assertEqual(0, rc) def test_evnetlet_no_dnspython(self): """"""Test eventlet pre-imported but dnspython not installed""""""","eventlet_no_dns = script % ('fake', '', 'yes', '', 1, 0) def test_evnetlet_no_dnspython(self): """"""Test eventlet imported but dnspython not installed""""""",9,2
openstack%2Fnova~master~I7612f931e95c5f2a5a0cf93ed87ee2d983bfae0a,openstack/nova,master,I7612f931e95c5f2a5a0cf93ed87ee2d983bfae0a,Libvirt: make use of flavor passed by spawn method,MERGED,2014-11-18 13:51:18.000000000,2014-12-08 15:41:05.000000000,2014-12-08 15:41:02.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 5170}, {'_account_id': 5441}, {'_account_id': 6873}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9569}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-18 13:51:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/5d7c38517f922afafecb61e80437c881cbd30bfe', 'message': 'Libvirt: make use of instance_type passed by spawn method\n\nMake use of the instance_type passed by the spawn method instead\nof reading from the DB.\n\nChange-Id: I7612f931e95c5f2a5a0cf93ed87ee2d983bfae0a\n'}, {'number': 2, 'created': '2014-11-18 15:13:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8f292c85fc059bc221e48a8a3cace87e0281aae9', 'message': 'Libvirt: make use of flavor passed by spawn method\n\nMake use of the flavor passed by the spawn method instead\nof reading from the DB.\n\nChange-Id: I7612f931e95c5f2a5a0cf93ed87ee2d983bfae0a\n'}, {'number': 3, 'created': '2014-11-20 10:47:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f031754da1a50cee53a2705a32883e7b1e3f605f', 'message': 'Libvirt: make use of flavor passed by spawn method\n\nMake use of the flavor passed by the spawn method instead\nof reading from the DB.\n\nChange-Id: I7612f931e95c5f2a5a0cf93ed87ee2d983bfae0a\n'}, {'number': 4, 'created': '2014-11-23 09:17:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3578f3c1874eeec9144dee55e18f2ef11081da85', 'message': 'Libvirt: make use of flavor passed by spawn method\n\nMake use of the flavor passed by the spawn method instead\nof reading from the DB.\n\nChange-Id: I7612f931e95c5f2a5a0cf93ed87ee2d983bfae0a\n'}, {'number': 5, 'created': '2014-11-25 12:00:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e74866c8e52296bd1bf6f17b9e9b368e3db3d1ed', 'message': 'Libvirt: make use of flavor passed by spawn method\n\nMake use of the flavor passed by the spawn method instead\nof reading from the DB.\n\nChange-Id: I7612f931e95c5f2a5a0cf93ed87ee2d983bfae0a\n'}, {'number': 6, 'created': '2014-11-30 15:54:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e59fcd958a4cde357df96043a284bc70333c06ac', 'message': 'Libvirt: make use of flavor passed by spawn method\n\nMake use of the flavor passed by the spawn method instead\nof reading from the DB.\n\nChange-Id: I7612f931e95c5f2a5a0cf93ed87ee2d983bfae0a\n'}, {'number': 7, 'created': '2014-12-06 17:13:20.000000000', 'files': ['nova/virt/libvirt/driver.py', 'nova/tests/unit/virt/libvirt/test_driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/2af0183a32061e66ba9eaf04ad6fad238c772549', 'message': 'Libvirt: make use of flavor passed by spawn method\n\nMake use of the flavor passed by the spawn method instead\nof reading from the DB.\n\nChange-Id: I7612f931e95c5f2a5a0cf93ed87ee2d983bfae0a\n'}]",0,135285,2af0183a32061e66ba9eaf04ad6fad238c772549,73,12,7,1653,,,0,"Libvirt: make use of flavor passed by spawn method

Make use of the flavor passed by the spawn method instead
of reading from the DB.

Change-Id: I7612f931e95c5f2a5a0cf93ed87ee2d983bfae0a
",git fetch https://review.opendev.org/openstack/nova refs/changes/85/135285/4 && git format-patch -1 --stdout FETCH_HEAD,['nova/virt/libvirt/driver.py'],1,5d7c38517f922afafecb61e80437c881cbd30bfe,libvirt-use-instance-type," write_to_disk=True, instance_type=instance_type) context=None, instance_type=None): if instance_type is None: with utils.temporary_mutation(context, read_deleted=""yes""): flavor = objects.Flavor.get_by_id(context, instance['instance_type_id']) else: flavor = instance_type block_device_info=None, write_to_disk=False, instance_type=None): context, instance_type=instance_type)"," write_to_disk=True) context=None): with utils.temporary_mutation(context, read_deleted=""yes""): flavor = objects.Flavor.get_by_id(context, instance['instance_type_id']) block_device_info=None, write_to_disk=False): context)",13,7
openstack%2Fironic-inspector~master~I49191290b7941e88f7fcb25dba6a324e21acb63c,openstack/ironic-inspector,master,I49191290b7941e88f7fcb25dba6a324e21acb63c,Store good MAC's and interfaces in node_info after processing,MERGED,2014-12-08 13:59:54.000000000,2014-12-08 15:39:26.000000000,2014-12-08 15:39:25.000000000,"[{'_account_id': 3}, {'_account_id': 7419}, {'_account_id': 10239}]","[{'number': 1, 'created': '2014-12-08 13:59:54.000000000', 'files': ['ironic_discoverd/test.py', 'ironic_discoverd/discoverd.py'], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/0047d87f2dcbe464caec8fd1e8fe86d28c032bc8', 'message': ""Store good MAC's and interfaces in node_info after processing\n\nThis makes life easier for post-processing hooks and removes need\nto pass valid_macs to _process_node.\n\nChange-Id: I49191290b7941e88f7fcb25dba6a324e21acb63c\n""}]",0,140005,0047d87f2dcbe464caec8fd1e8fe86d28c032bc8,7,3,1,10239,,,0,"Store good MAC's and interfaces in node_info after processing

This makes life easier for post-processing hooks and removes need
to pass valid_macs to _process_node.

Change-Id: I49191290b7941e88f7fcb25dba6a324e21acb63c
",git fetch https://review.opendev.org/openstack/ironic-inspector refs/changes/05/140005/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic_discoverd/test.py', 'ironic_discoverd/discoverd.py']",2,0047d87f2dcbe464caec8fd1e8fe86d28c032bc8,bp/returning-to-ramdisk," node_info['interfaces'] = valid_interfaces node_info['macs'] = valid_macs updated = _process_node(ironic, node, node_info)def _process_node(ironic, node, node_info): for mac in node_info['macs']:"," updated = _process_node(ironic, node, node_info, valid_macs)def _process_node(ironic, node, node_info, valid_macs): for mac in valid_macs:",18,9
openstack%2Fpython-barbicanclient~master~I4b6e6ba962bdbd4e15c73a1cee9c24e12c3b946d,openstack/python-barbicanclient,master,I4b6e6ba962bdbd4e15c73a1cee9c24e12c3b946d,Trivial change to docs,MERGED,2014-12-04 23:31:17.000000000,2014-12-08 15:29:22.000000000,2014-12-08 15:29:22.000000000,"[{'_account_id': 3}, {'_account_id': 7136}, {'_account_id': 8004}, {'_account_id': 10873}]","[{'number': 1, 'created': '2014-12-04 23:31:17.000000000', 'files': ['doc/source/usage.rst'], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/2c55869545abdae2da43c2719120cebd63ef11ee', 'message': 'Trivial change to docs\n\nChange-Id: I4b6e6ba962bdbd4e15c73a1cee9c24e12c3b946d\n'}]",0,139265,2c55869545abdae2da43c2719120cebd63ef11ee,8,4,1,7973,,,0,"Trivial change to docs

Change-Id: I4b6e6ba962bdbd4e15c73a1cee9c24e12c3b946d
",git fetch https://review.opendev.org/openstack/python-barbicanclient refs/changes/65/139265/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/usage.rst'],1,2c55869545abdae2da43c2719120cebd63ef11ee,,============ Client Usage ============,======== Usage ========,3,3
openstack%2Fpuppet-monasca~master~I313328e6c595309dd0aef25b374b5e848d59b522,openstack/puppet-monasca,master,I313328e6c595309dd0aef25b374b5e848d59b522,Pull in http_check.py fix in monasca-agent 0.0.14,MERGED,2014-12-08 15:06:03.000000000,2014-12-08 15:15:00.000000000,2014-12-08 15:15:00.000000000,"[{'_account_id': 3}, {'_account_id': 9500}, {'_account_id': 11155}]","[{'number': 1, 'created': '2014-12-08 15:06:03.000000000', 'files': ['manifests/agent.pp'], 'web_link': 'https://opendev.org/openstack/puppet-monasca/commit/a6329b0db39117266b5f95967cdda931ddb6b396', 'message': 'Pull in http_check.py fix in monasca-agent 0.0.14\n\nChange-Id: I313328e6c595309dd0aef25b374b5e848d59b522\n'}]",0,140023,a6329b0db39117266b5f95967cdda931ddb6b396,6,3,1,8126,,,0,"Pull in http_check.py fix in monasca-agent 0.0.14

Change-Id: I313328e6c595309dd0aef25b374b5e848d59b522
",git fetch https://review.opendev.org/openstack/puppet-monasca refs/changes/23/140023/1 && git format-patch -1 --stdout FETCH_HEAD,['manifests/agent.pp'],1,a6329b0db39117266b5f95967cdda931ddb6b396,feature/get_http_fix," ensure => '1.0.14',"," ensure => '1.0.13',",1,1
openstack%2Fcloudkitty~master~Id78f3a786bc91a34aa9751c7bb3a4fcc276d1778,openstack/cloudkitty,master,Id78f3a786bc91a34aa9751c7bb3a4fcc276d1778,Adding image collection to the collector,MERGED,2014-11-27 04:47:52.000000000,2014-12-08 15:09:30.000000000,2014-12-08 15:09:30.000000000,"[{'_account_id': 3}, {'_account_id': 7042}, {'_account_id': 7923}]","[{'number': 1, 'created': '2014-11-27 04:47:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cloudkitty/commit/fe1484e0223ecc412263680fa5b1a8ef590168cb', 'message': ""adding image collection to the collector\n\nEasy addition to see how extra collections can be added.\n\nDid rewrite get_active_instances to active_resources.\nMakes it more useful, plus gets rid of the unsafe prefix\nif someone puts 'active_instances' in the conf.\n\nRelies on the current empty_frame patch, but will rebase\nonce that is fixed properly.\n\nChange-Id: Id78f3a786bc91a34aa9751c7bb3a4fcc276d1778\n""}, {'number': 2, 'created': '2014-12-08 14:28:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cloudkitty/commit/999f6b2c6ec9c52ccc2dc0aa0a5fc9d356ef867d', 'message': ""Adding image collection to the collector\n\nEasy addition to see how extra collections can be added.\n\nDid rewrite get_active_instances to active_resources.\nMakes it more useful, plus gets rid of the unsafe prefix\nif someone puts 'active_instances' in the conf.\n\nRelies on the current empty_frame patch, but will rebase\nonce that is fixed properly.\n\nChange-Id: Id78f3a786bc91a34aa9751c7bb3a4fcc276d1778\n""}, {'number': 3, 'created': '2014-12-08 14:30:44.000000000', 'files': ['etc/cloudkitty/cloudkitty.conf.sample', 'cloudkitty/transformer/ceilometer.py', 'cloudkitty/collector/ceilometer.py', 'cloudkitty/config.py'], 'web_link': 'https://opendev.org/openstack/cloudkitty/commit/595d0eb54396840c8297b26a239877747c74083f', 'message': ""Adding image collection to the collector\n\nEasy addition to see how extra collections can be added.\n\nDid rewrite get_active_instances to active_resources.\nMakes it more useful, plus gets rid of the unsafe prefix\nif someone puts 'active_instances' in the conf.\n\nChange-Id: Id78f3a786bc91a34aa9751c7bb3a4fcc276d1778\n""}]",2,137510,595d0eb54396840c8297b26a239877747c74083f,13,3,3,10420,,,0,"Adding image collection to the collector

Easy addition to see how extra collections can be added.

Did rewrite get_active_instances to active_resources.
Makes it more useful, plus gets rid of the unsafe prefix
if someone puts 'active_instances' in the conf.

Change-Id: Id78f3a786bc91a34aa9751c7bb3a4fcc276d1778
",git fetch https://review.opendev.org/openstack/cloudkitty refs/changes/10/137510/1 && git format-patch -1 --stdout FETCH_HEAD,"['etc/cloudkitty/cloudkitty.conf.sample', 'cloudkitty/transformer/ceilometer.py', 'cloudkitty/collector/ceilometer.py']",3,fe1484e0223ecc412263680fa5b1a8ef590168cb,get_storage," def active_resources(self, meter, start, end=None, project_id=None, q_filter=None): """"""Resources that were active during the timespan."""""" resource_stats = self._conn.statistics.list(meter_name=meter, return [resource.groupby['resource_id'] for resource in resource_stats] active_instance_ids = self.active_resources('instance', start, end, project_id, q_filter) def get_image(self, start, end=None, project_id=None, q_filter=None): active_image_ids = self.active_resources('image.size', start, end, project_id, q_filter) image_data = [] for image_id in active_image_ids: if not self._cacher.has_resource_detail('storage', image_id): raw_resource = self._conn.resources.get(image_id) image = self.t_ceilometer.strip_resource_data('storage', raw_resource) self._cacher.add_resource_detail('storage', image_id, image) image = self._cacher.get_resource_detail('storage', image_id) image_data.append(self.t_cloudkitty.format_item(image, 'image', 1)) return self.t_cloudkitty.format_service('storage', image_data)"," def get_active_instances(self, start, end=None, project_id=None, q_filter=None): """"""Instance that were active during the timespan."""""" instance_stats = self._conn.statistics.list(meter_name='instance', return [instance.groupby['resource_id'] for instance in instance_stats] active_instance_ids = self.get_active_instances(start, end, project_id, q_filter)",29,8
openstack%2Fneutron-specs~master~I67b0474b40dd7bd6615cbfd9974d9edce4376582,openstack/neutron-specs,master,I67b0474b40dd7bd6615cbfd9974d9edce4376582,DHCP agent customization,ABANDONED,2014-06-11 12:49:07.000000000,2014-12-08 15:07:45.000000000,,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 261}, {'_account_id': 2035}, {'_account_id': 6854}, {'_account_id': 7141}, {'_account_id': 7448}, {'_account_id': 8124}, {'_account_id': 9200}]","[{'number': 1, 'created': '2014-06-11 12:49:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/979be8d667e315797db82507b1a26b7394d968d0', 'message': 'DHCP agent customization\n\nThis blueprint add hooks in dhcp agent deployment, update, undeployment\nworkflows in order to allow their customization.\n\nChange-Id: I67b0474b40dd7bd6615cbfd9974d9edce4376582\n'}, {'number': 2, 'created': '2014-06-12 13:11:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/4bb9a964fcf8e4f54d0286783b33f795dc561b62', 'message': 'DHCP agent customization\n\nThis blueprint add hooks in dhcp agent deployment, update, undeployment\nworkflows in order to allow their customization.\n\nChange-Id: I67b0474b40dd7bd6615cbfd9974d9edce4376582\n'}, {'number': 3, 'created': '2014-06-20 07:26:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/f86817874b8a9ad5ae18b3638e13d533ba4a754f', 'message': 'DHCP agent customization\n\nThis blueprint add mechanism system in dhcp agent deployment, update,\nundeployment workflows in order to allow their customization.\n\nChange-Id: I67b0474b40dd7bd6615cbfd9974d9edce4376582\n'}, {'number': 4, 'created': '2014-06-25 09:12:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/3feb308acc4f8c2ffe5311622ca02aed2d482e5e', 'message': 'DHCP agent customization\n\nThis blueprint add mechanism system in dhcp agent deployment, update,\nundeployment workflows in order to allow their customization.\n\nChange-Id: I67b0474b40dd7bd6615cbfd9974d9edce4376582\n'}, {'number': 5, 'created': '2014-07-07 19:31:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/b31b0ab38524ff162dcf1c4f67167307ea61cc1d', 'message': 'DHCP agent customization\n\nThis blueprint add mechanism system in dhcp agent deployment, update,\nundeployment workflows in order to allow their customization.\n\nChange-Id: I67b0474b40dd7bd6615cbfd9974d9edce4376582\n'}, {'number': 6, 'created': '2014-10-14 19:14:50.000000000', 'files': ['specs/kilo/dhcp-agent-customization.rst'], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/96a735c8a7e1f407bd6b24f7ca61f17895eebaf8', 'message': 'DHCP agent customization\n\nThis blueprint add mechanism system in dhcp agent deployment, update,\nundeployment workflows in order to allow their customization.\n\nChange-Id: I67b0474b40dd7bd6615cbfd9974d9edce4376582\n'}]",42,99356,96a735c8a7e1f407bd6b24f7ca61f17895eebaf8,46,9,6,8124,,,0,"DHCP agent customization

This blueprint add mechanism system in dhcp agent deployment, update,
undeployment workflows in order to allow their customization.

Change-Id: I67b0474b40dd7bd6615cbfd9974d9edce4376582
",git fetch https://review.opendev.org/openstack/neutron-specs refs/changes/56/99356/4 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/dhcp-agent-customization.rst'],1,979be8d667e315797db82507b1a26b7394d968d0,bp/dhcp-agent-customization,"======================== DHCP agent customization ======================== Launchpad blueprint: https://blueprints.launchpad.net/neutron/+spec/dhcp-agent-customization This blueprint add hooks in dhcp agent deployment, update, undeployment workflows in order to allow their customization. Problem description =================== Current dhcp-agent design is not intended to support customization of dhcp (un)deployment workflows. Indeed dhcp-agent manager is an internal class without stable interface. Such capability is expected in order to add extra features: * Start business specific applications or proxies on isolated network dhcps after deployment, * Reload previous applications after dhcp update, * Stop previous applications before dhcp undeployment, * Add custom iptables rules to secure previous applications. Proposed change =============== Define hook methods (see below) in current dhcp-agent manager, integrate them in deployment, update and undeployment workflows and load dhcp-agent manager implementation using the new configuration option **dhcp_manager** in section AGENT in order to allow developers to use dhcp-agent manager implementation subclass with business specific hook method implementations. Hook methods: * deploy_dhcp_precommit * deploy_dhcp_postcommit * update_dhcp_precommit * update_dhcp_postcommit * undeploy_dhcp_precommit * undeploy_dhcp_postcommit Alternatives ------------ Reuse ML2 mechanism design in dhcp-agent manager to allow customization of dhcp management workflows: * Define a dhcp service driver API with previous hook methods, * Implement a dhcp service manager, loading dhcp service drivers from configuration and calling driver hook methods, * Integrate dhcp service manager in dhcp-agent manager workflow. This alternative implies more changes and should be preferred if loading multiple dhcp service drivers is mandatory. Data model impact ----------------- None REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- None Performance Impact ------------------ None Other deployer impact --------------------- None Developer impact ---------------- It will affect developers working on the dhcp-agent. Implementation ============== Assignee(s) ----------- Primary assignee: Cedric Brandily <cbrandily> Other contributors: <launchpad-id or None> Work Items ---------- 1. Allow to define dhcp-agent manager implementation through the configuration option **dhcp_manager** in section AGENT, 2. Add hook methods in current dhcp-agent manager and integrate them in dhcp management workflows. Dependencies ============ None Testing ======= The code will be covered with unit tests. Tempest tests will ensure the refactor does not introduce any regressions. Documentation Impact ==================== The developer doc might need a section on dhcp-agent hook points. References ========== https://review.openstack.org/#/q/topic:bp/dhcp-agent-customization,n,z ",,144,0
openstack%2Fdevstack-gate~master~I29e2245e5b2be9bf57ba74a0fe00f335d1b170ec,openstack/devstack-gate,master,I29e2245e5b2be9bf57ba74a0fe00f335d1b170ec,Add conf to replace nova V2 endpoint with V2.1 API,MERGED,2014-12-04 04:14:45.000000000,2014-12-08 15:05:39.000000000,2014-12-08 15:05:38.000000000,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 6133}, {'_account_id': 6167}, {'_account_id': 6786}, {'_account_id': 8556}, {'_account_id': 8871}]","[{'number': 1, 'created': '2014-12-04 04:14:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/619d7d5c7c6629759413bc2ad72e88d958311ba0', 'message': 'Add conf to replace nova V2 endpoint with V2.1 API\n\nNow Nova contains both v2 and v2.1 APIs as its REST API, and we are\ntesting v2 API only on the gate. To test v2.1 API also, this patch\nadd a new item DEVSTACK_GATE_NOVA_REPLACE_V2_ENDPOINT_WITH_V21_API\nwhich allows us to switch a compute endpoint. The default value is\nfor testing v2 API and we will be able to test v2.1 API by setting\n1 as the value.\n\nDevstack patch for adding same configuration -\nIaaa530bda6bcdae75e86be8dbe572abe4396e8e9\n\nChange-Id: I29e2245e5b2be9bf57ba74a0fe00f335d1b170ec\n'}, {'number': 2, 'created': '2014-12-04 04:18:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/9cf11ee89e75b18b9980fbe3140b9d2cdeb69a0f', 'message': 'Add conf to replace nova V2 endpoint with V2.1 API\n\nNow Nova contains both v2 and v2.1 APIs as its REST API, and we are\ntesting v2 API only on the gate. To test v2.1 API also, this patch\nadd a new item DEVSTACK_GATE_NOVA_REPLACE_V2_ENDPOINT_WITH_V21_API\nwhich allows us to switch a compute endpoint. The default value is\nfor testing v2 API and we will be able to test v2.1 API by setting\n1 as the value.\n\nDevstack patch for adding same configuration in devstack-\nIaaa530bda6bcdae75e86be8dbe572abe4396e8e9\n\nChange-Id: I29e2245e5b2be9bf57ba74a0fe00f335d1b170ec\n'}, {'number': 3, 'created': '2014-12-05 00:24:36.000000000', 'files': ['devstack-vm-gate.sh', 'devstack-vm-gate-wrap.sh'], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/3b1ac65d5de04924e50f2fee058e279ab76fed46', 'message': 'Add conf to replace nova V2 endpoint with V2.1 API\n\nNow Nova contains both v2 and v2.1 APIs as its REST API, and we are\ntesting v2 API only on the gate. To test v2.1 API also, this patch\nadd a new item DEVSTACK_GATE_NOVA_REPLACE_V2_ENDPOINT_WITH_V21_API\nwhich allows us to switch a compute endpoint. The default value is\nfor testing v2 API and we will be able to test v2.1 API by setting\n1 as the value.\n\nDevstack patch for adding same configuration in devstack-\nIaaa530bda6bcdae75e86be8dbe572abe4396e8e9\n\nChange-Id: I29e2245e5b2be9bf57ba74a0fe00f335d1b170ec\n'}]",2,138949,3b1ac65d5de04924e50f2fee058e279ab76fed46,21,7,3,8556,,,0,"Add conf to replace nova V2 endpoint with V2.1 API

Now Nova contains both v2 and v2.1 APIs as its REST API, and we are
testing v2 API only on the gate. To test v2.1 API also, this patch
add a new item DEVSTACK_GATE_NOVA_REPLACE_V2_ENDPOINT_WITH_V21_API
which allows us to switch a compute endpoint. The default value is
for testing v2 API and we will be able to test v2.1 API by setting
1 as the value.

Devstack patch for adding same configuration in devstack-
Iaaa530bda6bcdae75e86be8dbe572abe4396e8e9

Change-Id: I29e2245e5b2be9bf57ba74a0fe00f335d1b170ec
",git fetch https://review.opendev.org/openstack/devstack-gate refs/changes/49/138949/1 && git format-patch -1 --stdout FETCH_HEAD,"['devstack-vm-gate.sh', 'devstack-vm-gate-wrap.sh']",2,619d7d5c7c6629759413bc2ad72e88d958311ba0,(detached,# Set to 1 to replace Nova V2 endpoint with V2.1 API export DEVSTACK_GATE_NOVA_REPLACE_V2_ENDPOINT_WITH_V21_API=${DEVSTACK_GATE_NOVA_REPLACE_V2_ENDPOINT_WITH_V21_API:-0} ,,7,0
openstack%2Frequirements~master~I6a97e3b462f8d0985532384ea169ebee4c9b3c2d,openstack/requirements,master,I6a97e3b462f8d0985532384ea169ebee4c9b3c2d,Drop greenio dependency,MERGED,2014-12-03 14:39:30.000000000,2014-12-08 15:05:25.000000000,2014-12-08 15:05:24.000000000,"[{'_account_id': 3}, {'_account_id': 24}, {'_account_id': 6786}, {'_account_id': 8623}, {'_account_id': 9107}]","[{'number': 1, 'created': '2014-12-03 14:39:30.000000000', 'files': ['global-requirements.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/3704f5cb0bae0345ae4578e92d71ce549d3243b8', 'message': 'Drop greenio dependency\n\ngreenio dependency was added to add a new ""greenio executor"" to Oslo\nMessaging, but I abandonned the changeset:\nhttps://review.openstack.org/#/c/108652/\n\nNo OpenStack component use greenio.\n\nChange-Id: I6a97e3b462f8d0985532384ea169ebee4c9b3c2d\n'}]",0,138748,3704f5cb0bae0345ae4578e92d71ce549d3243b8,12,5,1,9107,,,0,"Drop greenio dependency

greenio dependency was added to add a new ""greenio executor"" to Oslo
Messaging, but I abandonned the changeset:
https://review.openstack.org/#/c/108652/

No OpenStack component use greenio.

Change-Id: I6a97e3b462f8d0985532384ea169ebee4c9b3c2d
",git fetch https://review.opendev.org/openstack/requirements refs/changes/48/138748/1 && git format-patch -1 --stdout FETCH_HEAD,['global-requirements.txt'],1,3704f5cb0bae0345ae4578e92d71ce549d3243b8,drop_greenio,,greenio>=0.6.0,0,1
openstack%2Fcinder~master~Ia25288b9ac149408235085b1d9d95dcb33d1475f,openstack/cinder,master,Ia25288b9ac149408235085b1d9d95dcb33d1475f,Fixes intermittent NFS driver mount failure,ABANDONED,2014-12-08 14:31:36.000000000,2014-12-08 15:00:59.000000000,,"[{'_account_id': 8247}, {'_account_id': 9003}]","[{'number': 1, 'created': '2014-12-08 14:31:36.000000000', 'files': ['cinder/tests/test_nfs.py', 'cinder/volume/drivers/nfs.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/43f6e3790c87a9a9e17e6441c0770b949417e25b', 'message': ""Fixes intermittent NFS driver mount failure\n\nDuring cinder volume driver initialization, NFS drivers often\nfail to mount the NFS share backing their volumes, complaining\nthat the share in question is 'busy or already mounted'.\n\nThis commit introduces a retry loop around the ensure_mounted()\ncall inside set_nas_security_options() so that if there is contention\nbetween volume process and backup process mounting the same share\nthe driver will not be stopped from loading.\n\nChange-Id: Ia25288b9ac149408235085b1d9d95dcb33d1475f\nCloses-bug: 1395823\n""}]",0,140019,43f6e3790c87a9a9e17e6441c0770b949417e25b,4,2,1,9003,,,0,"Fixes intermittent NFS driver mount failure

During cinder volume driver initialization, NFS drivers often
fail to mount the NFS share backing their volumes, complaining
that the share in question is 'busy or already mounted'.

This commit introduces a retry loop around the ensure_mounted()
call inside set_nas_security_options() so that if there is contention
between volume process and backup process mounting the same share
the driver will not be stopped from loading.

Change-Id: Ia25288b9ac149408235085b1d9d95dcb33d1475f
Closes-bug: 1395823
",git fetch https://review.opendev.org/openstack/cinder refs/changes/19/140019/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/test_nfs.py', 'cinder/volume/drivers/nfs.py']",2,43f6e3790c87a9a9e17e6441c0770b949417e25b,bug/1395823,"import timeimport sixfrom cinder.i18n import _, _LE cfg.IntOpt('nfs_mount_attempts', default=3, help=('The number of attempts to mount nfs shares before ' 'raising an error. At least one attempt will be ' 'made to mount an nfs share, regardless of the ' 'value specified.')), num_attempts = max(1, self.configuration.nfs_mount_attempts) for attempt in range(num_attempts): try: self._remotefsclient.mount(nfs_share, mnt_flags) return except Exception as e: if attempt == (num_attempts - 1): LOG.error(_LE('Mount failure for %(share)s after ' '%(count)d attempts.') % { 'share': nfs_share, 'count': num_attempts}) raise exception.NfsException(e) LOG.debug('Mount attempt %d failed: %s.\nRetrying mount ...' % (attempt, six.text_type(e))) time.sleep(1)","from cinder.i18n import _ self._remotefsclient.mount(nfs_share, mnt_flags)",83,2
openstack%2Fnova~master~Ia2c21fde43ed7708d070e6db67567e367c7012dc,openstack/nova,master,Ia2c21fde43ed7708d070e6db67567e367c7012dc,Remove AbstractFieldType,ABANDONED,2014-11-11 14:00:16.000000000,2014-12-08 14:59:34.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 4393}, {'_account_id': 5170}, {'_account_id': 5511}, {'_account_id': 9008}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-11 14:00:16.000000000', 'files': ['nova/objects/fields.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/8c6d0e10052a0dad6d9cafd3f4bae720a4aeaa48', 'message': ""Remove AbstractFieldType\n\nAbstractFieldType had a single user: FieldType. FieldType implements\nall of AbstractFieldType's methods, and all specific field types\ninherit from FieldType. In other words, AbstractFieldType was dead\ncode. I'm assuming this wasn't originally the case and has happened\nover time, but we might as well remove it now because it's confusing.\n\nAlso, the signatures of AbstractFieldType's coerce, from_primitive,\nand to_primitive abstract methods were incorrect, as these are static\nmethods.\n\nChange-Id: Ia2c21fde43ed7708d070e6db67567e367c7012dc\n""}]",0,133694,8c6d0e10052a0dad6d9cafd3f4bae720a4aeaa48,13,10,1,9555,,,0,"Remove AbstractFieldType

AbstractFieldType had a single user: FieldType. FieldType implements
all of AbstractFieldType's methods, and all specific field types
inherit from FieldType. In other words, AbstractFieldType was dead
code. I'm assuming this wasn't originally the case and has happened
over time, but we might as well remove it now because it's confusing.

Also, the signatures of AbstractFieldType's coerce, from_primitive,
and to_primitive abstract methods were incorrect, as these are static
methods.

Change-Id: Ia2c21fde43ed7708d070e6db67567e367c7012dc
",git fetch https://review.opendev.org/openstack/nova refs/changes/94/133694/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/objects/fields.py'],1,8c6d0e10052a0dad6d9cafd3f4bae720a4aeaa48,object_compare_and_swap,"class FieldType(object): @staticmethod def coerce(obj, attr, value): return value @staticmethod def from_primitive(obj, attr, value): return value @staticmethod def to_primitive(obj, attr, value): return value """"""Returns a short stringified version of a value.""""""","import abcclass AbstractFieldType(six.with_metaclass(abc.ABCMeta, object)): @abc.abstractmethod def coerce(self, obj, attr, value): pass @abc.abstractmethod def from_primitive(self, obj, attr, value): pass @abc.abstractmethod def to_primitive(self, obj, attr, value): pass @abc.abstractmethod pass @abc.abstractmethod def stringify(self, value): """"""Returns a short stringified version of a value."""""" pass class FieldType(AbstractFieldType): @staticmethod def coerce(obj, attr, value): return value @staticmethod def from_primitive(obj, attr, value): return value @staticmethod def to_primitive(obj, attr, value): return value def describe(self):",11,34
openstack%2Fdevstack~master~Ibf62bf6f6300eeb0ab2f22086b0ff1c05c69d86b,openstack/devstack,master,Ibf62bf6f6300eeb0ab2f22086b0ff1c05c69d86b,Horizon front page test fix,MERGED,2014-12-02 12:25:51.000000000,2014-12-08 14:58:15.000000000,2014-12-08 14:58:14.000000000,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 4978}, {'_account_id': 7509}, {'_account_id': 8648}, {'_account_id': 9317}, {'_account_id': 9981}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-12-02 12:25:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/9c522adc22a3613f67fd4fb94ad190b50ebf505c', 'message': 'Horizon front page test fix\n\nFixes the test string in horizon front page test so it is not failing\nwhen header tag gets new attributes\n\nChange-Id: Ibf62bf6f6300eeb0ab2f22086b0ff1c05c69d86b\n'}, {'number': 2, 'created': '2014-12-03 10:33:52.000000000', 'files': ['exercises/horizon.sh'], 'web_link': 'https://opendev.org/openstack/devstack/commit/6623049ec48058afcb207855a9754df197a8cbb6', 'message': 'Horizon front page test fix\n\nFixes the test string in horizon front page test so it is not failing\nwhen header tag gets new attributes\n\nChange-Id: Ibf62bf6f6300eeb0ab2f22086b0ff1c05c69d86b\n'}]",0,138337,6623049ec48058afcb207855a9754df197a8cbb6,20,8,2,7509,,,0,"Horizon front page test fix

Fixes the test string in horizon front page test so it is not failing
when header tag gets new attributes

Change-Id: Ibf62bf6f6300eeb0ab2f22086b0ff1c05c69d86b
",git fetch https://review.opendev.org/openstack/devstack refs/changes/37/138337/2 && git format-patch -1 --stdout FETCH_HEAD,['exercises/horizon.sh'],1,9c522adc22a3613f67fd4fb94ad190b50ebf505c,horizon_front_page_test_fix,"curl http://$SERVICE_HOST 2>/dev/null | grep -q 'Log In' || die $LINENO ""Horizon front page not functioning!""","curl http://$SERVICE_HOST 2>/dev/null | grep -q '<h3>Log In</h3>' || die $LINENO ""Horizon front page not functioning!""",1,1
openstack%2Fceilometer~master~Ia5d22c51f2f84b5c70af855f6f5fe7b8d7d89586,openstack/ceilometer,master,Ia5d22c51f2f84b5c70af855f6f5fe7b8d7d89586,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:40:40.000000000,2014-12-08 14:46:48.000000000,2014-12-08 14:46:47.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 4491}, {'_account_id': 6676}, {'_account_id': 7052}, {'_account_id': 7478}]","[{'number': 1, 'created': '2014-12-05 03:40:40.000000000', 'files': ['doc/source/contributing/source.rst', 'CONTRIBUTING.rst', 'doc/source/contributing/user.rst'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/a409d0cd370303adfe7e1c291212052d21c78141', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Ia5d22c51f2f84b5c70af855f6f5fe7b8d7d89586\n'}]",2,139312,a409d0cd370303adfe7e1c291212052d21c78141,10,6,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Ia5d22c51f2f84b5c70af855f6f5fe7b8d7d89586
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/12/139312/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/contributing/source.rst', 'CONTRIBUTING.rst', 'doc/source/contributing/user.rst']",3,a409d0cd370303adfe7e1c291212052d21c78141,infra-manual, * http://docs.openstack.org/infra/manual/developers.html, * http://wiki.openstack.org/HowToContribute,4,4
openstack%2Fnova~master~Ie7fdb2928d957c03ed788c2ddd29fe798c645fce,openstack/nova,master,Ie7fdb2928d957c03ed788c2ddd29fe798c645fce,Adds global API version check for microversions,MERGED,2014-11-25 03:06:05.000000000,2014-12-08 14:29:15.000000000,2014-12-08 14:29:12.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 2271}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 5292}, {'_account_id': 5754}, {'_account_id': 6062}, {'_account_id': 6167}, {'_account_id': 8556}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9420}, {'_account_id': 9569}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 10559}, {'_account_id': 12175}]","[{'number': 1, 'created': '2014-11-25 03:06:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/82ed5d7dcc4b71593922b7d99d5172785776b061', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 2, 'created': '2014-11-26 04:07:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f10c9bac0657bb8a3bffbfce98951d719de8b495', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 3, 'created': '2014-11-26 04:14:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e648faf2967bbe7698798b83a7bdc92ef06e056f', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 4, 'created': '2014-11-26 14:13:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/32142a22916aecdcbca3f56ecc20ccd037628f14', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 5, 'created': '2014-11-26 23:45:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/234e0a9285c6e78242ce90cc9ba4211268a0a321', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 6, 'created': '2014-11-28 00:10:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/4f9de1f842979917e834ea2ba7fe4302faf0ef25', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 7, 'created': '2014-11-28 04:32:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/67f6a34ef6b0fc88407bea3e91a11c08f4e380d9', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 8, 'created': '2014-11-29 06:16:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8cb4aa3c79c403ecca4031f5989a6711db338775', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 9, 'created': '2014-12-01 03:49:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/fa58a47a32ce8a9b0429b2b75c8f0955437a2bf1', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 10, 'created': '2014-12-01 23:48:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/5c17962b953d7bea7d525adeef6b100f7e2d66a0', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 11, 'created': '2014-12-02 04:52:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e17750a0e3da1d1b06099cbad7eb9d246ad3d682', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 12, 'created': '2014-12-02 06:05:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/05217ea23969665a60fdc871c6e713ca97b93390', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 13, 'created': '2014-12-02 13:18:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/36ee2d30562606adcc9c5a7c13eade85d8a49ed1', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}, {'number': 14, 'created': '2014-12-05 01:56:44.000000000', 'files': ['nova/api/openstack/wsgi.py', 'nova/exception.py', 'nova/tests/unit/api/openstack/compute/test_microversions.py', 'nova/api/openstack/api_version_request.py', 'nova/api/openstack/rest_api_version_history.rst', 'nova/tests/unit/api/openstack/test_api_version_request.py', 'nova/tests/unit/api/openstack/test_wsgi.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/d8a17851b4a4d8d20df9ac9ae270f379c373a37e', 'message': 'Adds global API version check for microversions\n\nAdds a check for a request that the version requested is within the\nglobal API version range supported by the REST API. Both the minimum\nand maximum are currently set to ""2.1"". The maximum will be increased\neverytime an API change is made.\n\nAlso sets up some template/doc files for clearly documenting\nthe REST API changes with each microversion increment.\n\nPartially implements blueprint api-microversions\n\nChange-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce\n'}]",40,136951,d8a17851b4a4d8d20df9ac9ae270f379c373a37e,144,19,14,5292,,,0,"Adds global API version check for microversions

Adds a check for a request that the version requested is within the
global API version range supported by the REST API. Both the minimum
and maximum are currently set to ""2.1"". The maximum will be increased
everytime an API change is made.

Also sets up some template/doc files for clearly documenting
the REST API changes with each microversion increment.

Partially implements blueprint api-microversions

Change-Id: Ie7fdb2928d957c03ed788c2ddd29fe798c645fce
",git fetch https://review.opendev.org/openstack/nova refs/changes/51/136951/5 && git format-patch -1 --stdout FETCH_HEAD,"['nova/api/openstack/wsgi.py', 'nova/tests/unit/api/openstack/compute/test_microversions.py', 'nova/api/openstack/api_version_request.py', 'nova/api/openstack/rest_api_version_history.rst', 'nova/tests/unit/api/openstack/test_wsgi.py']",5,82ed5d7dcc4b71593922b7d99d5172785776b061,bp/api-microversions,"import mock @mock.patch(""nova.api.openstack.api_version_request.max_api_version"") def test_resource_receives_api_version_request(self, mock_maxver): mock_maxver.return_value = api_version.APIVersionRequest(version)", def test_resource_receives_api_version_request(self):,98,5
openstack%2Fproject-config~master~Ideb99f804ef8708eb6df6e62cc23e97972839c85,openstack/project-config,master,Ideb99f804ef8708eb6df6e62cc23e97972839c85,Add job template for pushing images to dockerhub,ABANDONED,2014-10-31 12:01:11.000000000,2014-12-08 14:26:55.000000000,,"[{'_account_id': 2}, {'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 6316}, {'_account_id': 6547}, {'_account_id': 6554}, {'_account_id': 7369}]","[{'number': 1, 'created': '2014-10-31 12:01:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/85ef883f3fc52b05d302c44ddeeae09d7f699e50', 'message': 'Add job template for pushing images to dockerhub\n\nUse this template for pushing rally releases to dockerhub\n\nTo make this work we need \'docker\' node with docker installed\nOn this node should be create file ~/.dockercfg with contents:\n\n {""https://index.docker.io/v1/"":\n  {""auth"":""%AUTH-INFO%"",""email"":""%EMAIL%""}}\n\nWhere %AUTH-INFO% is base64 encoded ""login:password"" string.\nLogin name is represent repository name, like ""openstack"" or\n""stackforge"" on github.\n\nMore about docker repos: https://docs.docker.com/userguide/dockerrepos/\n\nChange-Id: Ideb99f804ef8708eb6df6e62cc23e97972839c85\n'}, {'number': 2, 'created': '2014-10-31 12:34:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/cfe3a7e6b51c55028b855b736d54a010f4ce1162', 'message': 'Add job template for pushing images to dockerhub\n\nUse this template for pushing rally releases to dockerhub\n\nTo make this work we need \'docker\' node with docker installed\nOn this node should be create file ~/.dockercfg with contents:\n\n {""https://index.docker.io/v1/"":\n  {""auth"":""%AUTH-INFO%"",""email"":""%EMAIL%""}}\n\nWhere %AUTH-INFO% is base64 encoded ""login:password"" string.\nLogin name is represent repository name, like ""openstack"" or\n""stackforge"" on github.\n\nMore about docker repos: https://docs.docker.com/userguide/dockerrepos/\n\nChange-Id: Ideb99f804ef8708eb6df6e62cc23e97972839c85\n'}, {'number': 3, 'created': '2014-10-31 12:52:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/12ac0df893da46ada349d09e0800b6e04d302c2d', 'message': 'Add job template for pushing images to dockerhub\n\nUse this template for pushing rally releases to dockerhub\n\nTo make this work we need \'docker\' node with docker installed\nOn this node should be create file ~/.dockercfg with contents:\n\n {""https://index.docker.io/v1/"":\n  {""auth"":""%AUTH-INFO%"",""email"":""%EMAIL%""}}\n\nWhere %AUTH-INFO% is base64 encoded ""login:password"" string.\nLogin name is represent repository name, like ""openstack"" or\n""stackforge"" on github.\n\nMore about docker repos: https://docs.docker.com/userguide/dockerrepos/\n\nChange-Id: Ideb99f804ef8708eb6df6e62cc23e97972839c85\n'}, {'number': 4, 'created': '2014-10-31 13:07:33.000000000', 'files': ['jenkins/jobs/docker.yaml', 'jenkins/jobs/projects.yaml', 'zuul/layout.yaml', 'jenkins/scripts/dockerhub-push.sh'], 'web_link': 'https://opendev.org/openstack/project-config/commit/ae017f841b91e036736296bed82e5ae45f23c1bb', 'message': 'Add job template for pushing images to dockerhub\n\nUse this template for pushing rally releases to dockerhub\n\nTo make this work we need \'docker\' node with docker installed\nOn this node should be create file ~/.dockercfg with contents:\n\n {""https://index.docker.io/v1/"":\n  {""auth"":""%AUTH-INFO%"",""email"":""%EMAIL%""}}\n\nWhere %AUTH-INFO% is base64 encoded ""login:password"" string.\nLogin name is represent repository name, like ""openstack"" or\n""stackforge"" on github.\n\nMore about docker repos: https://docs.docker.com/userguide/dockerrepos/\n\nChange-Id: Ideb99f804ef8708eb6df6e62cc23e97972839c85\n'}]",6,132222,ae017f841b91e036736296bed82e5ae45f23c1bb,18,7,4,7369,,,0,"Add job template for pushing images to dockerhub

Use this template for pushing rally releases to dockerhub

To make this work we need 'docker' node with docker installed
On this node should be create file ~/.dockercfg with contents:

 {""https://index.docker.io/v1/"":
  {""auth"":""%AUTH-INFO%"",""email"":""%EMAIL%""}}

Where %AUTH-INFO% is base64 encoded ""login:password"" string.
Login name is represent repository name, like ""openstack"" or
""stackforge"" on github.

More about docker repos: https://docs.docker.com/userguide/dockerrepos/

Change-Id: Ideb99f804ef8708eb6df6e62cc23e97972839c85
",git fetch https://review.opendev.org/openstack/project-config refs/changes/22/132222/3 && git format-patch -1 --stdout FETCH_HEAD,"['jenkins/jobs/projects.yaml', 'jenkins/jobs/docker.jobs', 'zuul/layout.yaml', 'jenkins/scripts/dockerhub-push.sh']",4,85ef883f3fc52b05d302c44ddeeae09d7f699e50,docker,"#!/bin/bash -xe # # Copyright 2014 Mirantis Inc. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # # Build docker image and puth it to hub.docker.com REPOSITORY=$1 PROJECT=$2 DOCKERFILEPATH=$3 TAG=`echo $ZUUL_REF | sed 's/^refs.tags.//'` IMAGE=""$REPOSITORY/$PROJECT:$TAG"" docker login -u $REPOSITORY docker build -t $IMAGE $DOCKERFILEPATH docker push $IMAGE ",,45,0
openstack%2Fneutron~master~I520a36ae53848b828ce33f4050606a7238f4cbce,openstack/neutron,master,I520a36ae53848b828ce33f4050606a7238f4cbce,Enforce log hints in ofagent and oneconvergence,MERGED,2014-12-02 12:29:41.000000000,2014-12-08 14:26:42.000000000,2014-12-08 14:26:41.000000000,"[{'_account_id': 3}, {'_account_id': 704}, {'_account_id': 841}, {'_account_id': 2592}, {'_account_id': 4656}, {'_account_id': 5170}, {'_account_id': 6854}, {'_account_id': 8124}, {'_account_id': 8873}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 12040}, {'_account_id': 13051}, {'_account_id': 14208}, {'_account_id': 14212}, {'_account_id': 14214}]","[{'number': 1, 'created': '2014-12-02 12:29:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/25bfb5cfe42263d2f157ae8922014687db140030', 'message': 'Enforce log hints in ofagent and oneconvergence\n\nThis change enforces log hints use and removes debug level log\ntranslation, modifications are validated through a hacking rule\nand the change respects loggging guidelines. Validate that hacking\nrules apply to directories:-\n    neutron/plugins/ofagent\n    neutron/plugins/oneconvergence\n\nChange-Id: I520a36ae53848b828ce33f4050606a7238f4cbce\nPartial-bug: #1320867\n'}, {'number': 2, 'created': '2014-12-03 07:33:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e401143f77a24d9eb3d61d9b80ce1d43f51c2745', 'message': 'Enforce log hints in ofagent and oneconvergence\n\nThis change enforces log hints use and removes debug level log\ntranslation, modifications are validated through a hacking rule\nand the change respects loggging guidelines. Validate that hacking\nrules apply to directories:-\n    neutron/plugins/ofagent\n    neutron/plugins/oneconvergence\n\nChange-Id: I520a36ae53848b828ce33f4050606a7238f4cbce\nPartial-bug: #1320867\n'}, {'number': 3, 'created': '2014-12-03 20:19:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/fae1f66981734183f6170de8d382ecf6dc61f381', 'message': 'Enforce log hints in ofagent and oneconvergence\n\nThis change enforces log hints use and removes debug level log\ntranslation, modifications are validated through a hacking rule\nand the change respects loggging guidelines. Validate that hacking\nrules apply to directories:-\n    neutron/plugins/ofagent\n    neutron/plugins/oneconvergence\n\nChange-Id: I520a36ae53848b828ce33f4050606a7238f4cbce\nPartial-bug: #1320867\n'}, {'number': 4, 'created': '2014-12-05 07:34:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/05967b0e25afa3d270ff586d201c4b4b324960a2', 'message': 'Enforce log hints in ofagent and oneconvergence\n\nThis change enforces log hints use and removes debug level log\ntranslation, modifications are validated through a hacking rule\nand the change respects loggging guidelines. Validate that hacking\nrules apply to directories:-\n    neutron/plugins/ofagent\n    neutron/plugins/oneconvergence\n\nChange-Id: I520a36ae53848b828ce33f4050606a7238f4cbce\nPartial-bug: #1320867\n'}, {'number': 5, 'created': '2014-12-06 16:52:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ea58f7f3d04722413c4b567b279209344e05898c', 'message': 'Enforce log hints in ofagent and oneconvergence\n\nThis change enforces log hints use and removes debug level log\ntranslation, modifications are validated through a hacking rule\nand the change respects loggging guidelines. Validate that hacking\nrules apply to directories:-\n    neutron/plugins/ofagent\n    neutron/plugins/oneconvergence\n\nChange-Id: I520a36ae53848b828ce33f4050606a7238f4cbce\nPartial-bug: #1320867\n'}, {'number': 6, 'created': '2014-12-07 21:00:39.000000000', 'files': ['neutron/plugins/oneconvergence/agent/nvsd_neutron_agent.py', 'neutron/plugins/oneconvergence/plugin.py', 'neutron/hacking/checks.py', 'neutron/plugins/oneconvergence/lib/nvsdlib.py', 'neutron/plugins/oneconvergence/lib/plugin_helper.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/907094bf12f069fe5e1a3ca19a91df5de9029a32', 'message': 'Enforce log hints in ofagent and oneconvergence\n\nThis change enforces log hints use and removes debug level log\ntranslation, modifications are validated through a hacking rule\nand the change respects loggging guidelines. Validate that hacking\nrules apply to directories:-\n    neutron/plugins/ofagent\n    neutron/plugins/oneconvergence\n\nChange-Id: I520a36ae53848b828ce33f4050606a7238f4cbce\nPartial-bug: #1320867\n'}]",0,138339,907094bf12f069fe5e1a3ca19a91df5de9029a32,127,27,6,1653,,,0,"Enforce log hints in ofagent and oneconvergence

This change enforces log hints use and removes debug level log
translation, modifications are validated through a hacking rule
and the change respects loggging guidelines. Validate that hacking
rules apply to directories:-
    neutron/plugins/ofagent
    neutron/plugins/oneconvergence

Change-Id: I520a36ae53848b828ce33f4050606a7238f4cbce
Partial-bug: #1320867
",git fetch https://review.opendev.org/openstack/neutron refs/changes/39/138339/2 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/oneconvergence/agent/nvsd_neutron_agent.py', 'neutron/plugins/oneconvergence/plugin.py', 'neutron/hacking/checks.py', 'neutron/plugins/oneconvergence/lib/nvsdlib.py', 'neutron/plugins/oneconvergence/lib/plugin_helper.py']",5,25bfb5cfe42263d2f157ae8922014687db140030,bug/1320867,"from neutron.i18n import _LE, _LW LOG.error(_LE(""Login Failed: %s""), e) LOG.error(_LE(""Unable to establish connection"" "" with Controller %s""), self.api_url) LOG.error(_LE(""Retrying after 1 second..."")) LOG.debug(""Login Successful %(uri)s "" ""%(status)s"", {'uri': self.api_url, 'status': response.status_code}) LOG.debug(""AuthToken = %s"", self.auth_token) else: LOG.error(_LE(""login failed"")) LOG.warning(_LW(""No Token, Re-login"")) LOG.debug(""request: %(method)s %(uri)s successful"", LOG.error(_LE(""request: Request failed from "" LOG.error(_LE(""Response is Null, Request timed out: %(method)s to "" ""%(uri)s""), {'method': method, 'uri': uri}) LOG.error(_LE(""Request %(method)s %(uri)s body = %(body)s failed "" ""with status %(status)s. Reason: %(reason)s)""), {'method': method, 'uri': uri, 'body': body, 'status': status, 'reason': response.reason}) LOG.error(_LE(""%(method)s to %(url)s, unexpected response code: "" ""%(status)d""), {'method': method, 'url': url, 'status': status}) LOG.error(_LE(""Request failed from Controller side with "" LOG.debug(""Success: %(method)s %(url)s status=%(status)s"","," LOG.error(_(""Login Failed: %s""), e) LOG.error(_(""Unable to establish connection"" "" with Controller %s""), self.api_url) LOG.error(_(""Retrying after 1 second..."")) LOG.debug(_(""Login Successful %(uri)s "" ""%(status)s""), {'uri': self.api_url, 'status': response.status_code}) LOG.debug(_(""AuthToken = %s""), self.auth_token) else: LOG.error(_(""login failed"")) LOG.warning(_(""No Token, Re-login"")) LOG.debug(_(""request: %(method)s %(uri)s successful""), LOG.error(_(""request: Request failed from "" LOG.error(_(""Response is Null, Request timed out: %(method)s to "" ""%(uri)s""), {'method': method, 'uri': uri}) LOG.error(_(""Request %(method)s %(uri)s body = %(body)s failed "" ""with status %(status)s""), {'method': method, 'uri': uri, 'body': body, 'status': status}) LOG.error(_(""%s""), response.reason) LOG.error(_(""%(method)s to %(url)s, unexpected response code: "" ""%(status)d""), {'method': method, 'url': url, 'status': status}) LOG.error(_(""Request failed from Controller side with "" LOG.debug(_(""Success: %(method)s %(url)s status=%(status)s""),",57,53
openstack%2Fcloudkitty~master~Ie28d106927b99d5166157501eefda2c4ca20841a,openstack/cloudkitty,master,Ie28d106927b99d5166157501eefda2c4ca20841a,Insert empty frame if no data,MERGED,2014-11-26 04:50:01.000000000,2014-12-08 14:22:57.000000000,2014-12-08 14:22:56.000000000,"[{'_account_id': 3}, {'_account_id': 7042}]","[{'number': 1, 'created': '2014-11-26 04:50:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cloudkitty/commit/caffb0d468e8ccf614f3a50b1dddee621eab2477', 'message': 'insert empty frame if no data\n\nquick hack to allow the billing window to move along if no\nsamples are present.\n\nChange-Id: Ie28d106927b99d5166157501eefda2c4ca20841a\n'}, {'number': 2, 'created': '2014-12-08 14:11:03.000000000', 'files': ['etc/cloudkitty/cloudkitty.conf.sample', 'cloudkitty/storage/sqlalchemy/__init__.py'], 'web_link': 'https://opendev.org/openstack/cloudkitty/commit/454e3c0ac2af31d66b65cad47a9445f818293216', 'message': 'Insert empty frame if no data\n\nQuick hack to allow the billing window to move along if no\nsamples are present.\n\nChange-Id: Ie28d106927b99d5166157501eefda2c4ca20841a\n'}]",0,137273,454e3c0ac2af31d66b65cad47a9445f818293216,9,2,2,10420,,,0,"Insert empty frame if no data

Quick hack to allow the billing window to move along if no
samples are present.

Change-Id: Ie28d106927b99d5166157501eefda2c4ca20841a
",git fetch https://review.opendev.org/openstack/cloudkitty refs/changes/73/137273/1 && git format-patch -1 --stdout FETCH_HEAD,['cloudkitty/storage/sqlalchemy/__init__.py'],1,caffb0d468e8ccf614f3a50b1dddee621eab2477,empty_frame," # HACK(adriant) Quick hack to allow billing windows to # progress. This check/insert probably ought to be moved # somewhere else. if not data[service]: empty_frame = {'vol': {'qty': 0, 'unit': 'None'}, 'billing': {'price': 0}, 'desc': ''} self._append_time_frame(service, empty_frame)",,7,0
openstack%2Ffuel-library~master~I5bcf0520d3257b376d1abb00076f9aba77a772e3,openstack/fuel-library,master,I5bcf0520d3257b376d1abb00076f9aba77a772e3,Fix OFED install failure on Ubuntu,MERGED,2014-12-08 08:39:33.000000000,2014-12-08 14:19:20.000000000,2014-12-08 14:19:18.000000000,"[{'_account_id': 3}, {'_account_id': 7195}, {'_account_id': 8786}, {'_account_id': 8935}, {'_account_id': 8971}, {'_account_id': 11968}]","[{'number': 1, 'created': '2014-12-08 08:39:33.000000000', 'files': ['deployment/puppet/osnailyfacter/examples/site.pp', 'deployment/puppet/osnailyfacter/manifests/cluster_ha.pp', 'deployment/puppet/mellanox_openstack/manifests/init.pp', 'deployment/puppet/osnailyfacter/manifests/cluster_simple.pp'], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/9feba84f428dff957042a5464aad343e17e8ff32', 'message': 'Fix OFED install failure on Ubuntu\n\nMoved ofed recompilation to puppet stage zero before\niser interface rename.\n\nCloses-bug: 1398978\nSigned-off-by: Gil Meir <gilmeir@mellanox.com>\nChange-Id: I5bcf0520d3257b376d1abb00076f9aba77a772e3\n'}]",0,139940,9feba84f428dff957042a5464aad343e17e8ff32,12,6,1,12177,,,0,"Fix OFED install failure on Ubuntu

Moved ofed recompilation to puppet stage zero before
iser interface rename.

Closes-bug: 1398978
Signed-off-by: Gil Meir <gilmeir@mellanox.com>
Change-Id: I5bcf0520d3257b376d1abb00076f9aba77a772e3
",git fetch https://review.opendev.org/openstack/fuel-library refs/changes/40/139940/1 && git format-patch -1 --stdout FETCH_HEAD,"['deployment/puppet/osnailyfacter/examples/site.pp', 'deployment/puppet/mellanox_openstack/manifests/init.pp', 'deployment/puppet/osnailyfacter/manifests/cluster_ha.pp', 'deployment/puppet/osnailyfacter/manifests/cluster_simple.pp']",4,9feba84f428dff957042a5464aad343e17e8ff32,mellanox,, if ($::mellanox_mode != 'disabled') { class { 'mellanox_openstack::ofed_recompile' : } } ,14,23
openstack%2Fbarbican~master~Ibc2700324495d01c571343937a9d1771ba9e5b85,openstack/barbican,master,Ibc2700324495d01c571343937a9d1771ba9e5b85,Update log messages to oslo.i18n,MERGED,2014-12-02 03:55:55.000000000,2014-12-08 14:15:11.000000000,2014-12-08 14:15:10.000000000,"[{'_account_id': 3}, {'_account_id': 994}, {'_account_id': 7136}, {'_account_id': 7789}, {'_account_id': 8004}, {'_account_id': 8623}, {'_account_id': 9234}, {'_account_id': 10873}]","[{'number': 1, 'created': '2014-12-02 03:55:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/bbeaded463e8133cdf5cb9076d3d021afa23c3e5', 'message': ""Update log messages to oslo.i18n\n\nThe OpenStack internationalization (i18n) system was moved from\noslo-incubator to the oslo.i18n library. This new library also has a\nrefined way to tag logging strings for translation as detailed in on\nthis page:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html\nThis CR updates Barbican's logging text messages accordingly. A\nseparate CR will update the barbican/openstack package structure from\noslo-incubator to remove all references to the old i18n system.\n\nChange-Id: Ibc2700324495d01c571343937a9d1771ba9e5b85\n""}, {'number': 2, 'created': '2014-12-02 03:58:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/ed9ebff027f7de6c09cca666a5846e125fab3b86', 'message': ""Update log messages to oslo.i18n\n\nThe OpenStack internationalization (i18n) system was moved from\noslo-incubator to the oslo.i18n library. This new library also has a\nrefined way to tag logging strings for translation as detailed in on\nthis page:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html\nThis CR updates Barbican's logging text messages accordingly. A\nseparate CR will update the barbican/openstack package structure from\noslo-incubator to remove all references to the old i18n system.\n\nChange-Id: Ibc2700324495d01c571343937a9d1771ba9e5b85\n""}, {'number': 3, 'created': '2014-12-02 04:49:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/597330877f074df7ea968aed35e70eecc473ff1e', 'message': ""Update log messages to oslo.i18n\n\nThe OpenStack internationalization (i18n) system was moved from\noslo-incubator to the oslo.i18n library. This new library also has a\nrefined way to tag logging strings for translation as detailed in on\nthis page:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html\nThis CR updates Barbican's logging text messages accordingly. A\nseparate CR will update the barbican/openstack package structure from\noslo-incubator to remove all references to the old i18n system.\n\nChange-Id: Ibc2700324495d01c571343937a9d1771ba9e5b85\n""}, {'number': 4, 'created': '2014-12-03 21:41:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/00059554e14050fa198268e122104ab016c3faba', 'message': ""Update log messages to oslo.i18n\n\nThe OpenStack internationalization (i18n) system was moved from\noslo-incubator to the oslo.i18n library. This new library also has a\nrefined way to tag logging strings for translation as detailed in on\nthis page:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html\nThis CR updates Barbican's logging text messages accordingly. A\nseparate CR will update the barbican/openstack package structure from\noslo-incubator to remove all references to the old i18n system.\n\nChange-Id: Ibc2700324495d01c571343937a9d1771ba9e5b85\n""}, {'number': 5, 'created': '2014-12-03 22:36:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/f36d178fd8f83b3ddfc5c2c0a10fc2e029141247', 'message': ""Update log messages to oslo.i18n\n\nThe OpenStack internationalization (i18n) system was moved from\noslo-incubator to the oslo.i18n library. This new library also has a\nrefined way to tag logging strings for translation as detailed in on\nthis page:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html\nThis CR updates Barbican's logging text messages accordingly. A\nseparate CR will update the barbican/openstack package structure from\noslo-incubator to remove all references to the old i18n system.\n\nChange-Id: Ibc2700324495d01c571343937a9d1771ba9e5b85\n""}, {'number': 6, 'created': '2014-12-08 01:05:18.000000000', 'files': ['barbican/plugin/simple_certificate_manager.py', 'barbican/api/controllers/orders.py', 'barbican/common/validators.py', 'barbican/openstack/common/gettextutils.py', 'barbican/queue/__init__.py', 'barbican/api/controllers/consumers.py', 'barbican/plugin/interface/secret_store.py', 'barbican/common/exception.py', 'barbican/plugin/symantec.py', 'requirements.txt', 'barbican/__init__.py', 'barbican/tasks/resources.py', 'barbican/api/__init__.py', 'barbican/plugin/crypto/p11_crypto.py', 'barbican/api/controllers/__init__.py', 'openstack-common.conf', 'barbican/i18n.py', 'barbican/locale/barbican.pot', 'barbican/plugin/dogtag.py', 'barbican/plugin/interface/certificate_manager.py', 'barbican/api/controllers/transportkeys.py', 'barbican/plugin/crypto/crypto.py', 'barbican/common/utils.py', 'barbican/api/controllers/versions.py', 'barbican/api/controllers/containers.py', 'barbican/plugin/crypto/simple_crypto.py', 'barbican/tests/tasks/test_resources.py', 'barbican/model/repositories.py', 'bin/barbican-keystone-listener.py', 'barbican/api/controllers/secrets.py', 'bin/barbican-worker.py', 'barbican/api/middleware/context.py', 'barbican/plugin/kmip_secret_store.py', 'barbican/tasks/keystone_consumer.py'], 'web_link': 'https://opendev.org/openstack/barbican/commit/f29d08610dbf5fd70ac48b7bf86c8d42d14d42d4', 'message': ""Update log messages to oslo.i18n\n\nThe OpenStack internationalization (i18n) system was moved from\noslo-incubator to the oslo.i18n library. This new library also has a\nrefined way to tag logging strings for translation as detailed in on\nthis page:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html\nThis CR updates Barbican's logging text messages accordingly. A\nseparate CR will update the barbican/openstack package structure from\noslo-incubator to remove all references to the old i18n system.\n\nChange-Id: Ibc2700324495d01c571343937a9d1771ba9e5b85\n""}]",25,138247,f29d08610dbf5fd70ac48b7bf86c8d42d14d42d4,33,8,6,7789,,,0,"Update log messages to oslo.i18n

The OpenStack internationalization (i18n) system was moved from
oslo-incubator to the oslo.i18n library. This new library also has a
refined way to tag logging strings for translation as detailed in on
this page:
http://docs.openstack.org/developer/oslo.i18n/guidelines.html
This CR updates Barbican's logging text messages accordingly. A
separate CR will update the barbican/openstack package structure from
oslo-incubator to remove all references to the old i18n system.

Change-Id: Ibc2700324495d01c571343937a9d1771ba9e5b85
",git fetch https://review.opendev.org/openstack/barbican refs/changes/47/138247/5 && git format-patch -1 --stdout FETCH_HEAD,"['barbican/plugin/simple_certificate_manager.py', 'barbican/api/controllers/orders.py', 'barbican/common/validators.py', 'barbican/openstack/common/gettextutils.py', 'barbican/queue/__init__.py', 'bin/logic_challenge.py', 'barbican/api/controllers/consumers.py', 'barbican/plugin/interface/secret_store.py', 'barbican/common/exception.py', 'barbican/plugin/symantec.py', 'requirements.txt', 'barbican/__init__.py', 'barbican/tasks/resources.py', 'barbican/api/__init__.py', 'barbican/plugin/crypto/p11_crypto.py', 'barbican/api/controllers/__init__.py', 'openstack-common.conf', 'barbican/i18n.py', 'barbican/locale/barbican.pot', 'barbican/plugin/dogtag.py', 'barbican/plugin/interface/certificate_manager.py', 'barbican/api/controllers/transportkeys.py', 'barbican/plugin/crypto/crypto.py', 'barbican/common/utils.py', 'barbican/api/controllers/versions.py', 'barbican/api/controllers/containers.py', 'barbican/plugin/crypto/simple_crypto.py', 'barbican/tests/tasks/test_resources.py', 'barbican/model/repositories.py', 'bin/barbican-keystone-listener.py', 'barbican/api/controllers/secrets.py', 'bin/barbican-worker.py', 'barbican/api/middleware/context.py', 'barbican/plugin/kmip_secret_store.py', 'barbican/tasks/keystone_consumer.py']",35,bbeaded463e8133cdf5cb9076d3d021afa23c3e5,logging-move-to-latest-gettext-approach-unit-tests,"from barbican import i18n as u LOG.error(u._LE( 'Error processing Keystone event, project_id=%(project_id)s, ' 'event resource=%(resource)s, event operation=%(operation)s, ' 'status=%(status)s, error message=%(message)s'), { 'project_id': project.project_id, 'resource': resource_type, 'operation': operation_type, 'status': status, 'message': message } ) LOG.info(u._LI( 'Successfully handled Keystone event, ' 'project_id=%(project_id)s, event resource=%(resource)s, ' 'event operation=%(operation)s'), { 'project_id': project_id, 'resource': resource_type, 'operation': operation_type } ) LOG.info(u._LI('No action is needed as there are no Barbican ' 'resources present for Keystone ' 'project_id=%s'), project_id) LOG.info(u._LI('Successfully completed Barbican resources cleanup for ' 'Keystone project_id=%s'), project_id)","from barbican.openstack.common import gettextutils as u LOG.error('Error processing Keystone event, project_id={0}, event ' 'resource={1}, event operation={2}, status={3}, error ' 'message={4}'.format(project.project_id, resource_type, operation_type, status, message)) LOG.info('Successfully handled Keystone event, project_id={0}, event ' 'resource={1}, event operation={2}'.format(project_id, resource_type, operation_type)) LOG.info('No action is needed as there are no Barbican resources ' 'present for Keystone project_id={0}'.format(project_id)) LOG.info('Successfully completed Barbican resources cleanup for ' 'Keystone project_id={0}'.format(project_id))",788,440
openstack%2Fproject-config~master~I47229a16ec52df0881b8978f9f97d2b8ebb5ae56,openstack/project-config,master,I47229a16ec52df0881b8978f9f97d2b8ebb5ae56,Correct misnamed check-{name}-dsvm-aio-build jobs,MERGED,2014-12-06 14:40:37.000000000,2014-12-08 14:12:24.000000000,2014-12-08 14:12:23.000000000,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-12-06 14:40:37.000000000', 'files': ['jenkins/jobs/projects.yaml', 'jenkins/jobs/os-ansible-jobs.yaml', 'zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/a1352b44dbd6fa66c1a5f3da601efa11f48de994', 'message': 'Correct misnamed check-{name}-dsvm-aio-build jobs\n\nChange-Id: I47229a16ec52df0881b8978f9f97d2b8ebb5ae56\n'}]",0,139803,a1352b44dbd6fa66c1a5f3da601efa11f48de994,7,3,1,5263,,,0,"Correct misnamed check-{name}-dsvm-aio-build jobs

Change-Id: I47229a16ec52df0881b8978f9f97d2b8ebb5ae56
",git fetch https://review.opendev.org/openstack/project-config refs/changes/03/139803/1 && git format-patch -1 --stdout FETCH_HEAD,"['jenkins/jobs/projects.yaml', 'jenkins/jobs/os-ansible-jobs.yaml', 'zuul/layout.yaml']",3,a1352b44dbd6fa66c1a5f3da601efa11f48de994,ansible-dsvm, - 'check-{name}-dsvm-aio-build' - name: check-os-ansible-deployment-dsvm-aio-build, - 'check-{name}-dvsm-aio-build' - name: check-os-ansible-deployment-dvsm-aio-build,4,4
openstack%2Fsahara~master~Ib064eaf60eef0bf338b9757e338ec61b80e6e812,openstack/sahara,master,Ib064eaf60eef0bf338b9757e338ec61b80e6e812,Specify CDH version,MERGED,2014-11-29 05:16:44.000000000,2014-12-08 14:03:41.000000000,2014-12-08 14:03:40.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7132}, {'_account_id': 7213}, {'_account_id': 7634}, {'_account_id': 7710}, {'_account_id': 7745}, {'_account_id': 8091}, {'_account_id': 8411}, {'_account_id': 10670}, {'_account_id': 12039}, {'_account_id': 13662}]","[{'number': 1, 'created': '2014-11-29 05:16:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/1d92627f8e63bb861549205edbb27c5b28aba802', 'message': ""Specify CDH version\n\nIn plugin.py we give '5.0.0' and '5.2.0' choices for users to\nselect. In deploy.py we will use this value as the fullVersion\nparameter passed to create_cluster. Now for there is no further\noperation difference between 5.0.0 and 5.2.0, we do not do other\nchanges, and do not create directories or use different classes\nimplementations like for vanilla.\n\nCloses-Bug: #1397155\n\nChange-Id: Ib064eaf60eef0bf338b9757e338ec61b80e6e812\n""}, {'number': 2, 'created': '2014-12-01 13:25:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/bd3a731372c64534cbcb9c5aa758e9e2ee83bb7d', 'message': 'Specify CDH version\n\nIn plugin.py we give ""5"" and ""5.2.0"" choices for users to\nselect. In deploy.py we will use this value as the fullVersion\nparameter passed to create_cluster. We treate ""5"" as ""5.0.0"".\nNow for there is no further operation difference between 5.0.0\nand 5.2.0, we do not do other changes, and do not create\ndirectories or use different classes implementations like for\nvanilla.\n\nCloses-Bug: #1397155\n\nChange-Id: Ib064eaf60eef0bf338b9757e338ec61b80e6e812\n'}, {'number': 3, 'created': '2014-12-02 13:59:11.000000000', 'files': ['sahara/plugins/cdh/deploy.py', 'sahara/plugins/cdh/plugin.py'], 'web_link': 'https://opendev.org/openstack/sahara/commit/7485334ea1b3cf8d0c0ab1eb98e88d840f6bd09f', 'message': 'Specify CDH version\n\nIn plugin.py we give ""5"" and ""5.2.0"" choices for users to\nselect. In deploy.py we will use this value as the fullVersion\nparameter passed to create_cluster. We treate ""5"" as ""5.0.0"".\nNow for there is no further operation difference between 5.0.0\nand 5.2.0, we do not do other changes, and do not create\ndirectories or use different classes implementations like for\nvanilla.\n\nCloses-Bug: #1397155\n\nChange-Id: Ib064eaf60eef0bf338b9757e338ec61b80e6e812\n'}]",3,137879,7485334ea1b3cf8d0c0ab1eb98e88d840f6bd09f,54,12,3,13662,,,0,"Specify CDH version

In plugin.py we give ""5"" and ""5.2.0"" choices for users to
select. In deploy.py we will use this value as the fullVersion
parameter passed to create_cluster. We treate ""5"" as ""5.0.0"".
Now for there is no further operation difference between 5.0.0
and 5.2.0, we do not do other changes, and do not create
directories or use different classes implementations like for
vanilla.

Closes-Bug: #1397155

Change-Id: Ib064eaf60eef0bf338b9757e338ec61b80e6e812
",git fetch https://review.opendev.org/openstack/sahara refs/changes/79/137879/1 && git format-patch -1 --stdout FETCH_HEAD,"['sahara/plugins/cdh/deploy.py', 'sahara/plugins/cdh/plugin.py']",2,1d92627f8e63bb861549205edbb27c5b28aba802,Bug1397155," return ['5.0.0', '5.2.0']", return ['5'],3,4
openstack%2Fsahara~master~I65e76b7c4d63e2fd0c4a52ec2a198a7510ccbaad,openstack/sahara,master,I65e76b7c4d63e2fd0c4a52ec2a198a7510ccbaad,Storm integration,MERGED,2014-11-27 19:47:57.000000000,2014-12-08 14:03:33.000000000,2014-12-08 14:03:33.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 7710}, {'_account_id': 7745}, {'_account_id': 8090}, {'_account_id': 8091}, {'_account_id': 8411}, {'_account_id': 8871}, {'_account_id': 8932}, {'_account_id': 10670}, {'_account_id': 12038}]","[{'number': 1, 'created': '2014-11-27 19:47:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/cc1e9688d1f6ee1fcd29f32c40ebe810e5bf1f0d', 'message': 'Storm integration\n\nThis patch implements the Storm plugin.\n\nChange-Id: I65e76b7c4d63e2fd0c4a52ec2a198a7510ccbaad\n'}, {'number': 2, 'created': '2014-11-28 04:54:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/cd01143463f29e3fb51b1b50fa55491158ded873', 'message': 'Storm integration\n\nThis patch implements the Storm plugin.\n\nChange-Id: I65e76b7c4d63e2fd0c4a52ec2a198a7510ccbaad\n'}, {'number': 3, 'created': '2014-12-02 19:17:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/f9bc08d882b50ef6a35897ee34f28cc1c54c9bc4', 'message': 'Storm integration\n\nThis patch implements the Storm plugin.\n\nImplements: bp storm-integration\nChange-Id: I65e76b7c4d63e2fd0c4a52ec2a198a7510ccbaad\n'}, {'number': 4, 'created': '2014-12-03 02:28:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/e86d07d8f42d38f2df8842a925a6253be3fe6247', 'message': 'Storm integration\n\nThis patch implements the Storm plugin.\n\nImplements: bp storm-integration\nChange-Id: I65e76b7c4d63e2fd0c4a52ec2a198a7510ccbaad\n'}, {'number': 5, 'created': '2014-12-03 13:43:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/5124b404b045d1bc3b9bf381f72f8a10a30d1fb2', 'message': 'Storm integration\n\nThis patch implements the Storm plugin.\n\nImplements: bp storm-integration\nChange-Id: I65e76b7c4d63e2fd0c4a52ec2a198a7510ccbaad\n'}, {'number': 6, 'created': '2014-12-03 14:03:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/77cc98f147af9bd7315700c62d08e40cf0ff07a6', 'message': 'Storm integration\n\nThis patch implements the Storm plugin.\n\nImplements: bp storm-integration\nChange-Id: I65e76b7c4d63e2fd0c4a52ec2a198a7510ccbaad\n'}, {'number': 7, 'created': '2014-12-04 11:29:00.000000000', 'files': ['sahara/plugins/storm/__init__.py', 'sahara/plugins/storm/run_scripts.py', 'sahara/plugins/storm/config_helper.py', 'sahara/plugins/storm/plugin.py'], 'web_link': 'https://opendev.org/openstack/sahara/commit/7dc31d33a4484f448ad43de4b3b80ad0e9afeedb', 'message': 'Storm integration\n\nThis patch implements the Storm plugin.\n\nImplements: bp storm-integration\nChange-Id: I65e76b7c4d63e2fd0c4a52ec2a198a7510ccbaad\n'}]",27,137699,7dc31d33a4484f448ad43de4b3b80ad0e9afeedb,72,12,7,8932,,,0,"Storm integration

This patch implements the Storm plugin.

Implements: bp storm-integration
Change-Id: I65e76b7c4d63e2fd0c4a52ec2a198a7510ccbaad
",git fetch https://review.opendev.org/openstack/sahara refs/changes/99/137699/7 && git format-patch -1 --stdout FETCH_HEAD,"['sahara/plugins/storm/__init__.py', 'sahara/plugins/storm/run_scripts.py', 'sahara/plugins/storm/config_helper.py', 'sahara/plugins/storm/plugin.py']",4,cc1e9688d1f6ee1fcd29f32c40ebe810e5bf1f0d,bp/storm-integration,"# Copyright (c) 2014 Hoang Do, Phuc Vo, P. Michiardi, D. Venzano # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or # implied. # See the License for the specific language governing permissions and # limitations under the License. from oslo.config import cfg from sahara import conductor from sahara import context from sahara.openstack.common import log as logging from sahara.plugins.general import exceptions as ex from sahara.plugins.general import utils from sahara.plugins import provisioning as p from sahara.plugins.storm import config_helper as c_helper from sahara.plugins.storm import run_scripts as run from sahara.utils import remote import yaml conductor = conductor.API LOG = logging.getLogger(__name__) CONF = cfg.CONF class StormProvider(p.ProvisioningPluginBase): def __init__(self): self.processes = { ""Zookeeper"": [""zookeeper""], ""Storm"": [""nimbus"", ""supervisor""] } def get_title(self): return ""Apache Storm"" def get_description(self): return ( ""This plugin provides an ability to launch Storm "" ""cluster without any management consoles."") def get_versions(self): return ['0.9.2', '0.9.1'] def get_configs(self, storm_version): return c_helper.get_plugin_configs() def get_node_processes(self, storm_version): return self.processes def validate(self, cluster): # validate Storm Master Node and Storm Slaves sm_count = sum([ng.count for ng in utils.get_node_groups(cluster, ""nimbus"")]) if sm_count != 1: raise ex.RequiredServiceMissingException(""Storm nimbus"") sl_count = sum([ng.count for ng in utils.get_node_groups(cluster, ""supervisor"")]) if sl_count < 1: raise ex.InvalidComponentCountException(""Storm supervisor"", ""1 or more"", sl_count) def update_infra(self, cluster): pass def configure_cluster(self, cluster): self._setup_instances(cluster) def start_cluster(self, cluster): sm_instance = utils.get_instance(cluster, ""nimbus"") sl_instances = utils.get_instances(cluster, ""supervisor"") zk_instance = utils.get_instances(cluster, ""zookeeper"") if zk_instance: self._start_zookeeper_processes(zk_instance) # start storm master if sm_instance: with remote.get_remote(sm_instance) as r: run.start_storm_nimbus_and_ui(r) LOG.info(""Storm master at '%s' has been started"", sm_instance.hostname()) # start storm slaves self._start_slave_processes(sl_instances) LOG.info('Cluster %s has been started successfully' % cluster.name) self._set_cluster_info(cluster) def _extract_configs_to_extra(self, cluster): st_master = utils.get_instance(cluster, ""nimbus"") zk_servers = utils.get_instances(cluster, ""zookeeper"") extra = dict() config_instances = '' if st_master is not None: if zk_servers is not None: zknames = [] for zk in zk_servers: zknames.append(zk.hostname()) config_instances = c_helper.generate_storm_config( st_master.hostname(), zknames) config = self._convert_dict_to_yaml(config_instances) supervisor_conf = c_helper.generate_slave_supervisor_conf() nimbus_ui_conf = c_helper.generate_master_supervisor_conf() zk_conf = c_helper.generate_zookeeper_conf() for ng in cluster.node_groups: extra[ng.id] = { 'st_instances': config, 'slave_sv_conf': supervisor_conf, 'master_sv_conf': nimbus_ui_conf, 'zk_conf': zk_conf } return extra def _start_slave_processes(self, sl_instances): with context.ThreadGroup() as tg: for i in sl_instances: tg.spawn('storm-start-sl-%s' % i.instance_name, self._start_slaves, i) def _start_slaves(self, instance): with instance.remote() as r: run.start_storm_supervisor(r) def _start_zookeeper_processes(self, zk_instances): with context.ThreadGroup() as tg: for i in zk_instances: tg.spawn('storm-start-zk-%s' % i.instance_name, self._start_zookeeper, i) def _start_zookeeper(self, instance): with instance.remote() as r: run.start_zookeeper(r) def _setup_instances(self, cluster, instances=None): extra = self._extract_configs_to_extra(cluster) if instances is None: instances = utils.get_instances(cluster) self._push_configs_to_nodes(cluster, extra, instances) def _push_configs_to_nodes(self, cluster, extra, new_instances): all_instances = utils.get_instances(cluster) with context.ThreadGroup() as tg: for instance in all_instances: if instance in new_instances: tg.spawn('storm-configure-%s' % instance.instance_name, self._push_configs_to_new_node, cluster, extra, instance) else: tg.spawn('storm-reconfigure-%s' % instance.instance_name, self._push_configs_to_existing_node, cluster, extra, instance) def _convert_dict_to_yaml(self, dict_to_convert): new_dict = dict_to_convert.copy() for key in dict_to_convert: if isinstance(dict_to_convert[key], basestring): new_dict[key] = ""\"""" + dict_to_convert[key] + ""\"""" stream = yaml.dump(new_dict, default_flow_style=False) stream = stream.replace(""\'"", """") return stream def _push_configs_to_new_node(self, cluster, extra, instance): ng_extra = extra[instance.node_group.id] files_supervisor = { '/etc/supervisor/supervisord.conf': ng_extra['slave_sv_conf'] } files_storm = { '/usr/local/storm/conf/storm.yaml': ng_extra['st_instances'] } files_zk = { '/opt/zookeeper/zookeeper-3.4.6/conf/zoo.cfg': ng_extra['zk_conf'] } files_supervisor_master = { '/etc/supervisor/supervisord.conf': ng_extra['master_sv_conf'] } with remote.get_remote(instance) as r: node_processes = instance.node_group.node_processes r.write_files_to(files_storm, run_as_root=True) if 'zookeeper' in node_processes: self._push_zk_configs(r, files_zk) if 'nimbus' in node_processes: self._push_supervisor_configs(r, files_supervisor_master) if 'supervisor' in node_processes: self._push_supervisor_configs(r, files_supervisor) def _set_cluster_info(self, cluster): st_master = utils.get_instance(cluster, ""nimbus"") info = {} if st_master: port = ""8080"" if port is not None: info['Strom'] = { 'Web UI': 'http://%s:%s' % (st_master.management_ip, port) } ctx = context.ctx() conductor.cluster_update(ctx, cluster, {'info': info}) def _push_zk_configs(self, r, files): r.write_files_to(files, run_as_root=True) def _push_supervisor_configs(self, r, files): r.append_to_files(files, run_as_root=True) ",,420,0
openstack%2Fsahara~master~Ie9d6d113c35f957ee3e123750a4d9d14ba812ed9,openstack/sahara,master,Ie9d6d113c35f957ee3e123750a4d9d14ba812ed9,Update plugin descriptions,MERGED,2014-12-03 16:20:42.000000000,2014-12-08 14:03:21.000000000,2014-12-08 14:03:20.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7132}, {'_account_id': 7213}, {'_account_id': 7555}, {'_account_id': 7710}, {'_account_id': 8090}, {'_account_id': 8411}, {'_account_id': 8871}, {'_account_id': 9740}, {'_account_id': 10670}, {'_account_id': 12039}]","[{'number': 1, 'created': '2014-12-03 16:20:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/11daf0b32ede1fcc3efc6ee2015d4472ba40db99', 'message': 'Update plugin descriptions\n\nWe have a feedback to improve the plugin improvements.\n\nChange-Id: Ie9d6d113c35f957ee3e123750a4d9d14ba812ed9\nCloses-bug: #1398827\n'}, {'number': 2, 'created': '2014-12-03 16:29:19.000000000', 'files': ['sahara/plugins/vanilla/plugin.py', 'sahara/plugins/cdh/plugin.py', 'sahara/plugins/hdp/ambariplugin.py'], 'web_link': 'https://opendev.org/openstack/sahara/commit/dbf39c4134e5eff5fe5e1aaeb2807050dfe346bc', 'message': 'Update plugin descriptions\n\nWe have a feedback to improve the plugin descriptions.\n\nChange-Id: Ie9d6d113c35f957ee3e123750a4d9d14ba812ed9\n'}]",3,138775,dbf39c4134e5eff5fe5e1aaeb2807050dfe346bc,21,12,2,6786,,,0,"Update plugin descriptions

We have a feedback to improve the plugin descriptions.

Change-Id: Ie9d6d113c35f957ee3e123750a4d9d14ba812ed9
",git fetch https://review.opendev.org/openstack/sahara refs/changes/75/138775/1 && git format-patch -1 --stdout FETCH_HEAD,"['sahara/plugins/vanilla/plugin.py', 'sahara/plugins/cdh/plugin.py', 'sahara/plugins/hdp/ambariplugin.py']",3,11daf0b32ede1fcc3efc6ee2015d4472ba40db99,bug/1398827, return _('The Hortonworks Sahara plugin automates the deployment ' 'of the Hortonworks Data Platform (HDP) on OpenStack.'), return _('The Hortonworks OpenStack plugin works with project ' 'Sahara to automate the deployment of the Hortonworks data' ' platform on OpenStack based public & private clouds'),8,9
openstack%2Fheat~master~Ibf4c2f61d1f1936816245178fd8cf794e347de8b,openstack/heat,master,Ibf4c2f61d1f1936816245178fd8cf794e347de8b,Create common stack_create functional test helper,MERGED,2014-11-20 14:39:33.000000000,2014-12-08 14:03:11.000000000,2014-12-08 14:03:10.000000000,"[{'_account_id': 3}, {'_account_id': 4571}, {'_account_id': 4715}, {'_account_id': 7385}, {'_account_id': 13009}]","[{'number': 1, 'created': '2014-11-20 14:39:33.000000000', 'files': ['heat_integrationtests/functional/test_update.py', 'heat_integrationtests/common/test.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/de603a8d24e547fdee715fbfcb74baeec8fbdad6', 'message': 'Create common stack_create functional test helper\n\nThe same pattern for creating a stack, defaulting arguments,\nsetting cleanup, and waiting for status is repeated in all\ntests, so move that logic to a common base-class helper.\n\nChange-Id: Ibf4c2f61d1f1936816245178fd8cf794e347de8b\n'}]",0,136008,de603a8d24e547fdee715fbfcb74baeec8fbdad6,9,5,1,4328,,,0,"Create common stack_create functional test helper

The same pattern for creating a stack, defaulting arguments,
setting cleanup, and waiting for status is repeated in all
tests, so move that logic to a common base-class helper.

Change-Id: Ibf4c2f61d1f1936816245178fd8cf794e347de8b
",git fetch https://review.opendev.org/openstack/heat refs/changes/08/136008/1 && git format-patch -1 --stdout FETCH_HEAD,"['heat_integrationtests/functional/test_update.py', 'heat_integrationtests/common/test.py']",2,de603a8d24e547fdee715fbfcb74baeec8fbdad6,functional_tests3," def stack_create(self, stack_name=None, template=None, files=None, parameters=None, environment=None): name = stack_name or self._stack_rand_name() templ = template or self.template templ_files = files or {} params = parameters or {} env = environment or {} self.client.stacks.create( stack_name=name, template=templ, files=templ_files, disable_rollback=True, parameters=params, environment=env ) self.addCleanup(self.client.stacks.delete, name) stack = self.client.stacks.get(name) stack_identifier = '%s/%s' % (name, stack.id) self._wait_for_stack_status(stack_identifier, 'CREATE_COMPLETE') return stack_identifier",,28,56
openstack%2Fheat~master~I5edc9f06d6b48bb04e8ed41365823021ef133909,openstack/heat,master,I5edc9f06d6b48bb04e8ed41365823021ef133909,Populate _template_classes global on first use,MERGED,2014-11-29 00:13:08.000000000,2014-12-08 14:03:03.000000000,2014-12-08 14:03:00.000000000,"[{'_account_id': 3}, {'_account_id': 4257}, {'_account_id': 4715}, {'_account_id': 7385}, {'_account_id': 7634}, {'_account_id': 8289}]","[{'number': 1, 'created': '2014-11-29 00:13:08.000000000', 'files': ['heat/engine/template.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/5e2ee8a42c3791986bfa9d69b7c6032cccb3252c', 'message': 'Populate _template_classes global on first use\n\nThis change ensures that the _template_classes global variable is always\ninitialized when the first template is created. This was done when\ninstantiating the Template base class, but it was missed when creating an\ninstance of any of its subclasses.\n\nChange-Id: I5edc9f06d6b48bb04e8ed41365823021ef133909\nCloses-Bug: 1397468\n'}]",0,137867,5e2ee8a42c3791986bfa9d69b7c6032cccb3252c,10,6,1,12606,,,0,"Populate _template_classes global on first use

This change ensures that the _template_classes global variable is always
initialized when the first template is created. This was done when
instantiating the Template base class, but it was missed when creating an
instance of any of its subclasses.

Change-Id: I5edc9f06d6b48bb04e8ed41365823021ef133909
Closes-Bug: 1397468
",git fetch https://review.opendev.org/openstack/heat refs/changes/67/137867/1 && git format-patch -1 --stdout FETCH_HEAD,['heat/engine/template.py'],1,5e2ee8a42c3791986bfa9d69b7c6032cccb3252c,bug/1397468," global _template_classes if _template_classes is None: mgr = _get_template_extension_manager() _template_classes = dict((tuple(name.split('.')), mgr[name].plugin) for name in mgr.names())"," global _template_classes if _template_classes is None: mgr = _get_template_extension_manager() _template_classes = dict((tuple(name.split('.')), mgr[name].plugin) for name in mgr.names()) ",6,7
openstack%2Fbarbican-specs~master~Ie53cf3c36afdd314e4a410acb27be1ba4f6ec71d,openstack/barbican-specs,master,Ie53cf3c36afdd314e4a410acb27be1ba4f6ec71d,Remove the tenant-secret association table,MERGED,2014-11-18 03:59:48.000000000,2014-12-08 14:02:29.000000000,2014-12-08 14:02:27.000000000,"[{'_account_id': 3}, {'_account_id': 994}, {'_account_id': 6783}, {'_account_id': 7789}, {'_account_id': 8004}, {'_account_id': 10273}, {'_account_id': 10873}]","[{'number': 1, 'created': '2014-11-18 03:59:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/5e720d11b8a2c584326dc45144209d864d0ff2fb', 'message': 'Remove the tenant-secret association table\n\nCurrently all secrets stored in Barbican are indirectly associated with\nprojects via a tenant-secret association table. This association was\nconfigured early in the project lifecycle, when it seemed that such an\nassociation would aid in retrieving different components of grouped\nsecrets with different RBAC behaviors. However, recent cross-project\ndiscussions have indicated that this feature would be better\nimplemented in a different way. Hence this association is adding an\nadditional join on selects with no benefit, and therefore should be\nremoved.\n\nChange-Id: Ie53cf3c36afdd314e4a410acb27be1ba4f6ec71d\n'}, {'number': 2, 'created': '2014-11-18 16:37:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/d85de8d2a0544ef81647aa11ed6ee84303574560', 'message': 'Remove the tenant-secret association table\n\nCurrently all secrets stored in Barbican are indirectly associated with\nprojects via a tenant-secret association table. This association was\nconfigured early in the project lifecycle, when it seemed that such an\nassociation would aid in retrieving different components of grouped\nsecrets with different RBAC behaviors. However, recent cross-project\ndiscussions have indicated that this feature would be better\nimplemented in a different way. Hence this association is adding an\nadditional join on selects with no benefit, and therefore should be\nremoved.\n\nChange-Id: Ie53cf3c36afdd314e4a410acb27be1ba4f6ec71d\n'}, {'number': 3, 'created': '2014-11-18 16:42:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/484810bb97dfd199e2c18c84a9a0eb0adecbe280', 'message': 'Remove the tenant-secret association table\n\nCurrently all secrets stored in Barbican are indirectly associated with\nprojects via a tenant-secret association table. This association was\nconfigured early in the project lifecycle, when it seemed that such an\nassociation would aid in retrieving different components of grouped\nsecrets with different RBAC behaviors. However, recent cross-project\ndiscussions have indicated that this feature would be better\nimplemented in a different way. Hence this association is adding an\nadditional join on selects with no benefit, and therefore should be\nremoved.\n\nChange-Id: Ie53cf3c36afdd314e4a410acb27be1ba4f6ec71d\n'}, {'number': 4, 'created': '2014-11-18 23:45:46.000000000', 'files': ['specs/kilo/data-remove-tenant-secret-assoc.rst'], 'web_link': 'https://opendev.org/openstack/barbican-specs/commit/686f3c88825b8b1e6982cba1b3212fbd8ee6b312', 'message': 'Remove the tenant-secret association table\n\nCurrently all secrets stored in Barbican are indirectly associated with\nprojects via a tenant-secret association table. This association was\nconfigured early in the project lifecycle, when it seemed that such an\nassociation would aid in retrieving different components of grouped\nsecrets with different RBAC behaviors. However, recent cross-project\ndiscussions have indicated that this feature would be better\nimplemented in a different way. Hence this association is adding an\nadditional join on selects with no benefit, and therefore should be\nremoved.\n\nChange-Id: Ie53cf3c36afdd314e4a410acb27be1ba4f6ec71d\n'}]",11,135158,686f3c88825b8b1e6982cba1b3212fbd8ee6b312,26,7,4,7789,,,0,"Remove the tenant-secret association table

Currently all secrets stored in Barbican are indirectly associated with
projects via a tenant-secret association table. This association was
configured early in the project lifecycle, when it seemed that such an
association would aid in retrieving different components of grouped
secrets with different RBAC behaviors. However, recent cross-project
discussions have indicated that this feature would be better
implemented in a different way. Hence this association is adding an
additional join on selects with no benefit, and therefore should be
removed.

Change-Id: Ie53cf3c36afdd314e4a410acb27be1ba4f6ec71d
",git fetch https://review.opendev.org/openstack/barbican-specs refs/changes/58/135158/3 && git format-patch -1 --stdout FETCH_HEAD,['specs/kilo/data-remove-tenant-secret-assoc.rst'],1,5e720d11b8a2c584326dc45144209d864d0ff2fb,bp/data-remove-tenant-secret-assoc,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Remove the tenant-secret association table ========================================== https://blueprints.launchpad.net/barbican/+spec/data-remove-tenant-secret-assoc Currently all secrets stored in Barbican are indirectly associated with projects via a tenant-secret association table. This association was configured early in the project lifecycle, when it seemed that such an association would aid in retrieving different components of grouped secrets with different RBAC behaviors. However, recent cross-project discussions have indicated that this feature would be better implemented in a different way. Hence this association is adding an additional join on selects with no benefit, and therefore should be removed. Problem Description =================== Secrets in Barbican are indirectly related to projects via a TenantSecret association table. This association is not utilized for any features in Barbican currently but was envisioned to allow multiple projects to access the same secret with different access privileges. The team has since had numerous discussions with OpenStack projects that wish to integrate with Barbican, and access control could either be accomplished via Keystone Trusts, or else via user and project level policy based on metadata configured at the secret level. Hence the TenantSecret association is not required for any envisioned Barbican use cases and therefore represents a wasteful join for all secret selects. Proposed Change =============== This blueprint proposes removing the unused TenantSecret association. Alternatives ------------ None Data model impact ----------------- The TenantSecret table would be removed, along with associated SQLAlchemy models and repository queries. This schema modification would require a non-trivial migration for zero downtime, as detailed in the 'Work Items' section below. REST API impact --------------- None Security impact --------------- None Notifications & Audit Impact ---------------------------- None Other end user impact --------------------- None Performance Impact ------------------ Removing the TenantSecret association removes one join from secret-related queries, and removes Python code needed to create the SQLAlchemy TenantSecret model. Hence performance and code readability should be improved. Other deployer impact --------------------- Migration scripts would need to be run on already existing deployments. There will be no other impact otherwise. Developer impact ---------------- None Implementation ============== Assignee(s) Primary assignee: john-wood-w Work Items ---------- The schema modification called for would need to be phased in to avoid breaking existing deployments with secrets stored using the current TenantSecret associations. The first phase would add the new and initially nullable 'project' FK relationship to the secret entity, with all new secrets added setting this FK, and still adding the TenantSecret relationships (until older versions are rotated out). Secret retrievals would first attempt the simple query by FK, and fallback to the current TenantSecret join. The following mods would be needed: * models.py: Add a 'secrets_assoc' relationship in the Tenant model class to the secrets model, similar to the 'Container' relationship. To the Secret model, add a 'project_id' FK reference similar to the Container model's 'tenant_id' FK reference, but with nullable=True. * repositories.py: In the SecretRepo repository class add a query for secrets without the TenantSecret join, but on errors revert to the existing TenantSecret join query. * plugin/resources.py: Add setting the new 'project_id' FK on secrets to the '_save_secret()' function. * store_crypto.py: Add setting the new 'project_id' FK on secrets to the '_store_secret_and_datum()' function. The second phase would provide an Alembic migration script to set the 'project' FK on all secrets that don't already have it set. The 'project' FK would then be set as nullable=False. The third phase would remove all logic related to the TenantSecret relationship. The following mods would be needed: * models.py: Remove TenantSecret SQLAlchemy model class. * repositories.py: Remove TenantSecret joins from queries in the SecretRepo repository class. Remove the ProjectSecretRepo repository class. Remove 'ProjectSecretRepo' from the Repositories class. Remove the 'get_project_secret_repository' function. * plugin/resources.py: Remove 'new_assoc' lines from the '_save_secret()'' function. * store_crypto.py: Remove 'new_assoc' lines from the '_store_secret_and_datum()' function. * api/test_resources.py: Remove 'project_secret_repo' lines from the '_test_should_add_new_secret_metadata_without_payload' test method. * test_repositories.py: Remove all 'TenantSecret' related lines. * test_store_crypto.py: Remove all 'project_secret_repo' lines. The fourth phase would provide an Alembic migration script to remove the TenantSecret relationship from all secrets, and to then drop the TenantSecret table altogether. Dependencies ============ None Testing ======= The current unit tests will also be modified to have this change reflected upon them. Migration testing across the different phases would be needed, at least manually. Documentation Impact ==================== None References ========== None ",,185,0
openstack%2Fpuppet-glance~master~I13c85c91781448890656f37ae548f8da3ac31147,openstack/puppet-glance,master,I13c85c91781448890656f37ae548f8da3ac31147,Fix spelling of OpenStack,MERGED,2014-12-08 07:34:14.000000000,2014-12-08 13:56:19.000000000,2014-12-08 13:56:18.000000000,"[{'_account_id': 3}, {'_account_id': 3153}, {'_account_id': 7155}]","[{'number': 1, 'created': '2014-12-08 07:34:14.000000000', 'files': ['spec/classes/glance_keystone_auth_spec.rb', 'README.md', 'manifests/keystone/auth.pp'], 'web_link': 'https://opendev.org/openstack/puppet-glance/commit/26dac12bb54a7966f9a2f08a9cb3569a2b6822e9', 'message': 'Fix spelling of OpenStack\n\nAccording to the OpenStack Documentation conventions it should\nbe OpenStack and not Openstack or openstack.\n\nhttps://wiki.openstack.org/wiki/Documentation/Conventions#OpenStack.2C_not_Openstack_or_openstack\n\nChange-Id: I13c85c91781448890656f37ae548f8da3ac31147\n'}]",0,139929,26dac12bb54a7966f9a2f08a9cb3569a2b6822e9,7,3,1,167,,,0,"Fix spelling of OpenStack

According to the OpenStack Documentation conventions it should
be OpenStack and not Openstack or openstack.

https://wiki.openstack.org/wiki/Documentation/Conventions#OpenStack.2C_not_Openstack_or_openstack

Change-Id: I13c85c91781448890656f37ae548f8da3ac31147
",git fetch https://review.opendev.org/openstack/puppet-glance refs/changes/29/139929/1 && git format-patch -1 --stdout FETCH_HEAD,"['spec/classes/glance_keystone_auth_spec.rb', 'README.md', 'manifests/keystone/auth.pp']",3,26dac12bb54a7966f9a2f08a9cb3569a2b6822e9,wording_openstack," service_description => 'OpenStack Image Service',"," service_description => 'Openstack Image Service',",8,8
openstack%2Foslo-incubator~master~I19efd9bb94e246e709b2a83c612ea2d499bdbf05,openstack/oslo-incubator,master,I19efd9bb94e246e709b2a83c612ea2d499bdbf05,Remove local.py,ABANDONED,2014-10-29 16:42:49.000000000,2014-12-08 13:55:41.000000000,,"[{'_account_id': 3}, {'_account_id': 2472}]","[{'number': 1, 'created': '2014-10-29 16:42:49.000000000', 'files': ['tests/unit/test_local.py', 'MAINTAINERS', 'openstack/common/local.py', 'openstack/common/log.py', 'tests/unit/test_log.py', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/bf35e4f48e0673803fe65a837051908c9cfde9a3', 'message': ""Remove local.py\n\nLet's drop openstack.common.local and provide an API in\nopenstack.common.log to set the context to use in logging.\n\nChange-Id: I19efd9bb94e246e709b2a83c612ea2d499bdbf05\n""}]",0,131821,bf35e4f48e0673803fe65a837051908c9cfde9a3,4,2,1,1669,,,0,"Remove local.py

Let's drop openstack.common.local and provide an API in
openstack.common.log to set the context to use in logging.

Change-Id: I19efd9bb94e246e709b2a83c612ea2d499bdbf05
",git fetch https://review.opendev.org/openstack/oslo-incubator refs/changes/21/131821/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/unit/test_local.py', 'MAINTAINERS', 'openstack/common/local.py', 'openstack/common/log.py', 'tests/unit/test_log.py', 'tox.ini']",6,bf35e4f48e0673803fe65a837051908c9cfde9a3,jd/remove-local,, tests.unit.test_local \ tests.unit.test_local \,19,131
openstack%2Fpuppet-openstack-specs~master~Ie72adb0086dbbf8893d76fc8641365586ec2f703,openstack/puppet-openstack-specs,master,Ie72adb0086dbbf8893d76fc8641365586ec2f703,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:57:55.000000000,2014-12-08 13:52:09.000000000,2014-12-08 13:52:08.000000000,"[{'_account_id': 3}, {'_account_id': 3153}, {'_account_id': 7155}, {'_account_id': 9410}]","[{'number': 1, 'created': '2014-12-05 03:57:55.000000000', 'files': ['README.md'], 'web_link': 'https://opendev.org/openstack/puppet-openstack-specs/commit/cd63fed428ea705f41bc0b97836c743e8589d7b7', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Ie72adb0086dbbf8893d76fc8641365586ec2f703\n'}]",0,139490,cd63fed428ea705f41bc0b97836c743e8589d7b7,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Ie72adb0086dbbf8893d76fc8641365586ec2f703
",git fetch https://review.opendev.org/openstack/puppet-openstack-specs refs/changes/90/139490/1 && git format-patch -1 --stdout FETCH_HEAD,['README.md'],1,cd63fed428ea705f41bc0b97836c743e8589d7b7,infra-manual, http://docs.openstack.org/infra/manual/developers.html#development-workflow, https://wiki.openstack.org/wiki/Gerrit_Workflow,1,1
openstack%2Fheat~master~Ib7b6ab2591c01e91d7a55f3384dae505fe1485a7,openstack/heat,master,Ib7b6ab2591c01e91d7a55f3384dae505fe1485a7,Fill test database from models description,MERGED,2014-10-24 13:04:42.000000000,2014-12-08 13:51:39.000000000,2014-12-08 13:51:38.000000000,"[{'_account_id': 3}, {'_account_id': 4328}, {'_account_id': 4715}, {'_account_id': 7385}, {'_account_id': 7491}, {'_account_id': 8289}]","[{'number': 1, 'created': '2014-10-24 13:04:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/a7824c2b3d4b7bf8c32319f53a0453c99beca67b', 'message': 'WIP: use matadata.create_all() to init database\n\nChange-Id: Ib7b6ab2591c01e91d7a55f3384dae505fe1485a7\n'}, {'number': 2, 'created': '2014-11-27 10:23:53.000000000', 'files': ['heat/tests/utils.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/638fd591f03fcbd36ff1baf8ec89748c1614f1ef', 'message': 'Fill test database from models description\n\nNow Heat models are the same as the tables state in DB after migrations,\nso we can use metadata.create_all() to fill a test database.\nThis will speed-up test database creation and reduce a necessity in DB\nmigrations on SQLite\n\nChange-Id: Ib7b6ab2591c01e91d7a55f3384dae505fe1485a7\n'}]",0,130776,638fd591f03fcbd36ff1baf8ec89748c1614f1ef,15,6,2,7491,,,0,"Fill test database from models description

Now Heat models are the same as the tables state in DB after migrations,
so we can use metadata.create_all() to fill a test database.
This will speed-up test database creation and reduce a necessity in DB
migrations on SQLite

Change-Id: Ib7b6ab2591c01e91d7a55f3384dae505fe1485a7
",git fetch https://review.opendev.org/openstack/heat refs/changes/76/130776/1 && git format-patch -1 --stdout FETCH_HEAD,"['heat/tests/utils.py', 'heat/db/sqlalchemy/migration.py', 'heat/tests/db/test_migrations.py', 'heat/db/sqlalchemy/models.py']",4,a7824c2b3d4b7bf8c32319f53a0453c99beca67b,use-metadata," auth_url = sqlalchemy.Column(sqlalchemy.Text) value = sqlalchemy.Column('value', sqlalchemy.Text)"," auth_url = sqlalchemy.Column(sqlalchemy.String) value = sqlalchemy.Column('value', sqlalchemy.String)",46,3
openstack%2Fnova~master~Ib7cb7a0926050e3d0873328f84e8554884a48468,openstack/nova,master,Ib7cb7a0926050e3d0873328f84e8554884a48468,Virt: change instance_type to flavor,MERGED,2014-11-18 14:32:40.000000000,2014-12-08 13:51:22.000000000,2014-12-08 13:51:19.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 5441}, {'_account_id': 6873}, {'_account_id': 8688}, {'_account_id': 9008}, {'_account_id': 9569}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-18 14:32:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/2fd7b60f5119a36199adc8bcc30cb0db7f30ab0d', 'message': 'Virt: change instance_type to flavor\n\nThe commit Ieebfcbc2478ba8d7036c1af30efbbf640aa7df73 made\nuse of instance_type. This should have been flavor.\n\nThis is related to blueprint flavor-instance-type-dedup.\n\nChange-Id: Ib7cb7a0926050e3d0873328f84e8554884a48468\n'}, {'number': 2, 'created': '2014-11-20 09:46:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/2b82699ad815fd70d0222d56f59b19fce52a3e0d', 'message': 'Virt: change instance_type to flavor\n\nThe commit Ieebfcbc2478ba8d7036c1af30efbbf640aa7df73 made\nuse of instance_type. This should have been flavor.\n\nThis is related to blueprint flavor-instance-type-dedup.\n\nChange-Id: Ib7cb7a0926050e3d0873328f84e8554884a48468\n'}, {'number': 3, 'created': '2014-11-23 09:17:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8c9fe0043b36bfa86b5f5c512125c31b8dd87ab1', 'message': 'Virt: change instance_type to flavor\n\nThe commit Ieebfcbc2478ba8d7036c1af30efbbf640aa7df73 made\nuse of instance_type. This should have been flavor.\n\nThis is related to blueprint flavor-instance-type-dedup.\n\nChange-Id: Ib7cb7a0926050e3d0873328f84e8554884a48468\n'}, {'number': 4, 'created': '2014-11-25 12:00:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ea2a1b133bcbe5183bf0afb2d983cc6984363267', 'message': 'Virt: change instance_type to flavor\n\nThe commit Ieebfcbc2478ba8d7036c1af30efbbf640aa7df73 made\nuse of instance_type. This should have been flavor.\n\nThis is related to blueprint flavor-instance-type-dedup.\n\nChange-Id: Ib7cb7a0926050e3d0873328f84e8554884a48468\n'}, {'number': 5, 'created': '2014-11-30 15:53:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d17638bbb18828334f232207645e4618316ed5f4', 'message': 'Virt: change instance_type to flavor\n\nThe commit Ieebfcbc2478ba8d7036c1af30efbbf640aa7df73 made\nuse of instance_type. This should have been flavor.\n\nThis is related to blueprint flavor-instance-type-dedup.\n\nChange-Id: Ib7cb7a0926050e3d0873328f84e8554884a48468\n'}, {'number': 6, 'created': '2014-12-06 17:11:35.000000000', 'files': ['nova/virt/hyperv/driver.py', 'nova/tests/unit/compute/test_compute_mgr.py', 'nova/virt/ironic/driver.py', 'nova/virt/libvirt/driver.py', 'nova/virt/xenapi/driver.py', 'nova/virt/fake.py', 'nova/virt/vmwareapi/driver.py', 'nova/tests/unit/compute/test_shelve.py', 'nova/tests/unit/compute/test_compute.py', 'nova/virt/driver.py', 'nova/compute/manager.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/34c5c649137d457fb726a3c2cac0458e4d368e89', 'message': 'Virt: change instance_type to flavor\n\nThe commit Ieebfcbc2478ba8d7036c1af30efbbf640aa7df73 made\nuse of instance_type. This should have been flavor.\n\nThis is related to blueprint flavor-instance-type-dedup.\n\nChange-Id: Ib7cb7a0926050e3d0873328f84e8554884a48468\n'}]",2,135302,34c5c649137d457fb726a3c2cac0458e4d368e89,58,13,6,1653,,,0,"Virt: change instance_type to flavor

The commit Ieebfcbc2478ba8d7036c1af30efbbf640aa7df73 made
use of instance_type. This should have been flavor.

This is related to blueprint flavor-instance-type-dedup.

Change-Id: Ib7cb7a0926050e3d0873328f84e8554884a48468
",git fetch https://review.opendev.org/openstack/nova refs/changes/02/135302/4 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/hyperv/driver.py', 'nova/tests/unit/compute/test_compute_mgr.py', 'nova/virt/ironic/driver.py', 'nova/virt/libvirt/driver.py', 'nova/virt/xenapi/driver.py', 'nova/virt/fake.py', 'nova/virt/vmwareapi/driver.py', 'nova/tests/unit/compute/test_shelve.py', 'nova/tests/unit/compute/test_compute.py', 'nova/virt/driver.py', 'nova/compute/manager.py']",11,2fd7b60f5119a36199adc8bcc30cb0db7f30ab0d,bp/flavor-instance-type-dedup," flavor = None if filter_properties is not None: flavor = filter_properties.get('instance_type') flavor=flavor) set_access_ip=False, flavor=None): flavor=flavor) flavor = None if filter_properties is not None: flavor = filter_properties.get('instance_type') flavor=flavor) flavor = None if filter_properties is not None: flavor = filter_properties.get('instance_type') flavor=flavor)"," instance_type = None if filter_properties is not None: instance_type = filter_properties.get('instance_type') instance_type=instance_type) set_access_ip=False, instance_type=None): instance_type=instance_type) instance_type = None if filter_properties is not None: instance_type = filter_properties.get('instance_type') instance_type=instance_type) instance_type = None if filter_properties is not None: instance_type = filter_properties.get('instance_type') instance_type=instance_type)",30,30
openstack%2Fpython-magnumclient~master~I2fab0ba4e8797bd53f4abbec41e52d41d37e4ecd,openstack/python-magnumclient,master,I2fab0ba4e8797bd53f4abbec41e52d41d37e4ecd,Add node object to the python client,MERGED,2014-12-07 17:01:51.000000000,2014-12-08 13:49:26.000000000,2014-12-08 13:49:25.000000000,"[{'_account_id': 3}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-12-07 17:01:51.000000000', 'files': ['magnumclient/api/nodes.py', 'magnumclient/api/shell.py', 'magnumclient/api/client.py'], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/cc477295d784cd1e26184d894ad9bb8c38e44c11', 'message': 'Add node object to the python client\n\nChange-Id: I2fab0ba4e8797bd53f4abbec41e52d41d37e4ecd\n'}]",0,139867,cc477295d784cd1e26184d894ad9bb8c38e44c11,6,2,1,2834,,,0,"Add node object to the python client

Change-Id: I2fab0ba4e8797bd53f4abbec41e52d41d37e4ecd
",git fetch https://review.opendev.org/openstack/python-magnumclient refs/changes/67/139867/1 && git format-patch -1 --stdout FETCH_HEAD,"['magnumclient/api/nodes.py', 'magnumclient/api/shell.py', 'magnumclient/api/client.py']",3,cc477295d784cd1e26184d894ad9bb8c38e44c11,,from magnumclient.api import nodes self.nodes = nodes.NodeManager(self.http_client),,128,0
openstack%2Fpython-magnumclient~master~I17c77c5f6133734fb72e3a288de79e15d5e1cb68,openstack/python-magnumclient,master,I17c77c5f6133734fb72e3a288de79e15d5e1cb68,Add image_id and node_count to bay-create,MERGED,2014-12-05 00:55:04.000000000,2014-12-08 13:48:49.000000000,2014-12-08 13:48:48.000000000,"[{'_account_id': 3}, {'_account_id': 5638}, {'_account_id': 7770}]","[{'number': 1, 'created': '2014-12-05 00:55:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/2b8862ed59067050ee5bff99920bfad55ff379dd', 'message': 'Add image_id and node_count to bay-create\n\nimage_id specifies the image to load from glance and node_count identifies\nthe number of nodes to start with the image.\n\nChange-Id: I17c77c5f6133734fb72e3a288de79e15d5e1cb68\n'}, {'number': 2, 'created': '2014-12-05 16:53:18.000000000', 'files': ['magnumclient/api/bays.py', 'magnumclient/api/shell.py'], 'web_link': 'https://opendev.org/openstack/python-magnumclient/commit/37846df50241ea563f200951dfe8b829cbe00ac3', 'message': 'Add image_id and node_count to bay-create\n\nimage_id specifies the image to load from glance and node_count identifies\nthe number of nodes to start with the image.\n\nChange-Id: I17c77c5f6133734fb72e3a288de79e15d5e1cb68\n'}]",0,139290,37846df50241ea563f200951dfe8b829cbe00ac3,9,3,2,2834,,,0,"Add image_id and node_count to bay-create

image_id specifies the image to load from glance and node_count identifies
the number of nodes to start with the image.

Change-Id: I17c77c5f6133734fb72e3a288de79e15d5e1cb68
",git fetch https://review.opendev.org/openstack/python-magnumclient refs/changes/90/139290/1 && git format-patch -1 --stdout FETCH_HEAD,"['magnumclient/api/bays.py', 'magnumclient/api/shell.py']",2,2b8862ed59067050ee5bff99920bfad55ff379dd,," print 'bays %s' % bays columns = ('uuid', 'name', 'type', 'image_id', 'node_count')@utils.arg('--image_id', metavar='<image_id>', help='The name or UUID of the base image to customize for the bay.') @utils.arg('--node_count', metavar='<node_count>', help='The bay node count.') opts['image_id'] = args.image_id opts['node_count'] = args.node_count print 'bay is %s' % bay"," columns = ('uuid', 'name', 'type')",12,3
openstack%2Fmagnum~master~I9eba14232c193688347b7ec2674c83f230404ae1,openstack/magnum,master,I9eba14232c193688347b7ec2674c83f230404ae1,Add a node object,MERGED,2014-12-07 16:59:57.000000000,2014-12-08 13:48:22.000000000,2014-12-08 13:48:21.000000000,"[{'_account_id': 3}, {'_account_id': 5638}, {'_account_id': 12385}]","[{'number': 1, 'created': '2014-12-07 16:59:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/4ceba2832f1be2477e069d6d1cac580c8ce986e6', 'message': 'Add a node object\n\nThe node object represents either a bare metal or virtual machine node\nthat is provisioned with an OS to run the containers, or alternatively,\nrun kubernetes.\n\nChange-Id: I9eba14232c193688347b7ec2674c83f230404ae1\n'}, {'number': 2, 'created': '2014-12-07 17:07:52.000000000', 'files': ['magnum/objects/__init__.py', 'magnum/db/sqlalchemy/api.py', 'README.rst', 'magnum/tests/test_functional.py', 'magnum/objects/node.py', 'magnum/api/controllers/v1/__init__.py', 'magnum/db/api.py', 'magnum/db/sqlalchemy/alembic/versions/2581ebaf0cb2_initial_migration.py', 'magnum/api/controllers/v1/node.py', 'magnum/db/sqlalchemy/models.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/39a1ed9f5ab882d9be9500bcc9128e7bd3e9270b', 'message': 'Add a node object\n\nThe node object represents either a bare metal or virtual machine node\nthat is provisioned with an OS to run the containers, or alternatively,\nrun kubernetes.\n\nChange-Id: I9eba14232c193688347b7ec2674c83f230404ae1\n'}]",0,139866,39a1ed9f5ab882d9be9500bcc9128e7bd3e9270b,9,3,2,2834,,,0,"Add a node object

The node object represents either a bare metal or virtual machine node
that is provisioned with an OS to run the containers, or alternatively,
run kubernetes.

Change-Id: I9eba14232c193688347b7ec2674c83f230404ae1
",git fetch https://review.opendev.org/openstack/magnum refs/changes/66/139866/2 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/objects/__init__.py', 'magnum/db/sqlalchemy/api.py', 'README.rst', 'magnum/tests/test_functional.py', 'magnum/objects/node.py', 'magnum/api/controllers/v1/__init__.py', 'magnum/db/api.py', 'magnum/db/sqlalchemy/alembic/versions/2581ebaf0cb2_initial_migration.py', 'magnum/api/controllers/v1/node.py', 'magnum/db/sqlalchemy/models.py']",10,4ceba2832f1be2477e069d6d1cac580c8ce986e6,,"class Node(Base): """"""Represents a node."""""" __tablename__ = 'node' __table_args__ = ( schema.UniqueConstraint('uuid', name='uniq_node0uuid'), table_args() ) id = Column(Integer, primary_key=True) uuid = Column(String(36)) type = Column(String(20)) image_id = Column(String(255)) ironic_node_id = Column(String(36)) ",,850,3
openstack%2Fnova~master~I85eb3c1347d057d1f292e747f950065b8f394147,openstack/nova,master,I85eb3c1347d057d1f292e747f950065b8f394147,Support macvtap for vif_type being hw_veb,MERGED,2014-10-06 17:01:41.000000000,2014-12-08 13:43:55.000000000,2014-12-08 13:43:52.000000000,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 1561}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 2750}, {'_account_id': 4187}, {'_account_id': 4393}, {'_account_id': 4395}, {'_account_id': 4727}, {'_account_id': 5170}, {'_account_id': 6598}, {'_account_id': 6685}, {'_account_id': 7543}, {'_account_id': 8247}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 12441}]","[{'number': 1, 'created': '2014-10-06 17:01:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/2206448ab5cc7db6bc9af4e63bca299621b2b32e', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\n'}, {'number': 2, 'created': '2014-10-06 17:08:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/6e1353005025e432fde71fe1c72614c258bbe7f1', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\nCo-Authored-By: Itzik Brown <ItzikB@mellanox.com>\n'}, {'number': 3, 'created': '2014-10-08 14:27:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1a40bfa346a8a67809758fdc93b01417cc0a5688', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\nCo-Authored-By: Itzik Brown <ItzikB@mellanox.com>\n'}, {'number': 4, 'created': '2014-10-09 15:55:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d977bfad300a3f1c9a50377a27a826138e565e9e', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\nCo-Authored-By: Itzik Brown <ItzikB@mellanox.com>\n'}, {'number': 5, 'created': '2014-10-10 12:02:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/910836e453f242b485d90adcad3ec82b2ae2437a', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\nCo-Authored-By: Itzik Brown <ItzikB@mellanox.com>\n'}, {'number': 6, 'created': '2014-10-14 19:12:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ba931f334c5f00c98562d38749696a8fe44016e2', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\nCo-Authored-By: Itzik Brown <ItzikB@mellanox.com>\n'}, {'number': 7, 'created': '2014-10-24 19:33:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c8b678f2e3de66c62cbc3a0fc1c41b21a482d0f4', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\nCo-Authored-By: Itzik Brown <ItzikB@mellanox.com>\n'}, {'number': 8, 'created': '2014-10-24 19:42:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e154a9e05c06495ccde3dae1db2b1c8ecc710af9', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\nCo-Authored-By: Itzik Brown <ItzikB@mellanox.com>\n'}, {'number': 9, 'created': '2014-10-30 13:49:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ba0075bd36bdfa762314e11d1af81023ddeac4df', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\nCo-Authored-By: Itzik Brown <ItzikB@mellanox.com>\n'}, {'number': 10, 'created': '2014-10-30 21:19:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b59e7e7527fdbff257782118207d49bc4859f41e', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\nCo-Authored-By: Itzik Brown <ItzikB@mellanox.com>\n'}, {'number': 11, 'created': '2014-11-13 13:47:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b2b039872781d448dd2bb78492d90c991d3400ed', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\nCo-Authored-By: Itzik Brown <ItzikB@mellanox.com>\n'}, {'number': 12, 'created': '2014-11-14 17:11:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/cdf7006a973f136b2e9f4f0298151836b3b90cea', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\nCo-Authored-By: Itzik Brown <ItzikB@mellanox.com>\n'}, {'number': 13, 'created': '2014-12-06 13:40:47.000000000', 'files': ['nova/pci/utils.py', 'nova/virt/libvirt/vif.py', 'nova/network/linux_net.py', 'nova/virt/libvirt/designer.py', 'nova/tests/unit/virt/libvirt/test_vif.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/386e38198d63cf2dc45507ca7e4dc82f0bcd9bb9', 'message': 'Support macvtap for vif_type being hw_veb\n\nThis patch programs VLAN into a VF pci device, and properly\nplug/unplug it into the instance.\n\nChange-Id: I85eb3c1347d057d1f292e747f950065b8f394147\nCloses-Bug: 1370348\nCo-Authored-By: Itzik Brown <ItzikB@mellanox.com>\n'}]",33,126357,386e38198d63cf2dc45507ca7e4dc82f0bcd9bb9,120,21,13,6685,,,0,"Support macvtap for vif_type being hw_veb

This patch programs VLAN into a VF pci device, and properly
plug/unplug it into the instance.

Change-Id: I85eb3c1347d057d1f292e747f950065b8f394147
Closes-Bug: 1370348
Co-Authored-By: Itzik Brown <ItzikB@mellanox.com>
",git fetch https://review.opendev.org/openstack/nova refs/changes/57/126357/3 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/libvirt/test_vif.py', 'nova/virt/libvirt/vif.py', 'nova/network/linux_net.py', 'nova/virt/libvirt/designer.py', 'nova/pci/pci_utils.py']",5,2206448ab5cc7db6bc9af4e63bca299621b2b32e,bug/1370348,"import globdef get_ifname_by_pci_address(pci_addr, pf_interface=False): """"""Get the interface name based on a VF's pci address The returned interface name is either the parent PF's or that of the VF itself based on the argument of pf_interface. """""" if pf_interface: dev_path = ""/sys/bus/pci/devices/%s/physfn/net"" % (pci_addr) else: dev_path = ""/sys/bus/pci/devices/%s/net"" % (pci_addr) def get_vf_num_by_pci_address(pci_addr): """"""Get the VF number based on a VF's pci address A VF is associated with an VF number, which ip link command uses to configure it. This number can be obtained from the PCI device filesystem. """""" VIRTFN_RE = re.compile(""virtfn(\d+)"") virtfns_path = ""/sys/bus/pci/devices/%s/physfn/virtfn*"" % (pci_addr) try: for vf_path in glob.iglob(virtfns_path): if re.search(pci_addr, os.readlink(vf_path)): t = VIRTFN_RE.search(vf_path) return t.group(1) except Exception: LOG.error(_LE(""PCI device %s not found"") % pci_addr) return None return","def get_ifname_by_pci_address(pci_addr): dev_path = ""/sys/bus/pci/devices/%s/net"" % (pci_addr)",133,7
openstack%2Fmurano-dashboard~master~Ie0aad70bb32b52f103dffeeb2795c9b94f025e95,openstack/murano-dashboard,master,Ie0aad70bb32b52f103dffeeb2795c9b94f025e95,Add test on upload package,MERGED,2014-09-05 11:21:40.000000000,2014-12-08 13:42:15.000000000,2014-12-08 13:42:15.000000000,"[{'_account_id': 3}, {'_account_id': 7225}, {'_account_id': 7226}, {'_account_id': 7549}, {'_account_id': 7562}, {'_account_id': 7600}, {'_account_id': 7821}, {'_account_id': 8040}, {'_account_id': 8824}, {'_account_id': 13149}]","[{'number': 1, 'created': '2014-09-05 11:21:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/8e19a5740914b66df5494cf4b7bc24c0ddd63707', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 2, 'created': '2014-09-09 09:42:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/6d82f4ae66fd58582b7cd3506c76f999eafd83cb', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 3, 'created': '2014-09-09 09:45:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/4051afcf6ecaf13edeaaaee9ce94e2b8353974a7', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 4, 'created': '2014-09-09 12:59:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/ecbb0099f98caa350fc160f714d5d5739c14ac06', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 5, 'created': '2014-09-09 13:44:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/a971bda69962bfdf471cb9efda202ba515b78689', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 6, 'created': '2014-09-09 13:45:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/4768d88bd5d668519be79a542181dccb1ebc4f73', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 7, 'created': '2014-09-10 07:32:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/2cf0758b1e02408fa3588d495921d4b1d52feabd', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 8, 'created': '2014-09-10 13:08:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/0b926dabf7ad8ea325afea59b491547c659c1314', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 9, 'created': '2014-09-18 08:16:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/7a7070e33f035b1d5b397340a45bbe99dc931bdb', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 10, 'created': '2014-09-25 09:19:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/dd247d44fd5a11da06f784ee5ad2f50260d87f9d', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 11, 'created': '2014-10-06 13:22:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/de58a435d9bcf9ae2bb4010609cbfe817ea50e39', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 12, 'created': '2014-12-04 17:10:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/dfa56219a421251bebf394047562ab1be2d587a3', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 13, 'created': '2014-12-05 10:19:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/d6820a70da51ee4ba54d9df7d8b5581123cdcb46', 'message': 'Add test on upload package\n\nPartially implements: blueprint dashboard-package-upload-checks\n\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 14, 'created': '2014-12-06 08:02:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/ee37bb34877d47122416c568e97c419614cb7e97', 'message': 'Add test on upload package\n\nSplit packages related test cases to a separate class\nPartially implements: blueprint dashboard-package-upload-checks\n\nCo-Authored-By: Ekaterina Chernova <efedorova@mirantis.com>\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 15, 'created': '2014-12-06 08:36:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/646a085533f294f79b3e75d32de858b74748c061', 'message': 'Add test on upload package\n\nSplit packages related test cases to a separate class\nPartially implements: blueprint dashboard-package-upload-checks\n\nCo-Authored-By: Ekaterina Chernova <efedorova@mirantis.com>\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 16, 'created': '2014-12-06 08:58:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/50b8308de44ce96f3062b63f5188cc7b8dbcade4', 'message': 'Add test on upload package\n\nSplit packages related test cases to a separate class\nPartially implements: blueprint dashboard-package-upload-checks\n\nCo-Authored-By: Ekaterina Chernova <efedorova@mirantis.com>\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}, {'number': 17, 'created': '2014-12-06 11:27:14.000000000', 'files': ['muranodashboard/tests/functional/utils.py', 'muranodashboard/tests/functional/sanity_check.py', 'muranodashboard/tests/functional/consts.py', 'muranodashboard/tests/functional/base.py'], 'web_link': 'https://opendev.org/openstack/murano-dashboard/commit/2950730f18d27b6b400437e53854f72337a15b93', 'message': 'Add test on upload package\n\nSplit packages related test cases to a separate class\nPartially implements: blueprint dashboard-package-upload-checks\n\nCo-Authored-By: Ekaterina Chernova <efedorova@mirantis.com>\nChange-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95\n'}]",16,119349,2950730f18d27b6b400437e53854f72337a15b93,96,10,17,8824,,,0,"Add test on upload package

Split packages related test cases to a separate class
Partially implements: blueprint dashboard-package-upload-checks

Co-Authored-By: Ekaterina Chernova <efedorova@mirantis.com>
Change-Id: Ie0aad70bb32b52f103dffeeb2795c9b94f025e95
",git fetch https://review.opendev.org/openstack/murano-dashboard refs/changes/49/119349/17 && git format-patch -1 --stdout FETCH_HEAD,"['muranodashboard/tests/functional/utils.py', 'muranodashboard/tests/functional/sanity_check.py', 'muranodashboard/tests/functional/base.py']",3,8e19a5740914b66df5494cf4b7bc24c0ddd63707,bug/1394572," class PackageTestBase(UITestCase): @classmethod def setUpClass(cls): super(PackageTestBase, cls).setUpClass() package_dir = os.path.join(os.path.dirname( os.path.realpath(__file__)), 'MockApp') cls.manifest = os.path.join(package_dir, 'manifest.yaml') utils.prepare_manifest(""MockApp"", cls.manifest) cls.archive = utils.compose_package(""MockApp"", package_dir) @classmethod def tearDownClass(cls): super(PackageTestBase, cls).tearDownClass() os.remove(cls.manifest) os.remove(cls.archive) for package in cls.murano_client.packages.list(): if package.name == 'MockApp': cls.murano_client.packages.delete(package.id)",,75,20
openstack%2Fmanila~master~I1ba612428f7bbf50674a1d101fbd87a8e9da48e8,openstack/manila,master,I1ba612428f7bbf50674a1d101fbd87a8e9da48e8,Add support of new driver interfaces to share manager,ABANDONED,2014-12-05 19:52:42.000000000,2014-12-08 13:33:29.000000000,,"[{'_account_id': 3}, {'_account_id': 6491}, {'_account_id': 8851}, {'_account_id': 11878}]","[{'number': 1, 'created': '2014-12-05 19:52:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/d24156d77a0492c62f04e8186325a5d939dd063a', 'message': 'Add support of new driver interface\n\nTo support network helpers we need to remove network related code from\nshare manager and drivers to separate helpers.\nTo allow do it smoothly we need to implement another driver interfaces which\nwill be used for network helpers support.\n\nPartially implements bp network-helper\n\nChange-Id: I1ba612428f7bbf50674a1d101fbd87a8e9da48e8\n'}, {'number': 2, 'created': '2014-12-06 13:32:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/a705cf82fa1b8343026e6685eeadda149d446c58', 'message': 'Add support of new driver interfaces to share manager\n\nTo support network helpers we need to move network related code from\nshare manager to drivers so they will be able to use any separate network\nhelpers.\n\nPartially implements bp network-helper\nChange-Id: I1ba612428f7bbf50674a1d101fbd87a8e9da48e8\n'}, {'number': 3, 'created': '2014-12-06 13:40:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/2ceb06fc6a57d19e16af92d10ecb8a8e66f29283', 'message': 'Add support of new driver interfaces to share manager\n\nTo support network helpers we need to move network related code from\nshare manager to drivers so they will be able to use any separate network\nhelpers.\n\nPartially implements bp network-helper\nChange-Id: I1ba612428f7bbf50674a1d101fbd87a8e9da48e8\n'}, {'number': 4, 'created': '2014-12-06 14:00:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/8bcf5b8405d0a05acd662e2c02c0d43c0af2747f', 'message': 'Add support of new driver interfaces to share manager\n\nTo support network helpers we need to move network related code from\nshare manager to drivers so they will be able to use any separate network\nhelpers.\n\nPartially implements bp network-helper\nChange-Id: I1ba612428f7bbf50674a1d101fbd87a8e9da48e8\n'}, {'number': 5, 'created': '2014-12-06 14:13:59.000000000', 'files': ['manila/share/manager.py', 'manila/share/driver.py', 'manila/tests/share/test_manager.py'], 'web_link': 'https://opendev.org/openstack/manila/commit/f8c60867ec01b2e6d4467d1c364ede21502cab93', 'message': 'Add support of new driver interfaces to share manager\n\nTo support network helpers we need to move network related code from\nshare manager to drivers so they will be able to use any separate network\nhelpers.\n\nPartially implements bp network-helper\nChange-Id: I1ba612428f7bbf50674a1d101fbd87a8e9da48e8\n'}]",18,139720,f8c60867ec01b2e6d4467d1c364ede21502cab93,12,4,5,8851,,,0,"Add support of new driver interfaces to share manager

To support network helpers we need to move network related code from
share manager to drivers so they will be able to use any separate network
helpers.

Partially implements bp network-helper
Change-Id: I1ba612428f7bbf50674a1d101fbd87a8e9da48e8
",git fetch https://review.opendev.org/openstack/manila refs/changes/20/139720/4 && git format-patch -1 --stdout FETCH_HEAD,"['manila/share/manager.py', 'manila/tests/share/test_manager.py']",2,d24156d77a0492c62f04e8186325a5d939dd063a,bp/network-helper," def delete_share_last_on_server_v2(self, with_sec_services): if with_sec_services: sec_service = self._create_security_service(share_net['id']) host=self.share_manager.host, share = self._create_share( share_network_id=share_net['id'], share_server_id=share_srv['id'], ) self.stubs.Set( self.share_manager.driver, 'teardown_server', mock.Mock()) try: self.share_manager.driver.teardown_server_v2 = mock.Mock() self.share_manager.delete_share(self.context, share['id']) self.assertTrue(self.share_manager.driver.teardown_server_v2.called) self.assertFalse(self.share_manager.driver.teardown_server.called) call_args = self.share_manager.driver.teardown_server_v2.call_args[0] call_kwargs = self.share_manager.driver.teardown_server_v2.call_args[1] self.assertEqual(share_srv['id'], call_args[0]['id']) if with_sec_services: self.assertEqual(1, len(call_kwargs['security_services'])) self.assertTrue( sec_service['id'], call_kwargs['security_services'][0]['id'], ) else: self.assertEqual([], call_kwargs['security_services']) finally: del self.share_manager.driver.teardown_server_v2 def test_delete_share_last_on_server_v2_with_sec_services(self): self.delete_share_last_on_server_v2(True) def test_delete_share_last_on_server_v2(self): self.delete_share_last_on_server_v2(False) def delete_share_last_on_server(self, with_sec_services): manager.CONF.delete_share_server_with_last_share = True share_net = self._create_share_network() if with_sec_services: sec_service = self._create_security_service(share_net['id']) share_srv = self._create_share_server( share_network_id=share_net['id'], host=self.share_manager.host, ) share = self._create_share( share_network_id=share_net['id'], share_server_id=share_srv['id'], ) self.stubs.Set( self.share_manager.driver, 'teardown_server', mock.Mock()) self.share_manager.delete_share(self.context, share['id']) self.assertEqual(call_args[0], share_srv.get('backend_details')) if with_sec_services: self.assertEqual( len(call_kwargs['security_services']), 1) self.assertTrue( call_kwargs['security_services'][0]['id'], sec_service['id']) else: self.assertEqual([], call_kwargs['security_services']) def test_delete_share_last_on_server_with_sec_services(self): self.delete_share_last_on_server(True) self.delete_share_last_on_server(False) def test_setup_server_using_driver_v2(self): share_server = {'id': 'fake_share_server_id'} metadata = 'fake_metadata' try: self.share_manager.driver.setup_server_v2 = mock.Mock() self.share_manager._setup_server(self.context, share_server, metadata) self.share_manager.driver.setup_server_v2.assert_called_once_with( share_server, metadata=metadata) finally: del self.share_manager.driver.setup_server_v2 def test_setup_server_using_driver_v2_error(self): share_server = {'id': 'fake_share_server_id'} metadata = 'fake_metadata' self.stubs.Set( self.share_manager.db, 'share_server_update', mock.Mock()) try: self.share_manager.driver.setup_server_v2 = mock.Mock( side_effect=Exception()) self.assertRaises( Exception, self.share_manager._setup_server, self.context, share_server, metadata, ) self.share_manager.driver.setup_server_v2.assert_called_once_with( share_server, metadata=metadata) self.share_manager.db.share_server_update.assert_has_calls([]) finally: del self.share_manager.driver.setup_server_v2"," def test_delete_share_last_on_server_with_sec_services(self): sec_service = self._create_security_service(share_net['id']) host=self.share_manager.host share = self._create_share(share_network_id=share_net['id'], share_server_id=share_srv['id']) share_id = share['id'] self.share_manager.driver = mock.Mock() self.share_manager.delete_share(self.context, share_id) self.assertEqual( call_args[0], share_srv.get('backend_details')) self.assertEqual( len(call_kwargs['security_services']), 1) self.assertTrue( call_kwargs['security_services'][0]['id'], sec_service['id']) share_net = self._create_share_network() share_srv = self._create_share_server( share_network_id=share_net['id'], host=self.share_manager.host ) share = self._create_share(share_network_id=share_net['id'], share_server_id=share_srv['id']) share_id = share['id'] self.share_manager.driver = mock.Mock() manager.CONF.delete_share_server_with_last_share = True self.share_manager.delete_share(self.context, share_id) self.share_manager.driver.teardown_server.assert_called_once_with( share_srv.get('backend_details'), security_services=[] )",130,57
openstack%2Ffuel-main~master~I1c59e8e580441fb845b8b47f4a5fc9bcde90be9b,openstack/fuel-main,master,I1c59e8e580441fb845b8b47f4a5fc9bcde90be9b,Fix search for devops node by fqdn,MERGED,2014-11-20 14:56:40.000000000,2014-12-08 13:28:03.000000000,2014-12-08 13:28:03.000000000,"[{'_account_id': 3}, {'_account_id': 6719}, {'_account_id': 8882}, {'_account_id': 8971}, {'_account_id': 9439}, {'_account_id': 10136}, {'_account_id': 11081}, {'_account_id': 11969}]","[{'number': 1, 'created': '2014-11-20 14:56:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-main/commit/706d578c49bd80b0012ce350e69a6eb4f87938d8', 'message': 'Fix search for devops node by fqdn\n\n- Nailgun fqdn is now full node hostname both on\nUbuntu and CentOS so search for devops node by\nnailgun fqdn was changed in test\n\nChange-Id: I1c59e8e580441fb845b8b47f4a5fc9bcde90be9b\nCloses-Bug: #1394587\n'}, {'number': 2, 'created': '2014-12-01 07:53:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-main/commit/d3cddd383c5221b43848457f3ec5e5a3dc25132f', 'message': 'Fix search for devops node by fqdn\n\n- Nailgun fqdn is now full node hostname both on\nUbuntu and CentOS so search for devops node by\nnailgun fqdn was changed in test\n\nChange-Id: I1c59e8e580441fb845b8b47f4a5fc9bcde90be9b\nCloses-Bug: #1394587\n'}, {'number': 3, 'created': '2014-12-02 16:18:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-main/commit/7b056c7d2e95101f5e15ec5b5a1a949eca4acb63', 'message': 'Fix search for devops node by fqdn\n\n- Nailgun fqdn is now full node hostname both on\nUbuntu and CentOS so search for devops node by\nnailgun fqdn was changed in test\n\nChange-Id: I1c59e8e580441fb845b8b47f4a5fc9bcde90be9b\nCloses-Bug: #1394587\n'}, {'number': 4, 'created': '2014-12-03 12:40:14.000000000', 'files': ['fuelweb_test/models/fuel_web_client.py', 'fuelweb_test/tests/tests_strength/test_neutron.py'], 'web_link': 'https://opendev.org/openstack/fuel-main/commit/18cbbd298511cfa6286a693146d711a1ed83b1f1', 'message': 'Fix search for devops node by fqdn\n\n- Nailgun fqdn is now full node hostname both on\nUbuntu and CentOS so search for devops node by\nnailgun fqdn was changed in test\n\nChange-Id: I1c59e8e580441fb845b8b47f4a5fc9bcde90be9b\nCloses-Bug: #1394587\n'}]",6,136016,18cbbd298511cfa6286a693146d711a1ed83b1f1,31,8,4,10136,,,0,"Fix search for devops node by fqdn

- Nailgun fqdn is now full node hostname both on
Ubuntu and CentOS so search for devops node by
nailgun fqdn was changed in test

Change-Id: I1c59e8e580441fb845b8b47f4a5fc9bcde90be9b
Closes-Bug: #1394587
",git fetch https://review.opendev.org/openstack/fuel-main refs/changes/16/136016/4 && git format-patch -1 --stdout FETCH_HEAD,['fuelweb_test/tests/tests_strength/test_neutron.py'],1,706d578c49bd80b0012ce350e69a6eb4f87938d8,fixL3reschedulingTest," logger.debug('node name with dhcp is {0}'.format(node)) node, self.env.nodes().slaves[0:6]) logger.debug(""new node with l3 is {0}"".format(node_with_l3)) node_with_l3, self.env.nodes().slaves[0:6])"," node_fqdn = node.split('.')[0] logger.debug('node name with dhcp is {0}'.format(node_fqdn)) node_fqdn, self.env.nodes().slaves[0:6]) node_with_l3_fqdn = node_with_l3.split('.')[0] logger.debug(""new node with l3 is {0}"".format(node_with_l3_fqdn)) node_with_l3_fqdn, self.env.nodes().slaves[0:6])",4,6
openstack%2Fnova~master~Ic8d6beca1e60d944807f27c1a234ccf02094985c,openstack/nova,master,Ic8d6beca1e60d944807f27c1a234ccf02094985c,Add 'service' parameter to 'os-host' API as a filter option,ABANDONED,2014-06-24 07:52:04.000000000,2014-12-08 13:21:20.000000000,,"[{'_account_id': 3}, {'_account_id': 1313}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 5292}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-06-24 07:52:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/54cb16af27089b204fa5cf16f72ab3be752c0f83', 'message': ""Add 'service' parameter to 'os-host' API as a filter option\n\nSupport listing hosts by specify a specific services as filter.\nCloses-Bug: # 1333557\n\nChange-Id: Ic8d6beca1e60d944807f27c1a234ccf02094985c\n""}, {'number': 2, 'created': '2014-11-07 14:21:36.000000000', 'files': ['nova/api/openstack/compute/contrib/hosts.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/58a849aabcaed7da8ac0bc5dfa47b7b2ba643693', 'message': ""Add 'service' parameter to 'os-host' API as a filter option\n\nSupport listing hosts by specify a specific services as filter.\nCloses-Bug: # 1333557\n\nChange-Id: Ic8d6beca1e60d944807f27c1a234ccf02094985c\n""}]",0,102141,58a849aabcaed7da8ac0bc5dfa47b7b2ba643693,24,9,2,1313,,,0,"Add 'service' parameter to 'os-host' API as a filter option

Support listing hosts by specify a specific services as filter.
Closes-Bug: # 1333557

Change-Id: Ic8d6beca1e60d944807f27c1a234ccf02094985c
",git fetch https://review.opendev.org/openstack/nova refs/changes/41/102141/2 && git format-patch -1 --stdout FETCH_HEAD,['nova/api/openstack/compute/contrib/hosts.py'],1,54cb16af27089b204fa5cf16f72ab3be752c0f83,bug/1333557, service = req.GET.get('service') if service: filters['topic'] = service,,3,0
openstack%2Fdesignate~master~I0e797edb9611ca243f0e1f391cdf0444d1df92eb,openstack/designate,master,I0e797edb9611ca243f0e1f391cdf0444d1df92eb,Add bashate tox env for DevStack plugin,MERGED,2014-11-28 18:30:19.000000000,2014-12-08 13:16:33.000000000,2014-12-08 13:16:33.000000000,"[{'_account_id': 3}, {'_account_id': 395}, {'_account_id': 741}, {'_account_id': 8094}, {'_account_id': 8099}]","[{'number': 1, 'created': '2014-11-28 18:30:19.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/designate/commit/79bbf5bd7ac749ce5e51d166196658ee480ad40b', 'message': 'Add bashate tox env for DevStack plugin\n\nChange-Id: I0e797edb9611ca243f0e1f391cdf0444d1df92eb\n'}]",0,137844,79bbf5bd7ac749ce5e51d166196658ee480ad40b,14,5,1,741,,,0,"Add bashate tox env for DevStack plugin

Change-Id: I0e797edb9611ca243f0e1f391cdf0444d1df92eb
",git fetch https://review.opendev.org/openstack/designate refs/changes/44/137844/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,79bbf5bd7ac749ce5e51d166196658ee480ad40b,,"[testenv:bashate] deps = bashate whitelist_externals = bash commands = bash -c ""find {toxinidir}/contrib/devstack \ -not \( -type d -name .?\* -prune \) \ # prune all 'dot' dirs -not \( -type d -name doc -prune \) \ # skip documentation -type f \ # only files -not -name \*~ \ # skip editors, readme, etc -not -name \*.md \ \( \ -name \*.sh -or \ -name \*rc -or \ -name functions\* -or \ -wholename \*/lib/\* \ # /lib files are shell, but \) \ # have no extension -print0 | xargs -0 bashate -v"" ",,17,0
openstack%2Fmagnum~master~I0ac4c9189b973f6be40d00b026094c85d3858e57,openstack/magnum,master,I0ac4c9189b973f6be40d00b026094c85d3858e57,Use versioned objects over RPC - WIP,ABANDONED,2014-12-05 06:13:53.000000000,2014-12-08 13:10:55.000000000,,"[{'_account_id': 3}, {'_account_id': 4257}]","[{'number': 1, 'created': '2014-12-05 06:13:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/c4242cb2fbf14d3d4c06928b9dba66137aee41ca', 'message': 'WIP patch\n\nChange-Id: I0ac4c9189b973f6be40d00b026094c85d3858e57\n'}, {'number': 2, 'created': '2014-12-05 07:07:15.000000000', 'files': ['magnum/api/auth.py', 'magnum/cmd/backend.py', 'magnum/openstack/common/threadgroup.py', 'magnum/backend/manager.py', 'magnum/common/context-duh.py', 'magnum/common/rpc.py', 'magnum/openstack/common/loopingcall.py', 'magnum/common/rpc/__init__.py', 'magnum/cmd/db_manage.py', 'magnum/openstack/common/eventlet_backdoor.py', 'magnum/api/controllers/v1/__init__.py', 'magnum/api/controllers/v1/bay.py', 'magnum/common/config.py', 'magnum/common/rpc/service.py', 'magnum/common/service.py', 'magnum/openstack/common/periodic_task.py', 'magnum/common/context.py', 'magnum/openstack/common/service.py', 'magnum/openstack/common/importutils.py', 'magnum/backend/api.py', 'magnum/openstack/common/systemd.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/db0d80e14103ed0711545dae339fbaa6ba9e61d9', 'message': ""Use versioned objects over RPC - WIP\n\nInitial take at using versioned objects over RPC.  Currently the\nbackend doesn't actually respond for some reason.\n\nChange-Id: I0ac4c9189b973f6be40d00b026094c85d3858e57\n""}]",1,139528,db0d80e14103ed0711545dae339fbaa6ba9e61d9,6,2,2,2834,,,0,"Use versioned objects over RPC - WIP

Initial take at using versioned objects over RPC.  Currently the
backend doesn't actually respond for some reason.

Change-Id: I0ac4c9189b973f6be40d00b026094c85d3858e57
",git fetch https://review.opendev.org/openstack/magnum refs/changes/28/139528/2 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/api/auth.py', 'magnum/cmd/backend.py', 'magnum/openstack/common/threadgroup.py', 'magnum/backend/manager.py', 'magnum/common/context-duh.py', 'magnum/common/rpc.py', 'magnum/openstack/common/loopingcall.py', 'magnum/common/rpc/__init__.py', 'magnum/cmd/db_manage.py', 'magnum/openstack/common/eventlet_backdoor.py', 'magnum/api/controllers/v1/__init__.py', 'magnum/api/controllers/v1/bay.py', 'magnum/common/config.py', 'magnum/common/rpc/service.py', 'magnum/common/service.py', 'magnum/openstack/common/periodic_task.py', 'magnum/common/context.py', 'magnum/openstack/common/service.py', 'magnum/openstack/common/importutils.py', 'magnum/backend/api.py', 'magnum/openstack/common/systemd.py']",21,c4242cb2fbf14d3d4c06928b9dba66137aee41ca,,"# Copyright 2012-2014 Red Hat, Inc. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """""" Helper module for systemd service readiness notification. """""" import os import socket import sys from magnum.openstack.common import log as logging LOG = logging.getLogger(__name__) def _abstractify(socket_name): if socket_name.startswith('@'): # abstract namespace socket socket_name = '\0%s' % socket_name[1:] return socket_name def _sd_notify(unset_env, msg): notify_socket = os.getenv('NOTIFY_SOCKET') if notify_socket: sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM) try: sock.connect(_abstractify(notify_socket)) sock.sendall(msg) if unset_env: del os.environ['NOTIFY_SOCKET'] except EnvironmentError: LOG.debug(""Systemd notification failed"", exc_info=True) finally: sock.close() def notify(): """"""Send notification to Systemd that service is ready. For details see http://www.freedesktop.org/software/systemd/man/sd_notify.html """""" _sd_notify(False, 'READY=1') def notify_once(): """"""Send notification once to Systemd that service is ready. Systemd sets NOTIFY_SOCKET environment variable with the name of the socket listening for notifications from services. This method removes the NOTIFY_SOCKET environment variable to ensure notification is sent only once. """""" _sd_notify(True, 'READY=1') def onready(notify_socket, timeout): """"""Wait for systemd style notification on the socket. :param notify_socket: local socket address :type notify_socket: string :param timeout: socket timeout :type timeout: float :returns: 0 service ready 1 service not ready 2 timeout occurred """""" sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM) sock.settimeout(timeout) sock.bind(_abstractify(notify_socket)) try: msg = sock.recv(512) except socket.timeout: return 2 finally: sock.close() if 'READY=1' in msg: return 0 else: return 1 if __name__ == '__main__': # simple CLI for testing if len(sys.argv) == 1: notify() elif len(sys.argv) >= 2: timeout = float(sys.argv[1]) notify_socket = os.getenv('NOTIFY_SOCKET') if notify_socket: retval = onready(notify_socket, timeout) sys.exit(retval) ",,1853,217
openstack%2Fpython-saharaclient~master~Icb4cc9e79d5974c1824fe9b43f56134b704b6b2e,openstack/python-saharaclient,master,Icb4cc9e79d5974c1824fe9b43f56134b704b6b2e,Updated from global requirements,MERGED,2014-12-08 08:58:36.000000000,2014-12-08 13:06:50.000000000,2014-12-08 13:06:49.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}]","[{'number': 1, 'created': '2014-12-08 08:58:36.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/ec70c7f2ae455b4012036a2d5fdd5b9634e7d528', 'message': 'Updated from global requirements\n\nChange-Id: Icb4cc9e79d5974c1824fe9b43f56134b704b6b2e\n'}]",0,139946,ec70c7f2ae455b4012036a2d5fdd5b9634e7d528,7,3,1,11131,,,0,"Updated from global requirements

Change-Id: Icb4cc9e79d5974c1824fe9b43f56134b704b6b2e
",git fetch https://review.opendev.org/openstack/python-saharaclient refs/changes/46/139946/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,ec70c7f2ae455b4012036a2d5fdd5b9634e7d528,openstack/requirements,oslo.utils>=1.0.0 # Apache-2.0,oslo.utils>=1.0.0 # Apache-2.0,1,1
openstack%2Fnova~master~I54be56d631812be15c489df2b83fd1e750b9162b,openstack/nova,master,I54be56d631812be15c489df2b83fd1e750b9162b,Remove unnecessary timeutils override cleanup,MERGED,2014-12-05 23:11:18.000000000,2014-12-08 13:06:30.000000000,2014-12-08 13:06:26.000000000,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 13909}]","[{'number': 1, 'created': '2014-12-05 23:11:18.000000000', 'files': ['nova/tests/unit/scheduler/test_host_manager.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/90da3dddbba69f7146793c9b9596ef999c107c20', 'message': 'Remove unnecessary timeutils override cleanup\n\nThe test cases never setup an override in timeutils so the cleanup\nis unnecessary.\n\nChange-Id: I54be56d631812be15c489df2b83fd1e750b9162b\n'}]",0,139753,90da3dddbba69f7146793c9b9596ef999c107c20,14,10,1,100,,,0,"Remove unnecessary timeutils override cleanup

The test cases never setup an override in timeutils so the cleanup
is unnecessary.

Change-Id: I54be56d631812be15c489df2b83fd1e750b9162b
",git fetch https://review.opendev.org/openstack/nova refs/changes/53/139753/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/unit/scheduler/test_host_manager.py'],1,90da3dddbba69f7146793c9b9596ef999c107c20,unnecessary-timeutils-cleanup,,from oslo.utils import timeutils self.addCleanup(timeutils.clear_time_override) self.addCleanup(timeutils.clear_time_override),0,3
openstack%2Fnova~master~I637a1dd6301e5645a47dbd65026acfab83e6ae26,openstack/nova,master,I637a1dd6301e5645a47dbd65026acfab83e6ae26,Cleanup timeutils override in tests/functional/test_servers,MERGED,2014-12-05 22:58:27.000000000,2014-12-08 13:06:08.000000000,2014-12-08 13:06:05.000000000,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 1779}, {'_account_id': 4393}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-12-05 22:58:27.000000000', 'files': ['nova/tests/functional/test_servers.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/39e2a51e4d8817fe156a9a1b1471ecbb835326f3', 'message': 'Cleanup timeutils override in tests/functional/test_servers\n\nThe ServersTest._force_reclaim helper method sets up a timeutils\noverride but never removes it. This allows the override to potentially\nleak into other tests causing spurious failures in completely unrelated\ntests.\n\nChange-Id: I637a1dd6301e5645a47dbd65026acfab83e6ae26\nCloses-bug: 1399817\n'}]",0,139749,39e2a51e4d8817fe156a9a1b1471ecbb835326f3,12,9,1,100,,,0,"Cleanup timeutils override in tests/functional/test_servers

The ServersTest._force_reclaim helper method sets up a timeutils
override but never removes it. This allows the override to potentially
leak into other tests causing spurious failures in completely unrelated
tests.

Change-Id: I637a1dd6301e5645a47dbd65026acfab83e6ae26
Closes-bug: 1399817
",git fetch https://review.opendev.org/openstack/nova refs/changes/49/139749/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/functional/test_servers.py'],1,39e2a51e4d8817fe156a9a1b1471ecbb835326f3,bug/1399817, self.addCleanup(timeutils.clear_time_override),,1,0
openstack%2Fnova~master~I6a59d62962ff660aec4898ad525c4b3c3b04faba,openstack/nova,master,I6a59d62962ff660aec4898ad525c4b3c3b04faba,Downgrade quota exceeded log messages,MERGED,2014-12-05 18:55:45.000000000,2014-12-08 13:05:47.000000000,2014-12-08 13:05:44.000000000,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 13909}]","[{'number': 1, 'created': '2014-12-05 18:55:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/7e65b846bd6cb9d29ba964f103a178f79bdd8b6d', 'message': 'Downgrade quota exceeded log messages\n\nA user going over quota is a everyday occurrence, no need to log as a\nwarning. This information is only useful when trying to debug, so move\nto debug level.\n\nChange-Id: I6a59d62962ff660aec4898ad525c4b3c3b04faba\n'}, {'number': 2, 'created': '2014-12-05 21:43:33.000000000', 'files': ['nova/compute/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/2e69b0edfb276bdd00cfd0e007d2795f9f4bcd7c', 'message': 'Downgrade quota exceeded log messages\n\nA user going over quota is a everyday occurrence, no need to log as a\nwarning. This information is only useful when trying to debug, so move\nto debug level.\n\nChange-Id: I6a59d62962ff660aec4898ad525c4b3c3b04faba\n'}]",0,139710,2e69b0edfb276bdd00cfd0e007d2795f9f4bcd7c,23,11,2,1849,,,0,"Downgrade quota exceeded log messages

A user going over quota is a everyday occurrence, no need to log as a
warning. This information is only useful when trying to debug, so move
to debug level.

Change-Id: I6a59d62962ff660aec4898ad525c4b3c3b04faba
",git fetch https://review.opendev.org/openstack/nova refs/changes/10/139710/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/compute/api.py'],1,7e65b846bd6cb9d29ba964f103a178f79bdd8b6d,nowarn," LOG.debug((""%(overs)s quota exceeded for %(pid)s,"" LOG.debug((""%(overs)s quota exceeded for %(pid)s,"""," LOG.warn(_(""%(overs)s quota exceeded for %(pid)s,"" LOG.warn(_(""%(overs)s quota exceeded for %(pid)s,""",2,2
openstack%2Fnova~master~Id52e8837152fa8654131ca79e50582e03622b765,openstack/nova,master,Id52e8837152fa8654131ca79e50582e03622b765,Remove unused cinder code,MERGED,2014-12-05 18:29:17.000000000,2014-12-08 13:05:27.000000000,2014-12-08 13:05:23.000000000,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 1779}, {'_account_id': 5170}, {'_account_id': 8247}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-12-05 18:29:17.000000000', 'files': ['nova/tests/unit/volume/test_cinder.py', 'nova/volume/cinder.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/5d2b31284db6e1d8ff3c4a244d21bd57e1b04437', 'message': ""Remove unused cinder code\n\nThe removed functions aren't used anywhere except in unit tests, so just\nremove them.\n\nChange-Id: Id52e8837152fa8654131ca79e50582e03622b765\n""}]",0,139700,5d2b31284db6e1d8ff3c4a244d21bd57e1b04437,13,9,1,1849,,,0,"Remove unused cinder code

The removed functions aren't used anywhere except in unit tests, so just
remove them.

Change-Id: Id52e8837152fa8654131ca79e50582e03622b765
",git fetch https://review.opendev.org/openstack/nova refs/changes/00/139700/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/volume/test_cinder.py', 'nova/volume/cinder.py']",2,5d2b31284db6e1d8ff3c4a244d21bd57e1b04437,remove_useless_policy,," @translate_volume_exception def get_volume_metadata(self, context, volume_id): vol = cinderclient(context).volumes.get(volume_id) return vol.metadata @translate_volume_exception def delete_volume_metadata(self, context, volume_id, keys): cinderclient(context).volumes.delete_metadata(volume_id, keys) @translate_volume_exception def update_volume_metadata(self, context, volume_id, metadata, delete=False): if delete: # Completely replace volume metadata with one given return cinderclient(context).volumes.update_all_metadata( volume_id, metadata) else: return cinderclient(context).volumes.set_metadata( volume_id, metadata) @translate_volume_exception def get_volume_metadata_value(self, context, volume_id, key): vol = cinderclient(context).volumes.get(volume_id) return vol.metadata.get(key) ",0,105
openstack%2Fnova~master~If83cd8ee770d57547f4cfcdbba4a9fbfd55d5983,openstack/nova,master,If83cd8ee770d57547f4cfcdbba4a9fbfd55d5983,Remove needless workaround in utils module,MERGED,2014-12-05 16:13:29.000000000,2014-12-08 13:05:04.000000000,2014-12-08 13:05:01.000000000,"[{'_account_id': 3}, {'_account_id': 100}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 6062}, {'_account_id': 9008}, {'_account_id': 9420}, {'_account_id': 9578}, {'_account_id': 9708}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 11531}, {'_account_id': 13909}]","[{'number': 1, 'created': '2014-12-05 16:13:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/70a7bb4c0e12f302d9268dbc423f9133e70c44bd', 'message': 'Remove needless workaround in utils module\n\nWorkaround for bug in netaddr libary has been removed due the to fix\nwas presented in 0.7.6 and nova currently requires 0.7.12 or newer version.\n\nChange-Id: If83cd8ee770d57547f4cfcdbba4a9fbfd55d5983\n'}, {'number': 2, 'created': '2014-12-05 16:31:04.000000000', 'files': ['nova/utils.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/373c581433472b2486e3015e1982bee032ef5656', 'message': 'Remove needless workaround in utils module\n\nWorkaround for bug in netaddr library has been removed due to the fix\nwas introduced in 0.7.6 and nova currently requires 0.7.12 or newer version.\n\nChange-Id: If83cd8ee770d57547f4cfcdbba4a9fbfd55d5983\nRelated-Bug: #957708\n'}]",2,139669,373c581433472b2486e3015e1982bee032ef5656,29,15,2,13909,,,0,"Remove needless workaround in utils module

Workaround for bug in netaddr library has been removed due to the fix
was introduced in 0.7.6 and nova currently requires 0.7.12 or newer version.

Change-Id: If83cd8ee770d57547f4cfcdbba4a9fbfd55d5983
Related-Bug: #957708
",git fetch https://review.opendev.org/openstack/nova refs/changes/69/139669/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/utils.py'],1,70a7bb4c0e12f302d9268dbc423f9133e70c44bd,remove_fix_for_bug_957708,, except UnboundLocalError: # NOTE(MotoKen): work around bug in netaddr 0.7.5 (see detail in # https://github.com/drkjam/netaddr/issues/2) return False,0,4
openstack%2Ftripleo-heat-templates~master~I09f2eb278fa0eba1dff80884f12b6f682c7b0484,openstack/tripleo-heat-templates,master,I09f2eb278fa0eba1dff80884f12b6f682c7b0484,Remove missing cinder-storage Neutron* parameters,MERGED,2014-12-06 02:09:41.000000000,2014-12-08 13:04:12.000000000,2014-12-08 13:04:12.000000000,"[{'_account_id': 3}, {'_account_id': 4328}, {'_account_id': 4330}, {'_account_id': 6796}, {'_account_id': 7585}]","[{'number': 1, 'created': '2014-12-06 02:09:41.000000000', 'files': ['overcloud-without-mergepy.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/b91f8cd4928d82a84ac291cd5320b9952a01c4ae', 'message': 'Remove missing cinder-storage Neutron* parameters\n\nIn I00af10e07feed6c9c97ee6cad545dbff88cd6afc we removed the\nNeutron* parameters from cinder-storage.yaml but we forgot to\nalso remove them from overcloud-without-mergepy.yaml.\n\nChange-Id: I09f2eb278fa0eba1dff80884f12b6f682c7b0484\n'}]",0,139774,b91f8cd4928d82a84ac291cd5320b9952a01c4ae,9,5,1,360,,,0,"Remove missing cinder-storage Neutron* parameters

In I00af10e07feed6c9c97ee6cad545dbff88cd6afc we removed the
Neutron* parameters from cinder-storage.yaml but we forgot to
also remove them from overcloud-without-mergepy.yaml.

Change-Id: I09f2eb278fa0eba1dff80884f12b6f682c7b0484
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/74/139774/1 && git format-patch -1 --stdout FETCH_HEAD,['overcloud-without-mergepy.yaml'],1,b91f8cd4928d82a84ac291cd5320b9952a01c4ae,sync_without_mergepy,, NeutronEnableTunnelling: {get_param: NeutronEnableTunnelling} NeutronNetworkType: {get_param: NeutronNetworkType} NeutronPassword: {get_param: NeutronPassword} NeutronPublicInterface: {get_param: NeutronPublicInterface},0,4
openstack%2Ftripleo-heat-templates~master~Icff2f17a301e5e95fa43549ec1566c0c0d5b5353,openstack/tripleo-heat-templates,master,Icff2f17a301e5e95fa43549ec1566c0c0d5b5353,Add missing novncproxy settings to controller,MERGED,2014-12-05 18:44:44.000000000,2014-12-08 13:00:24.000000000,2014-12-08 13:00:23.000000000,"[{'_account_id': 3}, {'_account_id': 4328}, {'_account_id': 6449}, {'_account_id': 7585}]","[{'number': 1, 'created': '2014-12-05 18:44:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/a2f736e5755ca9be8ed56589a2a15b2d4b4b52ca', 'message': 'Add missing novncproxy settings to controller\n\nThis patch adds the missing HAProxy novncproxy parameters to\ncontroller.yaml\n\nThese parameters were adding to overcloud-source.yaml\nin I0c6a3d6a8fd10da71abbf568633b28bdb5e56aa2.\n\nChange-Id: Icff2f17a301e5e95fa43549ec1566c0c0d5b5353\n'}, {'number': 2, 'created': '2014-12-05 18:52:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/0cab956adaa71c30cbe9929f742a8c821c161d4a', 'message': 'Add missing novncproxy settings to controller\n\nThis patch adds the missing HAProxy novncproxy parameters to\ncontroller.yaml\n\nThese parameters were adding to overcloud-source.yaml\nin I0c6a3d6a8fd10da71abbf568633b28bdb5e56aa2.\n\nChange-Id: Icff2f17a301e5e95fa43549ec1566c0c0d5b5353\n'}, {'number': 3, 'created': '2014-12-06 01:50:51.000000000', 'files': ['controller.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/4c35b1b35837e2abd3419a2bd23c4453a61695e9', 'message': 'Add missing novncproxy settings to controller\n\nThis patch adds the missing HAProxy novncproxy parameters to\ncontroller.yaml\n\nThese parameters were adding to overcloud-source.yaml\nin I0c6a3d6a8fd10da71abbf568633b28bdb5e56aa2.\n\nChange-Id: Icff2f17a301e5e95fa43549ec1566c0c0d5b5353\n'}]",0,139709,4c35b1b35837e2abd3419a2bd23c4453a61695e9,14,4,3,360,,,0,"Add missing novncproxy settings to controller

This patch adds the missing HAProxy novncproxy parameters to
controller.yaml

These parameters were adding to overcloud-source.yaml
in I0c6a3d6a8fd10da71abbf568633b28bdb5e56aa2.

Change-Id: Icff2f17a301e5e95fa43549ec1566c0c0d5b5353
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/09/139709/3 && git format-patch -1 --stdout FETCH_HEAD,['controller.yaml'],1,a2f736e5755ca9be8ed56589a2a15b2d4b4b52ca,sync_without_mergepy, - name: nova_novncproxy port: 6080 net_binds: *public_binds,,3,0
openstack%2Ftripleo-heat-templates~master~I3e4e0e1feb521dded2679fed508fa97e8dd27661,openstack/tripleo-heat-templates,master,I3e4e0e1feb521dded2679fed508fa97e8dd27661,Add missing HAProxy settings to controller,MERGED,2014-12-05 18:34:54.000000000,2014-12-08 13:00:17.000000000,2014-12-08 13:00:17.000000000,"[{'_account_id': 3}, {'_account_id': 4328}, {'_account_id': 6449}, {'_account_id': 7585}]","[{'number': 1, 'created': '2014-12-05 18:34:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/a36e2862146db1547ea543797c4ddd9b474dda14', 'message': 'Add missing HAProxy settings to controller\n\nThis patch adds the missing parameters to controller.yaml\n\nThese parameters were adding to overcloud-source.yaml\nin I1581c091b996422fb1374ea4c024d0a88453e10b.\n\nChange-Id: I3e4e0e1feb521dded2679fed508fa97e8dd27661\n'}, {'number': 2, 'created': '2014-12-05 18:52:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/125a0714f9d9273847cd5c0adc8402c63f85d97c', 'message': 'Add missing HAProxy settings to controller\n\nThis patch adds the missing parameters to controller.yaml\n\nThese parameters were adding to overcloud-source.yaml\nin I1581c091b996422fb1374ea4c024d0a88453e10b.\n\nChange-Id: I3e4e0e1feb521dded2679fed508fa97e8dd27661\n'}, {'number': 3, 'created': '2014-12-06 01:50:51.000000000', 'files': ['controller.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/42b81e8478ce01dd0a148ac9b46c4f59afc6480b', 'message': 'Add missing HAProxy settings to controller\n\nThis patch adds the missing parameters to controller.yaml\n\nThese parameters were adding to overcloud-source.yaml\nin I1581c091b996422fb1374ea4c024d0a88453e10b.\n\nChange-Id: I3e4e0e1feb521dded2679fed508fa97e8dd27661\n'}]",0,139706,42b81e8478ce01dd0a148ac9b46c4f59afc6480b,14,4,3,360,,,0,"Add missing HAProxy settings to controller

This patch adds the missing parameters to controller.yaml

These parameters were adding to overcloud-source.yaml
in I1581c091b996422fb1374ea4c024d0a88453e10b.

Change-Id: I3e4e0e1feb521dded2679fed508fa97e8dd27661
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/06/139706/1 && git format-patch -1 --stdout FETCH_HEAD,['controller.yaml'],1,a36e2862146db1547ea543797c4ddd9b474dda14,sync_without_mergepy, options: - option httpchk GET / options: # overwrite options as glace_reg needs auth for http req options: # overwrite options as ceil needs auth for http req options: - option httpchk GET /info,,6,0
openstack%2Ftripleo-heat-templates~master~If54dc111aec852f906c9e7ac1bf56f9dcaf678ea,openstack/tripleo-heat-templates,master,If54dc111aec852f906c9e7ac1bf56f9dcaf678ea,Add missing Neutron DVR params to without-mergepy,MERGED,2014-12-05 16:14:26.000000000,2014-12-08 13:00:11.000000000,2014-12-08 13:00:10.000000000,"[{'_account_id': 3}, {'_account_id': 360}, {'_account_id': 4328}, {'_account_id': 6449}, {'_account_id': 7585}]","[{'number': 1, 'created': '2014-12-05 16:14:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/8be9681f04b6392ba4554916d390d088b7958174', 'message': 'Add missing Neutron DVR params to without-mergepy\n\nThis patch adds the missing parameters to\novercloud-without-mergepy.yaml.\n\nThese parameters were adding to overcloud-source.yaml\nin I422c65e7d941593083d52ad7fdf0dfd1d2fb3155. Due to\nthe concurrent review window they never made it\ninto the new overcloud-without-mergepy.yaml\nimplementation.\n\nChange-Id: If54dc111aec852f906c9e7ac1bf56f9dcaf678ea\n'}, {'number': 2, 'created': '2014-12-05 18:52:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/ac5857e0af4b860bf52b504ecdb88b7d6445d228', 'message': 'Add missing Neutron DVR params to without-mergepy\n\nThis patch adds the missing parameters to\novercloud-without-mergepy.yaml.\n\nThese parameters were adding to overcloud-source.yaml\nin I422c65e7d941593083d52ad7fdf0dfd1d2fb3155. Due to\nthe concurrent review window they never made it\ninto the new overcloud-without-mergepy.yaml\nimplementation.\n\nChange-Id: If54dc111aec852f906c9e7ac1bf56f9dcaf678ea\n'}, {'number': 3, 'created': '2014-12-06 01:50:51.000000000', 'files': ['overcloud-without-mergepy.yaml', 'controller.yaml', 'compute.yaml', 'compute-config.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/cb62cd9838786d40334f9e7f0a5063c73b02a50d', 'message': 'Add missing Neutron DVR params to without-mergepy\n\nThis patch adds the missing parameters to\novercloud-without-mergepy.yaml.\n\nThese parameters were adding to overcloud-source.yaml\nin I422c65e7d941593083d52ad7fdf0dfd1d2fb3155. Due to\nthe concurrent review window they never made it\ninto the new overcloud-without-mergepy.yaml\nimplementation.\n\nChange-Id: If54dc111aec852f906c9e7ac1bf56f9dcaf678ea\n'}]",4,139671,cb62cd9838786d40334f9e7f0a5063c73b02a50d,17,5,3,360,,,0,"Add missing Neutron DVR params to without-mergepy

This patch adds the missing parameters to
overcloud-without-mergepy.yaml.

These parameters were adding to overcloud-source.yaml
in I422c65e7d941593083d52ad7fdf0dfd1d2fb3155. Due to
the concurrent review window they never made it
into the new overcloud-without-mergepy.yaml
implementation.

Change-Id: If54dc111aec852f906c9e7ac1bf56f9dcaf678ea
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/71/139671/1 && git format-patch -1 --stdout FETCH_HEAD,"['overcloud-without-mergepy.yaml', 'controller.yaml', 'compute.yaml', 'compute-config.yaml']",4,8be9681f04b6392ba4554916d390d088b7958174,sync_without_mergepy, router_distributed: {get_input: neutron_router_distributed} agent_mode: {get_input: neutron_agent_mode} metadata_proxy_shared_secret: {get_input: neutron_metadata_proxy_shared_secret} mechanism_drivers: {get_input: neutron_mechanism_drivers} allow_automatic_l3agent_failover: {get_input: neutron_allow_l3agent_failover} public_interface_raw_device: {get_input: neutron_public_interface_raw_device},,100,1
openstack%2Ftripleo-heat-templates~master~Ib4888bc91f30aeb3aba590b69e4919a93f577143,openstack/tripleo-heat-templates,master,Ib4888bc91f30aeb3aba590b69e4919a93f577143,Remove duplicate Neutron params in overcloud,MERGED,2014-12-05 16:14:26.000000000,2014-12-08 13:00:03.000000000,2014-12-08 13:00:03.000000000,"[{'_account_id': 3}, {'_account_id': 4328}, {'_account_id': 4330}, {'_account_id': 6449}]","[{'number': 1, 'created': '2014-12-05 16:14:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/d8661b47a8a0240f72e4fc81a6ff44cb119c3519', 'message': 'Remove duplicate Neutron params in overcloud\n\nIn I422c65e7d941593083d52ad7fdf0dfd1d2fb3155\n(Enable Neutron DVR support in TripleO installation)\nwe added duplicate parameters for NeutronPublicInterfaceRawDevice\nand NeutronNetworkType.\n\nIn preparation for syncing with overcloud-without-mergepy.yaml\nlets remove these dups.\n\nChange-Id: Ib4888bc91f30aeb3aba590b69e4919a93f577143\n'}, {'number': 2, 'created': '2014-12-05 18:52:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/a4d5725975543267a603b332751d2b907bdc8ba0', 'message': 'Remove duplicate Neutron params in overcloud\n\nIn I422c65e7d941593083d52ad7fdf0dfd1d2fb3155\n(Enable Neutron DVR support in TripleO installation)\nwe added duplicate parameters for NeutronPublicInterfaceRawDevice\nand NeutronNetworkType.\n\nIn preparation for syncing with overcloud-without-mergepy.yaml\nlets remove these dups.\n\nChange-Id: Ib4888bc91f30aeb3aba590b69e4919a93f577143\n'}, {'number': 3, 'created': '2014-12-06 01:50:51.000000000', 'files': ['overcloud-source.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/56c905774f37be86cf740e772fe3fb8d5e4719c1', 'message': 'Remove duplicate Neutron params in overcloud\n\nIn I422c65e7d941593083d52ad7fdf0dfd1d2fb3155\n(Enable Neutron DVR support in TripleO installation)\nwe added duplicate parameters for NeutronPublicInterfaceRawDevice\nand NeutronNetworkType.\n\nIn preparation for syncing with overcloud-without-mergepy.yaml\nlets remove these dups.\n\nChange-Id: Ib4888bc91f30aeb3aba590b69e4919a93f577143\n'}]",0,139670,56c905774f37be86cf740e772fe3fb8d5e4719c1,19,4,3,360,,,0,"Remove duplicate Neutron params in overcloud

In I422c65e7d941593083d52ad7fdf0dfd1d2fb3155
(Enable Neutron DVR support in TripleO installation)
we added duplicate parameters for NeutronPublicInterfaceRawDevice
and NeutronNetworkType.

In preparation for syncing with overcloud-without-mergepy.yaml
lets remove these dups.

Change-Id: Ib4888bc91f30aeb3aba590b69e4919a93f577143
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/70/139670/3 && git format-patch -1 --stdout FETCH_HEAD,['overcloud-source.yaml'],1,d8661b47a8a0240f72e4fc81a6ff44cb119c3519,sync_without_mergepy,," NeutronPublicInterfaceRawDevice: default: '' description: If set, the public interface is a vlan with this device as the raw device. type: string NeutronNetworkType: default: 'gre' description: The tenant network type for Neutron, either gre or vxlan. type: string",0,8
openstack%2Fnova~master~Ice00abcbb431b29e28beb0cd8810b8653ef66139,openstack/nova,master,Ice00abcbb431b29e28beb0cd8810b8653ef66139,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:42:35.000000000,2014-12-08 12:59:11.000000000,2014-12-08 12:59:08.000000000,"[{'_account_id': 3}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 7665}, {'_account_id': 8556}, {'_account_id': 8959}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-12-05 03:42:35.000000000', 'files': ['doc/source/devref/gerrit.rst', 'CONTRIBUTING.rst', 'doc/source/devref/development.environment.rst'], 'web_link': 'https://opendev.org/openstack/nova/commit/18e138a2a2c4ceb28928df9facf7bd46c662f415', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Ice00abcbb431b29e28beb0cd8810b8653ef66139\n'}]",0,139340,18e138a2a2c4ceb28928df9facf7bd46c662f415,14,10,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Ice00abcbb431b29e28beb0cd8810b8653ef66139
",git fetch https://review.opendev.org/openstack/nova refs/changes/40/139340/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'doc/source/devref/gerrit.rst', 'doc/source/devref/development.environment.rst']",3,18e138a2a2c4ceb28928df9facf7bd46c662f415,infra-manual,.. _GerritWorkflow: http://docs.openstack.org/infra/manual/developers.html#development-workflow .. _HowToContribute: http://docs.openstack.org/infra/manual/developers.html,.. _GerritWorkflow: http://wiki.openstack.org/GerritWorkflow .. _HowToContribute: http://wiki.openstack.org/HowToContribute,8,9
openstack%2Fnova~master~Ibcdba00652808158019600f8ad75e80c85f9423e,openstack/nova,master,Ibcdba00652808158019600f8ad75e80c85f9423e,Remove the volume api related useless policy rules,MERGED,2014-12-03 06:18:34.000000000,2014-12-08 12:58:48.000000000,2014-12-08 12:58:45.000000000,"[{'_account_id': 3}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 5754}, {'_account_id': 6062}, {'_account_id': 7634}, {'_account_id': 8247}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-12-03 06:18:34.000000000', 'files': ['nova/tests/unit/fake_policy.py', 'etc/nova/policy.json'], 'web_link': 'https://opendev.org/openstack/nova/commit/61497bf6c4910eedb64494599f7dde25f2500629', 'message': 'Remove the volume api related useless policy rules\n\nThere are some old policy rules used by old nova volume, those are\nuseless anymore, this patch clean them up.\n\nChange-Id: Ibcdba00652808158019600f8ad75e80c85f9423e\nCloses-Bug: #1398686\n'}]",0,138645,61497bf6c4910eedb64494599f7dde25f2500629,23,13,1,5754,,,0,"Remove the volume api related useless policy rules

There are some old policy rules used by old nova volume, those are
useless anymore, this patch clean them up.

Change-Id: Ibcdba00652808158019600f8ad75e80c85f9423e
Closes-Bug: #1398686
",git fetch https://review.opendev.org/openstack/nova refs/changes/45/138645/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/fake_policy.py', 'etc/nova/policy.json']",2,61497bf6c4910eedb64494599f7dde25f2500629,remove_useless_policy,," ""volume:create"": """", ""volume:get_all"": """", ""volume:get_volume_metadata"": """", ""volume:get_snapshot"": """", ""volume:get_all_snapshots"": """", ""volume_extension:types_manage"": ""rule:admin_api"", ""volume_extension:types_extra_specs"": ""rule:admin_api"", ""volume_extension:volume_admin_actions:reset_status"": ""rule:admin_api"", ""volume_extension:snapshot_admin_actions:reset_status"": ""rule:admin_api"", ""volume_extension:volume_admin_actions:force_delete"": ""rule:admin_api"", ",0,46
openstack%2Fnova~master~I86f583f7d0747a164dae4530e25f8b01c2144673,openstack/nova,master,I86f583f7d0747a164dae4530e25f8b01c2144673,Remove old Baremetal Host Manager,MERGED,2014-11-24 13:13:39.000000000,2014-12-08 12:58:28.000000000,2014-12-08 12:58:24.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 2889}, {'_account_id': 4393}, {'_account_id': 5170}, {'_account_id': 7166}, {'_account_id': 9008}, {'_account_id': 9569}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-24 13:13:39.000000000', 'files': ['nova/tests/unit/scheduler/test_baremetal_host_manager.py', 'nova/scheduler/baremetal_host_manager.py', 'nova/scheduler/ironic_host_manager.py', 'nova/scheduler/base_baremetal_host_manager.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/621fd0d2d4e3aa2d0934b2886ae53e9819bd1993', 'message': ""Remove old Baremetal Host Manager\n\nBaremetal was deprecated in Juno and its driver was removed in\n46ed619b9ab1d61582f36155eea0d4a88f31fd50\n\nThis cleans out the parts that's still left in the scheduler.\n\nChange-Id: I86f583f7d0747a164dae4530e25f8b01c2144673\n""}]",0,136751,621fd0d2d4e3aa2d0934b2886ae53e9819bd1993,15,12,1,6450,,,0,"Remove old Baremetal Host Manager

Baremetal was deprecated in Juno and its driver was removed in
46ed619b9ab1d61582f36155eea0d4a88f31fd50

This cleans out the parts that's still left in the scheduler.

Change-Id: I86f583f7d0747a164dae4530e25f8b01c2144673
",git fetch https://review.opendev.org/openstack/nova refs/changes/51/136751/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/scheduler/test_baremetal_host_manager.py', 'nova/scheduler/baremetal_host_manager.py', 'nova/scheduler/ironic_host_manager.py', 'nova/scheduler/base_baremetal_host_manager.py']",4,621fd0d2d4e3aa2d0934b2886ae53e9819bd1993,remove-baremetal,,"# Copyright (c) 2012 NTT DOCOMO, INC. # Copyright (c) 2011 OpenStack Foundation # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """""" Manage hosts in the current zone. """""" from oslo.serialization import jsonutils from nova.scheduler import host_manager class BaseBaremetalNodeState(host_manager.HostState): """"""Mutable and immutable information tracked for a host. This is an attempt to remove the ad-hoc data structures previously used and lock down access. """""" def update_from_compute_node(self, compute): """"""Update information about a host from its compute_node info."""""" self.vcpus_total = compute['vcpus'] self.vcpus_used = compute['vcpus_used'] self.free_ram_mb = compute['free_ram_mb'] self.total_usable_ram_mb = compute['memory_mb'] self.free_disk_mb = compute['free_disk_gb'] * 1024 stats = compute.get('stats', '{}') self.stats = jsonutils.loads(stats) def consume_from_instance(self, instance): """"""Consume nodes entire resources regardless of instance request."""""" self.free_ram_mb = 0 self.free_disk_mb = 0 self.vcpus_used = self.vcpus_total class BaseBaremetalHostManager(host_manager.HostManager): """"""Base class for Baremetal and Ironic HostManager classes."""""" def host_state_cls(self, host, node, **kwargs): """"""Factory function to create a new HostState. May be overridden in subclasses to extend functionality. """""" return BaseBaremetalNodeState(host, node, **kwargs) ",14,186
openstack%2Fnova~master~I980237ed9d27f122b4725b8c05015a81764dd6b8,openstack/nova,master,I980237ed9d27f122b4725b8c05015a81764dd6b8,Remove except Exception cases,MERGED,2014-11-20 15:59:32.000000000,2014-12-08 12:58:07.000000000,2014-12-08 12:58:04.000000000,"[{'_account_id': 3}, {'_account_id': 642}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 6802}, {'_account_id': 8412}, {'_account_id': 9008}, {'_account_id': 9569}, {'_account_id': 9578}, {'_account_id': 9708}, {'_account_id': 10068}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 12612}, {'_account_id': 13909}]","[{'number': 1, 'created': '2014-11-20 15:59:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ae551601e8004afbb7bf0ce8ea715c2c6cee4873', 'message': ""Remove except Exception cases\n\nReplaced 'except Exception' cases with the specific exceptions\nthat the give code can throw in the util module. In some cases\nwe added extra unit test to cover the specific exception.\nDebug logging is added to provide better debugging.\n\nChange-Id: I980237ed9d27f122b4725b8c05015a81764dd6b8\nPartial-Bug: #1223605\n""}, {'number': 2, 'created': '2014-11-20 17:53:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/5461d5376f023b4c79fd3b24a0e00b3940d5bf74', 'message': ""Remove except Exception cases\n\nReplaced 'except Exception' cases with the specific exceptions\nthat the given code can throw in the util module. In some cases\nwe added extra unit test to cover the specific exception.\nDebug logging is added to provide better debugging.\n\nChange-Id: I980237ed9d27f122b4725b8c05015a81764dd6b8\nPartial-Bug: #1223605\n""}, {'number': 3, 'created': '2014-11-20 17:55:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/37dddcaffa1fc9ff791283992a3ad6e200ceaffb', 'message': ""Remove except Exception cases\n\nReplaced 'except Exception' cases with the specific exceptions\nthat the given code can throw in the util module. In some cases\nwe added extra unit test to cover the specific exception.\nDebug logging is added to provide better debugging.\n\nChange-Id: I980237ed9d27f122b4725b8c05015a81764dd6b8\nPartial-Bug: #1223605\n""}, {'number': 4, 'created': '2014-11-21 12:32:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/33acb352414f26fc9175a61532221d1523e6e6e5', 'message': ""Remove except Exception cases\n\nReplaced 'except Exception' cases with the specific exceptions\nthat the given code can throw in the util module. In some cases\nwe added extra unit test to cover the specific exception.\nDebug logging is added to provide better debugging.\n\nChange-Id: I980237ed9d27f122b4725b8c05015a81764dd6b8\nPartial-Bug: #1223605\n""}, {'number': 5, 'created': '2014-11-26 09:19:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3eb781452da5901058f3411a2f9d29c602afbb35', 'message': ""Remove except Exception cases\n\nReplaced 'except Exception' cases with the specific exceptions\nthat the given code can throw in the util module. In some cases\nwe added extra unit test to cover the specific exception.\nDebug logging is added to provide better debugging.\n\nChange-Id: I980237ed9d27f122b4725b8c05015a81764dd6b8\nPartial-Bug: #1223605\n""}, {'number': 6, 'created': '2014-11-26 09:42:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/943f761be4c3797969b36c8057e8b0bed263fba0', 'message': ""Remove except Exception cases\n\nReplaced 'except Exception' cases with the specific exceptions\nthat the given code can throw in the util module. In some cases\nwe added extra unit test to cover the specific exception.\nDebug logging is added to provide better debugging.\n\nChange-Id: I980237ed9d27f122b4725b8c05015a81764dd6b8\nPartial-Bug: #1223605\n""}, {'number': 7, 'created': '2014-12-05 15:12:28.000000000', 'files': ['nova/tests/unit/test_utils.py', 'nova/utils.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/ea0451230e98cf0e640811407c4fd9a5224f0b97', 'message': ""Remove except Exception cases\n\nReplaced 'except Exception' cases with the specific exceptions\nthat the given code can throw in the util module. In some cases\nwe added extra unit test to cover the specific exception.\n\nChange-Id: I980237ed9d27f122b4725b8c05015a81764dd6b8\nPartial-Bug: #1223605\n""}]",9,136033,ea0451230e98cf0e640811407c4fd9a5224f0b97,59,17,7,13909,,,0,"Remove except Exception cases

Replaced 'except Exception' cases with the specific exceptions
that the given code can throw in the util module. In some cases
we added extra unit test to cover the specific exception.

Change-Id: I980237ed9d27f122b4725b8c05015a81764dd6b8
Partial-Bug: #1223605
",git fetch https://review.opendev.org/openstack/nova refs/changes/33/136033/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/test_utils.py', 'nova/utils.py']",2,ae551601e8004afbb7bf0ce8ea715c2c6cee4873,bug/1223605," is an empty string. If the input is invalid expression, return a tuple of two empty strings. except (ValueError, netaddr.core.AddrFormatError): except (ValueError, netaddr.core.AddrFormatError) as err: LOG.debug('Invalid ipv4 address: ""%s"". %s', address, err) except (ValueError, netaddr.core.AddrFormatError) as err: LOG.debug('Invalid ipv6 address: ""%s"". %s', address, err) netaddr.IPNetwork(address, version=6).cidr except (TypeError, netaddr.core.AddrFormatError) as err: LOG.debug(err)"," is a null string. If the input is invalid expression, return a null list. except Exception: except Exception: except Exception: str(netaddr.IPNetwork(address, version=6).cidr) except Exception:",12,7
openstack%2Fnova~master~Ie827aaacbcfbc01e32591d29aab79ef75aa0c5d9,openstack/nova,master,Ie827aaacbcfbc01e32591d29aab79ef75aa0c5d9,Assert order of DB index members,MERGED,2014-10-07 17:53:27.000000000,2014-12-08 12:55:19.000000000,2014-12-08 12:55:16.000000000,"[{'_account_id': 3}, {'_account_id': 100}, {'_account_id': 679}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 11642}]","[{'number': 1, 'created': '2014-10-07 17:53:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d53697c048ed7b7d3d462765e2fa06579f7f0f37', 'message': ""Assert order of DB index members\n\nOrder of columns can matter. Most SQL databases can use the leading\ncolumns for optimizing queries that don't include all of the columns.\nIgnoring the order can result in an index being added that may not\nbe able to be used to optimize the same queries.\n\nChange-Id: Ie827aaacbcfbc01e32591d29aab79ef75aa0c5d9\n""}, {'number': 2, 'created': '2014-12-04 23:50:41.000000000', 'files': ['nova/tests/unit/db/test_migrations.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/c15fcc6caaad50d3a560436045a51c2fbecea423', 'message': ""Assert order of DB index members\n\nOrder of columns can matter. Most SQL databases can use the leading\ncolumns for optimizing queries that don't include all of the columns.\nIgnoring the order can result in an index being added that may not\nbe able to be used to optimize the same queries.\n\nChange-Id: Ie827aaacbcfbc01e32591d29aab79ef75aa0c5d9\n""}]",0,126638,c15fcc6caaad50d3a560436045a51c2fbecea423,23,11,2,100,,,0,"Assert order of DB index members

Order of columns can matter. Most SQL databases can use the leading
columns for optimizing queries that don't include all of the columns.
Ignoring the order can result in an index being added that may not
be able to be used to optimize the same queries.

Change-Id: Ie827aaacbcfbc01e32591d29aab79ef75aa0c5d9
",git fetch https://review.opendev.org/openstack/nova refs/changes/38/126638/2 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/db/test_migrations.py'],1,d53697c048ed7b7d3d462765e2fa06579f7f0f37,db-index-order-matters.new," # NOTE(johannes): Order of columns can matter. Most SQL databases # can use the leading columns for optimizing queries that don't # include all of the covered columns. index_columns = [c.name for c in idx.columns] self.assertEqual(members, index_columns)"," index_columns = idx.columns.keys() self.assertEqual(sorted(members), sorted(index_columns))",5,2
openstack%2Fdesignate~master~I63f5da422c6690a7052cbde2983be0e9a466f451,openstack/designate,master,I63f5da422c6690a7052cbde2983be0e9a466f451,Add DynECT DevStack plugin,MERGED,2014-12-03 21:37:21.000000000,2014-12-08 12:18:07.000000000,2014-12-08 12:18:07.000000000,"[{'_account_id': 3}, {'_account_id': 741}, {'_account_id': 8094}, {'_account_id': 8099}]","[{'number': 1, 'created': '2014-12-03 21:37:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/designate/commit/6c74bc043e4f09b0da7c821944524a1aab8750b7', 'message': 'Add DynECT DevStack plugin\n\nChange-Id: I63f5da422c6690a7052cbde2983be0e9a466f451\n'}, {'number': 2, 'created': '2014-12-03 21:38:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/designate/commit/e7f90c97a3e387cb961a5783bca01774de3a4cea', 'message': 'Add DynECT DevStack plugin\n\nChange-Id: I63f5da422c6690a7052cbde2983be0e9a466f451\n'}, {'number': 3, 'created': '2014-12-03 22:43:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/designate/commit/357ecf2207934718c8bb4e43c9981bb468f99f04', 'message': 'Add DynECT DevStack plugin\n\nChange-Id: I63f5da422c6690a7052cbde2983be0e9a466f451\n'}, {'number': 4, 'created': '2014-12-03 23:08:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/designate/commit/f0f103718b1d6604d0e6e2d3d5f77d41691ee0ea', 'message': 'Add DynECT DevStack plugin\n\nChange-Id: I63f5da422c6690a7052cbde2983be0e9a466f451\n'}, {'number': 5, 'created': '2014-12-03 23:30:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/designate/commit/d6ec96b25688d1af1e7b02488c6afbbf087011ca', 'message': 'Add DynECT DevStack plugin\n\nChange-Id: I63f5da422c6690a7052cbde2983be0e9a466f451\n'}, {'number': 6, 'created': '2014-12-04 13:03:26.000000000', 'files': ['contrib/devstack/lib/designate_plugins/backend-dynect', 'contrib/vagrant/Vagrantfile'], 'web_link': 'https://opendev.org/openstack/designate/commit/4a2cec8e202dc8a55507364b1998e397f644d476', 'message': 'Add DynECT DevStack plugin\n\nChange-Id: I63f5da422c6690a7052cbde2983be0e9a466f451\n'}]",2,138866,4a2cec8e202dc8a55507364b1998e397f644d476,17,4,6,741,,,0,"Add DynECT DevStack plugin

Change-Id: I63f5da422c6690a7052cbde2983be0e9a466f451
",git fetch https://review.opendev.org/openstack/designate refs/changes/66/138866/3 && git format-patch -1 --stdout FETCH_HEAD,['contrib/devstack/lib/designate_plugins/backend-dynect'],1,6c74bc043e4f09b0da7c821944524a1aab8750b7,138885,"# lib/designate_plugins/backend-dynect # Configure the dynect backend # Requirements: # An active DynECT account / contract will be requied to use this DevStack # plugin. # Enable with: # DESIGNATE_BACKEND_DRIVER=dynect # Dependencies: # ``functions`` file # ``designate`` configuration # install_designate_backend - install any external requirements # configure_designate_backend - make configuration changes, including those to other services # init_designate_backend - initialize databases, etc. # start_designate_backend - start any external services # stop_designate_backend - stop any external services # cleanup_designate_backend - remove transient data and cache # Save trace setting DP_DYNECT_XTRACE=$(set +o | grep xtrace) set +o xtrace # Defaults # -------- DESIGNATE_DYNECT_CUSTOMER=${DESIGNATE_DYNECT_CUSTOMER:-customer} DESIGNATE_DYNECT_USERNAME=${DESIGNATE_DYNECT_USERNAME:-username} DESIGNATE_DYNECT_PASSWORD=${DESIGNATE_DYNECT_PASSWORD:-password} DESIGNATE_DYNECT_MASTERS=${DESIGNATE_DYNECT_MASTERS:-$DESIGNATE_SERVICE_HOST:$DESIGNATE_SERVICE_PORT_MDNS} DESIGNATE_DYNECT_SLAVES=${DESIGNATE_DYNECT_SLAVES:-xfrin1.dynect.net:53, xfrin2.dynect.net:53} # Entry Points # ------------ # install_designate_backend - install any external requirements function install_designate_backend { : } # configure_designate_backend - make configuration changes, including those to other services function configure_designate_backend { iniset $DESIGNATE_CONF backend:dynect masters $DESIGNATE_DYNECT_MASTERS iniset $DESIGNATE_CONF backend:dynect customer_name $DESIGNATE_DYNECT_CUSTOMER iniset $DESIGNATE_CONF backend:dynect username $DESIGNATE_DYNECT_USERNAME iniset $DESIGNATE_CONF backend:dynect password $DESIGNATE_DYNECT_PASSWORD iniset $DESIGNATE_CONF service:mdns slave_nameserver_ips_and_ports $DESIGNATE_DYNECT_SLAVES } # init_designate_backend - initialize databases, etc. function init_designate_backend { : } # start_designate_backend - start any external services function start_designate_backend { : } # stop_designate_backend - stop any external services function stop_designate_backend { : } # cleanup_designate_backend - remove transient data and cache function cleanup_designate_backend { : } # Restore xtrace $DP_DYNECT_XTRACE ",,73,0
openstack%2Fanchor~master~I5227ca07d2f234f55bf4dfc1556455f318a70ded,openstack/anchor,master,I5227ca07d2f234f55bf4dfc1556455f318a70ded,Adds check for M2Crypto get_extensions() support,MERGED,2014-12-05 20:30:17.000000000,2014-12-08 12:06:10.000000000,2014-12-08 12:06:10.000000000,"[{'_account_id': 3}, {'_account_id': 7063}, {'_account_id': 11397}, {'_account_id': 11716}]","[{'number': 1, 'created': '2014-12-05 20:30:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/anchor/commit/26478951489c5f30f5cdd0dc264af4aa117312ae', 'message': 'Adds check for M2Crypto get_extensions() support\n\nChange-Id: I5227ca07d2f234f55bf4dfc1556455f318a70ded\nCloses-Bug: 1399670\n'}, {'number': 2, 'created': '2014-12-05 20:34:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/anchor/commit/82b844a67e486b478bb6de6627ebe9c703f9a728', 'message': 'Adds check for M2Crypto get_extensions() support\n\nChange-Id: I5227ca07d2f234f55bf4dfc1556455f318a70ded\nCloses-Bug: 1399670\n'}, {'number': 3, 'created': '2014-12-08 12:03:27.000000000', 'files': ['anchor/certificate_ops.py'], 'web_link': 'https://opendev.org/openstack/anchor/commit/c783c72baacfe1bfed0b94ca097a388f2de8bd97', 'message': 'Adds check for M2Crypto get_extensions() support\n\nChange-Id: I5227ca07d2f234f55bf4dfc1556455f318a70ded\nCloses-Bug: 1399670\n'}]",0,139732,c783c72baacfe1bfed0b94ca097a388f2de8bd97,14,4,3,11397,,,0,"Adds check for M2Crypto get_extensions() support

Change-Id: I5227ca07d2f234f55bf4dfc1556455f318a70ded
Closes-Bug: 1399670
",git fetch https://review.opendev.org/openstack/anchor refs/changes/32/139732/3 && git format-patch -1 --stdout FETCH_HEAD,['anchor/certificate_ops.py'],1,26478951489c5f30f5cdd0dc264af4aa117312ae,bug/1399670," logger.error(""Parsing CSR failed due to non-pem encoding type or null CSR."") logger.exception(""Exception while parsing the CSR"") #Check that M2crypto supports get_extensions() try: csr.get_extensions() except AttributeError: raise validators.ValidationError(""Incorrect M2Crypto library version, cannot perform csr.get_extensions"") "," logger.exception(""failed while parsing the CSR"")",8,1
openstack%2Fnova-specs~master~I667acfa422ef78bdc80efb279911091116b1c1f0,openstack/nova-specs,master,I667acfa422ef78bdc80efb279911091116b1c1f0,Add Spec Backlog,MERGED,2014-11-12 12:18:01.000000000,2014-12-08 11:55:15.000000000,2014-12-08 11:55:14.000000000,"[{'_account_id': 3}, {'_account_id': 1561}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 2271}, {'_account_id': 6772}]","[{'number': 1, 'created': '2014-11-12 12:18:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/6e4f3036550cd0071bd248b8345fc77c15cb66ab', 'message': 'Add Spec Backlog\n\nThis is copying what has been done for keystone:\nIf546724fd535db7753a372389c3f90f3b060d9bc\n\nWe add a place for people to add rough ideas, and a place to put specs\nthat are no longer going to be implemented by the proposer.\n\nIncludes updating the README to tell people where the backlog specs will\nlive.\n\nWe need to add a backlog spec before we can complete this change, this\nis just a placeholder to advertise the backlog idea.\n\nChange-Id: I667acfa422ef78bdc80efb279911091116b1c1f0\n'}, {'number': 2, 'created': '2014-11-14 02:50:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/226cbdfc867a289e0213af011dca84779820dc3e', 'message': 'Add Spec Backlog\n\nThis is copying what has been done for keystone:\nIf546724fd535db7753a372389c3f90f3b060d9bc\n\nWe add a place for people to add rough ideas, and a place to put specs\nthat are no longer going to be implemented by the proposer.\n\nIncludes updating the README to tell people where the backlog specs will\nlive.\n\nWe need to add a backlog spec before we can complete this change, this\nis just a placeholder to advertise the backlog idea.\n\nChange-Id: I667acfa422ef78bdc80efb279911091116b1c1f0\n'}, {'number': 3, 'created': '2014-12-05 19:34:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/11cf635700284307f0a4dd0dc8b85da0e5742bae', 'message': 'Add Spec Backlog\n\nThis is copying what has been done for keystone:\nIf546724fd535db7753a372389c3f90f3b060d9bc\n\nWe add a place for people to add rough ideas, and a place to put specs\nthat are no longer going to be implemented by the proposer.\n\nIncludes updating the README to tell people where the backlog specs will\nlive.\n\nWe need to add a backlog spec before we can complete this change, this\nis just a placeholder to advertise the backlog idea.\n\nChange-Id: I667acfa422ef78bdc80efb279911091116b1c1f0\n'}, {'number': 4, 'created': '2014-12-05 19:46:47.000000000', 'files': ['doc/source/specs/backlog/index.rst', 'doc/source/index.rst', 'README.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/3fcd6897622464fc83d1ba5a8cd335c54391d33c', 'message': 'Add Spec Backlog\n\nThis is copying what has been done for keystone:\nIf546724fd535db7753a372389c3f90f3b060d9bc\n\nWe add a place for people to add rough ideas, and a place to put specs\nthat are no longer going to be implemented by the proposer.\n\nIncludes updating the README to tell people where the backlog specs will\nlive.\n\nWe need to add a backlog spec before we can complete this change, this\nis just a placeholder to advertise the backlog idea.\n\nChange-Id: I667acfa422ef78bdc80efb279911091116b1c1f0\n'}]",0,133936,3fcd6897622464fc83d1ba5a8cd335c54391d33c,20,6,4,782,,,0,"Add Spec Backlog

This is copying what has been done for keystone:
If546724fd535db7753a372389c3f90f3b060d9bc

We add a place for people to add rough ideas, and a place to put specs
that are no longer going to be implemented by the proposer.

Includes updating the README to tell people where the backlog specs will
live.

We need to add a backlog spec before we can complete this change, this
is just a placeholder to advertise the backlog idea.

Change-Id: I667acfa422ef78bdc80efb279911091116b1c1f0
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/36/133936/3 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/specs/backlog/index.rst', 'doc/source/index.rst', 'README.rst']",3,6e4f3036550cd0071bd248b8345fc77c15cb66ab,(detached,"For specifications that have been reviewed and approved but have not been implemented:: specs/backlog/approved Specifications in this directory indicate the original author has either become unavailable, or has indicated that they are not going to implement the specification. The specifications found here are available as projects for people looking to get involved with Nova. If you are interested in claiming a spec, start by posting a review for the specification that moves it from this directory to the next active release. Please set yourself as the new `primary assignee` and maintain the original author in the `other contributors` list. ",,39,0
openstack%2Fnova-specs~master~I8cdbd264136caf6b1f36b3960db541fbbda3b968,openstack/nova-specs,master,I8cdbd264136caf6b1f36b3960db541fbbda3b968,Added toctree nova/specs/kilo/*,ABANDONED,2014-10-23 09:57:28.000000000,2014-12-08 11:49:19.000000000,,"[{'_account_id': 3}, {'_account_id': 1849}]","[{'number': 1, 'created': '2014-10-23 09:57:28.000000000', 'files': ['specs/kilo/example.rst', 'doc/source/index.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/3f3cbc6ee020740b0237a0680f995ec946317db5', 'message': 'Added toctree nova/specs/kilo/*\n\nChange-Id: I8cdbd264136caf6b1f36b3960db541fbbda3b968\nCloses-Bug: 1384642\n'}]",0,130482,3f3cbc6ee020740b0237a0680f995ec946317db5,5,2,1,13071,,,0,"Added toctree nova/specs/kilo/*

Change-Id: I8cdbd264136caf6b1f36b3960db541fbbda3b968
Closes-Bug: 1384642
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/82/130482/1 && git format-patch -1 --stdout FETCH_HEAD,"['specs/kilo/example.rst', 'doc/source/index.rst']",2,3f3cbc6ee020740b0237a0680f995ec946317db5,bug/1384642,Kilo new specs: .. toctree:: :glob: :maxdepth: 1 specs/kilo/* ,,342,0
openstack%2Fnova-specs~master~I14fccc2bf0160060ea0374ea0428323686c81630,openstack/nova-specs,master,I14fccc2bf0160060ea0374ea0428323686c81630,Add a new filter to implement project isolation feature,ABANDONED,2014-08-22 13:43:54.000000000,2014-12-08 11:44:53.000000000,,"[{'_account_id': 3}, {'_account_id': 1770}, {'_account_id': 1849}, {'_account_id': 7166}]","[{'number': 1, 'created': '2014-08-22 13:43:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/0fd0f738191eb508a51af9399fa5d9016663ebd1', 'message': 'Add a new filter to implement project isolation feature\n\nThis blueprint proposes a new scheduler filter which\nroutes instance to a pre-associated host aggregates\n\nChange-Id: I14fccc2bf0160060ea0374ea0428323686c81630\n'}, {'number': 2, 'created': '2014-08-22 13:53:43.000000000', 'files': ['specs/juno/projects-to-aggregate.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/38acf0e6a432e9ccb1b0f046364c17ea5ee8e0f8', 'message': 'Add a new filter to implement project isolation feature\n\nThis blueprint proposes a new scheduler filter which\nroutes instance to a pre-associated host aggregates\n\nChange-Id: I14fccc2bf0160060ea0374ea0428323686c81630\n'}]",1,116280,38acf0e6a432e9ccb1b0f046364c17ea5ee8e0f8,8,4,2,1770,,,0,"Add a new filter to implement project isolation feature

This blueprint proposes a new scheduler filter which
routes instance to a pre-associated host aggregates

Change-Id: I14fccc2bf0160060ea0374ea0428323686c81630
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/80/116280/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/projects-to-aggregate.rst'],1,0fd0f738191eb508a51af9399fa5d9016663ebd1,bp/proposes,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ======================================================= Add a new filter to implement project isolation feature ======================================================= https://blueprints.launchpad.net/nova/+spec/projects-to-aggregate This blueprint proposes a new scheduler filter which routes instance to a pre-associated host aggregates Problem description =================== At present, there is no scheduler filter to route all instances created by an user to pre-associated aggregates without having to specify zones as part of the nova boot command. Proposed change =============== This filter aims to route tenant instances to pre-associated aggregate, * Administrator has to associate tenant projects to aggregate as metadata field with key as 'project_to_aggregate' and set of projectids as values * Hosts which are tagged under the aggregates are given high priority while creating instances for those specified projects * One tenant can be mapped to more than one host aggregate This change does not restrict projects which are not attached to any specific aggregate. (i.e) If a project is not tagged to any aggregate,then its instances are free to boot on any of the filtered hosts irrespective of the aggregates. Alternatives ------------ In each boot request, user has to specify the Zone to which the instance has to reach. Data model impact ----------------- None. REST API impact --------------- None. Security impact --------------- None. Notifications impact -------------------- None. Other end user impact --------------------- None. Performance Impact ------------------ As scheduler filters get called once per host for every instance being created, the database calls might impact the performance of the system in cases of huge multi node architectures. Other deployer impact --------------------- Config Options includes addition of the filter class in nova.conf under the scheduler_available_filters attribute. Developer impact ---------------- None. Implementation ============== Assignee(s) ----------- Primary assignee: Ram-nalluri Other contributors: himapriya Work Items ---------- * Implement the filter and develop unit test cases. Dependencies ============ None. Testing ======= Unit test cases are being developed. No special tempest tests are necessary to test the new filter. Documentation Impact ==================== We expect to have the following document changes: * The scheduler_available_filters flag should include project-to-aggregate filter class. * Documentation for the project-to-aggregate filter should be added. References ========== https://blueprints.launchpad.net/nova/+spec/projects-to-aggregate ",,117,0
openstack%2Fnova-specs~master~I8334912def165b3f0ba35d6f3805bd7c5a6d1a63,openstack/nova-specs,master,I8334912def165b3f0ba35d6f3805bd7c5a6d1a63,Improve instance boot time,ABANDONED,2014-08-04 14:31:43.000000000,2014-12-08 11:44:36.000000000,,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 6172}]","[{'number': 1, 'created': '2014-08-04 14:31:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/89041f403230b67d522e4ca9587f64d2ef0ffa0b', 'message': 'Improve instance boot time\n\nAn umbrella spec to improve the boot time of an instance.\n\nThis is for blueprint instance-boot\n\nChange-Id: I8334912def165b3f0ba35d6f3805bd7c5a6d1a63\n'}, {'number': 2, 'created': '2014-08-04 14:32:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/f779516e7a1805f9677af627c042f8b17924d826', 'message': 'Improve instance boot time\n\nAn umbrella spec to improve the boot time of an instance.\n\nThis is for blueprint improve-instance-boot-time\n\nChange-Id: I8334912def165b3f0ba35d6f3805bd7c5a6d1a63\n'}, {'number': 3, 'created': '2014-08-04 14:56:44.000000000', 'files': ['specs/juno/instance-boot.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/21b82a8296bc078c1b10c43028e3970311fbfc3f', 'message': 'Improve instance boot time\n\nAn umbrella spec to improve the boot time of an instance.\n\nThis is for blueprint improve-instance-boot-time\n\nChange-Id: I8334912def165b3f0ba35d6f3805bd7c5a6d1a63\n'}]",1,111745,21b82a8296bc078c1b10c43028e3970311fbfc3f,11,3,3,1653,,,0,"Improve instance boot time

An umbrella spec to improve the boot time of an instance.

This is for blueprint improve-instance-boot-time

Change-Id: I8334912def165b3f0ba35d6f3805bd7c5a6d1a63
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/45/111745/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/instance-boot.rst'],1,89041f403230b67d522e4ca9587f64d2ef0ffa0b,bp/improve-instance-boot-time,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ================================= Improve instance boot performance ================================= https://blueprints.launchpad.net/nova/+spec/improve-instance-boot-time This is an umbrella spec that will include various performance enhancements for booting a instance. Problem description =================== Current booting an instance has a large number of database accesses. These are costly and have a considerable price for large clouds. Proposed change =============== Reduce the amount of database access when booting an instance. This will include the following: * Pass the instance type to the spawn virt method. This will save reading it from the database * Pass config drive information Alternatives ------------ None really. Unless we want to bear the brunt of bad instance launch times. Data model impact ----------------- In some cases we will change some virt method signatures, for example adding the instance type to the spawn method. REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- None Performance Impact ------------------ This will improve performance. :) Other deployer impact --------------------- None Developer impact ---------------- None Implementation ============== Assignee(s) ----------- Primary assignee: garyk Work Items ---------- * Pass instance type to the spawn method * Pass config drive information to the spwan method Dependencies ============ None Testing ======= Consider using Rally to show the advantages. Documentation Impact ==================== None References ========== None ",,113,0
openstack%2Fnova-specs~master~Ie76e034fb3bb5c321da5a9debf23f54efc2c909f,openstack/nova-specs,master,Ie76e034fb3bb5c321da5a9debf23f54efc2c909f,Added VirtProperties object blueprint,ABANDONED,2014-08-01 15:16:33.000000000,2014-12-08 11:44:19.000000000,,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 782}, {'_account_id': 1779}, {'_account_id': 4393}]","[{'number': 1, 'created': '2014-08-01 15:16:33.000000000', 'files': ['specs/juno/virt-properties-object.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/983d7f1c11ccbcfb1492b5e713244fc6ec88d625', 'message': 'Added VirtProperties object blueprint\n\nChange-Id: Ie76e034fb3bb5c321da5a9debf23f54efc2c909f\n'}]",0,111308,983d7f1c11ccbcfb1492b5e713244fc6ec88d625,6,5,1,7750,,,0,"Added VirtProperties object blueprint

Change-Id: Ie76e034fb3bb5c321da5a9debf23f54efc2c909f
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/08/111308/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/virt-properties-object.rst'],1,983d7f1c11ccbcfb1492b5e713244fc6ec88d625,bp/Change-Id,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode =========================================== Virt Properties Objects Support =========================================== https://blueprints.launchpad.net/nova/+spec/convert-image-meta-into-nova-object In an effort to standardize which image properties are available to the nova drivers, a NovaObject needs to be created. This object will take a dictionary of values provided by Glance and verify that the keys are one of the allowed ones listed here: https://wiki.openstack.org/wiki/VirtDriverImageProperties The new VirtProperties object will be consumed by the nova drivers. It will not be backed by a database. Problem description =================== Each nova driver expects slightly different property names from glance image meta data. Proposed change =============== Create a NovaObject that will contain a standard list of properties. The object will be contructed from glance image meta properties. Each driver will be upgraded to use the new object instead of the properties dictionary. Alternatives ------------ This is the accepted direction of the project to solve this problem. However, alternatives would be: 1. Don't solve the problem and continue using unversioned and completely arbitrary dictionaries of properties. Data model impact ----------------- None. REST API impact --------------- None. Security impact --------------- None. Notifications impact -------------------- None. Other end user impact --------------------- None. Performance Impact ------------------ None. Other deployer impact --------------------- Moving to objects enhances the ability for deployers to incrementally roll out new code. It is, however, largely transparent for them. Developer impact ---------------- This is normal refactoring, so the impact is minimal. In general, objects-based code is easier to work with, so long-term it is a win for the developers. Implementation ============== Assignee(s) ----------- Primary assignee: dkliban Work Items ---------- * Create VirtProperties object * Upgrade libvirt driver * Upgrade xenapi driver * Upgrade vmwareapi driver Dependencies ============ None. Testing ======= In general, unit tests require minimal change when this happens, depending on how the tests are structured. Ideally, they are already mocking out database calls, which means the change to objects is a transparent one. In reality, this usually means minor tweaking to the tests to return whole data models, etc. Documentation Impact ==================== Need to publish the new list of standard virt property names. References ========== * https://blueprints.launchpad.net/nova/+spec/convert-image-meta-into-nova-object ",,125,0
openstack%2Fnova-specs~master~Icc68a18dcbaa0d93650f2ab0995635b4d8af992a,openstack/nova-specs,master,Icc68a18dcbaa0d93650f2ab0995635b4d8af992a,Add ironic boot mode filters,ABANDONED,2014-07-22 06:01:16.000000000,2014-12-08 11:43:37.000000000,,"[{'_account_id': 3}, {'_account_id': 782}, {'_account_id': 2889}]","[{'number': 1, 'created': '2014-07-22 06:01:16.000000000', 'files': ['specs/juno/add-ironic-boot-mode-filters.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/fb6717cb002382c981e8defdb25d1009b55eeb1c', 'message': 'Add ironic boot mode filters\n\nThis spec proposes to add new filters to be used with ironic virt driver\nto select the ironic node based on the given boot mode.\n\nChange-Id: Icc68a18dcbaa0d93650f2ab0995635b4d8af992a\n'}]",1,108582,fb6717cb002382c981e8defdb25d1009b55eeb1c,8,3,1,10574,,,0,"Add ironic boot mode filters

This spec proposes to add new filters to be used with ironic virt driver
to select the ironic node based on the given boot mode.

Change-Id: Icc68a18dcbaa0d93650f2ab0995635b4d8af992a
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/82/108582/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/add-ironic-boot-mode-filters.rst'],1,fb6717cb002382c981e8defdb25d1009b55eeb1c,add-ironic-boot-mode-filters,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ===================================== Add ironic boot mode filters ===================================== https://blueprints.launchpad.net/nova/+spec/add-ironic-boot-mode-filters This spec proposes to add new filter to be used with ironic virt driver to select the ironic node based on the given boot mode. Problem description =================== Operator wants an ability to specify the boot mode for deploying an image on an ironic node which supports the given boot mode. Proposed change =============== Add a filter to select the ironic node based on a given boot mode. 1. A filter which uses image property to select the node. The required boot mode is given as an image property - boot_mode=uefi|bios. The filter will extract this image property and compares it with the ironic node property ""supported_boot_modes"". 2. A filter which uses a key-value pair present in flavor extra_specs field. The required boot mode is given as boot_mode=uefi|bios in flavor extra_specs field. This filter will extract the ""boot_mode"" from flavor extra_specs filed and compares it with the ironic node property ""supported_boot_modes"". 3. Make necessary changes in nova.virt.ironic driver to update the ironic node's instance_info field with the requested boot_mode. Alternatives ------------ None Data model impact ----------------- None REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- None Performance Impact ------------------ None Other deployer impact --------------------- The operator needs to update the scheduler's nova.conf to activate filters, also he has to set approiate image property or nova flavor extra_specs field with boot_mode=ueif|bios. Developer impact ---------------- None Implementation ============== Assignee(s) ----------- Primary assignee: Faizan Barmawer Other contributors: <None> Work Items ---------- * New filter IronicBootModeFilter * Pass the boot_mode to ironic nodes instance_info field. Dependencies ============ https://blueprints.launchpad.net/nova/+spec/add-ironic-driver Testing ======= Unit testing. Documentation Impact ==================== This filter has to be appropriately documented in ironic virt driver documents. References ========== None ",,127,0
openstack%2Fnova-specs~master~I23106186368ff181707e46037d3e90158003981b,openstack/nova-specs,master,I23106186368ff181707e46037d3e90158003981b,Leverage the features of IBM GPFS to store cached images and instances,ABANDONED,2014-07-08 07:59:38.000000000,2014-12-08 11:43:16.000000000,,"[{'_account_id': 3}, {'_account_id': 782}, {'_account_id': 1779}, {'_account_id': 8037}, {'_account_id': 9067}]","[{'number': 1, 'created': '2014-07-08 07:59:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/f448395cd53d30f45fce439f5679ed2046f679ca', 'message': 'Leverage the features of IBM GPFS to store cached images and instances\n\nLeverage the features of IBM GPFS to optimally store cached images and\ninstance files.\nThe bock-level format-agnostic Copy-On-Write(COW) mechanism enables quick\ninstance provisioning and instance snapshot avoiding data copy.\n\nChange-Id: I23106186368ff181707e46037d3e90158003981b\nblueprint: gpfs-instance-store\n'}, {'number': 2, 'created': '2014-07-08 11:17:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/a82fab0a7cbb9174a08e65b373397d7f75a1208c', 'message': 'Leverage the features of IBM GPFS to store cached images and instances\n\nLeverage the features of IBM GPFS to optimally store cached images and\ninstance files.\nThe bock-level format-agnostic Copy-On-Write(COW) mechanism enables quick\ninstance provisioning and instance snapshot avoiding data copy.\n\nChange-Id: I23106186368ff181707e46037d3e90158003981b\nblueprint: gpfs-instance-store\n'}, {'number': 3, 'created': '2014-07-08 12:05:38.000000000', 'files': ['specs/juno/gpfs-instance-store.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/bd014d021a821c284749b94656ee1406e484012d', 'message': 'Leverage the features of IBM GPFS to store cached images and instances\n\nLeverage the features of IBM GPFS to optimally store cached images and\ninstance files.\nThe bock-level format-agnostic Copy-On-Write(COW) mechanism enables quick\ninstance provisioning and instance snapshot avoiding data copy.\n\nChange-Id: I23106186368ff181707e46037d3e90158003981b\nblueprint: gpfs-instance-store\n'}]",6,105385,bd014d021a821c284749b94656ee1406e484012d,19,5,3,8037,,,0,"Leverage the features of IBM GPFS to store cached images and instances

Leverage the features of IBM GPFS to optimally store cached images and
instance files.
The bock-level format-agnostic Copy-On-Write(COW) mechanism enables quick
instance provisioning and instance snapshot avoiding data copy.

Change-Id: I23106186368ff181707e46037d3e90158003981b
blueprint: gpfs-instance-store
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/85/105385/3 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/gpfs-instance-store.rst'],1,f448395cd53d30f45fce439f5679ed2046f679ca,bp/gpfs-instance-store,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Leverage the features of IBM GPFS to store cached images and instances ========================================== https://blueprints.launchpad.net/nova/+spec/gpfs-instance-store In this blueprint we add the IBM GPFS cluster file system as instance store and image cached. Leverage the features of GPFS to optimally store cached images and instance files. The bock-level format-agnostic Copy-On-Write(COW) mechanism enables quick instance provisioning and instance snapshot avoiding data copy. Problem description =================== When user selete GPFS as instance store and image cached store, GPFS Copy-On-Write feture will enhance the performence of spawn instances and instances snapshot. GPFS Copy-On-Write mechanism avoid the data copy when image cached to base directory and snapshot instance will be generated regardless of the image format. GPFS as glance image Store blueprint have been approved. The instance snapshot image cloud be updated to the same filesystem glance GPFS image store without http upload byte by byte. Proposed change =============== Add gpfs image download module for GPFS image cached with GPFS COW avoiding date copy Add gpfs imagebackend for instance gpfs store leaverage GPFS COW to enhance instance spawn Alternatives ------------ None Data model impact ----------------- None REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- None Performance Impact ------------------ None Other deployer impact --------------------- Instance storage should be GPFS filesystem. Developer impact ---------------- None Implementation ============== Add a new class Gpfs in nova/virt/libvirt/imagebackend.py Add a new file gpfs.py in nova/image/download/gpfs.py Assignee(s) ----------- Primary assignee: renminmin Work Items ---------- Implement gpfs image download module support cached image leavarage the GPFS COW feature without data copy. Implement gpfs imagebckend support instance files generate leaverage the GPFS COW Feature without date copy regardless image format. Dependencies ============ None Testing ======= No tempest changes. Comprehensive unit tests to test the functionality have been written Documentation Impact ==================== None References ========== None ",,123,0
openstack%2Fnova-specs~master~Ifd08e305f6feee47713edcfb118ca25d58236bc8,openstack/nova-specs,master,Ifd08e305f6feee47713edcfb118ca25d58236bc8,Support Identity V3 API,ABANDONED,2014-06-30 18:38:35.000000000,2014-12-08 11:43:03.000000000,,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 782}, {'_account_id': 1247}, {'_account_id': 1849}, {'_account_id': 1916}, {'_account_id': 2903}, {'_account_id': 6486}, {'_account_id': 6873}, {'_account_id': 7191}, {'_account_id': 8574}, {'_account_id': 11428}]","[{'number': 1, 'created': '2014-06-30 18:38:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/697732f1f71daec77c6b504fb66243ede19c8a80', 'message': 'Support Identity V3 API\n\nThis spec is for adding support to Nova to use the Identity V3\nAPI.\n\nbp support-keystone-v3-api\n\nChange-Id: Ifd08e305f6feee47713edcfb118ca25d58236bc8\n'}, {'number': 2, 'created': '2014-06-30 18:41:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/10aceebe8f9792ee3f8c2a7ff224d3e683b64876', 'message': 'Support Identity V3 API\n\nThis spec is for adding support to Nova to use the Identity V3\nAPI.\n\nbp support-keystone-v3-api\n\nChange-Id: Ifd08e305f6feee47713edcfb118ca25d58236bc8\n'}, {'number': 3, 'created': '2014-07-30 21:57:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/6b389d6e2954ceb4b810ada7baa99f9f42f5f121', 'message': 'Support Identity V3 API\n\nThis spec is for adding support to Nova to use the Identity V3\nAPI.\n\nbp support-keystone-v3-api\n\nChange-Id: Ifd08e305f6feee47713edcfb118ca25d58236bc8\n'}, {'number': 4, 'created': '2014-08-12 22:16:58.000000000', 'files': ['specs/juno/support-keystone-v3-api.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/bf10ce700190655ff3760baca5e236e001c0643d', 'message': 'Support Identity V3 API\n\nThis spec is for adding support to Nova to use the Identity V3\nAPI.\n\nbp support-keystone-v3-api\n\nChange-Id: Ifd08e305f6feee47713edcfb118ca25d58236bc8\n'}]",25,103617,bf10ce700190655ff3760baca5e236e001c0643d,50,12,4,6486,,,0,"Support Identity V3 API

This spec is for adding support to Nova to use the Identity V3
API.

bp support-keystone-v3-api

Change-Id: Ifd08e305f6feee47713edcfb118ca25d58236bc8
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/17/103617/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/support-keystone-v3-api.rst'],1,697732f1f71daec77c6b504fb66243ede19c8a80,bp/support-keystone-v3-api,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ======================= Support Identity v3 API ======================= Launchpad blueprint: https://blueprints.launchpad.net/nova/+spec/support-keystone-v3-api The Keystone server has supported v3 of the Identity API for some time, and the goal is to eventually deprecate the v2 API. In order to do this Nova and other applications using the Identity API need to be able to be configured to use the v3 API rather than the v2 API. Also, the novaclient library needs to be updated to support Identity v3 authentication. Problem description =================== Nova can't be configured to use the Identity v3 API. Nova doesn't use the Identity v3 API directly, but the auth_token middleware and the Neutron client library that Nova uses do. Nova doesn't use the auth_token middleware directly. auth_token is included in the paste pipeline. When a version of auth_token middleware comes out that supports Identity v3, Nova will just start using it. As such, there's nothing to do in Nova specifically for auth_token support of Identity v3. In the case of Cinder and Glance access, Nova passes the token to cinderclient or glanceclient. Because a token is available those clients don't need to use the Identity API to get a token and therefore nothing needs to be changed in Nova for Cinder and Glance access. It's only Neutron access that needs to be changed to support Identity v3 authentication. Identity v3 authentication requires the application to specify the user and project's domain ID or domain name if the user or project name is given (the user ID and project ID are unique, but the names are only unique within a domain). New configuration options are required to be able to specify the identity API version to use and the domain for the user or project when authentication is done. For Neutron access, Nova uses python-neutronclient. Once python-neutronclient supports Identity v3 authentication (typically provided by adding support for keystoneclient's session), then Nova can be changed to set up the client to use Identity v3 authentication. Also, python-novaclient apparently has some sort of support for doing authentication with the Identity v3 API, but the suggested way is to use keystoneclient's ""session"" support, which provides auth plugins for v2 and v3. python-novaclient will be enhanced to use keystoneclient's session support. Proposed change =============== Once python-neutronclient supports using v3 for authentication, Nova will be changed to use it. The support is provided via support for keystoneclient's ""session"" support. When using the session, rather than passing the username, password, etc. to the neutronclient, you instead pass a Session object. The Session object is initialized with the authentication plugin to use, so in the case of authenticating for Neutron access, Nova will use the v3 password plugin. In order to do v3 authentication, more information is needed. The identity API version to use and the user and project domains are required. These will be new configuration options. python-novaclient will be changed to support keystoneclient's ""session"" and in this way will support using Identity API v3 for authentication. Alternatives ------------ None. Data model impact ----------------- None. REST API impact --------------- None. Security impact --------------- None. Notifications impact -------------------- None. Other end user impact --------------------- None. Performance Impact ------------------ None. Other deployer impact --------------------- There will be new configuration options for Neutron access. Here are the current relevant options for Neutron authentication:: [neutron] #admin_username=<None> #admin_tenant_id=<None> #admin_tenant_name=<None> #admin_auth_url=http://localhost:5000/v2.0 In order to use v3 auth, ``admin_auth_url`` will need to be set to a v3 endpoint or an endpoint that can be converted to a v3 endpoint. This config option will support being set to an unversioned endpoint, like ``http://localhost:5000/``. Nova will do version discovery to find if the identity server supports v3 or is v2 only. If the server reports that it supports v3 then v3 authentication will be used, otherwise Nova will fall back to v2 authentication. This will also support ``admin_auth_url`` being set to a v2.0 endpoint or a v3 endpoint. The help text for ``admin_auth_url`` will be updated. V3 authentication requires that a domain is provided for a user name or project name (otherwise it's not unique). So there will be new options ``admin_user_domain_name`` and ``admin_project_domain_name``. Keystone creates a default domain named ``Default``, so that will be the default. The ``admin_tenant_name`` option will be renamed to ``admin_project_name`` and ``admin_tenant_id`` to ``admin_project_id`` (the old names will still work but will be deprecated). Developer impact ---------------- None. Implementation ============== Assignee(s) ----------- Primary assignee: blk-u <Brant Knudson> Other contributors: - mriedem <Matt Riedemann> - jamielennox <Jamie Lennox> Work Items ---------- * Rename ``admin_tenant_name`` option to ``admin_project_name`` and ``admin_tenant_id`` to ``admin_project_id``, leaving the old option names a deprecated names. * Once neutronclient supports keystoneclient's sessions ( https://blueprints.launchpad.net/python-neutronclient/+spec/keystone-api-v3-support ) * Update requirements.txt to the version of python-neutronclient that has support for sessions. * Change Nova to use a session for existing v2 auth for Neutron access. * Change Nova to also support using v3 auth for Neutron access * Add new config options for user and project domain. * Use the V3 password plugin if configured for v3. * Change Nova to default to v3 auth rather than v2 auth. * Change Nova to also support using unversioned endpoint for Neutron access. * Change Nova to default to unversioned endpoint and do discovery. * Change devstack to allow configuring Nova for v2, v3, or to use discovery for auth. * Provide a Tempest daily configuration that sets nova to use v2 for Neutron access. * Change python-novaclient to support keystoneclient's session (This is already in progress: https://review.openstack.org/#/c/85920/ ) Dependencies ============ * python-neutronclient needs to be updated to support v3 auth. Nova will depend on the version of the python-neutronclient package that has support for v3. This work is in progress: keystone-api-v3-support_ .. _keystone-api-v3-support: https://blueprints.launchpad.net/python-neutronclient/+spec/keystone-api-v3-support Testing ======= Nova will support using both v3 auth and v2 auth, with v3 being the default. Use of v3 auth will thus be tested on every commit using the normal Tempest testing. We should ensure that using v2 auth still works as long as it's supported, and that will be done by having a nightly job that configures Nova for v2 auth. (We'll need a way to configure the system for this using devstack, so support will be added to devstack to configure Nova for v2 auth.) Documentation Impact ==================== There's new config options as described above. References ========== * `Identity v3 auth`_ .. _`Identity v3 auth`: https://github.com/openstack/identity-api/blob/master/v3/src/markdown/identity-api-v3.md#authenticate-post-authtokens ",,222,0
openstack%2Fnova-specs~master~I37904158131e7bd35e305050048f18a130ac2ecf,openstack/nova-specs,master,I37904158131e7bd35e305050048f18a130ac2ecf,Add client token to CreateServer,ABANDONED,2014-06-12 06:28:46.000000000,2014-12-08 11:42:21.000000000,,"[{'_account_id': 3}, {'_account_id': 148}, {'_account_id': 642}, {'_account_id': 782}, {'_account_id': 1849}, {'_account_id': 2271}, {'_account_id': 4571}, {'_account_id': 5292}, {'_account_id': 5441}, {'_account_id': 6167}, {'_account_id': 6488}, {'_account_id': 6873}, {'_account_id': 8328}, {'_account_id': 8441}, {'_account_id': 8844}, {'_account_id': 8922}, {'_account_id': 8988}]","[{'number': 1, 'created': '2014-06-12 06:28:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/073021b7e68d3d95d4fafc68f7a54dee09093056', 'message': 'Add client token to CreateServer\n\nWith this blueprint, using client token, Nova can guarantee\nidempotence of Nova POST request.\n\nblueprint idempotentcy-client-token\n\nChange-Id: I37904158131e7bd35e305050048f18a130ac2ecf\n'}, {'number': 2, 'created': '2014-06-12 07:00:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/52a949590f23fe836f48322f3709c0ae1ccf87df', 'message': 'Add client token to CreateServer\n\nWith this blueprint, using client token, Nova can guarantee\nidempotence of Nova POST request.\n\nblueprint idempotentcy-client-token\n\nChange-Id: I37904158131e7bd35e305050048f18a130ac2ecf\n'}, {'number': 3, 'created': '2014-06-12 07:18:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/97fcdf3886b4450fc6a33480c10f563891567b39', 'message': 'Add client token to CreateServer\n\nWith this blueprint, using client token, Nova can guarantee\nidempotence of Nova POST request.\n\nblueprint idempotentcy-client-token\n\nChange-Id: I37904158131e7bd35e305050048f18a130ac2ecf\n'}, {'number': 4, 'created': '2014-07-02 09:15:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/3de13229725f75bb3064392336f9f191e3992ca2', 'message': 'Add client token to CreateServer\n\nWith this blueprint, using client token, Nova can guarantee\nidempotence of Nova POST request.\n\nblueprint idempotentcy-client-token\n\nChange-Id: I37904158131e7bd35e305050048f18a130ac2ecf\n'}, {'number': 5, 'created': '2014-07-03 01:52:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/594e758c297920319b892010eb6b788e1bfcefff', 'message': 'Add client token to CreateServer\n\nWith this blueprint, using client token, Nova can guarantee\nidempotence of Nova POST request.\n\nblueprint idempotentcy-client-token\n\nChange-Id: I37904158131e7bd35e305050048f18a130ac2ecf\n'}, {'number': 6, 'created': '2014-07-04 06:49:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/4bc796024439e86dd4defab54a06c5d2a3dba7f5', 'message': 'Add client token to CreateServer\n\nWith this blueprint, using client token, Nova can guarantee\nidempotence of Nova POST request.\n\nblueprint idempotentcy-client-token\n\nChange-Id: I37904158131e7bd35e305050048f18a130ac2ecf\n'}, {'number': 7, 'created': '2014-07-09 03:45:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/ce47ed2b974973d74e201f59c80ecd96ba580306', 'message': 'Add client token to CreateServer\n\nWith this blueprint, using client token, Nova can guarantee\nidempotence of Nova POST request.\n\nblueprint idempotentcy-client-token\n\nChange-Id: I37904158131e7bd35e305050048f18a130ac2ecf\n'}, {'number': 8, 'created': '2014-07-09 05:27:05.000000000', 'files': ['specs/juno/idempotentcy-client-token.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/78ab283422f6b7cb4f6b6cc47a34b452bfd114af', 'message': 'Add client token to CreateServer\n\nWith this blueprint, using client token, Nova can guarantee\nidempotence of Nova POST request.\n\nblueprint idempotentcy-client-token\n\nChange-Id: I37904158131e7bd35e305050048f18a130ac2ecf\n'}]",69,99576,78ab283422f6b7cb4f6b6cc47a34b452bfd114af,67,17,8,8922,,,0,"Add client token to CreateServer

With this blueprint, using client token, Nova can guarantee
idempotence of Nova POST request.

blueprint idempotentcy-client-token

Change-Id: I37904158131e7bd35e305050048f18a130ac2ecf
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/76/99576/8 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/idempotentcy-client-token.rst'],1,073021b7e68d3d95d4fafc68f7a54dee09093056,bp/idempotentcy-client-token,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Idempotency for OpenStack API ========================================== https://blueprints.launchpad.net/nova/+spec/idempotentcy-client-token In this blueprint, I suggest ClientToken that users can specify, and post requests which idempotency is guaranteed by this ClientToken. Problem description =================== * ClientToken is implemented in the run_instance of AmazonEC2. It is also implemented in Openstack EC2 run_instance. - https://bugs.launchpad.net/nova/+bug/1188327 - https://review.openstack.org/#/c/32060/ * There is no reason to ClientToken is limited to only the run_instance. I suggest implementation of ClientToken to other Openstack API. * Use case for End User #. User requested that boot own server to OpenStack. #. Unfortunately, the client has gone down. #. There is no way to know how was his server for user. #. User thought booting was fauilure(even if it went well), so user booted a new server with same param as 1. * For billing purpose, users need to create as many servers as they want. But there is a possibility that the overage charges for two servers. * So users need to way how is his server, how is his request. In this case, idempotency client token is so useful. To specify the client token by users, users can track his request. Even if how many times users requests POST method, it is guaranteed that the state of the POST request which was same with return of user's first POST request. * For a major reworking of something existing it would describe the problems in that feature that are being addressed. Proposed change =============== In curl command, you add header of request 'X-Client-Token: foo'. X-Client-Token should be uuid or any strings. In this name, I already implemented python-novaclient implementation. So, you can put request like below. 'nova --x-client-token boot --flavor 1 --image hoge ...' If you have good name instead of 'X-Client-Token', please tell me. After, if you retry same request parameter and same X-Client-Token, now OpenStack returns GET response of instance. You can correctly know instance's status and don't happen to boot needless instance. * Now, I target at CreateServer as first. * And I will implement this to any POST API in Nova in time. * In future, it is useful this specification is adopted to any POST API. Alternatives ------------ * TaskAPI https://blueprints.launchpad.net/nova/+spec/instance-tasks-api First, I wanted to use TaskAPI. But it is needed to set instance_id as required parameter to use this TaskAPI. I imagene the case that client can't get instance_id, so I can't use this TaskAPI what I want to resolve. * Instance tag https://blueprints.launchpad.net/nova/+spec/tag-instances This is add tag to instance not request. To prevent any needless request, it is needed to add tag for request. Data model impact ----------------- It is a decorator function. I made PoC as decorator, so if you want to idempotent, you set this decorator any APIs you want. * Every data related to this blueprint stored in memcached as below. <client_token>: <[url, body, tenant_id]> * Memcached data's life could be cahnged in 'memcache_expiration' configuration. REST API impact --------------- This idempotent feature decorator can be used with POST API to v3. * First POST request, it returns 202. * Second POST request, with same 'X-Client-Token' and same request parameter, it also returns 202 but response content is same as GET response. * Second POST request, with same 'X-Client-Token' and different request parameter, it returns 409 because that 'X-Client-Token' is already used. * Now, second POST request returns GET result with schema for GET, in the near future, I want to return POST schema with GET response content. See concrete example below of this link: https://github.com/ntt-sic/nova/commit/c9bc157b122907d7bd7e98b364137b7ecd47bd0f Security impact --------------- This decorator judge by tenant_id scope. So can specify same 'X-Client-Token' if tenant is differ. Notifications impact -------------------- None Other end user impact --------------------- If users don't specfiy this 'X-Client-Token', there is no effect to original POST request. In case of using python nova-client, need to modify it to accept 'X-Client-Token'. Performance Impact ------------------ None Other deployer impact --------------------- Now, this feature using memcached. So you need to add your /etc/nova/nova.conf as below. * memcached_servers=YourIP:11211 If memcached is not so appropriate, I re-implement with other way like DB. Developer impact ---------------- None Implementation ============== Assignee(s) ----------- Primary assignee: haruka tanizawa(h-tanizawa) Other contributors: None Work Items ---------- 1. Now second request returns GET result. I want to return same schema as first POST request. 2. About flavor, decorated function and show function is in different class. So, I need to resolve this problem. 3. Consolidate decorator's resolver. 4. I implemented idempotent.py temporarily. Appropriate file path is need to be considered. 5. This feature is decorator. And I applied this decorator to 'Create Server'. In nova, I also applied it to create keypair. I am going to apply other nova POST method. Dependencies ============ One of Heat blueprint depend on this blueprint. * Support API retry function with Idempotency in creating/updating a stack https://blueprints.launchpad.net/heat/+spec/support-retry-with-idempotency Testing ======= Add decorator unittest. Moreover I should tempest for testing multiple times request with same parameter and same client token. And combination of tenant, request URL, parameter, client token etc... Documentation Impact ==================== There are some documentation impacts. First, new request parameter is added. Users can use this if he wants. Second, response of POST request is differ from how many times request. References ========== Mailing list discussions - https://lists.launchpad.net/openstack/msg13082.html - http://lists.openstack.org/pipermail/openstack-dev/2013-October/017691.html Related specifications in EC2 - http://goo.gl/8gQX8X - http://goo.gl/Awphn9 ",,245,0
openstack%2Fnova-specs~master~I1c0d650e16904790a964c5626612c576ce1de213,openstack/nova-specs,master,I1c0d650e16904790a964c5626612c576ce1de213,Dedicate aggregates for specific tenants,ABANDONED,2014-06-11 20:49:12.000000000,2014-12-08 11:41:50.000000000,,"[{'_account_id': 3}, {'_account_id': 136}, {'_account_id': 782}, {'_account_id': 1501}, {'_account_id': 1849}, {'_account_id': 2033}, {'_account_id': 2468}, {'_account_id': 4573}, {'_account_id': 6816}, {'_account_id': 7166}]","[{'number': 1, 'created': '2014-06-11 20:49:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/740ed347956616ed7e1c7821881945556efe158f', 'message': 'Dedicate aggregates for specific tenants\n\nExtend the ""AggregateMultiTenancyIsolation"" scheduler filter\nin order to optionally exclude instances from non-defined tenant(s)\nto be deployed in the aggregate(s).\n\nChange-Id: I1c0d650e16904790a964c5626612c576ce1de213\n'}, {'number': 2, 'created': '2014-06-12 06:54:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/5f048c15ca15f89bd075224aad9da1f14504f807', 'message': 'Dedicate aggregates for specific tenants\n\nExtend the ""AggregateMultiTenancyIsolation"" scheduler filter\nin order to optionally exclude instances from being schedule\nfor deployment in hosts that don\'t belong to the defined\naggregate(s).\n\nChange-Id: I1c0d650e16904790a964c5626612c576ce1de213\n'}, {'number': 3, 'created': '2014-06-12 09:04:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/ecc6e032ade5341a0db365006811d5ef1452ecb3', 'message': 'Dedicate aggregates for specific tenants\n\nExtend the ""AggregateMultiTenancyIsolation"" scheduler filter\nin order to optionally exclude instances from being schedule\nfor deployment in hosts that don\'t belong to the defined\naggregate(s).\n\nChange-Id: I1c0d650e16904790a964c5626612c576ce1de213\n'}, {'number': 4, 'created': '2014-06-13 06:13:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/a9cf9ad2ef7ee89acd331e9414e6555c87512054', 'message': 'Dedicate aggregates for specific tenants\n\nExtend the ""AggregateMultiTenancyIsolation"" scheduler filter\nin order to optionally exclude instances from being schedule\nfor deployment in hosts that don\'t belong to the defined\naggregate(s).\n\nblueprint multi-tenancy-isolation-only-aggregates\n\nChange-Id: I1c0d650e16904790a964c5626612c576ce1de213\n'}, {'number': 5, 'created': '2014-06-29 22:23:59.000000000', 'files': ['specs/juno/tenant-aggregate-exclusive-filter.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/e2c8693d6718d6d3c4e77a064fe02afa59202a73', 'message': 'Dedicate aggregates for specific tenants\n\nExtend the ""AggregateMultiTenancyIsolation"" scheduler filter\nin order to optionally exclude instances from being schedule\nfor deployment in hosts that don\'t belong to the defined\naggregate(s).\n\nblueprint multi-tenancy-isolation-only-aggregates\n\nChange-Id: I1c0d650e16904790a964c5626612c576ce1de213\n'}]",40,99476,e2c8693d6718d6d3c4e77a064fe02afa59202a73,62,10,5,2033,,,0,"Dedicate aggregates for specific tenants

Extend the ""AggregateMultiTenancyIsolation"" scheduler filter
in order to optionally exclude instances from being schedule
for deployment in hosts that don't belong to the defined
aggregate(s).

blueprint multi-tenancy-isolation-only-aggregates

Change-Id: I1c0d650e16904790a964c5626612c576ce1de213
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/76/99476/3 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/tenant-aggregate-exclusive-filter.rst'],1,740ed347956616ed7e1c7821881945556efe158f,bp/multi-tenancy-isolation-only-aggregates,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ============================ Tenants to Aggregate Filter ============================ https://blueprints.launchpad.net/nova/+spec/multi-tenancy-isolation-only\ -aggregates Extend the ""AggregateMultiTenancyIsolation"" scheduler filter in order to optionally exclude instances from non-defined tenant(s) to be deployed in the aggregate(s). Problem description =================== The ""AggregateMultiTenancyIsolation"" scheduler filter allows the creation of instances from defined tenants in specific aggregates. If an aggregate has the metadata key ""filter_tenant_id"" defined with tenant ids, (ex: filter_tenant_id=tenant_id1,tenant_id2) instances from those tenants will be scheduled for deployment in the aggregate. However, it doesn't exclude instances from other tenants to be scheduled in the aggregate. Is not possible to exclusively dedicate an aggregate to a set of tenants. Proposed change =============== We propose the introduction of the ""filter_tenant_exclusive"" aggregate metadata key in the ""AggregateMultiTenancyIsolation"" scheduler filter. If defined and its value is ""true"" only instances from the defined tenants in ""filter_tenant_id"" will be scheduled for deployment to the aggregate. This will allow the exclusive dedication of an aggregate to a set of tenants. Alternatives ------------ Implement a new filter with this functionality. Data model impact ----------------- None REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- Current filter behavior is preserved and remains the default. New functionality needs to be explicitly enabled by adding the aggregate metadata ""filter_tenant_exclusive=true"". Performance Impact ------------------ The filter will have another DB query if ""filter_tenant_exclusive=true"". Depending in the deployment size it could have a performance impact. Documentation will reflect this. Other deployer impact --------------------- None Developer impact ---------------- None Implementation ============== Work Items ---------- * Modify ""AggregateMultiTenancyIsolation"" to dedicate an aggregate to a set of defined tenants if ""filter_tenant_exclusive"" aggregate metadata is defined and is ""true"". * Add a new DB query ""aggregate_metadata_get_all_by_key()"" in order to get all aggregates with a specific metadata key. * Change the scheduler filter documentation reflecting the new functionality. Assignee(s) ----------- Primary assignee: moreira-belmiro-email-lists Other contributors: jesse-pretorius Dependencies ============ None Testing ======= Additional unit tests: * Set aggregate metadata filter_tenant_exclusive=true. Validate that if the tenant is not in the filter_tenant_id list then the host fails. * Set aggregate metadata filter_tenant_exclusive=true. Validate that if the tenant is in the filter_tenant_id list then the host passes. * Set aggregate metadata filter_tenant_exclusive=false. Validate that the host passes for any tenant. * Validate db query ""aggregate_metadata_get_all_by_key()"". Documentation Impact ==================== ""AggregateMultiTenancyIsolation"" documentation will be updated with the new feature. References ========== None ",,142,0
openstack%2Fnova-specs~master~Id8009b458ee139b34c84522333f3f116f173b3f1,openstack/nova-specs,master,Id8009b458ee139b34c84522333f3f116f173b3f1,Add nova spec for bp/isnot-operator,ABANDONED,2014-06-03 09:57:49.000000000,2014-12-08 11:41:17.000000000,,"[{'_account_id': 3}, {'_account_id': 136}, {'_account_id': 782}, {'_account_id': 1561}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 7872}]","[{'number': 1, 'created': '2014-06-03 09:57:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/e00f283657f491c7b8a82c8fc88de1cb7692bebe', 'message': 'Add nova spec for bp/isnot-operator\n\nChange-Id: Id8009b458ee139b34c84522333f3f116f173b3f1\nPartially-implements: bp isnot-operator\n'}, {'number': 2, 'created': '2014-06-03 10:01:47.000000000', 'files': ['specs/juno/isnot-operator.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/32f18117de87e334b4b1eeb2265bedfc4ba4fa6a', 'message': 'Add nova spec for bp/isnot-operator\n\nChange-Id: Id8009b458ee139b34c84522333f3f116f173b3f1\nPartially-implements: bp isnot-operator\n'}]",4,97441,32f18117de87e334b4b1eeb2265bedfc4ba4fa6a,21,7,2,7872,,,0,"Add nova spec for bp/isnot-operator

Change-Id: Id8009b458ee139b34c84522333f3f116f173b3f1
Partially-implements: bp isnot-operator
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/41/97441/2 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/isnot-operator.rst'],1,e00f283657f491c7b8a82c8fc88de1cb7692bebe,bp/isnot-operator,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== IsNot operator for boolean specs ========================================== https://blueprints.launchpad.net/nova/+spec/isnot-operator This blueprint introduces an new operator in the extra specs filter to have an easy way to define a ""default"" behavior . Problem description =================== Defining a group of hosts in a host aggregate group with a boolean extra spec set: nova aggregate-create fast-io nova nova aggregate-set-metadata fast-io ssd=true nova aggregate-add-host fast-io node1 Defining a flavor that matches ssd=true will filter (using the extra-spec filter) only node1. In order to have a list of all other nodes (without ssd=true) it's needed to have a a second group that defined ssh=false. Proposed change =============== Add a ""isNot"" operator to extra_spec filter with that it is possible to check if the a certain extra_spec is not set. Alternatives ------------ Have a ""default"" value field for specs. Data model impact ----------------- None. REST API impact --------------- None. Security impact --------------- None. Notifications impact -------------------- None. Other end user impact --------------------- None. The user can specify the new operator within the nova-manage command. Performance Impact ------------------ None. Other deployer impact --------------------- None. Developer impact ---------------- None. Implementation ============== Assignee(s) ----------- Primary assignee: Marc Koderer <m-koderer> Work Items ---------- TBD Dependencies ============ None. Testing ======= Should be tested with unit test and inside Tempest. Documentation Impact ==================== New operator need to get documented. References ========== None. ",,118,0
openstack%2Fnova-specs~master~Id19c61a89f05a6b5c02e78dbfe48236060a27245,openstack/nova-specs,master,Id19c61a89f05a6b5c02e78dbfe48236060a27245,Smart Scheduler (Solver Scheduler) - Constraint based resource placement,ABANDONED,2014-05-29 18:12:37.000000000,2014-12-08 11:40:55.000000000,,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 681}, {'_account_id': 782}, {'_account_id': 1561}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 1981}, {'_account_id': 2271}, {'_account_id': 6772}, {'_account_id': 7166}, {'_account_id': 7494}, {'_account_id': 8328}, {'_account_id': 8514}, {'_account_id': 8890}, {'_account_id': 9100}, {'_account_id': 10257}]","[{'number': 1, 'created': '2014-05-29 18:12:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/38aa709fc7cfd0b10239f50ba989f0efa75b443f', 'message': ""Add a pluggable solver scheduler\n\nThis feature provides a solver scheduler driver, which enables complex\nconstraints-based scheduling to start within the Nova or Gantt project to\nbegin with.  The blueprint was approved and the code patches were submitted\nas part of the Icehouse cycle. This features supports for pluggable costs and\nconstraints that can be added easily.\nThe first patch set includes the driver and  an illustrative implementation\nof a solver is included and is based on PULP. Here the constraints are based\non the disk space and memory requirements, and optimization is done using the\nhost's free ram as a cost metric.\n\nProvides blueprint spec for: blueprint solver-scheduler\n\nChange-Id: Id19c61a89f05a6b5c02e78dbfe48236060a27245\n""}, {'number': 2, 'created': '2014-05-29 18:18:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/c43532b9567491787959630924c6fe759864f5bf', 'message': ""Add a pluggable solver scheduler\n\nThis feature provides a solver scheduler driver, which enables complex\nconstraints-based scheduling to start within the Nova or Gantt project to\nbegin with.  The blueprint was approved and the code patches were submitted\nas part of the Icehouse cycle. This features supports for pluggable costs and\nconstraints that can be added easily.\nThe first patch set includes the driver and  an illustrative implementation\nof a solver is included and is based on PULP. Here the constraints are based\non the disk space and memory requirements, and optimization is done using the\nhost's free ram as a cost metric.\n\nProvides blueprint spec for: blueprint solver-scheduler\n\nChange-Id: Id19c61a89f05a6b5c02e78dbfe48236060a27245\n""}, {'number': 3, 'created': '2014-05-29 18:19:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/550f8a0de09593791f13a3e0530c14051bdbc00b', 'message': ""Add a pluggable solver scheduler\n\nThis feature provides a solver scheduler driver, which enables complex\nconstraints-based scheduling to start within the Nova or Gantt project to\nbegin with.  The feature was approved and the code patches were submitted\nas part of the Icehouse cycle. This features supports for pluggable costs and\nconstraints that can be added easily.\nThe first patch set includes the driver and  an illustrative implementation\nof a solver is included and is based on PULP. Here the constraints are based\non the disk space and memory requirements, and optimization is done using the\nhost's free ram as a cost metric.\n\nProvides spec for: blueprint solver-scheduler\n\nChange-Id: Id19c61a89f05a6b5c02e78dbfe48236060a27245\n""}, {'number': 4, 'created': '2014-05-29 18:24:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/b9a2bfb3674e2c04f434404ef111e71c891f31b9', 'message': ""Add a pluggable solver scheduler\n\nThis feature provides a solver scheduler driver, which enables complex\nconstraints-based scheduling to start within the Nova or Gantt project to\nbegin with.  The feature was approved and the code patches were submitted\nas part of the Icehouse cycle. This features supports for pluggable costs and\nconstraints that can be added easily.\nThe first patch set includes the driver and  an illustrative implementation\nof a solver is included and is based on PULP. Here the constraints are based\non the disk space and memory requirements, and optimization is done using the\nhost's free ram as a cost metric.\n\nProvides spec for: blueprint solver-scheduler\n\nChange-Id: Id19c61a89f05a6b5c02e78dbfe48236060a27245\n""}, {'number': 5, 'created': '2014-05-29 18:51:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/6a4b98e4a3e675c14b83cb9a9c5f55fc65a52218', 'message': ""Smart Scheduler: A pluggable scheduler driver (Solver Scheduler)\n\nThis feature provides a smart scheduler driver, which enables complex\nconstraints-based scheduling to start within the Nova or Gantt project to\nbegin with.  The feature was approved and the code patches were submitted\nas part of the Icehouse cycle. This features supports for pluggable costs and\nconstraints that can be added easily.\nThe first patch set includes the driver and  an illustrative implementation\nof a solver is included and is based on PULP. Here the constraints are based\non the disk space and memory requirements, and optimization is done using the\nhost's free ram as a cost metric.\n\nProvides spec for: blueprint solver-scheduler\n\nChange-Id: Id19c61a89f05a6b5c02e78dbfe48236060a27245\n""}, {'number': 6, 'created': '2014-05-29 18:55:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/e677532d4ec7c54d1ff2f455fa9c3c8d1f591882', 'message': ""Smart Scheduler: A pluggable scheduler driver (Solver Scheduler)\n\nThis feature provides a smart scheduler driver, which enables complex\nconstraints-based scheduling to start within the Nova or Gantt project to\nbegin with.  The feature was approved and the code patches were submitted\nas part of the Icehouse cycle. This features supports for pluggable costs and\nconstraints that can be added easily.\nThe first patch set includes the driver and  an illustrative implementation\nof a solver is included and is based on PULP. Here the constraints are based\non the disk space and memory requirements, and optimization is done using the\nhost's free ram as a cost metric.\n\nProvides spec for: blueprint solver-scheduler\n\nChange-Id: Id19c61a89f05a6b5c02e78dbfe48236060a27245\n""}, {'number': 7, 'created': '2014-07-16 23:08:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/ed869b532e3151921cf6816146b444ac82711e52', 'message': ""Smart Scheduler (Solver Scheduler) - Constraint based resource placement\n\nOpenStack scheduler currently supports nice options via pluggable filters that\ndoes resource selection based on simple constraints e.g. Don't put instances\non a given host.  However for complex constraints, building a filter would be\nas complex as building a real constraint solver with complex constraints e.g.\nplace VMs while minimizing average (VM-storage-bandwidth).  On the other hand,\ncomplex solvers are available in open source (PULP, CVXOPT, COIN_OR).  Hence a\nnatural solution to the above problem is to design a pluggable scheduler that\nleverages existing solvers.  We believe that this will open new avenues for\ncomplex constraint (and objectives) based resource placement in large\nOpenStack deployments.\n\nProvides spec for: blueprint solver-scheduler\n\nChange-Id: Id19c61a89f05a6b5c02e78dbfe48236060a27245\n""}, {'number': 8, 'created': '2014-07-16 23:12:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/432332554b8a7cfbba9eb66d7c206783fe08ab72', 'message': ""Smart Scheduler (Solver Scheduler) - Constraint based resource placement\n\nOpenStack scheduler currently supports nice options via pluggable filters that\ndoes resource selection based on simple constraints e.g. Don't put instances\non a given host.  However for complex constraints, building a filter would be\nas complex as building a real constraint solver with complex constraints e.g.\nplace VMs while minimizing average (VM-storage-bandwidth).  On the other hand,\ncomplex solvers are available in open source (PULP, CVXOPT, COIN_OR).  Hence a\nnatural solution to the above problem is to design a pluggable scheduler that\nleverages existing solvers.  We believe that this will open new avenues for\ncomplex constraint (and objectives) based resource placement in large\nOpenStack deployments.\n\nProvides spec for: blueprint solver-scheduler\n\nChange-Id: Id19c61a89f05a6b5c02e78dbfe48236060a27245\n""}, {'number': 9, 'created': '2014-07-16 23:15:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/ee5b26c7b7acf774bf3cd630bf508c6c82f31bbb', 'message': ""Smart Scheduler (Solver Scheduler) - Constraint based resource placement\n\nOpenStack scheduler currently supports nice options via pluggable filters that\ndoes resource selection based on simple constraints e.g. Don't put instances\non a given host.  However for complex constraints, building a filter would be\nas complex as building a real constraint solver with complex constraints e.g.\nplace VMs while minimizing average (VM-storage-bandwidth).  On the other hand,\ncomplex solvers are available in open source (PULP, CVXOPT, COIN_OR).  Hence a\nnatural solution to the above problem is to design a pluggable scheduler that\nleverages existing solvers.  We believe that this will open new avenues for\ncomplex constraint (and objectives) based resource placement in large\nOpenStack deployments.\n\nProvides spec for: blueprint solver-scheduler\n\nChange-Id: Id19c61a89f05a6b5c02e78dbfe48236060a27245\n""}, {'number': 10, 'created': '2014-07-16 23:34:06.000000000', 'files': ['specs/juno/solver-scheduler.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/4a1a97ecfec86a0a6e1926644bdc0af90a82fb7e', 'message': ""Smart Scheduler (Solver Scheduler) - Constraint based resource placement\n\nOpenStack scheduler currently supports nice options via pluggable filters that\ndoes resource selection based on simple constraints e.g. Don't put instances\non a given host.  However for complex constraints, building a filter would be\nas complex as building a real constraint solver with complex constraints e.g.\nplace VMs while minimizing average (VM-storage-bandwidth).  On the other hand,\ncomplex solvers are available in open source (PULP, CVXOPT, COIN_OR).  Hence a\nnatural solution to the above problem is to design a pluggable scheduler that\nleverages existing solvers.  We believe that this will open new avenues for\ncomplex constraint (and objectives) based resource placement in large\nOpenStack deployments.\n\nProvides spec for: blueprint solver-scheduler\n\nChange-Id: Id19c61a89f05a6b5c02e78dbfe48236060a27245\n""}]",16,96543,4a1a97ecfec86a0a6e1926644bdc0af90a82fb7e,52,18,10,8514,,,0,"Smart Scheduler (Solver Scheduler) - Constraint based resource placement

OpenStack scheduler currently supports nice options via pluggable filters that
does resource selection based on simple constraints e.g. Don't put instances
on a given host.  However for complex constraints, building a filter would be
as complex as building a real constraint solver with complex constraints e.g.
place VMs while minimizing average (VM-storage-bandwidth).  On the other hand,
complex solvers are available in open source (PULP, CVXOPT, COIN_OR).  Hence a
natural solution to the above problem is to design a pluggable scheduler that
leverages existing solvers.  We believe that this will open new avenues for
complex constraint (and objectives) based resource placement in large
OpenStack deployments.

Provides spec for: blueprint solver-scheduler

Change-Id: Id19c61a89f05a6b5c02e78dbfe48236060a27245
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/43/96543/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/solver_scheduler.rst'],1,38aa709fc7cfd0b10239f50ba989f0efa75b443f,bp/solver-scheduler,"Solver Scheduler ================ The **Solver Scheduler** provides an extensible mechanism for making smarter, complex constraints optimization based resource scheduling in Nova. This driver supports pluggable Solvers, that can leverage existing complex constraint solving frameworks, available in open source such as PULP_, CVXOPT_, `Google OR-TOOLS`_, etc. This Scheduler is currently supported to work with Compute Nodes in Nova. .. _PULP: https://projects.coin-or.org/PuLP .. _CVXOPT: http://cvxopt.org/ .. _`Google OR-TOOLS`: https://code.google.com/p/or-tools/ The Nova compute resource placement can be described as a problem of placing a set of VMs on a set of physical hosts, where each VM has a set of resource requirements that have to be satisfied by a host with available resource capacity. In addition to the constraints, we optimize the solution for some cost metrics, so that the net cost of placing all VMs to certain hosts is minimized. A pluggable Solver used by the Solver Scheduler driver models the Nova compute placement request as a constraint optimization problem using a set of constraints derived from the placement request specification and a net cost value to optimize. A Solver implementation should model the constraints and costs and feed it to a constraint problem specification, which is eventually solved by using any external solvers such as the COIN-OR_ CLP_, CBC_, GLPK_, and so on. .. _COIN-OR: http://en.wikipedia.org/wiki/COIN-OR .. _CLP: http://en.wikipedia.org/wiki/COIN-OR#CLP .. _CBC: http://en.wikipedia.org/wiki/COIN-OR#CBC .. _GLPK: http://en.wikipedia.org/wiki/GNU_Linear_Programming_Kit The Nova compute resource placement optimization problem when subject to a set of linear constraints, can be formulated and solved as a `linear programming`_ problem. A **linear programming (LP)** problem involves maximizing or minimizing a linear function subject to linear constraints. .. _linear programming: http://en.wikipedia.org/wiki/Linear_programming Solvers ------- All Solver implementations will be in the module (:mod:`nova.scheduler.solvers`). A solver implementation should be a subclass of ``solvers.BaseHostSolver`` and they implement the ``host_solve`` method. This method returns a list of host-instance tuples after solving the constraints optimization problem. A Reference Solver Implementation --------------------------------- |HostsPulpSolver| is a reference solver implementation that models the Nova scheduling problem as a linear programming (LP) problem using the PULP_ modeling framework. This example implementation is a working solver that includes the required disk and memory as constraints, and uses the free ram as a cost metric to maximize (for spreading hosts), or minimize (for stacking) for the LP problem. An example LP problem formulation is provided below to describe how this example solver models the problem in LP. Consider there are 2 hosts `Host_1` and `Host_2`, with available resources described as a tuple (usable_disk_mb, usable_memory_mb, free_ram_mb): * `Host_1`: (2048, 2048, 2048) * `Host_2`: (4096, 1536, 1536) There are two VM requests with the following disk and memory requirements: * `VM_1`: (1024, 512) * `VM_2`: (1024, 512) To formulate this problem as a LP problem, we use the variables: `X11`, `X12`, `X21`, `X22`. Here, a variable `Xij` takes the value `1` if `VM_i` is placed on `Host_j`, `0` otherwise. If the problem objective is to minimize the cost metric of free_ram_mb, the mathematical LP formulation of this example is as follows: :: Minimize (2048*X11 + 2048*X21 + 1536*X12 + 1536*X22) subject to constraints: X11*1024 + X21*1024 <= 2048 (disk maximum supply for Host_1) X11*512 + X21*512 <= 2048 (memory maximum supply for Host_1) X12*1024 + X22*1024 <= 4096 (disk maximum supply for Host_2) X12*512 + X22*512 <= 1536 (memory maximum supply for Host_2) X11*1024 + X12*1024 >= 1024 (disk minimum demand for VM_1) X11*512 + X12*512 >= 512 (memory minimum demand for VM_1) X21*1024 + X22*1024 >= 1024 (disk minimum demand for VM_2) X21*512 + X22*512 >= 512 (memory minimum demand for VM_2) X11 + X12 == 1 (VM_1 can run in only 1 Host) X21 + X22 == 1 (VM_2 can run in only 1 Host) `X11` = 0, `X12` = 1, `X21` = 0, and `X22` = 1 happens to be the optimal solution, implying, both `VM_1` and `VM_2` will be hosted in `Host_2`. |HostsPulpSolver| models such LP problems using the PULP_ LP modeler written in Python. This problem is solved for an optimal solution using an external solver supported by PULP such as CLP_, CBC_, GLPK_, etc. By default, PULP_ uses the CBC_ solver, which is packaged with the `coinor.pulp`_ distribution. .. _`coinor.pulp`: https://pypi.python.org/pypi/coinor.pulp Additional Solver implementations are planned in the roadmap, that support pluggable constraints and costs. Configuration ------------- To use Solver Scheduler, the nova.conf should contain the following settings under the ``[solver_scheduler]`` namespace: `The Solver Scheduler driver to use (required):` ``scheduler_driver=nova.scheduler.solver_scheduler.ConstraintSolverScheduler`` `The Solver implementation to use:` ``scheduler_host_solver=nova.scheduler.solvers.hosts_pulp_solver.HostsPulpSolver`` When using the default provided Solver implementation |HostsPulpSolver|, the following default values of these settings can be modified: These are under the ``[DEFAULT]`` namespace, as they are also being used by the Filter Scheduler as well. `The ram weight multiplier. A negative value indicates stacking as opposed to spreading:` ``ram_weight_multiplier=1.0`` `Virtual disk to physical disk allocation ratio:` ``disk_allocation_ratio=1.0`` `Virtual ram to physical ram allocation ratio:` ``ram_allocation_ratio=1.5`` .. |HostsPulpSolver| replace:: :class:`HostsPulpSolver <nova.scheduler.solvers.hosts_pulp_solver.HostsPulpSolver>` ",,152,0
openstack%2Fnova-specs~master~I08b4172781eccfa28fee5fe30c29f1626f36ee59,openstack/nova-specs,master,I08b4172781eccfa28fee5fe30c29f1626f36ee59,Add utilization based weighers on top of MetricsWeigher,ABANDONED,2014-04-28 05:49:54.000000000,2014-12-08 11:39:53.000000000,,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 782}, {'_account_id': 1849}, {'_account_id': 4458}, {'_account_id': 4992}]","[{'number': 1, 'created': '2014-04-28 05:49:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/5ea711d9e4a973e088ba30e5b812118de702b01c', 'message': 'Add utilization based weighers on top of MetricsWeigher\n\nThis feature is to add some predefined weighers based on MetricsWeigher.\nThe weighers will tell which metrics should be monitored and how much they\nshould be considered, if an aspect is cared by some user, after experiments.\n\nThose weighers could be used by the users directly without or with less\nknowledge on monitors, metrics and their ratios. They are under the\ninfrastructure of Utilization Based Scheduling.\n\nblueprint add-utilization-based-weighers\nblueprint utilization-aware-scheduling\n\nChange-Id: I08b4172781eccfa28fee5fe30c29f1626f36ee59\n'}, {'number': 2, 'created': '2014-04-30 09:23:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/14842f2dcf045026783d3f0b514514056fb9fb8b', 'message': 'Add utilization based weighers on top of MetricsWeigher\n\nThis feature is to add some predefined weighers based on MetricsWeigher.\nThe weighers will tell which metrics should be monitored and how much they\nshould be considered, if an aspect is cared by some user, after experiments.\n\nThose weighers could be used by the users directly without or with less\nknowledge on monitors, metrics and their ratios. They are under the\ninfrastructure of Utilization Based Scheduling.\n\nblueprint add-utilization-based-weighers\n\nChange-Id: I08b4172781eccfa28fee5fe30c29f1626f36ee59\n'}, {'number': 3, 'created': '2014-06-04 08:38:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/239659372b8da08cd07c867004a795ca3fd44550', 'message': 'Add utilization based weighers on top of MetricsWeigher\n\nThis feature is to add some predefined weighers based on MetricsWeigher.\nThe weighers will tell which metrics should be monitored and how much they\nshould be considered, if an aspect is cared by some user, after experiments.\n\nThose weighers could be used by the users directly without or with less\nknowledge on monitors, metrics and their ratios. They are under the\ninfrastructure of Utilization Based Scheduling.\n\nblueprint add-utilization-based-weighers\n\nChange-Id: I08b4172781eccfa28fee5fe30c29f1626f36ee59\n'}, {'number': 4, 'created': '2014-06-26 09:44:27.000000000', 'files': ['specs/juno/add-utilization-based-weighers.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/c1d6decb77845a0d16a57773d23b1a467bf1e9f2', 'message': 'Add utilization based weighers on top of MetricsWeigher\n\nThis feature is to add some predefined weighers based on MetricsWeigher.\nThe weighers will tell which metrics should be monitored and how much they\nshould be considered, if an aspect is cared by some user, after experiments.\n\nThose weighers could be used by the users directly without or with less\nknowledge on monitors, metrics and their ratios. They are under the\ninfrastructure of Utilization Based Scheduling.\n\nblueprint add-utilization-based-weighers\n\nChange-Id: I08b4172781eccfa28fee5fe30c29f1626f36ee59\n'}]",15,90647,c1d6decb77845a0d16a57773d23b1a467bf1e9f2,32,6,4,4458,,,0,"Add utilization based weighers on top of MetricsWeigher

This feature is to add some predefined weighers based on MetricsWeigher.
The weighers will tell which metrics should be monitored and how much they
should be considered, if an aspect is cared by some user, after experiments.

Those weighers could be used by the users directly without or with less
knowledge on monitors, metrics and their ratios. They are under the
infrastructure of Utilization Based Scheduling.

blueprint add-utilization-based-weighers

Change-Id: I08b4172781eccfa28fee5fe30c29f1626f36ee59
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/47/90647/2 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/add-utilization-based-weighers.rst'],1,5ea711d9e4a973e088ba30e5b812118de702b01c,bp/add-utilization-based-weighers,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Add utilization based weighers ========================================== Include the URL of your launchpad blueprint: https://blueprints.launchpad.net/nova/+spec/add-utilization-based-weighers Utilization Based Scheduling (UBS) has implemented a MetricsWeigher to set the ratios for different metrics to specify how much they are considered when scheduling. However, it is flexible so not easy for users to use it because probably users don't know how many metrics are available and what each metric exactly means. The plan is to create some predefined weighers as samples from advanced scheduling algorithms based on the experimental results. For instance, CPUUtilWeigher inheriting from MetricsWeigher can consider CPU load, CPU utilization percentage, and CPU frequency if needed, and we set proper ratios on them. Problem description =================== Utilization Based Scheduling (UBS) has implemented a MetricsWeigher to set the ratios for different metrics to specify how much they are considered when scheduling. For instance, at least users need to set: - weight_setting, which means how the metrics are going to be weighed. This should be in the form of ""<name1>=<ratio1>, <name2>=<ratio2>, ..."", where <nameX> is one of the metrics to be weighed, and <ratioX> is the corresponding ratio. Additionally, users need to add <name1>, <name2>, ... into the nova.conf file in order that the resource tracker will load the corresponding monitors to support collecting metrics. However, users probably don't know what <nameX> exactly means, and how to set the proper ratio for each. Proposed change =============== In our plan, we hope to create some predefined weighers based on some experimental results, like RAMWeigher, where users don't need to set weight_setting, and others, but weight_multiplier only if they like. In the implementation, the weighers are going to be implemented as python classes inheriting from class MetricsWeigher. And we hope to generate a sample nova.conf being filled in proper monitors to get those metrics if possible, or explain that in a README file. For example, if a user cares CPU utilization for a period of time and doesn't want to schedule its VM on an ""always-busy"" host. We could figure out that CPU load would be an important aspect to measure by experiments, and pre-define a CPUUtilWeigher based on that for that user to use directly. Alternatives ------------ None Data model impact ----------------- None REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- Users should have more meaningful weighers to use, which is more convenient. Performance Impact ------------------ None Other deployer impact --------------------- None Developer impact ---------------- None Implementation ============== Assignee(s) ----------- Primary assignee: shane-wang Other contributors: lianhao-lu Work Items ---------- We plan to implement CPUUtilWeigher first since CPU monitor has been merged. Dependencies ============ If we hope to implement more meaningful weighers based on MetricsWeigher. We have to depend on the approval and the implementation of blueprint add-useful-metrics whose nova spec proposal is at https://review.openstack.org/#/c/89766/. Testing ======= Unit tests are enough to cover testing. Some specific weighers may need specific monitors which are supported on the 3rd party hardware platforms. Documentation Impact ==================== None References ========== None ",,147,0
openstack%2Fnova-specs~master~I81c3be3e99e96abb7dbf75ca3ff057bbede3e5fd,openstack/nova-specs,master,I81c3be3e99e96abb7dbf75ca3ff057bbede3e5fd,Add useful metrics into utilization based scheduling,ABANDONED,2014-04-23 07:54:53.000000000,2014-12-08 11:38:51.000000000,,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 782}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 4458}, {'_account_id': 6062}, {'_account_id': 11235}]","[{'number': 1, 'created': '2014-04-23 07:54:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/0c99677994aaee7cef210da9f8d5418edfea4604', 'message': 'Add useful metrics into utilization based scheduling\n\nThis feature is to add more useful metrics to monitor under the infrastructure\nof Utilization Based Scheduling.\n\nblueprint add-useful-metrics\nblueprint utilization-aware-scheduling\n\nChange-Id: I81c3be3e99e96abb7dbf75ca3ff057bbede3e5fd\n'}, {'number': 2, 'created': '2014-04-30 07:02:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/d9ae2ee65e94a71e7a7c4456612352198cb76df3', 'message': 'Add useful metrics into utilization based scheduling\n\nThis feature is to add more useful metrics to monitor under the infrastructure\nof Utilization Based Scheduling.\n\nblueprint add-useful-metrics\n\nChange-Id: I81c3be3e99e96abb7dbf75ca3ff057bbede3e5fd\n'}, {'number': 3, 'created': '2014-06-03 08:19:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/368c2a5438de59082e5dff478366f40f15adb9db', 'message': 'Add useful metrics into utilization based scheduling\n\nThis feature is to add more useful metrics to monitor under the infrastructure\nof Utilization Based Scheduling.\n\nblueprint add-useful-metrics\n\nChange-Id: I81c3be3e99e96abb7dbf75ca3ff057bbede3e5fd\n'}, {'number': 4, 'created': '2014-06-26 02:41:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/baa9ca37f72d4166cf8c0b978ff08a6a6bb56bc2', 'message': 'Add useful metrics into utilization based scheduling\n\nThis feature is to add more useful metrics to monitor under the infrastructure\nof Utilization Based Scheduling.\n\nblueprint add-useful-metrics\n\nChange-Id: I81c3be3e99e96abb7dbf75ca3ff057bbede3e5fd\n'}, {'number': 5, 'created': '2014-06-26 04:48:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/b0b9955cc3015420f18f56a9f344ee3b35d53b1d', 'message': 'Add useful metrics into utilization based scheduling\n\nThis feature is to add more useful metrics to monitor under the infrastructure\nof Utilization Based Scheduling.\n\nblueprint add-useful-metrics\n\nChange-Id: I81c3be3e99e96abb7dbf75ca3ff057bbede3e5fd\n'}, {'number': 6, 'created': '2014-07-03 05:12:23.000000000', 'files': ['specs/juno/add-useful-metrics.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/ad1080df4dca7de85e73245adde1ff92d32fdd3c', 'message': 'Add useful metrics into utilization based scheduling\n\nThis feature is to add more useful metrics to monitor under the infrastructure\nof Utilization Based Scheduling.\n\nblueprint add-useful-metrics\n\nChange-Id: I81c3be3e99e96abb7dbf75ca3ff057bbede3e5fd\n'}]",37,89766,ad1080df4dca7de85e73245adde1ff92d32fdd3c,56,8,6,4458,,,0,"Add useful metrics into utilization based scheduling

This feature is to add more useful metrics to monitor under the infrastructure
of Utilization Based Scheduling.

blueprint add-useful-metrics

Change-Id: I81c3be3e99e96abb7dbf75ca3ff057bbede3e5fd
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/66/89766/6 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/add-useful-metrics.rst'],1,0c99677994aaee7cef210da9f8d5418edfea4604,bp/add-useful-metrics,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ==================================================== Add useful metrics into Utilization Based Scheduling ==================================================== https://blueprints.launchpad.net/nova/+spec/add-useful-metrics Add some useful metrics into UBS, such as network monitor, power consumption monitor, and chipset temperature monitor and so on. With them, UBS has more options to do scheduling based on those metrics by setting the ratio of each metric in weigher. Problem description =================== The framework for Utilization Based Scheduling has already been implemented in Icehouse: https://blueprints.launchpad.net/nova/+spec/utilization-aware-scheduling And some of the monitors like CPU monitor were implemented under the above blueprint. The plan is to add power consumption monitor and chip temperature monitor etc into the UBS infrastructure. Proposed change =============== Implement those monitors to monitor those useful metrics under nova/compute/monitors. Alternatives ------------ None Data model impact ----------------- None REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- A user will be able to set ratios of those useful metrics to specify how the metrics are going to be weighed, including the existing CPU utilization, ongoing power consumption of the host, and chipset temperature and so on, to pick up a host where an instance could be created. Performance Impact ------------------ None Other deployer impact --------------------- None Developer impact ---------------- None Implementation ============== Assignee(s) ----------- Primary assignee: Edwin Zhai<edwin.zhai@intel.com> Lianhao Lu<lianhao.lu@intel.com> Shane Wang<shane.wang@intel.com> Work Items ---------- Code was posted in Icehouse - https://review.openstack.org/#/c/65631/ https://review.openstack.org/#/c/64403/ https://review.openstack.org/#/c/64404/ https://review.openstack.org/#/c/44007/ Dependencies ============ None Testing ======= Unit tests and 3rd party testing. Note that some specific monitors such as power consumption and chipset temperature can only be tested on the hosts with Intel node manager and IPMI support. Network monitor is generic which can be tested on each host. Documentation Impact ==================== None References ========== None ",,127,0
openstack%2Fnova-specs~master~I22c8f9bbf5c34b87fefdea3ab911efc0e1ca3508,openstack/nova-specs,master,I22c8f9bbf5c34b87fefdea3ab911efc0e1ca3508,Add support for USB controller,ABANDONED,2014-04-17 15:25:23.000000000,2014-12-08 11:38:21.000000000,,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 100}, {'_account_id': 782}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 2271}, {'_account_id': 8290}, {'_account_id': 9847}, {'_account_id': 10510}]","[{'number': 1, 'created': '2014-04-17 15:25:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/87021160d33195234882588d50f068e217f5680f', 'message': 'Add support for USB controller\n\nSupport to create USB controller for VM\n\nblueprint add-usb-controller\n\nChange-Id: I22c8f9bbf5c34b87fefdea3ab911efc0e1ca3508\n'}, {'number': 2, 'created': '2014-04-24 17:03:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/80e17e3cdc99f979f868a3b578d9386b2e852cd9', 'message': 'Add support for USB controller\n\nSupport to create USB controller for VM\n\nblueprint add-usb-controller\n\nChange-Id: I22c8f9bbf5c34b87fefdea3ab911efc0e1ca3508\n'}, {'number': 3, 'created': '2014-05-04 13:49:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/e68a16d9b4594c303e48c6e7dd419ba2ca99145c', 'message': 'Add support for USB controller\n\nSupport to create USB controller for VM\n\nblueprint add-usb-controller\n\nChange-Id: I22c8f9bbf5c34b87fefdea3ab911efc0e1ca3508\n'}, {'number': 4, 'created': '2014-05-04 14:08:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/72d5cab908cc80800016c85354d7f82bb729e40d', 'message': 'Add support for USB controller\n\nSupport to create USB controller for VM\n\nblueprint add-usb-controller\n\nChange-Id: I22c8f9bbf5c34b87fefdea3ab911efc0e1ca3508\n'}, {'number': 5, 'created': '2014-06-07 01:21:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/a0f8508a20d3cccde1281f19371a8a0342212b69', 'message': 'Add support for USB controller\n\nSupport to create USB controller for VM\n\nblueprint add-usb-controller\n\nChange-Id: I22c8f9bbf5c34b87fefdea3ab911efc0e1ca3508\n'}, {'number': 6, 'created': '2014-06-10 14:07:24.000000000', 'files': ['specs/juno/add-usb-controller.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/d5cd07ba387411580fcaa20ef3817e87ba3d2af1', 'message': 'Add support for USB controller\n\nSupport to create USB controller for VM\n\nblueprint add-usb-controller\n\nChange-Id: I22c8f9bbf5c34b87fefdea3ab911efc0e1ca3508\n'}]",24,88334,d5cd07ba387411580fcaa20ef3817e87ba3d2af1,61,10,6,10510,,,0,"Add support for USB controller

Support to create USB controller for VM

blueprint add-usb-controller

Change-Id: I22c8f9bbf5c34b87fefdea3ab911efc0e1ca3508
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/34/88334/3 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/add-usb-controller.rst'],1,87021160d33195234882588d50f068e217f5680f,bp/add-usb-controller,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ============================== Add support for USB controller ============================== Include the URL of your launchpad blueprint: https://blueprints.launchpad.net/nova/+spec/add-usb-controller Users have requirement of using USB devices, the detailed information can reference the bp of https://blueprints.launchpad.net/nova/+spec/usb-passthrough. If not specify appropriate type of USB controller for USB device, USB device will use the default piix3-usb-uhci, the default USB controller's speed may mismatch with USB device. This result in the following problems. 1. The low speed of USB device. 2. If use spice client to redirect USB device to VM, the mismatched speed may prevent the connection. As described above, I think USB controller is valuable in Openstack. Problem description =================== Use cases: 1. The administrator creates a flavor with USB controller information, the key parameter is type like ehci. 2. The administrator creates a VM with flavor created above, the system creates USB controller for the VM. Proposed change =============== We should add the following functions to nova: 1. When create flavor, specify a key-value for USB controller, the schema may like: {""usb_controllers"":[{""type"":""ehci""}]} 2. Nova should support to create USB controller in libvirt driver when create VM. Alternatives ------------ None Data model impact ----------------- We should add USB controller data model in libvirt driver for constructing USB controller xml when create VM. REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- None Performance Impact ------------------ None Other deployer impact --------------------- None Developer impact ---------------- None Implementation ============== Assignee(s) ----------- Primary assignee: <Jing Yuan> Work Items ---------- Step 1: Implement function of construct USB controller xml in libvirt driver. Step 2: Implement function of create USB controller when create VM. Dependencies ============ None Testing ======= It is necessary to add tempest for this new function. Documentation Impact ==================== None References ========== https://blueprints.launchpad.net/nova/+spec/usb-passthrough ",,136,0
openstack%2Ffuel-library~master~I4206f6beb16cf697be3a80dfd20c6caa6354c6bf,openstack/fuel-library,master,I4206f6beb16cf697be3a80dfd20c6caa6354c6bf,Improve monitoring in neutron-agent-l3 OCF,ABANDONED,2014-10-10 15:20:30.000000000,2014-12-08 11:34:14.000000000,,"[{'_account_id': 3}, {'_account_id': 8971}]","[{'number': 1, 'created': '2014-10-10 15:20:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/1c924b6c90a7760a05d3f8d6b855375b3813bb05', 'message': 'Improve monitoring in neutron-agent-dhcp OCF\n\n* Added OCF_RESKEY_amqp_server_port parameter to neoutron-agent-dhcp OCF\n  script\n* Monitoring function monitors the connections to AMQP. Pacemaker will\n  restart the service in case of problems with AMQP.\n\nChange-Id: I4206f6beb16cf697be3a80dfd20c6caa6354c6bf\nImplements: bp/pacemaker-improvements\n'}, {'number': 2, 'created': '2014-10-17 14:38:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/6c01298c16e8fa190d4854395b7014372d87459e', 'message': 'Improve monitoring in neutron-agent-dhcp OCF\n\n* Added OCF_RESKEY_amqp_server_port parameter to neoutron-agent-dhcp OCF\n  script\n* Monitoring function monitors the connections to AMQP. Pacemaker will\n  restart the service in case of problems with AMQP.\n\n\nChange-Id: I4206f6beb16cf697be3a80dfd20c6caa6354c6bf\nImplements: bp/pacemaker-improvements\n'}, {'number': 3, 'created': '2014-10-20 13:15:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/d45e9ab79345880feee18dc16e1c3e63f41e350f', 'message': 'Improve monitoring in neutron-agent-l3 OCF\n\n* Added OCF_RESKEY_amqp_server_port parameter to neutron-agent-l2 OCF\n  script\n* Renamed Router to L3 Server\n\nChange-Id: I4206f6beb16cf697be3a80dfd20c6caa6354c6bf\nImplements: bp/pacemaker-improvements\n'}, {'number': 4, 'created': '2014-10-20 13:16:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/30eaefa0a923a4edd0f9b751aceaf3aff6221c44', 'message': 'Improve monitoring in neutron-agent-l3 OCF\n\n* Added OCF_RESKEY_amqp_server_port parameter to neutron-agent-l2 OCF\n  script\n* Renamed Router to L3 Server\n\nChange-Id: I4206f6beb16cf697be3a80dfd20c6caa6354c6bf\nImplements: bp/pacemaker-improvements\n'}, {'number': 5, 'created': '2014-11-24 11:02:22.000000000', 'files': ['deployment/puppet/cluster/files/ocf/neutron-agent-l3', 'deployment/puppet/neutron/manifests/agents/l3.pp'], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/7b37179f1c5862881985aaf3617846c65725c767', 'message': 'Improve monitoring in neutron-agent-l3 OCF\n\n* Added OCF_RESKEY_amqp_server_port parameter to neutron-agent-l2 OCF\n  script\n* Renamed Router to L3 Server\n\nImplements: bp/pacemaker-improvements\nSigned-off-by: Sergii Golovatiuk <sgolovatiuk@mirantis.com>\nChange-Id: I4206f6beb16cf697be3a80dfd20c6caa6354c6bf\n'}]",0,127551,7b37179f1c5862881985aaf3617846c65725c767,32,2,5,11090,,,0,"Improve monitoring in neutron-agent-l3 OCF

* Added OCF_RESKEY_amqp_server_port parameter to neutron-agent-l2 OCF
  script
* Renamed Router to L3 Server

Implements: bp/pacemaker-improvements
Signed-off-by: Sergii Golovatiuk <sgolovatiuk@mirantis.com>
Change-Id: I4206f6beb16cf697be3a80dfd20c6caa6354c6bf
",git fetch https://review.opendev.org/openstack/fuel-library refs/changes/51/127551/2 && git format-patch -1 --stdout FETCH_HEAD,['deployment/puppet/neutron/files/ocf/neutron-agent-l3'],1,1c924b6c90a7760a05d3f8d6b855375b3813bb05,bp/pacemaker-improvements-neutron-agent-l3,"OCF_RESKEY_neutron_server_port_default=""9696"" OCF_RESKEY_amqp_server_port_default=""5672"": ${OCF_RESKEY_neutron_server_port=${OCF_RESKEY_neutron_server_port_default}} : ${OCF_RESKEY_amqp_server_port=${OCF_RESKEY_amqp_server_port_default}}Resource agent for the OpenStack Neutron L3 Service (neutron-l3-agent)Location of the OpenStack L3 Server server binary (neutron-l3-agent)<shortdesc lang=""en"">OpenStack L3 Server server binary (neutron-l3-agent)</shortdesc>Location of the OpenStack Neutron Service (neutron-server) configuration file<shortdesc lang=""en"">OpenStack L3 Server (neutron-server) config file</shortdesc><shortdesc lang=""en"">OpenStack L3 Server (neutron-l3-agent) config file</shortdesc><parameter name=""log_file"" unique=""0"" required=""0""> <longdesc lang=""en""> The log file to use for this OpenStack L3 Service (neutron-l3-agent) instance </longdesc> <shortdesc lang=""en"">OpenStack L3 Service (neutron-l3-agent) log file</shortdesc> <content type=""string"" default=""${OCF_RESKEY_log_file_default}"" /> </parameter> local rc local pid local network_amqp_check # If status returned anything but success, return that immediately if [ $rc -ne $OCF_SUCCESS ]; then return $rc fi # Check the connections according to the PID. # We are sure to hit the scheduler process and not other Neutron process with the same connection behavior (for example neutron-server) pid=`cat $OCF_RESKEY_pid` # check the connections according to the PID network_amqp_check=`netstat -punt | grep -s ""$OCF_RESKEY_amqp_server_port"" | grep -s ""$pid"" | grep -qs ""ESTABLISHED""` rc=$? if [ $rc -ne 0 ]; then ocf_log err ""Neutron L3 Server is not connected to the Neutron server: $rc"" return $OCF_NOT_RUNNING fi ocf_log debug ""OpenStack L3 Server (neutron-l3-agent) monitor succeeded"" return $OCF_SUCCESS ocf_log info ""OpenStack L3 Server (neutron-l3-agent) already running"" ocf_log info ""OpenStack L3 Server (neutron-l3-agent) started"" ocf_log err ""OpenStack L3 Server (neutron-l3-agent) couldn't be stopped"" ocf_log debug ""OpenStack L3 Server (neutron-l3-agent) still hasn't stopped yet. Waiting ..."" ocf_log info ""OpenStack L3 Server (neutron-l3-agent) failed to stop after ${shutdown_timeout}s \ ocf_log info ""OpenStack L3 Server (neutron-l3-agent) stopped""","Resource agent for the OpenStack Router (neutron-l3-agent)Location of the OpenStack Router server binary (neutron-l3-agent)<shortdesc lang=""en"">OpenStack Router server binary (neutron-l3-agent)</shortdesc>Location of the OpenStack Router (neutron-server) configuration file<shortdesc lang=""en"">OpenStack Router (neutron-server) config file</shortdesc><shortdesc lang=""en"">OpenStack Router (neutron-l3-agent) config file</shortdesc><parameter name=""log_file"" unique=""0"" required=""0""> <longdesc lang=""en""> The log file to use for this OpenStack L3 Service (neutron-l3-agent) instance </longdesc> <shortdesc lang=""en"">OpenStack L3 Service (neutron-l3-agent) log file</shortdesc> <content type=""string"" default=""${OCF_RESKEY_log_file_default}"" /> </parameter> return $rc ocf_log info ""OpenStack neutron-l3-agent already running"" ocf_log info ""OpenStack Router (neutron-l3-agent) started"" ocf_log err ""OpenStack Router ($OCF_RESKEY_binary) couldn't be stopped"" ocf_log debug ""OpenStack Router ($OCF_RESKEY_binary) still hasn't stopped yet. Waiting ..."" ocf_log info ""OpenStack Router ($OCF_RESKEY_binary) failed to stop after ${shutdown_timeout}s \ ocf_log info ""OpenStack Router ($OCF_RESKEY_binary) stopped""",46,24
openstack%2Fironic~master~Ib265fa49a0601985793c0f9cb1f394d05496958b,openstack/ironic,master,Ib265fa49a0601985793c0f9cb1f394d05496958b,Fix for broken deploy of iscsi_ilo driver,MERGED,2014-12-05 11:24:07.000000000,2014-12-08 11:31:27.000000000,2014-12-08 11:31:26.000000000,"[{'_account_id': 3}, {'_account_id': 2889}, {'_account_id': 8106}, {'_account_id': 9315}, {'_account_id': 10239}, {'_account_id': 12081}]","[{'number': 1, 'created': '2014-12-05 11:24:07.000000000', 'files': ['ironic/tests/test_images.py', 'ironic/common/images.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/4d7157c57d216eae203adecec8b8e66ae7eb5efa', 'message': 'Fix for broken deploy of iscsi_ilo driver\n\nThis commit fixes the issue that iscsi_ilo driver fails\nduring deploy while creating the bootable ISO image.\n\nChange-Id: Ib265fa49a0601985793c0f9cb1f394d05496958b\nCloses-Bug: 1399522\n'}]",0,139602,4d7157c57d216eae203adecec8b8e66ae7eb5efa,13,6,1,9315,,,0,"Fix for broken deploy of iscsi_ilo driver

This commit fixes the issue that iscsi_ilo driver fails
during deploy while creating the bootable ISO image.

Change-Id: Ib265fa49a0601985793c0f9cb1f394d05496958b
Closes-Bug: 1399522
",git fetch https://review.opendev.org/openstack/ironic refs/changes/02/139602/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/tests/test_images.py', 'ironic/common/images.py']",2,4d7157c57d216eae203adecec8b8e66ae7eb5efa,bug/1399522," fetch(context, kernel_uuid, kernel_path) fetch(context, ramdisk_uuid, ramdisk_path)"," fetch(context, kernel_uuid, kernel_path, CONF.force_raw_images) fetch(context, ramdisk_uuid, ramdisk_path, CONF.force_raw_images)",4,4
openstack%2Frally~master~I2ca0664e5f5957c77f1c15c97e08af869bc97877,openstack/rally,master,I2ca0664e5f5957c77f1c15c97e08af869bc97877,deprecated,ABANDONED,2014-12-08 11:23:36.000000000,2014-12-08 11:26:03.000000000,,"[{'_account_id': 9180}, {'_account_id': 14135}]","[{'number': 1, 'created': '2014-12-08 11:23:36.000000000', 'files': ['rally/utils.py', 'rally/benchmark/sla/base.py', 'rally/benchmark/engine.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/1c1c656af4a68fc7962cd128b81f14620b2bb729', 'message': 'deprecated\n\nChange-Id: I2ca0664e5f5957c77f1c15c97e08af869bc97877\n'}]",0,139975,1c1c656af4a68fc7962cd128b81f14620b2bb729,4,2,1,9180,,,0,"deprecated

Change-Id: I2ca0664e5f5957c77f1c15c97e08af869bc97877
",git fetch https://review.opendev.org/openstack/rally refs/changes/75/139975/1 && git format-patch -1 --stdout FETCH_HEAD,"['rally/utils.py', 'rally/benchmark/sla/base.py', 'rally/benchmark/engine.py']",3,1c1c656af4a68fc7962cd128b81f14620b2bb729,deprecated_decorator," def _check_deprecated(self, config): # we should check every type for being deprecated only once checked_runners = [] checked_contexts = [] checked_slas = [] for scenario in config.keys(): if rutils.is_deprecated(base_scenario.Scenario. get_scenario_by_name(scenario)): LOG.warning(_(""Scenario %s is deprecated"") % scenario_name) for block in config[scenario]: runner = block[""runner""][""type""] if block.get(""runner"") else None if runner and runner not in checked_runners: if rutils.is_deprecated(base_runner.ScenarioRunner. _get_cls(runner)): LOG.warning(_(""Runner %s is deprecated"") % runner) checked_runners.append(runner) for context in block.get(""context"", []): if context not in checked_contexts: if rutils.is_deprecated(base_ctx.Context. get_by_name(context)): LOG.warning(_(""Context %s is deprecated"") % context) checked_contexts.append(context) for sla in block.get(""sla"", []): if sla not in checked_slas: if rutils.is_deprecated(base_sla.SLA.get_by_name(sla)): LOG.warning(_(""SLA %s is deprecated"") % sla) checked_slas.append(sla) self._check_deprecated(self.config)",,43,0
openstack%2Fheat~master~I6016699514e75bd88c27f9f8f7d871b39d31343d,openstack/heat,master,I6016699514e75bd88c27f9f8f7d871b39d31343d,Add support for pass through references,MERGED,2014-10-31 08:25:08.000000000,2014-12-08 11:23:30.000000000,2014-12-04 18:44:43.000000000,"[{'_account_id': 3}, {'_account_id': 1633}, {'_account_id': 4328}, {'_account_id': 4571}, {'_account_id': 4715}, {'_account_id': 7404}, {'_account_id': 8246}, {'_account_id': 8289}, {'_account_id': 8505}, {'_account_id': 8871}]","[{'number': 1, 'created': '2014-10-31 08:25:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/5d95ad224b0a9cd6f01f453947a6a8b273765a3d', 'message': 'Add support for pass through references\n\nTo make template resources more transparent we now support\nthe user adding an attribute __id__ to a nested stack. This\nis then used by get_resource.\n\nbp env-nested-usability\n\nChange-Id: I6016699514e75bd88c27f9f8f7d871b39d31343d\n'}, {'number': 2, 'created': '2014-11-13 02:12:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/38496d1f72fc42c96b6dea07e581ea802963707a', 'message': 'Add support for pass through references\n\nTo make template resources more transparent we now support\nthe user adding an attribute __id__ to a nested stack. This\nis then used by get_resource and get_attr.\n\nNote: __id__ is the name of the resource in the nested stack.\n\nbp env-nested-usability\nChange-Id: I6016699514e75bd88c27f9f8f7d871b39d31343d\n'}, {'number': 3, 'created': '2014-11-20 22:54:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/02158fa012f725e45b00a6ff18331eb216a7f352', 'message': 'Add support for pass through references\n\nTo make template resources more transparent we now support\nthe user adding an attribute __id__ to a nested stack. This\nis then used by get_resource and get_attr.\n\nNote: __id__ is the name of the resource in the nested stack.\n\nbp env-nested-usability\nChange-Id: I6016699514e75bd88c27f9f8f7d871b39d31343d\n'}, {'number': 4, 'created': '2014-11-21 04:33:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/fbca1f38e652d2c2c591bb07d2f2a6d3eb9b77c4', 'message': 'Add support for pass through references\n\nTo make template resources more transparent we now support\nthe user adding an output ""OS::stack_id"" to a nested stack. This\nis then used by get_resource in the outer stack.\n\nNote: it is expected to use this as follows:\noutputs:\n  OS::stack_id:\n    value: {get_resource: my_server}\n\nbp env-nested-usability\nChange-Id: I6016699514e75bd88c27f9f8f7d871b39d31343d\n'}, {'number': 5, 'created': '2014-11-24 02:39:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/7ff62561e185839c7cb6986529a7e7d47af6c4fe', 'message': 'Add support for pass through references\n\nTo make template resources more transparent we now support\nthe user adding an output ""OS::stack_id"" to a nested stack. This\nis then used by get_resource in the outer stack.\n\nNote: it is expected to use this as follows:\noutputs:\n  OS::stack_id:\n    value: {get_resource: my_server}\n\nbp env-nested-usability\nChange-Id: I6016699514e75bd88c27f9f8f7d871b39d31343d\n'}, {'number': 6, 'created': '2014-12-03 03:20:43.000000000', 'files': ['heat/tests/test_provider_template.py', 'heat/engine/resources/template_resource.py', 'doc/source/template_guide/composition.rst'], 'web_link': 'https://opendev.org/openstack/heat/commit/b37062b98def271e7a94014e2c83d5828ab46536', 'message': 'Add support for pass through references\n\nTo make template resources more transparent we now support\nthe user adding an output ""OS::stack_id"" to a nested stack. This\nis then used by get_resource in the outer stack.\n\nNote: it is expected to use this as follows:\noutputs:\n  OS::stack_id:\n    value: {get_resource: my_server}\n\nbp env-nested-usability\nChange-Id: I6016699514e75bd88c27f9f8f7d871b39d31343d\n'}]",6,132190,b37062b98def271e7a94014e2c83d5828ab46536,38,10,6,4715,,,0,"Add support for pass through references

To make template resources more transparent we now support
the user adding an output ""OS::stack_id"" to a nested stack. This
is then used by get_resource in the outer stack.

Note: it is expected to use this as follows:
outputs:
  OS::stack_id:
    value: {get_resource: my_server}

bp env-nested-usability
Change-Id: I6016699514e75bd88c27f9f8f7d871b39d31343d
",git fetch https://review.opendev.org/openstack/heat refs/changes/90/132190/3 && git format-patch -1 --stdout FETCH_HEAD,"['heat/tests/test_provider_template.py', 'heat/engine/resources/template_resource.py']",2,5d95ad224b0a9cd6f01f453947a6a8b273765a3d,bp/env-nested-usability, stack = self.nested() if stack is None: if '__id__' in stack.outputs: return stack.output('__id__') , if not self.nested():,71,1
openstack%2Fhorizon~master~I7425d654d9fee51b6b74e1cda888e12100a9cf3d,openstack/horizon,master,I7425d654d9fee51b6b74e1cda888e12100a9cf3d,Remove port_id from floating ip disassociate,MERGED,2014-11-25 11:38:03.000000000,2014-12-08 10:58:15.000000000,2014-12-08 10:58:14.000000000,"[{'_account_id': 3}, {'_account_id': 4264}, {'_account_id': 6610}, {'_account_id': 9531}, {'_account_id': 12355}, {'_account_id': 14107}]","[{'number': 1, 'created': '2014-11-25 11:38:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/add29d8ec9f77242c96892352ccadc1cbd3d96c6', 'message': 'Remove port_id from floating ip disassociate\n\nRemove the redundant port_id from parameters of FloatingIpManager\ndisassociate function and the related.\n\nChange-Id: I7425d654d9fee51b6b74e1cda888e12100a9cf3d\nCloses-Bug: #1396050\n'}, {'number': 2, 'created': '2014-12-01 09:27:02.000000000', 'files': ['openstack_dashboard/dashboards/project/access_and_security/floating_ips/tests.py', 'openstack_dashboard/api/network_base.py', 'openstack_dashboard/dashboards/project/access_and_security/floating_ips/tables.py', 'openstack_dashboard/test/api_tests/network_tests.py', 'openstack_dashboard/api/neutron.py', 'openstack_dashboard/dashboards/project/instances/tests.py', 'openstack_dashboard/dashboards/project/instances/tables.py', 'openstack_dashboard/api/network.py', 'openstack_dashboard/api/nova.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/ec93149311342ac9f5c427dd6aef3f7f779ff738', 'message': 'Remove port_id from floating ip disassociate\n\nRemove the redundant port_id from parameters of FloatingIpManager\ndisassociate function and the related.\n\nChange-Id: I7425d654d9fee51b6b74e1cda888e12100a9cf3d\nCloses-Bug: #1396050\n'}]",2,137023,ec93149311342ac9f5c427dd6aef3f7f779ff738,15,6,2,9531,,,0,"Remove port_id from floating ip disassociate

Remove the redundant port_id from parameters of FloatingIpManager
disassociate function and the related.

Change-Id: I7425d654d9fee51b6b74e1cda888e12100a9cf3d
Closes-Bug: #1396050
",git fetch https://review.opendev.org/openstack/horizon refs/changes/23/137023/2 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/api/network_base.py', 'openstack_dashboard/dashboards/project/access_and_security/floating_ips/tests.py', 'openstack_dashboard/dashboards/project/access_and_security/floating_ips/tables.py', 'openstack_dashboard/test/api_tests/network_tests.py', 'openstack_dashboard/api/neutron.py', 'openstack_dashboard/dashboards/project/instances/tests.py', 'openstack_dashboard/api/network.py', 'openstack_dashboard/dashboards/project/instances/tables.py', 'openstack_dashboard/api/nova.py']",9,add29d8ec9f77242c96892352ccadc1cbd3d96c6,bug/1396050," def disassociate(self, floating_ip_id):"," def disassociate(self, floating_ip_id, port_id):",12,24
openstack%2Fceilometer~master~Ia3dc237c2cb76b9accd661b283accce133526420,openstack/ceilometer,master,Ia3dc237c2cb76b9accd661b283accce133526420,Updated from global requirements,MERGED,2014-12-05 23:59:00.000000000,2014-12-08 10:48:04.000000000,2014-12-08 10:48:03.000000000,"[{'_account_id': 3}, {'_account_id': 1669}, {'_account_id': 7052}, {'_account_id': 7761}]","[{'number': 1, 'created': '2014-12-05 23:59:00.000000000', 'files': ['requirements.txt', 'requirements-py3.txt'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/cc7d8013c23b38bbaad5420d837fe5b2f07ba83f', 'message': 'Updated from global requirements\n\nChange-Id: Ia3dc237c2cb76b9accd661b283accce133526420\n'}]",0,139759,cc7d8013c23b38bbaad5420d837fe5b2f07ba83f,8,4,1,11131,,,0,"Updated from global requirements

Change-Id: Ia3dc237c2cb76b9accd661b283accce133526420
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/59/139759/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'requirements-py3.txt']",2,cc7d8013c23b38bbaad5420d837fe5b2f07ba83f,openstack/requirements,oslo.concurrency>=0.3.0 # Apache-2.0,oslo.concurrency>=0.1.0 # Apache-2.0,2,2
openstack%2Ftripleo-heat-templates~master~I8b1155ca0a28392e5d5ade57d53bf810d8b5f053,openstack/tripleo-heat-templates,master,I8b1155ca0a28392e5d5ade57d53bf810d8b5f053,Add missing Keystone params to without-mergepy,MERGED,2014-12-05 15:14:24.000000000,2014-12-08 10:46:44.000000000,2014-12-08 10:46:44.000000000,"[{'_account_id': 3}, {'_account_id': 4328}, {'_account_id': 4330}, {'_account_id': 6449}]","[{'number': 1, 'created': '2014-12-05 15:14:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/f978ecb0b48f76cf06279fea30a211683771e348', 'message': 'Add missing Keystone params to without-mergepy\n\nThis patch adds the missing KeystoneSSLCertificate and\nKeystoneSSLCertificateKey to overcloud-without-mergepy.yaml.\n\nThese parameters were adding to overcloud-source.yaml\nin Icf46132230512a31b6dec3c07164c95b13dd8f73. Due to\nthe concurrent review window they never made it\ninto the new overcloud-without-mergepy.yaml\nimplementation.\n\nChange-Id: I8b1155ca0a28392e5d5ade57d53bf810d8b5f053\n'}, {'number': 2, 'created': '2014-12-05 18:52:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/a821f1bbee5dbf1e8911e01666fa93e98cf1eb6f', 'message': 'Add missing Keystone params to without-mergepy\n\nThis patch adds the missing KeystoneSSLCertificate and\nKeystoneSSLCertificateKey to overcloud-without-mergepy.yaml.\n\nThese parameters were adding to overcloud-source.yaml\nin Icf46132230512a31b6dec3c07164c95b13dd8f73. Due to\nthe concurrent review window they never made it\ninto the new overcloud-without-mergepy.yaml\nimplementation.\n\nChange-Id: I8b1155ca0a28392e5d5ade57d53bf810d8b5f053\n'}, {'number': 3, 'created': '2014-12-06 01:50:51.000000000', 'files': ['overcloud-without-mergepy.yaml', 'controller.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/9f7ccef25e7f8dde0b630a817d6acf780bc07a0a', 'message': 'Add missing Keystone params to without-mergepy\n\nThis patch adds the missing KeystoneSSLCertificate and\nKeystoneSSLCertificateKey to overcloud-without-mergepy.yaml.\n\nThese parameters were adding to overcloud-source.yaml\nin Icf46132230512a31b6dec3c07164c95b13dd8f73. Due to\nthe concurrent review window they never made it\ninto the new overcloud-without-mergepy.yaml\nimplementation.\n\nChange-Id: I8b1155ca0a28392e5d5ade57d53bf810d8b5f053\n'}]",0,139656,9f7ccef25e7f8dde0b630a817d6acf780bc07a0a,15,4,3,360,,,0,"Add missing Keystone params to without-mergepy

This patch adds the missing KeystoneSSLCertificate and
KeystoneSSLCertificateKey to overcloud-without-mergepy.yaml.

These parameters were adding to overcloud-source.yaml
in Icf46132230512a31b6dec3c07164c95b13dd8f73. Due to
the concurrent review window they never made it
into the new overcloud-without-mergepy.yaml
implementation.

Change-Id: I8b1155ca0a28392e5d5ade57d53bf810d8b5f053
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/56/139656/3 && git format-patch -1 --stdout FETCH_HEAD,"['overcloud-without-mergepy.yaml', 'controller.yaml']",2,f978ecb0b48f76cf06279fea30a211683771e348,sync_without_mergepy, KeystoneSSLCertificate: default: '' description: Keystone certificate for verifying token validity. type: string KeystoneSSLCertificateKey: default: '' description: Keystone key for signing tokens. type: string hidden: true ssl: certificate: {get_param: KeystoneSSLCertificate} certificate_key: {get_param: KeystoneSSLCertificateKey},,23,0
openstack%2Ftripleo-heat-templates~master~I182671b84d0a21d7018eb136003968f101384716,openstack/tripleo-heat-templates,master,I182671b84d0a21d7018eb136003968f101384716,Add missing Rabbit params to without-mergepy,MERGED,2014-12-05 15:02:05.000000000,2014-12-08 10:46:31.000000000,2014-12-08 10:46:30.000000000,"[{'_account_id': 3}, {'_account_id': 360}, {'_account_id': 4328}, {'_account_id': 4330}, {'_account_id': 6449}, {'_account_id': 7336}, {'_account_id': 7585}]","[{'number': 1, 'created': '2014-12-05 15:02:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/57e2e06e299f5ffb256154c4c2a7e3957d9267bf', 'message': 'Add missing Rabbit params to without-mergepy\n\nThis patch adds the missing RabbitClientUseSSL and\nRabbitClientPort to overcloud-without-mergepy.yaml.\n\nThese parameters were adding to overcloud-source.yaml\nin I7b7613cb60b9095ba5665c335c496fea4514391a. Due to\nthe concurrent review window they never made it\ninto the new overcloud-without-mergepy.yaml\nimplementation.\n\nChange-Id: I182671b84d0a21d7018eb136003968f101384716\n'}, {'number': 2, 'created': '2014-12-05 18:52:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/c5d0d69e2b228890db7c0c90d19382b509f9ce7a', 'message': 'Add missing Rabbit params to without-mergepy\n\nThis patch adds the missing RabbitClientUseSSL and\nRabbitClientPort to overcloud-without-mergepy.yaml.\n\nThese parameters were adding to overcloud-source.yaml\nin I7b7613cb60b9095ba5665c335c496fea4514391a. Due to\nthe concurrent review window they never made it\ninto the new overcloud-without-mergepy.yaml\nimplementation.\n\nChange-Id: I182671b84d0a21d7018eb136003968f101384716\n'}, {'number': 3, 'created': '2014-12-06 01:50:51.000000000', 'files': ['overcloud-without-mergepy.yaml', 'controller.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/bf9b176e3512f652cdb6fc091a58a4f845bc9619', 'message': 'Add missing Rabbit params to without-mergepy\n\nThis patch adds the missing RabbitClientUseSSL and\nRabbitClientPort to overcloud-without-mergepy.yaml.\n\nThese parameters were adding to overcloud-source.yaml\nin I7b7613cb60b9095ba5665c335c496fea4514391a. Due to\nthe concurrent review window they never made it\ninto the new overcloud-without-mergepy.yaml\nimplementation.\n\nChange-Id: I182671b84d0a21d7018eb136003968f101384716\n'}]",1,139649,bf9b176e3512f652cdb6fc091a58a4f845bc9619,18,7,3,360,,,0,"Add missing Rabbit params to without-mergepy

This patch adds the missing RabbitClientUseSSL and
RabbitClientPort to overcloud-without-mergepy.yaml.

These parameters were adding to overcloud-source.yaml
in I7b7613cb60b9095ba5665c335c496fea4514391a. Due to
the concurrent review window they never made it
into the new overcloud-without-mergepy.yaml
implementation.

Change-Id: I182671b84d0a21d7018eb136003968f101384716
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/49/139649/2 && git format-patch -1 --stdout FETCH_HEAD,"['overcloud-without-mergepy.yaml', 'controller.yaml']",2,57e2e06e299f5ffb256154c4c2a7e3957d9267bf,sync_without_mergepy," RabbitClientUseSSL: default: false description: > Rabbit client subscriber parameter to specify an SSL connection to the RabbitMQ host. type: string RabbitClientPort: default: 5672 description: Set rabbit subscriber port, change this if using SSL type: number rabbit_client_use_ssl: {get_param: RabbitClientUseSSL} rabbit_port: {get_param: RabbitClientPort}",,24,0
openstack%2Fceilometer~master~Ia0474726960ce2b4b611fda0a1c304bb8ad96922,openstack/ceilometer,master,Ia0474726960ce2b4b611fda0a1c304bb8ad96922,Retry to connect database when DB2 or mongodb is restarted,MERGED,2014-11-18 07:48:33.000000000,2014-12-08 10:13:22.000000000,2014-12-08 10:13:21.000000000,"[{'_account_id': 3}, {'_account_id': 2284}, {'_account_id': 3012}, {'_account_id': 4491}, {'_account_id': 6537}, {'_account_id': 6676}, {'_account_id': 7052}, {'_account_id': 7478}, {'_account_id': 7729}, {'_account_id': 8871}, {'_account_id': 8992}, {'_account_id': 10987}, {'_account_id': 12927}, {'_account_id': 13273}]","[{'number': 1, 'created': '2014-11-18 07:48:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/5319054a5880b51a8b1645a594a22507813ccefb', 'message': 'Retry to connect database when DB2 or mongodb is restart\n\nThis patch uses retrying package to reconnect to database,When\nCeilometer try to connect to DB2 nosql or mongodb and AutoReconnect\nexception occurred. This give some benifit to tolerance DB restart.\n\nChange-Id: Ia0474726960ce2b4b611fda0a1c304bb8ad96922\nCloses-Bug: #1389985\n'}, {'number': 2, 'created': '2014-11-20 04:27:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/9513dd40994fcb39aebc01a3c2c8fad30d60b7e4', 'message': 'Retry to connect database when DB2 or mongodb is restart\n\nThe patch https://review.openstack.org/#/c/122387 works fine\nwith operations with get, record and update functions.\nBut exception would still occured with the operation of db.collection.find() function.\n\nThis patch can give some benifit to tolerance DB restart with find() function.\nSince ceilometer mostly use ""yield"" to get each find() element, we can only except the\n""AutoReconnect"" exceptions in ""yield"" functions.\n\nChange-Id: Ia0474726960ce2b4b611fda0a1c304bb8ad96922\nCloses-Bug: #1389985\n'}, {'number': 3, 'created': '2014-11-20 04:36:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/e09f3c88c683832f356985af901e5c4bb759d089', 'message': 'Retry to connect database when DB2 or mongodb is restart\n\nThe patch https://review.openstack.org/#/c/122387 works fine\nwith operations with get, record and update functions.\nBut exception would still occured with the operation of db.collection.find() function.\n\nThis patch can give some benifit to tolerance DB restart with find() function.\nSince ceilometer mostly use ""yield"" to get each find() element, we can only except the\n""AutoReconnect"" exceptions in ""yield"" functions.\n\nChange-Id: Ia0474726960ce2b4b611fda0a1c304bb8ad96922\nCloses-Bug: #1389985\n'}, {'number': 4, 'created': '2014-11-21 06:14:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/2efd4c21189490c9f930a7fa37b5458778fd0555', 'message': 'Retry to connect database when DB2 or mongodb is restart\n\nThe patch https://review.openstack.org/#/c/122387 works fine\nwith operations with get, record and update functions.\nBut exception would still occured with the operation of\ndb.collection.find() function.\n\nThis patch can give some benifit to tolerance DB restart\nwith find() function.\nThis patch also remove ""test_mongo_find"" test case since\nIt doesn\'t raise AutoReconnect exception at all.\n\nChange-Id: Ia0474726960ce2b4b611fda0a1c304bb8ad96922\nCloses-Bug: #1389985\n'}, {'number': 5, 'created': '2014-11-24 16:05:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/d2c245590f8d2518491e549c63665dd0e8d9f6dd', 'message': 'Retry to connect database when DB2 or mongodb is restart\n\nThe patch https://review.openstack.org/#/c/122387 works fine\nwith operations with get, record and update functions.\nBut exception would still occured with the operation of\ndb.collection.find() function.\n\nThis patch can give some benifit to tolerance DB restart\nwith find() function.\nThis patch also remove ""test_mongo_find"" test case since\nIt doesn\'t raise AutoReconnect exception at all.\n\nChange-Id: Ia0474726960ce2b4b611fda0a1c304bb8ad96922\nCloses-Bug: #1389985\n'}, {'number': 6, 'created': '2014-11-25 12:38:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/50b1dee8b896f5f8de9d7eda855f0eb54557f1a8', 'message': 'Retry to connect database when DB2 or mongodb is restart\n\nThe patch https://review.openstack.org/#/c/122387 works fine\nwith operations with get, record and update functions.\nBut exception would still occured with the operation of\ndb.collection.find() function.\n\nThis patch can give some benifit to tolerance DB restart\nwith find() function.\nThis patch also remove ""test_mongo_find"" test case since\nIt doesn\'t raise AutoReconnect exception at all.\n\nChange-Id: Ia0474726960ce2b4b611fda0a1c304bb8ad96922\nCloses-Bug: #1389985\n'}, {'number': 7, 'created': '2014-11-28 14:31:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/69bff8d467a0d28517f68802da7d932bacce1043', 'message': 'Retry to connect database when DB2 or mongodb is restart\n\nThe patch https://review.openstack.org/#/c/122387 works fine\nwith operations with get, record and update functions.\nBut exception would still occured with the operation of\ndb.collection.find() function.\n\nThis patch can give some benifit to tolerate DB restart\nwith find() function.\nThis patch also remove ""test_mongo_find"" test case since\nit doesn\'t raise AutoReconnect exception at all.\n\nChange-Id: Ia0474726960ce2b4b611fda0a1c304bb8ad96922\nCloses-Bug: #1389985\n'}, {'number': 8, 'created': '2014-12-08 02:51:08.000000000', 'files': ['ceilometer/tests/storage/test_storage_scenarios.py', 'ceilometer/storage/mongo/utils.py'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/8c6841d3c00931204eaba0e9058707629120c1da', 'message': 'Retry to connect database when DB2 or mongodb is restarted\n\nThe patch https://review.openstack.org/#/c/122387 works fine\nwith operations with get, record and update functions.\nBut exception would still occured with the operation of\ndb.collection.find() function.\n\nThis patch can give some benefit to tolerate DB restart\nwith find() function.\nThis patch also removes ""test_mongo_find"" test case since\nit doesn\'t raise AutoReconnect exception at all.\n\nChange-Id: Ia0474726960ce2b4b611fda0a1c304bb8ad96922\nCloses-Bug: #1389985\n'}]",39,135186,8c6841d3c00931204eaba0e9058707629120c1da,66,14,8,12927,,,0,"Retry to connect database when DB2 or mongodb is restarted

The patch https://review.openstack.org/#/c/122387 works fine
with operations with get, record and update functions.
But exception would still occured with the operation of
db.collection.find() function.

This patch can give some benefit to tolerate DB restart
with find() function.
This patch also removes ""test_mongo_find"" test case since
it doesn't raise AutoReconnect exception at all.

Change-Id: Ia0474726960ce2b4b611fda0a1c304bb8ad96922
Closes-Bug: #1389985
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/86/135186/8 && git format-patch -1 --stdout FETCH_HEAD,"['ceilometer/storage/pymongo_base.py', 'ceilometer/tests/storage/test_pymongo_base.py']",2,5319054a5880b51a8b1645a594a22507813ccefb,bug/1389985,"from pymongo import errors @tests_db.run_with('mongodb', 'db2') class RetryOnErrorTest(test_storage_scenarios.DBTestBase, tests_db.MixinTestsWithBackendScenarios): def test_raise_auto_reconnect_error(self): class TestException(Exception): pass self.attempts = 3 def _mock_get_meters(): def _raise_exceptions(): self.attempts -= 1 if self.attempts <= 0: raise TestException(""Exit"") raise errors.AutoReconnect(""Fake Exception"") return _raise_exceptions with mock.patch.object(self.conn, ""get_meters"") as meters: meters.side_effect = _mock_get_meters() try: self.conn.get_meters() except TestException: self.assertEqual(meters.call_count, 3)",,59,0
openstack%2Ftraining-guides~master~I2f081a840aacffc7c6d35be52a46a19f3a96531a,openstack/training-guides,master,I2f081a840aacffc7c6d35be52a46a19f3a96531a,Imported Translations from Transifex,MERGED,2014-12-08 06:00:18.000000000,2014-12-08 10:04:55.000000000,2014-12-08 10:04:55.000000000,"[{'_account_id': 3}, {'_account_id': 11889}]","[{'number': 1, 'created': '2014-12-08 06:00:18.000000000', 'files': ['doc/training-guides/locale/ja.po'], 'web_link': 'https://opendev.org/openstack/training-guides/commit/c43558071595b087727fa553e40be69c7b7109c4', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I2f081a840aacffc7c6d35be52a46a19f3a96531a\n'}]",0,139913,c43558071595b087727fa553e40be69c7b7109c4,6,2,1,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: I2f081a840aacffc7c6d35be52a46a19f3a96531a
",git fetch https://review.opendev.org/openstack/training-guides refs/changes/13/139913/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/training-guides/locale/ja.po'],1,c43558071595b087727fa553e40be69c7b7109c4,transifex/translations,"""POT-Creation-Date: 2014-12-07 18:31+0000\n"" ""PO-Revision-Date: 2014-12-08 01:01+0000\n""""href=\""http://docs.openstack.org/infra/manual/developers.html#development-"" ""workflow\"">OpenStack Gerrit Workflow</link>, <link ""msgstr """"""href=\""http://docs.openstack.org/infra/manual/developers.html#development-"" ""workflow\"">OpenStack Gerrit Workflow</link>, <link """"---------------- Grab the code:: git clone """"href=\""http://docs.openstack.org/infra/manual/developers.html#development-"" ""workflow\"">Gerrit Workflow</link> and the <link href=\""http://git-"" ""scm.com/doc\"">Git docs</link>."" msgstr """"msgstr ""クラウドコンピューティングは、利用者が必要とする機能に応じて、さまざまなサービスモデルを提供します。""msgstr ""SaaS: Software-as-a-Service。クラウド環境でソフトウェアを使用する機能を利用者に提供します。Web ベースの電子メールなどがあります。""","""POT-Creation-Date: 2014-11-30 18:03+0000\n"" ""PO-Revision-Date: 2014-12-01 04:11+0000\n""""href=\""https://wiki.openstack.org/wiki/Gerrit_Workflow\"">OpenStack Gerrit "" ""Workflow</link>, <link ""msgstr ""OpenStack に変更を提出することに関する詳細は、<link href=\""operator-getting-started-lab.html#submit-doc-bug\"">fixing a documentation bug</link>、<link href=\""https://wiki.openstack.org/wiki/Gerrit_Workflow\"">OpenStack Gerrit Workflow</link>、<link href=\""https://wiki.openstack.org/wiki/Documentation/HowTo\"">OpenStack Documentation HowTo</link>、<link href=\""http://git-scm.com/doc\"">Git Documentation</link> にあります。""""href=\""https://wiki.openstack.org/wiki/Gerrit_Workflow\"">OpenStack Gerrit "" ""Workflow</link>, <link """"---------------- Grab the code from GitHub:: git clone """"href=\""https://wiki.openstack.org/wiki/Gerrit_Workflow\"">Gerrit "" ""Workflow</link> and the <link href=\""http://git-scm.com/doc\"">Git "" ""docs</link>."" msgstr ""ブランチモデルの詳細は <link href=\""https://wiki.openstack.org/wiki/Gerrit_Workflow\"">Gerrit Workflow</link> と <link href=\""http://git-scm.com/doc\"">Git docs</link> にあります。""msgstr """"msgstr """"",14,14
openstack%2Foslo-specs~master~I1b9d2b527be2dd695025c51d2baaea3943060a92,openstack/oslo-specs,master,I1b9d2b527be2dd695025c51d2baaea3943060a92,oslo.messaging functional tests proposal,ABANDONED,2014-11-25 15:13:33.000000000,2014-12-08 09:58:25.000000000,,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 2813}, {'_account_id': 6928}, {'_account_id': 7536}, {'_account_id': 8415}, {'_account_id': 13290}]","[{'number': 1, 'created': '2014-11-25 15:13:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/46c7759ebbb30f2380f16fa53416bb073e05c613', 'message': '    blueprint: https://blueprints.launchpad.net/oslo.messaging/+spec/oslo-functional-testing-apps\n\n    Implement a messenger application with open API built on top\n    of oslo libraries. Tests for the messenger functionality would also\n    cover oslo.messaging, but in terms of the application.\n\nChange-Id: I1b9d2b527be2dd695025c51d2baaea3943060a92\n'}, {'number': 2, 'created': '2014-11-25 15:16:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/be7abaaf12d5755ba8f6de5c7768709ffd3a5e44', 'message': '    blueprint: https://blueprints.launchpad.net/oslo.messaging/+spec/oslo-functional-testing-apps\n\n    Implement a messenger application with open API built on top\n    of oslo libraries. Tests for the messenger functionality would also\n    cover oslo.messaging, but in terms of the application.\n\nChange-Id: I1b9d2b527be2dd695025c51d2baaea3943060a92\n'}, {'number': 3, 'created': '2014-11-25 15:19:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/2d1a13187a068903a0b26daa35273f94cd175b0e', 'message': '    blueprint: https://blueprints.launchpad.net/oslo.messaging/+spec/oslo-functional-testing-apps\n\n    Implement a messenger application with open API built on top\n    of oslo libraries. Tests for the messenger functionality would also\n    cover oslo.messaging, but in terms of the application.\n\nChange-Id: I1b9d2b527be2dd695025c51d2baaea3943060a92\n'}, {'number': 4, 'created': '2014-11-25 15:31:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/1a16b776dfb5d6c683a216c720a5460b878d27ee', 'message': 'oslo.messaging functional tests proposal\n\n    blueprint: https://blueprints.launchpad.net/oslo.messaging/+spec/oslo-functional-testing-apps\n\n    Implement a messenger application with open API built on top\n    of oslo libraries. Tests for the messenger functionality would also\n    cover oslo.messaging, but in terms of the application.\n\nChange-Id: I1b9d2b527be2dd695025c51d2baaea3943060a92\n'}, {'number': 5, 'created': '2014-11-25 15:44:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/c468f9cbe010ff80e6960321451d9910895b302b', 'message': 'oslo.messaging functional tests proposal\n\nblueprint: https://blueprints.launchpad.net/oslo.messaging/+spec/oslo-functional-testing-apps\n\nImplement a messenger application with open API built on top\nof oslo libraries. Tests for the messenger functionality would also\ncover oslo.messaging, but in terms of the application.\n\nChange-Id: I1b9d2b527be2dd695025c51d2baaea3943060a92\n'}, {'number': 6, 'created': '2014-12-02 16:20:22.000000000', 'files': ['specs/kilo/oslo-functional-testing-apps.rst'], 'web_link': 'https://opendev.org/openstack/oslo-specs/commit/cf796c64acc4fc2dbf7d1d4e45a0a915f4afd106', 'message': 'oslo.messaging functional tests proposal\n\n    blueprint: https://blueprints.launchpad.net/oslo.messaging/+spec/oslo-functional-testing-apps\n\n    Implement a messenger application with open API built on top\n    of oslo libraries. Tests for the messenger functionality would also\n    cover oslo.messaging, but in terms of the application.\n\nChange-Id: I1b9d2b527be2dd695025c51d2baaea3943060a92\n'}]",2,137094,cf796c64acc4fc2dbf7d1d4e45a0a915f4afd106,17,7,6,13290,,,0,"oslo.messaging functional tests proposal

    blueprint: https://blueprints.launchpad.net/oslo.messaging/+spec/oslo-functional-testing-apps

    Implement a messenger application with open API built on top
    of oslo libraries. Tests for the messenger functionality would also
    cover oslo.messaging, but in terms of the application.

Change-Id: I1b9d2b527be2dd695025c51d2baaea3943060a92
",git fetch https://review.opendev.org/openstack/oslo-specs refs/changes/94/137094/4 && git format-patch -1 --stdout FETCH_HEAD,['specs/kilo/oslo-messaging-functional-testing.rst'],1,46c7759ebbb30f2380f16fa53416bb073e05c613,bp/https,"================================================================== Provide a simple application for oslo.messaging functional testing ================================================================== https://blueprints.launchpad.net/oslo.messaging/+spec/oslo-functional-testing-apps Make a simple application using oslo.messaging API. Provide some automated functional tests on the application. Make a devstack gate with these tests. Problem description =================== For now we have no any functional coverage of the oslo.messaging. We have some unit tests, but as you know it is not enough to be confident when pushing changes. Unit tests tell us all covered methods perform as they are expected to. There is still a need of some more high-level automatic check. Proposed change =============== There could be a messenger application with open API built on top of oslo libraries. Tests for the messenger functionality would also cover oslo.messaging, but in terms of the application. Lets specify some high-level scope of the messenger. 1. Client is a dynamic entity which represents the person on the network ready for chat. It can appear online, send textual messages to other clients, then disappear. Cleints speak to each other directly. The problem for now is to discover other clients on the network. 2. Server is a static entity with known address. It's main purpose is to manage clients lists. It serves for clients to discover each other. 3. Clients list is to be synchronized between all participants of conversation. Clients list is published by the server periodically or it can be obtained by request. 4. Clients list item contains information about a client: - Name - name of the client - Address (IP:Port) - address to identify client on the network - Online/Offline, etc. 6. Message is an object which is sent between clients. Clients speak to each other directly. This means that Server has no idea about messages and doesn't keep message history. 5. History is simply speaking a message list, stored conversation between two clients or a group of clients. .. note:: Group chats are not detailed yet, but such feature will help us covering multicast functionality. .. note:: Such an application may cover not only oslo.messaging, but if we would like to have some persistence of message history we could use oslo.db APIs to save/restore the data. Returning to our main purpose, how can the functional test look like: .. code-block:: python cfg.setOption('server', '192.168.0.101:9090') cfg.setOption('messaging.driver', 'zmq') # means we can substitute any driver for test client1 = messenger.Client(cfg, 'Bob') client2 = messenger.Client(cfg, 'Alice') sv1 = messenger.Server(cfg) sv1.run() client1.run() client2.run() time.sleep(2) # wait for all instances discover each other assertEquals(sv1.clientsList, client1.clientsList, client2.clientsList) client1.sendMessage('Alice', 'Hi, there!') asserEquals(client1.history['Alice'], client2.history['Bob']) Alternatives ------------ https://github.com/openstack/taskflow/tree/master/taskflow/examples The examples all get tested during unit test runs to ensure they work as expected. We can implement a set of examples for oslo.messaging in the same manner as in taskflow, but such approach seems to be more synthetic than the app. The solid application may need more effort to implement but it seems to be more natural way of using the API. Impact on Existing APIs ----------------------- None Security impact --------------- None Performance Impact ------------------ None Configuration Impact -------------------- None Developer Impact ---------------- Any future changes to oslo.messaging API should be reflected in the application. We should pay additional attention to the application as we do for supporting unit tests. Testing Impact -------------- None Implementation ============== Assignee(s) ----------- Primary assignee: ozamiatin@mirantis.com Other contributors: ipekelny@mirantis.com dmakogon@mirantis.com Milestones ---------- Target Milestone for completion: * Kilo-1 Work Items ---------- * Develop the application layer * Implement functional tests * Make devstack gate on jenkins Incubation ========== N/A Documentation Impact ==================== N/A Dependencies ============ oslo.messaging References ========== None ",,179,0
openstack%2Fceilometer~master~If9ffcca3f0de8f45483f30bc30ded91dd9d40d77,openstack/ceilometer,master,If9ffcca3f0de8f45483f30bc30ded91dd9d40d77,Add alarm_name field to alarm notification,MERGED,2014-11-03 08:56:25.000000000,2014-12-08 09:57:27.000000000,2014-12-08 09:57:25.000000000,"[{'_account_id': 3}, {'_account_id': 3012}, {'_account_id': 4491}, {'_account_id': 6537}, {'_account_id': 6676}, {'_account_id': 7634}, {'_account_id': 7729}, {'_account_id': 8871}, {'_account_id': 10987}, {'_account_id': 11034}, {'_account_id': 13273}]","[{'number': 1, 'created': '2014-11-03 08:56:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/92ccf196f43390d00e33588638f7b8b8f3f7beea', 'message': 'Add alarm_name field to alarm notification\n\nCurrently, alarm notification only has the alarm_id which can be used\nto identify an alarm. But in some use cases, user may want to use\nsome customized information to mark the alarm. So this patch added the\nalarm_name field into the notification. The name of alarm is tenant\nunique and can be defined by user when alarm is created. When the alarm\nconsumer (e.g. OS::Heat::ScalingPolicy) receives an alarm, it can now\nidentify the alarm using the customized info stored in alarm_name field.\n\nChange-Id: If9ffcca3f0de8f45483f30bc30ded91dd9d40d77\n'}, {'number': 2, 'created': '2014-11-04 07:32:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/5f53e24efc1da530baec224812d33ae0a212a043', 'message': 'Add alarm_name field to alarm notification\n\nCurrently, alarm notification only has the alarm_id which can be used\nto identify an alarm. But in some use cases, user may want to use\nsome customized information to mark the alarm. So this patch added the\nalarm_name field into the notification. The name of alarm is tenant\nunique and can be defined by user when alarm is created. When the alarm\nconsumer (e.g. OS::Heat::ScalingPolicy) receives an alarm, it can now\nidentify the alarm using the customized info stored in alarm_name field.\n\nChange-Id: If9ffcca3f0de8f45483f30bc30ded91dd9d40d77\n'}, {'number': 3, 'created': '2014-12-05 02:04:29.000000000', 'files': ['ceilometer/alarm/service.py', 'ceilometer/tests/alarm/test_rpc.py', 'ceilometer/alarm/notifier/log.py', 'ceilometer/alarm/notifier/test.py', 'ceilometer/alarm/notifier/__init__.py', 'ceilometer/alarm/notifier/rest.py', 'ceilometer/tests/alarm/test_notifier.py', 'ceilometer/alarm/rpc.py', 'ceilometer/alarm/notifier/trust.py'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/a48e6645377150805b0deb344553927ad614a00f', 'message': 'Add alarm_name field to alarm notification\n\nCurrently, alarm notification only has the alarm_id which can be used\nto identify an alarm. But in some use cases, user may want to use\nsome customized information to mark the alarm. So this patch added the\nalarm_name field into the notification. The name of alarm is tenant\nunique and can be defined by user when alarm is created. When the alarm\nconsumer (e.g. OS::Heat::ScalingPolicy) receives an alarm, it can now\nidentify the alarm using the customized info stored in alarm_name field.\n\nChange-Id: If9ffcca3f0de8f45483f30bc30ded91dd9d40d77\nCloses-Bug: #1399067\n'}]",0,132522,a48e6645377150805b0deb344553927ad614a00f,28,11,3,11034,,,0,"Add alarm_name field to alarm notification

Currently, alarm notification only has the alarm_id which can be used
to identify an alarm. But in some use cases, user may want to use
some customized information to mark the alarm. So this patch added the
alarm_name field into the notification. The name of alarm is tenant
unique and can be defined by user when alarm is created. When the alarm
consumer (e.g. OS::Heat::ScalingPolicy) receives an alarm, it can now
identify the alarm using the customized info stored in alarm_name field.

Change-Id: If9ffcca3f0de8f45483f30bc30ded91dd9d40d77
Closes-Bug: #1399067
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/22/132522/1 && git format-patch -1 --stdout FETCH_HEAD,"['ceilometer/alarm/notifier/log.py', 'ceilometer/alarm/service.py', 'ceilometer/tests/alarm/test_rpc.py', 'ceilometer/alarm/notifier/test.py', 'ceilometer/alarm/notifier/__init__.py', 'ceilometer/alarm/notifier/rest.py', 'ceilometer/tests/alarm/test_notifier.py', 'ceilometer/alarm/notifier/trust.py', 'ceilometer/alarm/rpc.py']",9,92ccf196f43390d00e33588638f7b8b8f3f7beea,add_alarm_name_to_notification," 'alarm_name': alarm.name,",,38,24
openstack%2Fhorizon~master~I3a406330143831c2fe45682ca8e547cd28c8a2ae,openstack/horizon,master,I3a406330143831c2fe45682ca8e547cd28c8a2ae,Imported Translations from Transifex,MERGED,2014-12-07 06:03:38.000000000,2014-12-08 09:55:57.000000000,2014-12-08 09:55:57.000000000,"[{'_account_id': 3}, {'_account_id': 6610}, {'_account_id': 9317}]","[{'number': 1, 'created': '2014-12-07 06:03:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/8f7b155b90bc3dd4cea0d1ef0257ad5eaf9e9fbc', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I3a406330143831c2fe45682ca8e547cd28c8a2ae\n'}, {'number': 2, 'created': '2014-12-08 06:03:42.000000000', 'files': ['openstack_dashboard/locale/ru/LC_MESSAGES/django.po', 'horizon/locale/en_AU/LC_MESSAGES/djangojs.po', 'openstack_dashboard/locale/de/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en_AU/LC_MESSAGES/django.po'], 'web_link': 'https://opendev.org/openstack/horizon/commit/a315f54e85ba1822fa56b1e831fc822d6e0a58e1', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I3a406330143831c2fe45682ca8e547cd28c8a2ae\n'}]",0,139847,a315f54e85ba1822fa56b1e831fc822d6e0a58e1,9,3,2,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: I3a406330143831c2fe45682ca8e547cd28c8a2ae
",git fetch https://review.opendev.org/openstack/horizon refs/changes/47/139847/2 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/locale/ru/LC_MESSAGES/django.po', 'openstack_dashboard/locale/de/LC_MESSAGES/django.po']",2,8f7b155b90bc3dd4cea0d1ef0257ad5eaf9e9fbc,transifex/translations,"""PO-Revision-Date: 2014-12-06 13:50+0000\n"" ""Last-Translator: Ettore Atalan <atalanttore@googlemail.com>\n""msgstr ""Verschlüsselungsverfahren""msgstr ""Schlüsselgröße (Bit)""msgstr ""Verschlüsselung erstellen""msgstr ""Verschlüsselung""msgstr ""Volumendetails: %(volume_name)s""msgstr ""Gruppenverwaltung: %(group_name)s""msgstr ""Details zur übergeordneten Sicherungskopie können nicht abgerufen werden: %s""msgstr ""Sicherungsdetails: %(backup_name)s""msgstr ""Legen sie die Details für die Datenbanksicherung fest.""msgstr ""Datensicherungsdetails""msgstr ""Firewall %s bearbeiten""msgstr ""Abbilddetails: %(image_name)s""msgstr ""Netzwerkdetail: %(network_name)s""msgstr ""Ressourcendetail: %s""msgstr ""Volumensicherungsdetails""","""PO-Revision-Date: 2014-12-05 23:08+0000\n"" ""Last-Translator: openstackjenkins <jenkins@openstack.org>\n""msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr """"msgstr ""Details zur Eltern-Sicherungskopie können nicht abgerufen werden: %s""msgstr """"msgstr ""Legen sie die Details für die Datenbank-Sicherung fest.""msgstr ""Sicherungs-Details""msgstr """"msgstr """"msgstr """"msgstr """"msgstr ""Datenträger-Sicherung Details""",26,26
openstack%2Ftripleo-image-elements~master~Id520ea27f2803447eff654d14ba8cbb388502a52,openstack/tripleo-image-elements,master,Id520ea27f2803447eff654d14ba8cbb388502a52,Change the kill_metadata executable strings in Neutron,MERGED,2014-12-02 23:41:06.000000000,2014-12-08 09:44:02.000000000,2014-12-08 09:44:01.000000000,"[{'_account_id': 3}, {'_account_id': 6876}, {'_account_id': 6969}, {'_account_id': 7448}, {'_account_id': 8449}, {'_account_id': 8688}, {'_account_id': 10652}, {'_account_id': 11655}]","[{'number': 1, 'created': '2014-12-02 23:41:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/e4f4697dd06ea6c7b5da25c0f20c1020a2960432', 'message': 'Change the kill_metadata executable strings in Neutron\n\nThe scripts 80-neutron-dhcp-agent and 80-neutron-router modifies\nthe neutron rootwrap filter files l3.filters and dhcp.filters.\nThey replaces the string ""/usr/bin/python"" with the path to\nthe python found in the NEUTRON_VENV_DIR/bin directory.\n\nThe neutron patch https://review.openstack.org/#/c/118296\nreplaced ""/usr/bin/python"" with ""python"" in the kill_metadata\nfilters. As a result, the 80-neutron-dhcp-agent and 80-neutron-\nrouter scripts need to be modified to make the same replacement\nof ""/usr/bin/python"" with ""python"".\n\nIf this were not corrected, the neutron L3 agent will get a\nrootwrap error (no filters matched) when it tries to kill a\nmetadata-proxy process and caused the cleanup of a deleted\nrouter to fail.\n\nChange-Id: Id520ea27f2803447eff654d14ba8cbb388502a52\n'}, {'number': 2, 'created': '2014-12-03 01:51:58.000000000', 'files': ['elements/neutron-router/install.d/neutron-source-install/80-neutron-router', 'elements/neutron-dhcp-agent/install.d/neutron-source-install/80-neutron-dhcp-agent'], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/d1c9cbebe5e94e9616dc7ee41cd94ace25cdb36e', 'message': 'Change the kill_metadata executable strings in Neutron\n\nThe scripts 80-neutron-dhcp-agent and 80-neutron-router modifies\nthe neutron rootwrap filter files l3.filters and dhcp.filters.\nThey replaces the string ""/usr/bin/python"" with the path to\nthe python found in the NEUTRON_VENV_DIR/bin directory.\n\nThe neutron patch https://review.openstack.org/#/c/118296\nreplaced ""/usr/bin/python"" with ""python"" in the kill_metadata\nfilters. As a result, the 80-neutron-dhcp-agent and 80-neutron-\nrouter scripts need to be modified to make the same replacement\nof ""/usr/bin/python"" with ""python"".\n\nIf this were not corrected, the neutron L3 agent will get a\nrootwrap error (no filters matched) when it tries to kill a\nmetadata-proxy process and caused the cleanup of a deleted\nrouter to fail.\n\nCloses-Bug: #1398591\nChange-Id: Id520ea27f2803447eff654d14ba8cbb388502a52\n'}]",1,138579,d1c9cbebe5e94e9616dc7ee41cd94ace25cdb36e,20,8,2,6876,,,0,"Change the kill_metadata executable strings in Neutron

The scripts 80-neutron-dhcp-agent and 80-neutron-router modifies
the neutron rootwrap filter files l3.filters and dhcp.filters.
They replaces the string ""/usr/bin/python"" with the path to
the python found in the NEUTRON_VENV_DIR/bin directory.

The neutron patch https://review.openstack.org/#/c/118296
replaced ""/usr/bin/python"" with ""python"" in the kill_metadata
filters. As a result, the 80-neutron-dhcp-agent and 80-neutron-
router scripts need to be modified to make the same replacement
of ""/usr/bin/python"" with ""python"".

If this were not corrected, the neutron L3 agent will get a
rootwrap error (no filters matched) when it tries to kill a
metadata-proxy process and caused the cleanup of a deleted
router to fail.

Closes-Bug: #1398591
Change-Id: Id520ea27f2803447eff654d14ba8cbb388502a52
",git fetch https://review.opendev.org/openstack/tripleo-image-elements refs/changes/79/138579/2 && git format-patch -1 --stdout FETCH_HEAD,"['elements/neutron-router/install.d/neutron-source-install/80-neutron-router', 'elements/neutron-dhcp-agent/install.d/neutron-source-install/80-neutron-dhcp-agent']",2,e4f4697dd06ea6c7b5da25c0f20c1020a2960432,bug/1398591," sed -i ""s, python, ${NEUTRON_VENV_DIR}/bin/python,"" /etc/neutron/rootwrap.d/dhcp.filters"," sed -i ""s, /usr/bin/python, ${NEUTRON_VENV_DIR}/bin/python,"" /etc/neutron/rootwrap.d/dhcp.filters",2,2
openstack%2Fheat~master~I1b5c1690779bf1890c85fd40e5abb6835a7fe31f,openstack/heat,master,I1b5c1690779bf1890c85fd40e5abb6835a7fe31f,Imported Translations from Transifex,MERGED,2014-12-04 06:01:52.000000000,2014-12-08 09:41:04.000000000,2014-12-08 09:41:03.000000000,"[{'_account_id': 3}, {'_account_id': 3098}, {'_account_id': 6577}, {'_account_id': 9542}]","[{'number': 1, 'created': '2014-12-04 06:01:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/e31ba9537f55856cc709f42a3f697db0f9809f51', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I1b5c1690779bf1890c85fd40e5abb6835a7fe31f\n'}, {'number': 2, 'created': '2014-12-05 06:01:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/57ed11bb0de8cd6dd80bf9d0ecd6eefc9a174973', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I1b5c1690779bf1890c85fd40e5abb6835a7fe31f\n'}, {'number': 3, 'created': '2014-12-06 06:00:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/ae8461fb7f6e4db78ea8229a31408809ec8e1433', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I1b5c1690779bf1890c85fd40e5abb6835a7fe31f\n'}, {'number': 4, 'created': '2014-12-07 06:00:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/32938675b5bb1656c576ce4c7478a301d0627666', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I1b5c1690779bf1890c85fd40e5abb6835a7fe31f\n'}, {'number': 5, 'created': '2014-12-08 06:00:53.000000000', 'files': ['heat/locale/ko_KR/LC_MESSAGES/heat-log-error.po', 'heat/locale/pt_BR/LC_MESSAGES/heat-log-error.po', 'heat/locale/heat.pot', 'heat/locale/fr/LC_MESSAGES/heat-log-error.po', 'heat/locale/heat-log-warning.pot', 'heat/locale/heat-log-info.pot', 'heat/locale/es/LC_MESSAGES/heat-log-error.po', 'heat/locale/heat-log-error.pot', 'heat/locale/de/LC_MESSAGES/heat-log-error.po', 'heat/locale/fr/LC_MESSAGES/heat-log-info.po', 'heat/locale/es/LC_MESSAGES/heat-log-info.po'], 'web_link': 'https://opendev.org/openstack/heat/commit/17a3b0005d229e24f3f57caafc134ad91069ea37', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I1b5c1690779bf1890c85fd40e5abb6835a7fe31f\n'}]",0,138958,17a3b0005d229e24f3f57caafc134ad91069ea37,22,4,5,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: I1b5c1690779bf1890c85fd40e5abb6835a7fe31f
",git fetch https://review.opendev.org/openstack/heat refs/changes/58/138958/3 && git format-patch -1 --stdout FETCH_HEAD,"['heat/locale/ko_KR/LC_MESSAGES/heat-log-error.po', 'heat/locale/pt_BR/LC_MESSAGES/heat-log-error.po', 'heat/locale/heat.pot', 'heat/locale/fr/LC_MESSAGES/heat-log-error.po', 'heat/locale/heat-log-warning.pot', 'heat/locale/heat-log-info.pot', 'heat/locale/es/LC_MESSAGES/heat-log-error.po', 'heat/locale/heat-log-error.pot', 'heat/locale/de/LC_MESSAGES/heat-log-error.po', 'heat/locale/fr/LC_MESSAGES/heat-log-info.po', 'heat/locale/es/LC_MESSAGES/heat-log-info.po']",11,e31ba9537f55856cc709f42a3f697db0f9809f51,transifex/translations,"""POT-Creation-Date: 2014-12-04 06:01+0000\n"" ""PO-Revision-Date: 2014-12-04 05:08+0000\n""#: heat/cloudinit/loguserdata.py:88 #, python-format msgid ""Provision began: %s"" msgstr """" #: heat/cloudinit/loguserdata.py:90 #, python-format msgid ""Provision done: %s"" msgstr """" #: heat/common/lifecycle_plugin_utils.py:116 #, python-format msgid ""done with class=%(c)s, stackid=%(sid)s, action=%(a)s"" msgstr """" #: heat/common/wsgi.py:292 heat/openstack/common/service.py:331 #, python-format msgid ""Starting %d workers"" msgstr ""Iniciando %d trabajadores"" #: heat/common/wsgi.py:310 msgid ""Caught keyboard interrupt. Exiting."" msgstr ""Se ha generado interrupción de teclado. Saliendo."" #: heat/common/wsgi.py:333 #, python-format msgid ""Child %d exiting normally"" msgstr ""El hijo %d está saliendo de forma normal"" #: heat/common/wsgi.py:336 #, python-format msgid ""Started child %s"" msgstr ""Se ha iniciado el hijo %s"" #: heat/common/wsgi.py:359 msgid ""Starting single process server"" msgstr ""Iniciando servidor de proceso individual"" #: heat/engine/resource.py:555#: heat/engine/resource.py:707#: heat/engine/resource.py:732#: heat/engine/resource.py:769#: heat/engine/resource.py:785#: heat/engine/resource.py:790#: heat/engine/resource.py:836#: heat/engine/resource.py:869#: heat/engine/scheduler.py:214#: heat/engine/stack.py:532#: heat/engine/stack.py:965#: heat/engine/stack.py:974#: heat/engine/stack.py:991 heat/engine/stack.py:1003#: heat/engine/stack.py:1019#: heat/engine/stack.py:1040#: heat/engine/stack_resource.py:257#: heat/engine/clients/os/cinder.py:62#: heat/engine/clients/os/cinder.py:93 #, python-format msgid ""Volume (%(volume)s) not found: %(ex)s"" msgstr """" #: heat/engine/clients/os/cinder.py:101 #, python-format msgid ""VolumeSnapshot (%(snapshot)s) not found: %(ex)s"" msgstr """" #: heat/engine/resources/eip.py:106 heat/engine/resources/eip.py:120#: heat/engine/resources/instance.py:80#: heat/engine/resources/instance.py:89#: heat/engine/resources/instance.py:95#: heat/engine/resources/instance.py:462#: heat/engine/resources/os_database.py:362#: heat/engine/resources/sahara_templates.py:190#: heat/engine/resources/sahara_templates.py:203#: heat/engine/resources/sahara_templates.py:348#: heat/engine/resources/sahara_templates.py:361#: heat/engine/resources/server.py:980#: heat/engine/resources/swiftsignal.py:248#: heat/engine/resources/swiftsignal.py:255#: heat/engine/resources/aws/autoscaling_group.py:253","""POT-Creation-Date: 2014-11-24 06:00+0000\n"" ""PO-Revision-Date: 2014-11-10 07:27+0000\n""#: heat/engine/resource.py:549#: heat/engine/resource.py:698#: heat/engine/resource.py:723#: heat/engine/resource.py:760#: heat/engine/resource.py:776#: heat/engine/resource.py:781#: heat/engine/resource.py:827#: heat/engine/resource.py:860#: heat/engine/scheduler.py:213#: heat/engine/stack.py:530#: heat/engine/stack.py:963#: heat/engine/stack.py:972#: heat/engine/stack.py:989 heat/engine/stack.py:1001#: heat/engine/stack.py:1017#: heat/engine/stack.py:1037#: heat/engine/stack_resource.py:256#: heat/engine/clients/os/cinder.py:61#: heat/engine/resources/eip.py:102 heat/engine/resources/eip.py:116#: heat/engine/resources/instance.py:77#: heat/engine/resources/instance.py:86#: heat/engine/resources/instance.py:92#: heat/engine/resources/instance.py:453#: heat/engine/resources/os_database.py:352#: heat/engine/resources/sahara_templates.py:150#: heat/engine/resources/sahara_templates.py:163#: heat/engine/resources/sahara_templates.py:308#: heat/engine/resources/sahara_templates.py:321#: heat/engine/resources/server.py:964#: heat/engine/resources/swiftsignal.py:244#: heat/engine/resources/swiftsignal.py:251#: heat/engine/resources/aws/autoscaling_group.py:252#: heat/openstack/common/service.py:331 #, python-format msgid ""Starting %d workers"" msgstr ""Iniciando %d trabajadores"" ",1292,777
openstack%2Ftricircle~master~I1e00aaae1660a58d2462896ea141bf240d81c932,openstack/tricircle,master,I1e00aaae1660a58d2462896ea141bf240d81c932,Check and sync flavor extra_spces,MERGED,2014-12-08 08:50:27.000000000,2014-12-08 09:32:55.000000000,2014-12-08 09:32:55.000000000,"[{'_account_id': 3}, {'_account_id': 9684}]","[{'number': 1, 'created': '2014-12-08 08:50:27.000000000', 'files': ['novaproxy/nova/compute/manager_proxy.py', 'README.md'], 'web_link': 'https://opendev.org/openstack/tricircle/commit/8103bf5350e7fc4cd98ae392b6a277708124fef3', 'message': ""Check and sync flavor extra_spces\n\nAdd the function to sync the flavor's extra_spces between cascading\nand cascaded when instance is launched or rebooted.\n\nChange-Id: I1e00aaae1660a58d2462896ea141bf240d81c932\n""}]",0,139943,8103bf5350e7fc4cd98ae392b6a277708124fef3,6,2,1,9684,,,0,"Check and sync flavor extra_spces

Add the function to sync the flavor's extra_spces between cascading
and cascaded when instance is launched or rebooted.

Change-Id: I1e00aaae1660a58d2462896ea141bf240d81c932
",git fetch https://review.opendev.org/openstack/tricircle refs/changes/43/139943/1 && git format-patch -1 --stdout FETCH_HEAD,"['novaproxy/nova/compute/manager_proxy.py', 'README.md']",2,8103bf5350e7fc4cd98ae392b6a277708124fef3,, - Sync image only during first time usage but not uploading or patch location can works by just modify the option 'sync_strategy=nova' in /etc/glance-sync.conf file and restart the glance sync service.," - Sync image only during first time usage but not uploading or patch location is still in testing phase, may not work properly.",55,24
openstack%2Ffuel-library~master~Idcefa8346fd3948897550248d3f6bfa2694ac7d0,openstack/fuel-library,master,Idcefa8346fd3948897550248d3f6bfa2694ac7d0,Remove all AMQP checks from OCF scripts,MERGED,2014-12-08 08:21:19.000000000,2014-12-08 09:19:01.000000000,2014-12-08 09:19:01.000000000,"[{'_account_id': 3}, {'_account_id': 6926}, {'_account_id': 7468}, {'_account_id': 8971}, {'_account_id': 9387}, {'_account_id': 11090}]","[{'number': 1, 'created': '2014-12-08 08:21:19.000000000', 'files': ['deployment/puppet/vmware/files/ocf/nova-compute', 'deployment/puppet/heat/templates/heat_engine_ubuntu.ocf.erb', 'deployment/puppet/heat/templates/heat_engine_centos.ocf.erb', 'deployment/puppet/vmware/files/ocf/nova-network', 'deployment/puppet/ceilometer/files/ocf/ceilometer-agent-central', 'deployment/puppet/cluster/files/ocf/neutron-agent-dhcp'], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/6d2b4a82790bba5c1da1b4f2b7578f75d9178cd7', 'message': 'Remove all AMQP checks from OCF scripts\n\nRemove AMQP checks from OCF scripts\nas they are introducing race conditions\nand triggering false-positive restart\nactions breaking deployments\n\nChange-Id: Idcefa8346fd3948897550248d3f6bfa2694ac7d0\nCloses-bug: #1399907\n'}]",0,139937,6d2b4a82790bba5c1da1b4f2b7578f75d9178cd7,14,6,1,8786,,,0,"Remove all AMQP checks from OCF scripts

Remove AMQP checks from OCF scripts
as they are introducing race conditions
and triggering false-positive restart
actions breaking deployments

Change-Id: Idcefa8346fd3948897550248d3f6bfa2694ac7d0
Closes-bug: #1399907
",git fetch https://review.opendev.org/openstack/fuel-library refs/changes/37/139937/1 && git format-patch -1 --stdout FETCH_HEAD,"['deployment/puppet/vmware/files/ocf/nova-compute', 'deployment/puppet/heat/templates/heat_engine_ubuntu.ocf.erb', 'deployment/puppet/heat/templates/heat_engine_centos.ocf.erb', 'deployment/puppet/vmware/files/ocf/nova-network', 'deployment/puppet/ceilometer/files/ocf/ceilometer-agent-central', 'deployment/puppet/cluster/files/ocf/neutron-agent-dhcp']",6,6d2b4a82790bba5c1da1b4f2b7578f75d9178cd7,bug/1399907,," if [[ ""$OCF_RESKEY_amqp_server_port"" != 'none' ]] ; then # Check the connections according to the PID, if has been invoked as a monitor action. if [ ""${__OCF_ACTION}"" = ""monitor"" ]; then # We are sure to hit the scheduler process and not other Neutron process with the same connection behavior (for example neutron-server) pid=`get_worker_pid` # check the connections according to the PID network_amqp_check=`lsof -nPp ${pid} | grep -s ':${OCF_RESKEY_amqp_server_port}\s\+(ESTABLISHED)'` rc=$? if [ $rc -ne 0 ]; then ocf_log err ""Neutron DHCP Agent is not connected to the AMQP server : $rc"" return $OCF_NOT_RUNNING fi fi fi",0,79
openstack%2Fdevstack~master~I75ebc825db11948d48fc027ff1abf704ad9931a0,openstack/devstack,master,I75ebc825db11948d48fc027ff1abf704ad9931a0,lib/nova: Change encoding from LATIN1 to UTF-8,ABANDONED,2014-11-27 18:47:12.000000000,2014-12-08 09:03:17.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 5511}, {'_account_id': 6962}, {'_account_id': 7118}, {'_account_id': 9656}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-27 18:47:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/8687dbc0fb85460d7fc0952bdc40a9a830062338', 'message': 'lib/nova: Change encoding from LATIN1 to UTF-8\n\nThis was noticed when setting up DevStack with PostgreSQL:\n\n  [. . .]\n  2014-11-26 14:21:18.263 | + recreate_database nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + recreate_database_postgresql nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + psql -h127.0.0.1 -Uroot -dtemplate1 -c \'DROP DATABASE IF EXISTS nova\'\n  2014-11-26 14:21:18.273 | NOTICE:  database ""nova"" does not exist, skipping\n  2014-11-26 14:21:18.273 | DROP DATABASE\n  2014-11-26 14:21:18.275 | + createdb -h 127.0.0.1 -Uroot -l C -T template0 -E latin1 nova\n  2014-11-26 14:21:20.464 | + /usr/bin/nova-manage db sync\n  [. . .]\n\nOnce DevStack configuration is complete, the encoding for nova database\nstill reflects as LATIN1\n\nThis was originally introduced in this old commit to workaround bug\n\n  commit 6a57f2649d6d8b28c8fa1a03c8b5eb8b8b6789aa\n  Author: Vincent Untz <vuntz@suse.com>\n  Date:   Thu Jun 14 10:07:42 2012 +0200\n\n      Use latin1 character set when creating the nova database\n\nSo, update Nova database creation setup so that creates databases with\nUTF8 encoding by default when using PostgrSQL.\n\nChange-Id: I75ebc825db11948d48fc027ff1abf704ad9931a0\nCloses-bug: #1397078\n'}, {'number': 2, 'created': '2014-11-27 19:37:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/ad04b4c6adab3a28397448c85f8278dc84645a74', 'message': 'lib/nova: Change encoding from LATIN1 to UTF-8\n\nThis was noticed when setting up DevStack with PostgreSQL:\n\n  [. . .]\n  2014-11-26 14:21:18.263 | + recreate_database nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + recreate_database_postgresql nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + psql -h127.0.0.1 -Uroot -dtemplate1 -c \'DROP DATABASE IF EXISTS nova\'\n  2014-11-26 14:21:18.273 | NOTICE:  database ""nova"" does not exist, skipping\n  2014-11-26 14:21:18.273 | DROP DATABASE\n  2014-11-26 14:21:18.275 | + createdb -h 127.0.0.1 -Uroot -l C -T template0 -E latin1 nova\n  2014-11-26 14:21:20.464 | + /usr/bin/nova-manage db sync\n  [. . .]\n\nOnce DevStack configuration is complete, the encoding for nova database\nstill reflects as LATIN1\n\nThis was originally introduced in this old commit to workaround bug 829209\n\n  commit 6a57f2649d6d8b28c8fa1a03c8b5eb8b8b6789aa\n  Author: Vincent Untz <vuntz@suse.com>\n  Date:   Thu Jun 14 10:07:42 2012 +0200\n\n      Use latin1 character set when creating the nova database\n\nSo, update Nova database creation setup so that creates databases with\nUTF8 encoding by default when using PostgrSQL.\n\nChange-Id: I75ebc825db11948d48fc027ff1abf704ad9931a0\nCloses-bug: #1397078\n'}, {'number': 3, 'created': '2014-11-27 19:49:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/89344faaf7ed2e8d93ee248554104289ee5821d0', 'message': 'lib/nova: Change encoding from LATIN1 to UTF-8\n\nThis was noticed when setting up DevStack with PostgreSQL:\n\n  [. . .]\n  2014-11-26 14:21:18.263 | + recreate_database nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + recreate_database_postgresql nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + psql -h127.0.0.1 -Uroot -dtemplate1 -c \'DROP DATABASE IF EXISTS nova\'\n  2014-11-26 14:21:18.273 | NOTICE:  database ""nova"" does not exist, skipping\n  2014-11-26 14:21:18.273 | DROP DATABASE\n  2014-11-26 14:21:18.275 | + createdb -h 127.0.0.1 -Uroot -l C -T template0 -E latin1 nova\n  2014-11-26 14:21:20.464 | + /usr/bin/nova-manage db sync\n  [. . .]\n\nOnce DevStack configuration is complete, the encoding for nova database\nstill reflects as LATIN1\n\nThis was originally introduced in this old commit to workaround bug 829209\n\n  commit 6a57f2649d6d8b28c8fa1a03c8b5eb8b8b6789aa\n  Author: Vincent Untz <vuntz@suse.com>\n  Date:   Thu Jun 14 10:07:42 2012 +0200\n\n      Use latin1 character set when creating the nova database\n\nSo, update Nova database creation setup so that encoding is set to UTF-8 by default when using PostgrSQL.\n\nChange-Id: I75ebc825db11948d48fc027ff1abf704ad9931a0\nCloses-bug: #1397078\n'}, {'number': 4, 'created': '2014-11-27 19:49:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/fca26ae0c0476902e2bc091760a9494f01f85c52', 'message': 'lib/nova: Change encoding from LATIN1 to UTF-8\n\nThis was noticed when setting up DevStack with PostgreSQL:\n\n  [. . .]\n  2014-11-26 14:21:18.263 | + recreate_database nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + recreate_database_postgresql nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + psql -h127.0.0.1 -Uroot -dtemplate1 -c \'DROP DATABASE IF EXISTS nova\'\n  2014-11-26 14:21:18.273 | NOTICE:  database ""nova"" does not exist, skipping\n  2014-11-26 14:21:18.273 | DROP DATABASE\n  2014-11-26 14:21:18.275 | + createdb -h 127.0.0.1 -Uroot -l C -T template0 -E latin1 nova\n  2014-11-26 14:21:20.464 | + /usr/bin/nova-manage db sync\n  [. . .]\n\nOnce DevStack configuration is complete, the encoding for nova database\nstill reflects as LATIN1\n\nThis was originally introduced in this old commit to workaround bug 829209\n\n  commit 6a57f2649d6d8b28c8fa1a03c8b5eb8b8b6789aa\n  Author: Vincent Untz <vuntz@suse.com>\n  Date:   Thu Jun 14 10:07:42 2012 +0200\n\n      Use latin1 character set when creating the nova database\n\nSo, update Nova database creation setup so that encoding is\nset to UTF-8 by default when using PostgrSQL.\n\nChange-Id: I75ebc825db11948d48fc027ff1abf704ad9931a0\nCloses-bug: #1397078\n'}, {'number': 5, 'created': '2014-11-27 19:50:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/f0772c76939959691cb63ef95d8d8cdb81dd9ba3', 'message': 'lib/nova: Change encoding from LATIN1 to UTF-8\n\nThis was noticed when setting up DevStack with PostgreSQL:\n\n  [. . .]\n  2014-11-26 14:21:18.263 | + recreate_database nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + recreate_database_postgresql nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + psql -h127.0.0.1 -Uroot -dtemplate1 -c \'DROP DATABASE IF EXISTS nova\'\n  2014-11-26 14:21:18.273 | NOTICE:  database ""nova"" does not exist, skipping\n  2014-11-26 14:21:18.273 | DROP DATABASE\n  2014-11-26 14:21:18.275 | + createdb -h 127.0.0.1 -Uroot -l C -T template0 -E latin1 nova\n  2014-11-26 14:21:20.464 | + /usr/bin/nova-manage db sync\n  [. . .]\n\nOnce DevStack configuration is complete, the encoding for nova database\nstill reflects as LATIN1\n\nThis was originally introduced in this old commit to workaround bug 829209\n\n  commit 6a57f2649d6d8b28c8fa1a03c8b5eb8b8b6789aa\n  Author: Vincent Untz <vuntz@suse.com>\n  Date:   Thu Jun 14 10:07:42 2012 +0200\n\n      Use latin1 character set when creating the nova database\n\nSo, update Nova database creation setup so that encoding is set to UTF-8 by default when using PostgrSQL.\n\nChange-Id: I75ebc825db11948d48fc027ff1abf704ad9931a0\nCloses-bug: #1397078\n'}, {'number': 6, 'created': '2014-11-27 19:50:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/cf78cc7074ec22aa1e6280f97d381d0d59e1db61', 'message': 'lib/nova: Change encoding from LATIN1 to UTF-8\n\nThis was noticed when setting up DevStack with PostgreSQL:\n\n  [. . .]\n  2014-11-26 14:21:18.263 | + recreate_database nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + recreate_database_postgresql nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + psql -h127.0.0.1 -Uroot -dtemplate1 -c \'DROP DATABASE IF EXISTS nova\'\n  2014-11-26 14:21:18.273 | NOTICE:  database ""nova"" does not exist, skipping\n  2014-11-26 14:21:18.273 | DROP DATABASE\n  2014-11-26 14:21:18.275 | + createdb -h 127.0.0.1 -Uroot -l C -T template0 -E latin1 nova\n  2014-11-26 14:21:20.464 | + /usr/bin/nova-manage db sync\n  [. . .]\n\nOnce DevStack configuration is complete, the encoding for nova database\nstill reflects as LATIN1\n\nThis was originally introduced in this old commit to workaround bug 829209\n\n  commit 6a57f2649d6d8b28c8fa1a03c8b5eb8b8b6789aa\n  Author: Vincent Untz <vuntz@suse.com>\n  Date:   Thu Jun 14 10:07:42 2012 +0200\n\n      Use latin1 character set when creating the nova database\n\nSo, update Nova database creation setup so that encoding is set to UTF-8 by\ndefault when using PostgrSQL.\n\nChange-Id: I75ebc825db11948d48fc027ff1abf704ad9931a0\nCloses-bug: #1397078\n'}, {'number': 7, 'created': '2014-11-27 19:50:55.000000000', 'files': ['lib/nova'], 'web_link': 'https://opendev.org/openstack/devstack/commit/03fa0be7747360316d56f47aba291d3cbf6a3a15', 'message': 'lib/nova: Change encoding from LATIN1 to UTF-8\n\nThis was noticed when setting up DevStack with PostgreSQL:\n\n  [. . .]\n  2014-11-26 14:21:18.263 | + recreate_database nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + recreate_database_postgresql nova latin1\n  2014-11-26 14:21:18.263 | + local db=nova\n  2014-11-26 14:21:18.263 | + local charset=latin1\n  2014-11-26 14:21:18.263 | + psql -h127.0.0.1 -Uroot -dtemplate1 -c \'DROP DATABASE IF EXISTS nova\'\n  2014-11-26 14:21:18.273 | NOTICE:  database ""nova"" does not exist, skipping\n  2014-11-26 14:21:18.273 | DROP DATABASE\n  2014-11-26 14:21:18.275 | + createdb -h 127.0.0.1 -Uroot -l C -T template0 -E latin1 nova\n  2014-11-26 14:21:20.464 | + /usr/bin/nova-manage db sync\n  [. . .]\n\nOnce DevStack configuration is complete, the encoding for nova database\nstill reflects as LATIN1\n\nThis was originally introduced in this old commit to workaround bug 829209\n\n  commit 6a57f2649d6d8b28c8fa1a03c8b5eb8b8b6789aa\n  Author: Vincent Untz <vuntz@suse.com>\n  Date:   Thu Jun 14 10:07:42 2012 +0200\n\n      Use latin1 character set when creating the nova database\n\nSo, update Nova database creation setup so that encoding is set to UTF-8 by\ndefault when using PostgreSQL.\n\nChange-Id: I75ebc825db11948d48fc027ff1abf704ad9931a0\nCloses-bug: #1397078\n'}]",3,137692,03fa0be7747360316d56f47aba291d3cbf6a3a15,18,7,7,6962,,,0,"lib/nova: Change encoding from LATIN1 to UTF-8

This was noticed when setting up DevStack with PostgreSQL:

  [. . .]
  2014-11-26 14:21:18.263 | + recreate_database nova latin1
  2014-11-26 14:21:18.263 | + local db=nova
  2014-11-26 14:21:18.263 | + local charset=latin1
  2014-11-26 14:21:18.263 | + recreate_database_postgresql nova latin1
  2014-11-26 14:21:18.263 | + local db=nova
  2014-11-26 14:21:18.263 | + local charset=latin1
  2014-11-26 14:21:18.263 | + psql -h127.0.0.1 -Uroot -dtemplate1 -c 'DROP DATABASE IF EXISTS nova'
  2014-11-26 14:21:18.273 | NOTICE:  database ""nova"" does not exist, skipping
  2014-11-26 14:21:18.273 | DROP DATABASE
  2014-11-26 14:21:18.275 | + createdb -h 127.0.0.1 -Uroot -l C -T template0 -E latin1 nova
  2014-11-26 14:21:20.464 | + /usr/bin/nova-manage db sync
  [. . .]

Once DevStack configuration is complete, the encoding for nova database
still reflects as LATIN1

This was originally introduced in this old commit to workaround bug 829209

  commit 6a57f2649d6d8b28c8fa1a03c8b5eb8b8b6789aa
  Author: Vincent Untz <vuntz@suse.com>
  Date:   Thu Jun 14 10:07:42 2012 +0200

      Use latin1 character set when creating the nova database

So, update Nova database creation setup so that encoding is set to UTF-8 by
default when using PostgreSQL.

Change-Id: I75ebc825db11948d48fc027ff1abf704ad9931a0
Closes-bug: #1397078
",git fetch https://review.opendev.org/openstack/devstack refs/changes/92/137692/7 && git format-patch -1 --stdout FETCH_HEAD,['lib/nova'],1,8687dbc0fb85460d7fc0952bdc40a9a830062338,utf8-to-utf8-for-nova-db, recreate_database nova utf8 recreate_database $NOVA_CELLS_DB utf8 recreate_database nova_bm utf8," # Explicitly use latin1: to avoid lp#829209, nova expects the database to # use latin1 by default, and then upgrades the database to utf8 (see the # 082_essex.py in nova) recreate_database nova latin1 recreate_database $NOVA_CELLS_DB latin1 recreate_database nova_bm latin1",3,6
openstack%2Fneutron~master~I7876ca7e4652f8152d1a8a0015cc897b09b31899,openstack/neutron,master,I7876ca7e4652f8152d1a8a0015cc897b09b31899,Make sudo check in ip_lib.IpNetnsCommand.execute optional,MERGED,2014-10-01 12:12:27.000000000,2014-12-08 08:51:20.000000000,2014-12-08 08:51:19.000000000,"[{'_account_id': 3}, {'_account_id': 261}, {'_account_id': 704}, {'_account_id': 748}, {'_account_id': 1561}, {'_account_id': 2035}, {'_account_id': 2592}, {'_account_id': 4656}, {'_account_id': 5170}, {'_account_id': 6502}, {'_account_id': 6659}, {'_account_id': 6788}, {'_account_id': 6854}, {'_account_id': 7448}, {'_account_id': 7787}, {'_account_id': 8124}, {'_account_id': 8645}, {'_account_id': 8788}, {'_account_id': 8873}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 9970}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 12444}, {'_account_id': 14208}, {'_account_id': 14212}, {'_account_id': 14214}]","[{'number': 1, 'created': '2014-10-01 12:12:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3a4eb25a59b4e7385a4f65e155cfd3cb39e4f39a', 'message': 'Make sudo check in ip_lib.IpNetnsCommand.execute optional\n\nIf the process runs as root the root_helper and sudo check\nare not required.\n\nRelated-Bug: #1365453\nChange-Id: I7876ca7e4652f8152d1a8a0015cc897b09b31899\n'}, {'number': 2, 'created': '2014-10-03 14:09:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/bd5a54aded6a367b74fad088b1515f7922a34edc', 'message': 'Make sudo check in ip_lib.IpNetnsCommand.execute optional\n\nIf the process runs as root the root_helper and sudo check\nare not required.\n\nRelated-Bug: #1365453\nChange-Id: I7876ca7e4652f8152d1a8a0015cc897b09b31899\n'}, {'number': 3, 'created': '2014-10-05 16:08:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/06ebfc22242b7e0c623552fa5ecc892e1e086646', 'message': 'Make sudo check in ip_lib.IpNetnsCommand.execute optional\n\nIf the process runs as root the root_helper and sudo check\nare not required.\n\nRelated-Bug: #1365453\nChange-Id: I7876ca7e4652f8152d1a8a0015cc897b09b31899\n'}, {'number': 4, 'created': '2014-10-12 12:32:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/fe7708889124811f8eb901249fc778eed3db8533', 'message': ""Make sudo check in ip_lib.IpNetnsCommand.execute optional\n\nIf the process runs as root the root_helper and sudo check\nare not required.\n\nWhen creating HA routers, keepalived is started as the root user\n(This is the same as the metadata proxy, for example).\nLater in this patch series I tell keepalive to invoke a Neutron script\n(Called neutron-keepalived-state-change). Since keepalive is run, the script runs as\nroot as well, so I set root_helper to None. That is why I have to remove this check\nin this patch. I can't use root_helper in the Neutron script because keepalived\ndoesn't open the script it invokes with a shell,\nso when I try to use sudo it says that a terminal is required.\n\nRelated-Bug: #1365453\nChange-Id: I7876ca7e4652f8152d1a8a0015cc897b09b31899\n""}, {'number': 5, 'created': '2014-11-17 14:21:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/01992076d638e883d869eca0488a15339b3f233c', 'message': 'Make sudo check in ip_lib.IpNetnsCommand.execute optional\n\nIf the process runs as root the root_helper and sudo check\nare not required.\n\nRelated-Bug: #1365453\nCloses-Bug: #1393184\nPartially-Implements: bp/report-ha-router-master\nChange-Id: I7876ca7e4652f8152d1a8a0015cc897b09b31899\n'}, {'number': 6, 'created': '2014-11-17 14:24:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/99c93ce00011d8c5f582327c61fc4751bb832150', 'message': 'Make sudo check in ip_lib.IpNetnsCommand.execute optional\n\nIf the process runs as root the root_helper and sudo check\nare not required.\n\nRelated-Bug: #1365453\nCloses-Bug: #1393184\nPartially-Implements: blueprint report-ha-router-master\nChange-Id: I7876ca7e4652f8152d1a8a0015cc897b09b31899\n'}, {'number': 7, 'created': '2014-11-19 15:16:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9e68920aab8dbfe79d1dc6f8cd654591384b72a3', 'message': 'Make sudo check in ip_lib.IpNetnsCommand.execute optional\n\nIf the process runs as root the root_helper and sudo check\nare not required.\n\nRelated-Bug: #1365453\nCloses-Bug: #1393184\nPartially-Implements: blueprint report-ha-router-master\nChange-Id: I7876ca7e4652f8152d1a8a0015cc897b09b31899\n'}, {'number': 8, 'created': '2014-12-07 15:26:52.000000000', 'files': ['neutron/tests/unit/test_linux_ip_lib.py', 'neutron/agent/linux/ip_lib.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/be34d3aa72a2bb2315fffe17a3e75dca1ce1c82b', 'message': 'Make sudo check in ip_lib.IpNetnsCommand.execute optional\n\nIf the process runs as root the root_helper and sudo check\nare not required.\n\nCloses-Bug: #1393184\nChange-Id: I7876ca7e4652f8152d1a8a0015cc897b09b31899\n'}]",15,125337,be34d3aa72a2bb2315fffe17a3e75dca1ce1c82b,193,43,8,8873,,,0,"Make sudo check in ip_lib.IpNetnsCommand.execute optional

If the process runs as root the root_helper and sudo check
are not required.

Closes-Bug: #1393184
Change-Id: I7876ca7e4652f8152d1a8a0015cc897b09b31899
",git fetch https://review.opendev.org/openstack/neutron refs/changes/37/125337/8 && git format-patch -1 --stdout FETCH_HEAD,['neutron/agent/linux/ip_lib.py'],1,3a4eb25a59b4e7385a4f65e155cfd3cb39e4f39a,bug/1393184,import os if not self._parent.root_helper and os.geteuid() != 0:, if not self._parent.root_helper:,3,1
openstack%2Frequirements~master~I861724be09ede35ecedde7537e72610ec570dcb0,openstack/requirements,master,I861724be09ede35ecedde7537e72610ec570dcb0,Update urllib>=1.8.3,MERGED,2014-12-05 07:06:17.000000000,2014-12-08 08:51:05.000000000,2014-12-08 08:51:04.000000000,"[{'_account_id': 3}, {'_account_id': 24}, {'_account_id': 308}, {'_account_id': 1653}, {'_account_id': 2472}, {'_account_id': 2750}, {'_account_id': 5638}, {'_account_id': 6786}, {'_account_id': 7575}, {'_account_id': 9171}, {'_account_id': 9172}, {'_account_id': 14101}]","[{'number': 1, 'created': '2014-12-05 07:06:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/7486eee253dcbe9fc1619f9d60728cba98e1cb11', 'message': ""Update urllib>=1.8.3\n\nRecent commit 969bfba931f132199662851c6f309be67761c4f9 in oslo.\nvmware uses urllib3 connection api's that is available only\nfrom 1.8.3.\n\nChange-Id: I861724be09ede35ecedde7537e72610ec570dcb0\n""}, {'number': 2, 'created': '2014-12-08 06:09:23.000000000', 'files': ['global-requirements.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/ad9d7a14f62074fb658e4443efd4fe93dff0dd87', 'message': ""Update urllib>=1.8.3\n\nRecent commit 969bfba931f132199662851c6f309be67761c4f9 in oslo.\nvmware uses urllib3 connection api's that is available only\nfrom 1.8.3.\n\nThis patch will unblock Minesweeper CI from failing in starting\nup nova-compute.\n\nChange-Id: I861724be09ede35ecedde7537e72610ec570dcb0\n""}]",0,139545,ad9d7a14f62074fb658e4443efd4fe93dff0dd87,18,12,2,7575,,,0,"Update urllib>=1.8.3

Recent commit 969bfba931f132199662851c6f309be67761c4f9 in oslo.
vmware uses urllib3 connection api's that is available only
from 1.8.3.

This patch will unblock Minesweeper CI from failing in starting
up nova-compute.

Change-Id: I861724be09ede35ecedde7537e72610ec570dcb0
",git fetch https://review.opendev.org/openstack/requirements refs/changes/45/139545/1 && git format-patch -1 --stdout FETCH_HEAD,['global-requirements.txt'],1,7486eee253dcbe9fc1619f9d60728cba98e1cb11,,urllib3>=1.8.3,urllib3>=1.7.1,1,1
openstack%2Fopenstack-planet~master~I1839fcf487ba1f8ae3a77fe4a711ffd4767b1a88,openstack/openstack-planet,master,I1839fcf487ba1f8ae3a77fe4a711ffd4767b1a88,"Revert ""Workflow documentation is now in infra-manual""",MERGED,2014-12-05 19:18:56.000000000,2014-12-08 08:47:36.000000000,2014-12-05 19:30:57.000000000,"[{'_account_id': 3}, {'_account_id': 287}, {'_account_id': 308}, {'_account_id': 5263}]","[{'number': 1, 'created': '2014-12-05 19:18:56.000000000', 'files': ['classic_fancy/index.html.tmpl'], 'web_link': 'https://opendev.org/openstack/openstack-planet/commit/6b0ffea7f173a47bafc043b8f7677af845c6b734', 'message': 'Revert ""Workflow documentation is now in infra-manual""\n\nThis reverts commit ce8f827918f7768774fb58c58223471e893cf1d0.\n\nTurns out there are a _few_ projects where the contribution links\nweren\'t specific to developer contribution. Such as this one.\n\nChange-Id: I1839fcf487ba1f8ae3a77fe4a711ffd4767b1a88\n'}]",0,139713,6b0ffea7f173a47bafc043b8f7677af845c6b734,9,4,1,5263,,,0,"Revert ""Workflow documentation is now in infra-manual""

This reverts commit ce8f827918f7768774fb58c58223471e893cf1d0.

Turns out there are a _few_ projects where the contribution links
weren't specific to developer contribution. Such as this one.

Change-Id: I1839fcf487ba1f8ae3a77fe4a711ffd4767b1a88
",git fetch https://review.opendev.org/openstack/openstack-planet refs/changes/13/139713/1 && git format-patch -1 --stdout FETCH_HEAD,['classic_fancy/index.html.tmpl'],1,6b0ffea7f173a47bafc043b8f7677af845c6b734,infra-manual," <li><a href=""http://wiki.openstack.org/HowToContribute"">Contribute</a></li>"," <li><a href=""http://docs.openstack.org/infra/manual/developers.html"">Contribute</a></li>",1,1
openstack%2Fopenstack-manuals~master~I829c2070eaa849981a36795538bb8247877732a1,openstack/openstack-manuals,master,I829c2070eaa849981a36795538bb8247877732a1,Remove NameVirtualHost from https configuration,MERGED,2014-12-03 07:34:50.000000000,2014-12-08 07:52:51.000000000,2014-12-08 07:52:50.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 612}, {'_account_id': 3191}, {'_account_id': 6772}]","[{'number': 1, 'created': '2014-12-03 07:34:50.000000000', 'files': ['doc/common/section_dashboard-configure-https.xml'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/7966e03b6becc7e84d43e82ce074db11979068d6', 'message': 'Remove NameVirtualHost from https configuration\n\nUnder \'Configure the dashboard for HTTPS\' , when I apply 2. and restart\napache I receive this error:\n\n""NameVirtualHost has no effect and will be removed in the next release\n/etc/apache2/ports.conf"".\n\nSo step 2 is not required now\n\nChange-Id: I829c2070eaa849981a36795538bb8247877732a1\nCloses-bug: #1397359\n'}]",0,138656,7966e03b6becc7e84d43e82ce074db11979068d6,11,5,1,3191,,,0,"Remove NameVirtualHost from https configuration

Under 'Configure the dashboard for HTTPS' , when I apply 2. and restart
apache I receive this error:

""NameVirtualHost has no effect and will be removed in the next release
/etc/apache2/ports.conf"".

So step 2 is not required now

Change-Id: I829c2070eaa849981a36795538bb8247877732a1
Closes-bug: #1397359
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/56/138656/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/common/section_dashboard-configure-https.xml'],1,7966e03b6becc7e84d43e82ce074db11979068d6,remove-NameVirtualHost,, <filename>/etc/apache2/ports.conf</filename> file and add the following line:</para> <programlisting>NameVirtualHost *:443</programlisting> </step> <step> <para>Edit the,0,6
openstack%2Fapi-site~master~Ic047259b4d01747e233290509d61bea33a5a8d7a,openstack/api-site,master,Ic047259b4d01747e233290509d61bea33a5a8d7a,Updates to API guide - wip,ABANDONED,2014-06-04 20:45:19.000000000,2014-12-08 07:13:07.000000000,,"[{'_account_id': 3}, {'_account_id': 612}, {'_account_id': 2448}]","[{'number': 1, 'created': '2014-06-04 20:45:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/api-site/commit/ed0a0fb1e37b7a01d7397267a6e0ebce399c6a63', 'message': 'Updates to API guide - wip\n\nCloses-Bug: #1326515\n\nChange-Id: Ic047259b4d01747e233290509d61bea33a5a8d7a\nauthor: diane fleming\n'}, {'number': 2, 'created': '2014-06-05 16:35:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/api-site/commit/b14f3685b51d438040ad79b7010d394bdbd3adbb', 'message': 'Updates to API guide - wip\n\nCloses-Bug: #1326515\n\nChange-Id: Ic047259b4d01747e233290509d61bea33a5a8d7a\nauthor: diane fleming\n'}, {'number': 3, 'created': '2014-06-05 20:01:37.000000000', 'files': ['api-guide/samples/fault.xml', 'api-guide/figures/Arrow_east.svg', 'api-guide/samples/fault.json', 'api-guide/samples/notfound.json', 'api-guide/samples/overlimit.json', 'api-guide/samples/notfound.xml', 'api-guide/samples/image-fault.xml', 'api-guide/samples/overlimit.xml', 'api-guide/figures/Check_mark_23x20_02.svg', 'api-guide/samples/server-fault.xml', 'api-guide/section_faults.xml', 'api-guide/samples/image-fault.json', 'api-guide/figures/Check_mark_23x20_02.png', 'api-guide/ch_api_concepts.xml', 'api-guide/figures/Arrow_east.png', 'api-guide/samples/server-fault.json'], 'web_link': 'https://opendev.org/openstack/api-site/commit/68c26a6e694ced4523f999852d0813050b2243f4', 'message': 'Updates to API guide - wip\n\nCloses-Bug: #1326515\n\nChange-Id: Ic047259b4d01747e233290509d61bea33a5a8d7a\nauthor: diane fleming\n'}]",0,97925,68c26a6e694ced4523f999852d0813050b2243f4,19,3,3,2448,,,0,"Updates to API guide - wip

Closes-Bug: #1326515

Change-Id: Ic047259b4d01747e233290509d61bea33a5a8d7a
author: diane fleming
",git fetch https://review.opendev.org/openstack/api-site refs/changes/25/97925/2 && git format-patch -1 --stdout FETCH_HEAD,"['api-guide/samples/fault.xml', 'api-guide/figures/Arrow_east.svg', 'api-guide/samples/fault.json', 'api-guide/samples/notfound.json', 'api-guide/samples/overlimit.json', 'api-guide/samples/notfound.xml', 'pom.xml', 'api-guide/samples/image-fault.xml', 'api-guide/samples/overlimit.xml', 'api-guide/figures/Check_mark_23x20_02.svg', 'api-guide/samples/server-fault.xml', 'api-guide/section_faults.xml', 'api-guide/samples/image-fault.json', 'api-guide/figures/Check_mark_23x20_02.png', 'api-guide/ch_api_concepts.xml', 'api-guide/figures/Arrow_east.png', 'api-guide/samples/server-fault.json']",17,ed0a0fb1e37b7a01d7397267a6e0ebce399c6a63,1326515,"{ ""server"": { ""id"": ""52415800-8b69-11e0-9b19-734f0000ffff"", ""tenant_id"": ""1234"", ""user_id"": ""5678"", ""name"": ""sample-server"", ""created"": ""2010-08-10T12:00:00Z"", ""hostId"": ""e4d909c290d0fb1ca068ffafff22cbd0"", ""status"": ""ERROR"", ""progress"": 66, ""image"" : { ""id"": ""52415800-8b69-11e0-9b19-734f6f007777"" }, ""flavor"" : { ""id"": ""52415800-8b69-11e0-9b19-734f216543fd"" }, ""fault"" : { ""code"" : 404, ""created"": ""2010-08-10T11:59:59Z"", ""message"" : ""Could not find image 52415800-8b69-11e0-9b19-734f6f007777"", ""details"" : ""Fault details"" }, ""links"": [ { ""rel"": ""self"", ""href"": ""http://servers.api.openstack.org/v2/1234/servers/52415800-8b69-11e0-9b19-734f000004d2"" }, { ""rel"": ""bookmark"", ""href"": ""http://servers.api.openstack.org/1234/servers/52415800-8b69-11e0-9b19-734f000004d2"" } ] } } ",,502,2
openstack%2Fha-guide~master~I9e3e0284f3ce1583bc52c8ae3b5faeda49a3f7d1,openstack/ha-guide,master,I9e3e0284f3ce1583bc52c8ae3b5faeda49a3f7d1,Imported Translations from Transifex,MERGED,2014-12-08 06:00:07.000000000,2014-12-08 07:11:19.000000000,2014-12-08 07:11:19.000000000,"[{'_account_id': 3}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-12-08 06:00:07.000000000', 'files': ['doc/high-availability-guide/locale/ja.po'], 'web_link': 'https://opendev.org/openstack/ha-guide/commit/cead2e8ef6c7c41088f3e4347a9660da2be3e699', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I9e3e0284f3ce1583bc52c8ae3b5faeda49a3f7d1\n'}]",0,139912,cead2e8ef6c7c41088f3e4347a9660da2be3e699,6,2,1,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: I9e3e0284f3ce1583bc52c8ae3b5faeda49a3f7d1
",git fetch https://review.opendev.org/openstack/ha-guide refs/changes/12/139912/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/high-availability-guide/locale/ja.po'],1,cead2e8ef6c7c41088f3e4347a9660da2be3e699,transifex/translations,"""POT-Creation-Date: 2014-12-05 08:47+0000\n"" ""PO-Revision-Date: 2014-12-08 01:01+0000\n"" ""Last-Translator: Tomoyuki KATO <tomo@dream.daynight.jp>\n""msgstr ""Ubuntu と Debian の RabbitMQ インストールに関する公式マニュアル""msgstr ""Fedora と RHEL の RabbitMQ インストールに関する公式マニュアル""","""POT-Creation-Date: 2014-12-03 07:51+0000\n"" ""PO-Revision-Date: 2014-12-03 07:51+0000\n"" ""Last-Translator: openstackjenkins <jenkins@openstack.org>\n""msgstr """"msgstr """"",5,5
openstack%2Fopenstack-manuals~master~I9356e4605ce2ff0e72037221d148d1f1b9e555e3,openstack/openstack-manuals,master,I9356e4605ce2ff0e72037221d148d1f1b9e555e3,Imported Translations from Transifex,MERGED,2014-12-08 06:12:41.000000000,2014-12-08 07:10:27.000000000,2014-12-08 07:10:26.000000000,"[{'_account_id': 3}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-12-08 06:12:41.000000000', 'files': ['doc/install-guide/locale/ja.po', 'doc/image-guide/locale/ja.po'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/88b7331f76c1c9f1a98428574409cab174770c3d', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I9356e4605ce2ff0e72037221d148d1f1b9e555e3\n'}]",0,139918,88b7331f76c1c9f1a98428574409cab174770c3d,6,2,1,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: I9356e4605ce2ff0e72037221d148d1f1b9e555e3
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/18/139918/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/install-guide/locale/ja.po', 'doc/image-guide/locale/ja.po']",2,88b7331f76c1c9f1a98428574409cab174770c3d,transifex/translations,"""POT-Creation-Date: 2014-12-07 06:37+0000\n"" ""PO-Revision-Date: 2014-12-08 02:41+0000\n"" ""Last-Translator: Tomoyuki KATO <tomo@dream.daynight.jp>\n""msgstr ""公式 Debian イメージ""msgstr ""Debian は、直接ダウンロードするイメージをまだ提供していません。代わりに、<package>openstack-debian-images</package> という名前のパッケージが、イメージを構築するためのシンプルなスクリプトを提供します。このパッケージは、Debian Unstable、Debian Jessie、wheezy バックポートリポジトリー経由で利用可能です。以下を実行して、Wheezy イメージを作成します。<placeholder-1/>""msgstr ""Wheezy イメージを構築している場合、<package>cloud-init</package>、<package>cloud-utils</package>、<package>cloud-initramfs-growroot</package> のようなパッケージが wheezy バックポートから取得されます。また、Wheezy の <package>bootlogd</package> の最新版は、複数コンソールへのログインをサポートしません。これは OpenStack Dashboard のコンソールと <placeholder-1/> のコンソールを動作させるために必要になります。しかしながら、<link href=\""http://archive.gplhost.com/debian/pool/juno-backports/main/s/sysvinit/bootlogd_2.88dsf-41+deb7u2_amd64.deb\"">修正バージョンが非公式 GPLHost リポジトリーから取得できます</link>。イメージの上にインストールするために、パラメーターとしてこの種類のスクリプトを用いて、<placeholder-2/> スクリプトの <option>--hook-script</option> オプションを使用できます。""","""POT-Creation-Date: 2014-12-04 23:29+0000\n"" ""PO-Revision-Date: 2014-12-04 07:24+0000\n"" ""Last-Translator: openstackjenkins <jenkins@openstack.org>\n""msgstr """"msgstr """"msgstr """"",17,17
openstack%2Fpython-neutronclient~master~I1ad19326fe7a7bccabf6a39066659a5ae8c0055b,openstack/python-neutronclient,master,I1ad19326fe7a7bccabf6a39066659a5ae8c0055b,print the real error cmd argument,ABANDONED,2014-10-22 05:57:28.000000000,2014-12-08 06:47:57.000000000,,"[{'_account_id': 3}, {'_account_id': 333}, {'_account_id': 11114}]","[{'number': 1, 'created': '2014-10-22 05:57:28.000000000', 'files': ['neutronclient/shell.py'], 'web_link': 'https://opendev.org/openstack/python-neutronclient/commit/2be574e112cf0b4b223016c520d65e53b256c2d5', 'message': 'print the real error cmd argument\n\nthe find_command in cliff commandmanager will loop all the arguments, but the\nreal one in effect is the first argument in cmdline.\n\nChange-Id: I1ad19326fe7a7bccabf6a39066659a5ae8c0055b\nCloses-Bug: 1380030\n'}]",0,130122,2be574e112cf0b4b223016c520d65e53b256c2d5,6,3,1,11114,,,0,"print the real error cmd argument

the find_command in cliff commandmanager will loop all the arguments, but the
real one in effect is the first argument in cmdline.

Change-Id: I1ad19326fe7a7bccabf6a39066659a5ae8c0055b
Closes-Bug: 1380030
",git fetch https://review.opendev.org/openstack/python-neutronclient refs/changes/22/130122/1 && git format-patch -1 --stdout FETCH_HEAD,['neutronclient/shell.py'],1,2be574e112cf0b4b223016c520d65e53b256c2d5,bug/1380030, cmd_info = self.command_manager.find_command([argv[0]]), cmd_info = self.command_manager.find_command(argv),1,1
openstack%2Ffuel-library~master~I146f96c2215aff95af9fd53682f501f3a1b90349,openstack/fuel-library,master,I146f96c2215aff95af9fd53682f501f3a1b90349,Set neutron OCF scripts umask to 0022,MERGED,2014-12-04 18:00:16.000000000,2014-12-08 06:34:40.000000000,2014-12-05 11:49:03.000000000,"[{'_account_id': 3}, {'_account_id': 6502}, {'_account_id': 6926}, {'_account_id': 7125}, {'_account_id': 7604}, {'_account_id': 8786}, {'_account_id': 8971}, {'_account_id': 9387}, {'_account_id': 14168}]","[{'number': 1, 'created': '2014-12-04 18:00:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/0d5dcbe1ac29ea5021174703e041d68e48e9a65a', 'message': 'Set neutron OCF scripts umask to 0022\n\nFor some reason pacemaker sets umask to 0026\nwhich leads to 0751 rights set for neutron\nagents scripts, which in turn create\nsome of files with these rights set, which\nin turn can make metadata proxy process\nunmanagable, making router hang.\n\nChange-Id: I146f96c2215aff95af9fd53682f501f3a1b90349\nCloses-bug: #1392330\n'}, {'number': 2, 'created': '2014-12-04 18:03:06.000000000', 'files': ['deployment/puppet/cluster/files/ocf/neutron-agent-l3', 'deployment/puppet/cluster/files/ocf/neutron-agent-ovs', 'deployment/puppet/cluster/files/ocf/neutron-agent-metadata', 'deployment/puppet/cluster/files/ocf/neutron-agent-dhcp'], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/d33dd473bf49f88f70cb1e5b693979b866d8ee60', 'message': 'Set neutron OCF scripts umask to 0022\n\nFor some reason pacemaker sets umask to 0026\nwhich leads to 0751 rights set for neutron\nagents scripts, which in turn create\nsome of files with these rights set, which\nin turn can make metadata proxy process\nunmanagable, making router hang.\n\nChange-Id: I146f96c2215aff95af9fd53682f501f3a1b90349\nCloses-bug: #1392330\nRelated-bug: #1397284'}]",0,139130,d33dd473bf49f88f70cb1e5b693979b866d8ee60,19,9,2,8786,,,0,"Set neutron OCF scripts umask to 0022

For some reason pacemaker sets umask to 0026
which leads to 0751 rights set for neutron
agents scripts, which in turn create
some of files with these rights set, which
in turn can make metadata proxy process
unmanagable, making router hang.

Change-Id: I146f96c2215aff95af9fd53682f501f3a1b90349
Closes-bug: #1392330
Related-bug: #1397284",git fetch https://review.opendev.org/openstack/fuel-library refs/changes/30/139130/2 && git format-patch -1 --stdout FETCH_HEAD,"['deployment/puppet/cluster/files/ocf/neutron-agent-l3', 'deployment/puppet/cluster/files/ocf/neutron-agent-ovs', 'deployment/puppet/cluster/files/ocf/neutron-agent-metadata', 'deployment/puppet/cluster/files/ocf/neutron-agent-dhcp']",4,0d5dcbe1ac29ea5021174703e041d68e48e9a65a,,umask 0022,,4,0
openstack%2Fos-loganalyze~master~I7c401b114efe61ff5828d61975394cc9cc4bb37a,openstack/os-loganalyze,master,I7c401b114efe61ff5828d61975394cc9cc4bb37a,Add ignored Editors to .gitignore,MERGED,2014-11-12 15:52:24.000000000,2014-12-08 06:16:09.000000000,2014-12-08 06:16:08.000000000,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 7069}, {'_account_id': 12182}]","[{'number': 1, 'created': '2014-11-12 15:52:24.000000000', 'files': ['.gitignore'], 'web_link': 'https://opendev.org/openstack/os-loganalyze/commit/2c00077885c7a2757569244eba545c47b8f4a9e5', 'message': 'Add ignored Editors to .gitignore\n\nChange-Id: I7c401b114efe61ff5828d61975394cc9cc4bb37a\n'}]",0,133982,2c00077885c7a2757569244eba545c47b8f4a9e5,8,4,1,4395,,,0,"Add ignored Editors to .gitignore

Change-Id: I7c401b114efe61ff5828d61975394cc9cc4bb37a
",git fetch https://review.opendev.org/openstack/os-loganalyze refs/changes/82/133982/1 && git format-patch -1 --stdout FETCH_HEAD,['.gitignore'],1,2c00077885c7a2757569244eba545c47b8f4a9e5,,ChangeLog # Editors *~ .*.swp *.swo *.swn ,ChangeLog,7,1
openstack%2Fdevstack-gate~master~Id30cdb9bc94591a957d6f77be84e077a0adad7d9,openstack/devstack-gate,master,Id30cdb9bc94591a957d6f77be84e077a0adad7d9,Add is_debian and uses_debs functions,MERGED,2014-10-24 12:37:02.000000000,2014-12-08 06:14:57.000000000,2014-12-08 06:14:56.000000000,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 6133}, {'_account_id': 6609}, {'_account_id': 7069}, {'_account_id': 7118}, {'_account_id': 12100}, {'_account_id': 12182}]","[{'number': 1, 'created': '2014-10-24 12:37:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/e9a944e57508c2333d65001f7f1d31699e1f2a5c', 'message': 'Add option for is_debian to support it\n\nCreate a new is_debian method and use it in the same cases\nas Ubuntu to support Debian distros in tests\n\nChange-Id: Id30cdb9bc94591a957d6f77be84e077a0adad7d9\n'}, {'number': 2, 'created': '2014-11-17 09:41:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/5cb542ec264237b8907ee2d889e58cdc59d56e33', 'message': 'Add option for is_debian to support it\n\nCreate a new is_debian method and use it in the same cases\nas Ubuntu to support Debian distros in tests\n\nChange-Id: Id30cdb9bc94591a957d6f77be84e077a0adad7d9\n'}, {'number': 3, 'created': '2014-11-19 08:43:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/68d58e3481b6b729a5e6c23d2a17f6c30eed1bb2', 'message': 'Add option for is_debian to support it\n\nCreate a new is_debian method and use it in the same cases\nas Ubuntu to support Debian distros in tests\n\nChange-Id: Id30cdb9bc94591a957d6f77be84e077a0adad7d9\n'}, {'number': 4, 'created': '2014-11-21 09:18:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/8352167b3fe318a7999a2d79e4a11ebe969108af', 'message': 'Add is_debian and is_ubuntu_or_debian functions\n\nCreate a new is_debian method to check for Debian\ndistro. Create an is_ubuntu_or_debian function and use\nit in the same cases as Ubuntu to support Debian\ndistros in tests. Also keeping the is_ubuntu and\nis_debian methods for some special cases if needed.\n\nChange-Id: Id30cdb9bc94591a957d6f77be84e077a0adad7d9\n'}, {'number': 5, 'created': '2014-12-01 11:07:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/5e106cd67cb950562f963586aa2da0380abf6b51', 'message': 'Add is_debian and is_ubuntu_or_debian functions\n\nCreate a new is_debian method to check for Debian\ndistro. Create an is_ubuntu_or_debian function and use\nit in the same cases as Ubuntu to support Debian\ndistros in tests. Also keeping the is_ubuntu and\nis_debian methods for some special cases if needed.\n\nChange-Id: Id30cdb9bc94591a957d6f77be84e077a0adad7d9\n'}, {'number': 6, 'created': '2014-12-04 10:54:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/8b8fe44d9cf227f8e6c59b39f505e5307d38d04c', 'message': 'Add is_debian and is_ubuntu_or_debian functions\n\nCreate a new is_debian method to check for Debian\ndistro. Create an is_ubuntu_or_debian function and use\nit in the same cases as Ubuntu to support Debian\ndistros in tests. Also keeping the is_ubuntu and\nis_debian methods for some special cases if needed.\n\nChange-Id: Id30cdb9bc94591a957d6f77be84e077a0adad7d9\n'}, {'number': 7, 'created': '2014-12-05 09:22:41.000000000', 'files': ['devstack-vm-gate.sh', 'functions.sh'], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/a34d377fb29f45559b3f63696039ecf7f3c3c741', 'message': 'Add is_debian and uses_debs functions\n\nCreate a new is_debian method to check for Debian\ndistro. Create an uses_debs function and use\nit in the same cases as Ubuntu to support Debian\ndistros in tests. Also keeping the is_ubuntu and\nis_debian methods for some special cases if needed.\n\nChange-Id: Id30cdb9bc94591a957d6f77be84e077a0adad7d9\n'}]",8,130768,a34d377fb29f45559b3f63696039ecf7f3c3c741,43,8,7,6133,,,0,"Add is_debian and uses_debs functions

Create a new is_debian method to check for Debian
distro. Create an uses_debs function and use
it in the same cases as Ubuntu to support Debian
distros in tests. Also keeping the is_ubuntu and
is_debian methods for some special cases if needed.

Change-Id: Id30cdb9bc94591a957d6f77be84e077a0adad7d9
",git fetch https://review.opendev.org/openstack/devstack-gate refs/changes/68/130768/7 && git format-patch -1 --stdout FETCH_HEAD,"['devstack-vm-gate.sh', 'functions.sh']",2,e9a944e57508c2333d65001f7f1d31699e1f2a5c,fix_is_debian,"function is_debian { lsb_release -i 2>/dev/null | grep -iq ""debian"" } elif is_ubuntu || is_debian; then if is_ubuntu || is_debian; then if is_ubuntu || is_debian; then if is_ubuntu || is_debian; then if is_ubuntu || is_debian; then if is_ubuntu || is_debian; then", elif is_ubuntu; then if is_ubuntu; then if is_ubuntu; then if is_ubuntu; then if is_ubuntu; then if is_ubuntu; then,10,7
openstack%2Fcinder~master~I92e7e8b56e3a21644aa2bff3288c1bdc80d45cc6,openstack/cinder,master,I92e7e8b56e3a21644aa2bff3288c1bdc80d45cc6,Fix a clone volume problem in VMAX driver,MERGED,2014-12-05 23:06:23.000000000,2014-12-08 05:32:22.000000000,2014-12-08 05:32:21.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 2759}, {'_account_id': 6491}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-05 23:06:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/30031366b15ce870754b3f8ef902219f843fe2c4', 'message': 'Fix a clone volume problem in VMAX driver\n\nIf the source volume contains meta members of different sizes,\ncreate cloned volume will fail with error ""ERROR_FAMILY_NOT_SUPPORTED"".\nThis patched fixed this problem.\n\nChange-Id: I92e7e8b56e3a21644aa2bff3288c1bdc80d45cc6\nCloses-Bug: #1391205\n'}, {'number': 2, 'created': '2014-12-06 22:33:44.000000000', 'files': ['cinder/volume/drivers/emc/emc_vmax_provision.py', 'cinder/volume/drivers/emc/emc_vmax_common.py', 'cinder/volume/drivers/emc/emc_vmax_utils.py', 'cinder/volume/drivers/emc/emc_vmax_fast.py', 'cinder/tests/test_emc_vmax.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/8ddecd772b85a70b1edc7a69048a3ea43c6c8c30', 'message': 'Fix a clone volume problem in VMAX driver\n\nIf the source volume contains meta members of different sizes,\ncreate cloned volume will fail with error ""ERROR_FAMILY_NOT_SUPPORTED"".\nThis patched fixed this problem.\n\nChange-Id: I92e7e8b56e3a21644aa2bff3288c1bdc80d45cc6\nCloses-Bug: #1391205\n'}]",0,139751,8ddecd772b85a70b1edc7a69048a3ea43c6c8c30,19,10,2,6491,,,0,"Fix a clone volume problem in VMAX driver

If the source volume contains meta members of different sizes,
create cloned volume will fail with error ""ERROR_FAMILY_NOT_SUPPORTED"".
This patched fixed this problem.

Change-Id: I92e7e8b56e3a21644aa2bff3288c1bdc80d45cc6
Closes-Bug: #1391205
",git fetch https://review.opendev.org/openstack/cinder refs/changes/51/139751/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/emc/emc_vmax_provision.py', 'cinder/volume/drivers/emc/emc_vmax_common.py', 'cinder/volume/drivers/emc/emc_vmax_utils.py', 'cinder/volume/drivers/emc/emc_vmax_fast.py', 'cinder/tests/test_emc_vmax.py']",5,30031366b15ce870754b3f8ef902219f843fe2c4,bug/1391205," SyncType=None, SourceElement=None, TargetElement=None, def _getinstance_syncsvsv(self, objectpath): svInstance = {} svInstance['SyncedElement'] = 'SyncedElement' svInstance['SystemElement'] = 'SystemElement' svInstance['PercentSynced'] = 100 return svInstance EMCVMAXUtils, 'get_volume_meta_head', return_value=[EMCVMAXCommonData.test_volume]) @mock.patch.object( EMCVMAXUtils, 'get_meta_members_capacity_in_bit', return_value=[1234567, 7654321]) @mock.patch.object( '_get_pool_and_storage_system', return_value=(None, EMCVMAXCommonData.storage_system)) def test_create_snapshot_different_sizes_meta_no_fast_success( self, mock_volume_type, mock_volume, mock_meta, mock_size, mock_pool): common = self.driver.common volumeDict = {'classname': u'Symm_StorageVolume', 'keybindings': EMCVMAXCommonData.keybindings} common.provision.create_volume_from_pool = ( mock.Mock(return_value=(volumeDict, 0L))) common.provision.get_volume_dict_from_job = ( mock.Mock(return_value=volumeDict)) @mock.patch.object( EMCVMAXUtils, 'get_volume_meta_head', return_value=[EMCVMAXCommonData.test_volume]) @mock.patch.object( EMCVMAXUtils, 'get_meta_members_capacity_in_bit', return_value=[1234567]) def test_create_volume_from_same_size_meta_snapshot( self, mock_volume_type, mock_sync_sv, mock_meta, mock_size): self.data.test_volume, self.data.test_volume) @mock.patch.object( EMCVMAXUtils, 'get_volume_meta_head', return_value=None) def test_create_clone_simple_volume_no_fast_success(self, mock_volume_type, mock_volume, mock_sync_sv, mock_simple_volume): return_value={'volume_backend_name': 'ISCSIFAST'}) EMCVMAXFast, 'get_pool_associated_to_policy', 'get_volume_meta_head', return_value=[EMCVMAXCommonData.test_volume]) @mock.patch.object( EMCVMAXUtils, 'get_meta_members_capacity_in_bit', return_value=[1234567, 7654321]) '_get_pool_and_storage_system', return_value=(None, EMCVMAXCommonData.storage_system)) def test_create_snapshot_different_sizes_meta_fast_success( self, mock_volume_type, mock_volume,mock_meta, mock_size, mock_pool, mock_policy): common = self.driver.common volumeDict = {'classname': u'Symm_StorageVolume', 'keybindings': EMCVMAXCommonData.keybindings} common.provision.create_volume_from_pool = ( mock.Mock(return_value=(volumeDict, 0L))) common.provision.get_volume_dict_from_job = ( mock.Mock(return_value=volumeDict)) common.fast.is_volume_in_default_SG = ( mock.Mock(return_value=True)) return_value={'volume_backend_name': 'ISCSIFAST'}) 'get_volume_meta_head', return_value=[EMCVMAXCommonData.test_volume]) 'get_meta_members_capacity_in_bit', return_value=[1234567]) def test_create_volume_from_same_size_meta_snapshot( self, mock_volume_type, mock_sync_sv, mock_meta, mock_size): common = self.driver.common self.driver.common.utils.find_storage_configuration_service = ( mock.Mock(return_value=EMCVMAXCommonData.storage_system)) self.driver.common._get_or_create_default_storage_group = ( mock.Mock(return_value=EMCVMAXCommonData.default_storage_group)) self.driver.common.fast.is_volume_in_default_SG = ( mock.Mock(return_value=True)) self.data.test_volume, self.data.test_volume) @mock.patch.object( EMCVMAXUtils, 'get_volume_meta_head', return_value=None) self, mock_type, mock_rep_service, mock_sync_sv, mock_meta): return_value={'volume_backend_name': 'ISCSIFAST'}) 'get_volume_meta_head', return_value=None) def test_create_clone_simple_volume_fast_success(self, mock_volume_type, mock_volume, mock_sync_sv, mock_simple_volume): self.driver.common.utils.find_storage_configuration_service = ( mock.Mock(return_value=EMCVMAXCommonData.storage_system)) self.driver.common._get_or_create_default_storage_group = ( mock.Mock(return_value=EMCVMAXCommonData.default_storage_group)) self.driver.common.fast.is_volume_in_default_SG = ( mock.Mock(return_value=True)) return_value={'volume_backend_name': 'ISCSIFAST'}) @mock.patch.object( FakeDB, 'volume_get', return_value=EMCVMAXCommonData.test_source_volume) @mock.patch.object( EMCVMAXFast, 'get_pool_associated_to_policy', return_value=1) @mock.patch.object( EMCVMAXUtils, 'get_volume_meta_head', return_value=[EMCVMAXCommonData.test_volume]) @mock.patch.object( EMCVMAXUtils, 'get_meta_members_capacity_in_bit', return_value=[1234567, 7654321]) '_get_pool_and_storage_system', return_value=(None, EMCVMAXCommonData.storage_system)) def test_create_clone_fast_failed(self, mock_volume_type, mock_vol, mock_policy, mock_meta, mock_size, mock_pool): self.driver.common._modify_and_get_composite_volume_instance = ( mock.Mock(return_value=(1L, None))) return_value={'volume_backend_name': 'FCFAST'}) EMCVMAXFast, 'get_pool_associated_to_policy', 'get_volume_meta_head', return_value=[EMCVMAXCommonData.test_volume]) @mock.patch.object( EMCVMAXUtils, 'get_meta_members_capacity_in_bit', return_value=[1234567, 7654321]) '_get_pool_and_storage_system', return_value=(None, EMCVMAXCommonData.storage_system)) def test_create_snapshot_different_sizes_meta_fast_success( self, mock_volume_type, mock_volume, mock_meta, mock_size, mock_pool, mock_policy): common = self.driver.common volumeDict = {'classname': u'Symm_StorageVolume', 'keybindings': EMCVMAXCommonData.keybindings} common.provision.create_volume_from_pool = ( mock.Mock(return_value=(volumeDict, 0L))) common.provision.get_volume_dict_from_job = ( mock.Mock(return_value=volumeDict)) common.fast.is_volume_in_default_SG = ( mock.Mock(return_value=True)) return_value={'volume_backend_name': 'FCFAST'}) 'get_volume_meta_head', def test_create_clone_simple_volume_fast_success(self, mock_volume_type, mock_volume, mock_sync_sv, mock_meta): self.driver.common.utils.find_storage_configuration_service = ( mock.Mock(return_value=EMCVMAXCommonData.storage_system)) self.driver.common._get_or_create_default_storage_group = ( mock.Mock(return_value=EMCVMAXCommonData.default_storage_group)) self.driver.common.fast.is_volume_in_default_SG = ( mock.Mock(return_value=True)) EMCVMAXCommonData.test_source_volume) FakeDB, 'volume_get', return_value=EMCVMAXCommonData.test_source_volume) @mock.patch.object( EMCVMAXFast, 'get_pool_associated_to_policy', return_value=1) @mock.patch.object( 'get_volume_meta_head', return_value=[EMCVMAXCommonData.test_volume]) @mock.patch.object( EMCVMAXUtils, 'get_meta_members_capacity_in_bit', return_value=[1234567, 7654321]) '_get_pool_and_storage_system', return_value=(None, EMCVMAXCommonData.storage_system)) def test_create_clone_fast_failed(self, mock_volume_type, mock_vol, mock_policy, mock_meta, mock_size, mock_pool): self.driver.common._modify_and_get_composite_volume_instance = ( mock.Mock(return_value=(1L, None)))"," SyncType=None, SourceElement=None, '_find_storage_sync_sv_sv', return_value=(None, None)) def test_create_snapshot_no_fast_success( self, mock_volume_type, mock_volume, mock_sync_sv): FakeDB, 'volume_get', return_value=EMCVMAXCommonData.test_source_volume) @mock.patch.object( def test_create_volume_from_snapshot_no_fast_success( self, mock_volume_type, mock_volume, mock_sync_sv): self.data.test_volume, EMCVMAXCommonData.test_source_volume) def test_create_clone_no_fast_success(self, mock_volume_type, mock_volume, mock_sync_sv): return_value={'volume_backend_name': 'ISCSIFAST', 'FASTPOLICY': 'FC_GOLD1'}) @mock.patch.object( EMCVMAXUtils, 'find_storage_masking_group', return_value=EMCVMAXCommonData.storagegroupname) EMCVMAXCommon, '_find_storage_sync_sv_sv', return_value=(None, None)) @mock.patch.object( EMCVMAXUtils, 'find_storage_configuration_service', 'find_controller_configuration_service', return_value=1) '_get_or_create_default_storage_group', return_value=1) def test_create_snapshot_fast_success( self, mock_volume_type, mock_storage_group, mock_volume, mock_sync_sv, mock_storage_config_service, mock_controller_service, mock_default_sg): return_value={'volume_backend_name': 'ISCSIFAST', 'FASTPOLICY': 'FC_GOLD1'}) @mock.patch.object( EMCVMAXUtils, 'find_storage_masking_group', return_value=EMCVMAXCommonData.storagegroupname) @mock.patch.object( FakeDB, 'volume_get', return_value=EMCVMAXCommonData.test_source_volume) 'find_storage_configuration_service', return_value=1) 'find_controller_configuration_service', return_value=1) @mock.patch.object( EMCVMAXCommon, '_get_or_create_default_storage_group', return_value=1) def test_create_volume_from_snapshot_fast_success( self, mock_volume_type, mock_storage_group, mock_volume, mock_sync_sv, mock_storage_config_service, mock_controller_service, mock_default_sg): self.data.test_volume, EMCVMAXCommonData.test_source_volume) self, mock_volume_type, mock_rep_service, mock_sync_sv): return_value={'volume_backend_name': 'ISCSIFAST', 'FASTPOLICY': 'FC_GOLD1'}) @mock.patch.object( EMCVMAXUtils, 'find_storage_masking_group', return_value=EMCVMAXCommonData.storagegroupname) 'find_storage_configuration_service', return_value=1) @mock.patch.object( EMCVMAXUtils, 'find_controller_configuration_service', return_value=1) @mock.patch.object( EMCVMAXCommon, '_get_or_create_default_storage_group', return_value=1) def test_create_clone_fast_success(self, mock_volume_type, mock_storage_group, mock_volume, mock_sync_sv, mock_storage_config_service, mock_controller_service, mock_default_sg): return_value={'volume_backend_name': 'ISCSIFAST', 'FASTPOLICY': 'FC_GOLD1'}) '_find_storage_sync_sv_sv', return_value=(None, None)) def test_create_clone_fast_failed(self, mock_volume_type, mock_sync_sv): @mock.patch.object( FakeDB, 'volume_get', return_value=EMCVMAXCommonData.test_source_volume) @mock.patch.object( EMCVMAXCommon, '_find_storage_sync_sv_sv', return_value=(None, None)) def test_create_snapshot_no_fast_success( self, mock_volume_type, mock_volume, mock_sync_sv): self.data.test_volume['volume_name'] = ""vmax-1234567"" self.driver.create_snapshot(self.data.test_volume) def test_create_snapshot_no_fast_failed(self): self.data.test_volume['volume_name'] = ""vmax-1234567"" self.assertRaises(exception.VolumeBackendAPIException, self.driver.create_snapshot, self.data.test_volume) @mock.patch.object( volume_types, 'get_volume_type_extra_specs', return_value={'volume_backend_name': 'FCNoFAST'}) @mock.patch.object( FakeDB, 'volume_get', return_value=EMCVMAXCommonData.test_source_volume) @mock.patch.object( EMCVMAXCommon, '_find_storage_sync_sv_sv', return_value=(None, None)) def test_create_volume_from_snapshot_no_fast_success( self, mock_volume_type, mock_volume, mock_sync_sv): self.data.test_volume['volume_name'] = ""vmax-1234567"" self.driver.create_volume_from_snapshot( self.data.test_volume, EMCVMAXCommonData.test_source_volume) def test_create_volume_from_snapshot_no_fast_failed(self): self.data.test_volume['volume_name'] = ""vmax-1234567"" self.assertRaises(exception.VolumeBackendAPIException, self.driver.create_volume_from_snapshot, self.data.test_volume, EMCVMAXCommonData.test_source_volume) @mock.patch.object( volume_types, 'get_volume_type_extra_specs', return_value={'volume_backend_name': 'FCNoFAST'}) @mock.patch.object( FakeDB, 'volume_get', return_value=EMCVMAXCommonData.test_source_volume) @mock.patch.object( EMCVMAXCommon, '_find_storage_sync_sv_sv', return_value=(None, None)) def test_create_clone_no_fast_success(self, mock_volume_type, mock_volume, mock_sync_sv): self.data.test_volume['volume_name'] = ""vmax-1234567"" self.driver.create_cloned_volume(self.data.test_volume, EMCVMAXCommonData.test_source_volume) def test_create_clone_no_fast_failed(self): self.data.test_volume['volume_name'] = ""vmax-1234567"" self.assertRaises(exception.VolumeBackendAPIException, self.driver.create_cloned_volume, self.data.test_volume, EMCVMAXCommonData.test_source_volume) @mock.patch.object( volume_types, 'get_volume_type_extra_specs', return_value={'volume_backend_name': 'FCNoFAST'}) return_value={'volume_backend_name': 'FCFAST', 'FASTPOLICY': 'FC_GOLD1'}) @mock.patch.object( EMCVMAXUtils, 'find_storage_masking_group', return_value=EMCVMAXCommonData.storagegroupname) EMCVMAXCommon, '_find_storage_sync_sv_sv', return_value=(None, None)) @mock.patch.object( EMCVMAXUtils, 'find_storage_configuration_service', 'find_controller_configuration_service', return_value=1) '_get_or_create_default_storage_group', return_value=1) def test_create_snapshot_fast_success(self, mock_volume_type, mock_storage_group, mock_volume, mock_sync_sv, mock_storage_config_service, mock_controller_config_service, mock_default_sg): return_value={'volume_backend_name': 'FCFAST', 'FASTPOLICY': 'FC_GOLD1'}) @mock.patch.object( EMCVMAXUtils, 'find_storage_masking_group', return_value=EMCVMAXCommonData.storagegroupname) 'find_storage_configuration_service', return_value=1) @mock.patch.object( EMCVMAXUtils, 'find_controller_configuration_service', return_value=1) @mock.patch.object( EMCVMAXCommon, '_get_or_create_default_storage_group', return_value=1) def test_create_volume_from_snapshot_fast_success( self, mock_volume_type, mock_storage_group, mock_volume, mock_sync_sv, mock_storage_config_service, mock_controller_config_service, mock_default_sg): self.data.test_volume['volume_name'] = ""vmax-1234567"" self.driver.create_volume_from_snapshot( self.data.test_volume, EMCVMAXCommonData.test_source_volume) @mock.patch.object( volume_types, 'get_volume_type_extra_specs', return_value={'storagetype: pool': 'gold', 'volume_backend_name': 'FCFAST'}) @mock.patch.object( EMCVMAXUtils, 'find_replication_service', @mock.patch.object( EMCVMAXCommon, '_find_storage_sync_sv_sv', return_value=(None, None)) def test_create_volume_from_snapshot_fast_failed(self, mock_volume_type, mock_rep_service, mock_sync_sv): self.assertRaises(exception.VolumeBackendAPIException, self.driver.create_volume_from_snapshot, self.data.test_volume, EMCVMAXCommonData.test_source_volume) @mock.patch.object( volume_types, 'get_volume_type_extra_specs', return_value={'volume_backend_name': 'FCFAST', 'FASTPOLICY': 'FC_GOLD1'}) @mock.patch.object( EMCVMAXUtils, 'find_storage_masking_group', return_value=EMCVMAXCommonData.storagegroupname) @mock.patch.object( FakeDB, 'volume_get', return_value=EMCVMAXCommonData.test_source_volume) @mock.patch.object( EMCVMAXCommon, '_find_storage_sync_sv_sv', return_value=(None, None)) @mock.patch.object( EMCVMAXUtils, 'find_storage_configuration_service', return_value=1) @mock.patch.object( EMCVMAXUtils, 'find_controller_configuration_service', return_value=1) @mock.patch.object( EMCVMAXCommon, '_get_or_create_default_storage_group', return_value=1) def test_create_clone_fast_success(self, mock_volume_type, mock_storage_group, mock_volume, mock_sync_sv, mock_storage_config_service, mock_controller_config_service, mock_default_sg): self.data.test_volume['volume_name'] = ""vmax-1234567"" EMCVMAXCommonData.test_source_volume) 'find_replication_service', return_value=None) '_find_storage_sync_sv_sv', return_value=(None, None)) def test_create_clone_fast_failed(self, mock_volume_type, mock_rep_service, mock_sync_sv):",507,389
openstack%2Ftempest~master~If4d04f1ac2908409d86e719607f9fa0b1241b997,openstack/tempest,master,If4d04f1ac2908409d86e719607f9fa0b1241b997,Support creating users with a default_project_id,MERGED,2014-11-20 11:46:25.000000000,2014-12-08 05:13:39.000000000,2014-12-08 05:13:38.000000000,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 5689}, {'_account_id': 8556}, {'_account_id': 8576}, {'_account_id': 8577}, {'_account_id': 8623}, {'_account_id': 8871}, {'_account_id': 9098}, {'_account_id': 10090}, {'_account_id': 10385}, {'_account_id': 10969}, {'_account_id': 12946}]","[{'number': 1, 'created': '2014-11-20 11:46:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/70fe592b4ad5c1c4d3d9128a8a36ddc25a294d0e', 'message': 'Adding support in the client to create users with a default_project_id.\nAlso adding a test to make sure that such users automatically get a token\nthat is scoped to their project - even when requesting an unscoped token.\n\nChange-Id: If4d04f1ac2908409d86e719607f9fa0b1241b997\n'}, {'number': 2, 'created': '2014-11-23 13:37:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/fbe6828d45d31e27bd23cf3ca8af4f249a6c37c7', 'message': 'Support creating users with a default_project_id\n\nAdd support in the client to pass the default_project_id attribute.\nAlso adding a test to make sure that such users automatically get a token\nthat is scoped to their project - even when requesting an unscoped token.\n\nChange-Id: If4d04f1ac2908409d86e719607f9fa0b1241b997\n'}, {'number': 3, 'created': '2014-11-24 12:14:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/2cdcdc931a5c5c08d008726d75d23e5734cbd1d7', 'message': 'Support creating users with a default_project_id\n\nAdd support in the client to pass the default_project_id attribute.\nAlso adding a test to make sure that such users automatically get a token\nthat is scoped to their project - even when requesting an unscoped token.\n\nChange-Id: If4d04f1ac2908409d86e719607f9fa0b1241b997\n'}, {'number': 4, 'created': '2014-11-25 08:21:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/e5e3f76035222e9dc57756db1b725203727ded42', 'message': 'Support creating users with a default_project_id\n\nAdd support in the client to pass the default_project_id attribute.\nAlso adding a test to make sure that such users automatically get a token\nthat is scoped to their project - even when requesting an unscoped token.\n\nChange-Id: If4d04f1ac2908409d86e719607f9fa0b1241b997\n'}, {'number': 5, 'created': '2014-11-25 12:15:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/16df5c3000a283c560abe7cdcc52d65f757c7644', 'message': 'Support creating users with a default_project_id\n\nAdd support in the client to pass the default_project_id attribute.\nAlso adding a test to make sure that such users automatically get a token\nthat is scoped to their project - even when requesting an unscoped token.\n\nChange-Id: If4d04f1ac2908409d86e719607f9fa0b1241b997\n'}, {'number': 6, 'created': '2014-11-26 08:16:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/85433690c6b853307de84a8f48b9c6b210df6eb8', 'message': 'Support creating users with a default_project_id\n\nAdd support in the client to pass the default_project_id attribute.\nAlso adding a test to make sure that such users automatically get a token\nthat is scoped to their project - even when requesting an unscoped token.\n\nChange-Id: If4d04f1ac2908409d86e719607f9fa0b1241b997\n'}, {'number': 7, 'created': '2014-11-27 07:29:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5f54a834f2c3330d28083fc13ef4f88aefe6444a', 'message': 'Support creating users with a default_project_id\n\nAdd support in the client to pass the default_project_id attribute.\nAlso adding a test to make sure that such users automatically get a token\nthat is scoped to their project - even when requesting an unscoped token.\n\nChange-Id: If4d04f1ac2908409d86e719607f9fa0b1241b997\n'}, {'number': 8, 'created': '2014-12-02 12:41:29.000000000', 'files': ['tempest/services/identity/v3/json/identity_client.py', 'tempest/api/identity/admin/v3/test_default_project_id.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/3434547582f6343138a5d9810ab2c4969fc36998', 'message': 'Support creating users with a default_project_id\n\nAdd support in the client to pass the default_project_id attribute.\nAlso adding a test to make sure that such users automatically get a token\nthat is scoped to their project - even when requesting an unscoped token.\n\nChange-Id: If4d04f1ac2908409d86e719607f9fa0b1241b997\n'}]",18,135922,3434547582f6343138a5d9810ab2c4969fc36998,50,13,8,12946,,,0,"Support creating users with a default_project_id

Add support in the client to pass the default_project_id attribute.
Also adding a test to make sure that such users automatically get a token
that is scoped to their project - even when requesting an unscoped token.

Change-Id: If4d04f1ac2908409d86e719607f9fa0b1241b997
",git fetch https://review.opendev.org/openstack/tempest refs/changes/22/135922/8 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/scenario/Identity/test_default_project_id.py', 'tempest/services/identity/v3/json/identity_client.py', 'tempest/scenario/Identity/__init__.py']",3,70fe592b4ad5c1c4d3d9128a8a36ddc25a294d0e,default_project_id,,,94,0
openstack%2Fmistral~master~I04390a0b305e970aa609db27fa0c091657132fa0,openstack/mistral,master,I04390a0b305e970aa609db27fa0c091657132fa0,Refactor policies tests,MERGED,2014-12-04 09:53:30.000000000,2014-12-08 05:13:20.000000000,2014-12-08 05:13:19.000000000,"[{'_account_id': 3}, {'_account_id': 8592}, {'_account_id': 8731}]","[{'number': 1, 'created': '2014-12-04 09:53:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/2f13e923887e9fef35c91192e0ff611e8e688304', 'message': 'Refactor timeout policy test\n\nChange-Id: I04390a0b305e970aa609db27fa0c091657132fa0\n'}, {'number': 2, 'created': '2014-12-05 09:49:08.000000000', 'files': ['mistral/tests/unit/engine1/test_policies.py'], 'web_link': 'https://opendev.org/openstack/mistral/commit/946c8a86a39e072b66f7662041896821dfabbdae', 'message': 'Refactor policies tests\n\nChange-Id: I04390a0b305e970aa609db27fa0c091657132fa0\n'}]",3,139003,946c8a86a39e072b66f7662041896821dfabbdae,10,3,2,7700,,,0,"Refactor policies tests

Change-Id: I04390a0b305e970aa609db27fa0c091657132fa0
",git fetch https://review.opendev.org/openstack/mistral refs/changes/03/139003/2 && git format-patch -1 --stdout FETCH_HEAD,['mistral/tests/unit/engine1/test_policies.py'],1,2f13e923887e9fef35c91192e0ff611e8e688304,refactor_policy_test," task_db = self._assert_single_item(exec_db.tasks, name=""task1"")"," # We need to fix this action_context: workbook_name: wb execution_id: 123 task_id: 3121 task_db = exec_db.tasks[0] self.assertEqual(states.SUCCESS, exec_db.state)",1,7
openstack%2Fmistral~master~I1885f329f25e1424ed2b76325e007d7b5a028e93,openstack/mistral,master,I1885f329f25e1424ed2b76325e007d7b5a028e93,"Fix creating triggers with the same pattern, wf and wf-input",MERGED,2014-11-27 13:58:25.000000000,2014-12-08 05:12:29.000000000,2014-12-08 05:12:28.000000000,"[{'_account_id': 3}, {'_account_id': 7700}, {'_account_id': 8592}, {'_account_id': 8731}, {'_account_id': 9432}]","[{'number': 1, 'created': '2014-11-27 13:58:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/480f0f3515a8c8e52e7a5e1b4e851bf7b1d0dadd', 'message': 'Fix creating triggers with the same pattern, wf and wf-input\n\nCloses-bug: #1383146\n\nChange-Id: I1885f329f25e1424ed2b76325e007d7b5a028e93\n'}, {'number': 2, 'created': '2014-11-27 14:13:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/13f387d477234ff59d6dea90bbfcc8d896b50d49', 'message': 'Fix creating triggers with the same pattern, wf and wf-input\n\nCloses-bug: #1383146\n\nChange-Id: I1885f329f25e1424ed2b76325e007d7b5a028e93\n'}, {'number': 3, 'created': '2014-11-28 08:54:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/56938a333f2d2ecd874d0acc7403710d76d1c1df', 'message': 'Fix creating triggers with the same pattern, wf and wf-input\n\nCloses-bug: #1383146\n\nChange-Id: I1885f329f25e1424ed2b76325e007d7b5a028e93\n'}, {'number': 4, 'created': '2014-12-01 12:34:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/bd252be3e6b7d309267e86d9c3b816e2ca975080', 'message': 'Fix creating triggers with the same pattern, wf and wf-input\n\nCloses-bug: #1383146\n\nChange-Id: I1885f329f25e1424ed2b76325e007d7b5a028e93\n'}, {'number': 5, 'created': '2014-12-03 15:04:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/09b49090cc72a96cf94eff527060f14a6d25d5b3', 'message': 'Fix creating triggers with the same pattern, wf and wf-input\n\nCloses-bug: #1383146\n\nChange-Id: I1885f329f25e1424ed2b76325e007d7b5a028e93\n'}, {'number': 6, 'created': '2014-12-04 09:27:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/cb67e1933dba56b85c9e75483ab59c3f09db3b5d', 'message': 'Fix creating triggers with the same pattern, wf and wf-input\n\nCloses-bug: #1383146\n\nChange-Id: I1885f329f25e1424ed2b76325e007d7b5a028e93\n'}, {'number': 7, 'created': '2014-12-04 09:58:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/3c4fcd687e9a2d182327f09ae5ddc4f5f8987ea6', 'message': 'Fix creating triggers with the same pattern, wf and wf-input\n\nCloses-bug: #1383146\n\nChange-Id: I1885f329f25e1424ed2b76325e007d7b5a028e93\n'}, {'number': 8, 'created': '2014-12-05 12:45:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/0b47742dc512150d8ac6ad3472c3f9cf81e687d2', 'message': 'Fix creating triggers with the same pattern, wf and wf-input\n\nCloses-bug: #1383146\n\nChange-Id: I1885f329f25e1424ed2b76325e007d7b5a028e93\n'}, {'number': 9, 'created': '2014-12-05 13:21:30.000000000', 'files': ['mistral/tests/unit/test_trigger_service.py', 'mistral/tests/unit/api/v2/test_cron_triggers.py', 'mistral/tests/unit/db/v2/test_sqlalchemy_db_api.py', 'mistral/db/v2/sqlalchemy/models.py'], 'web_link': 'https://opendev.org/openstack/mistral/commit/a10c83aecfb978c044b374aaeefff5c8095faf1b', 'message': 'Fix creating triggers with the same pattern, wf and wf-input\n\nCloses-bug: #1383146\n\nChange-Id: I1885f329f25e1424ed2b76325e007d7b5a028e93\n'}]",6,137647,a10c83aecfb978c044b374aaeefff5c8095faf1b,29,5,9,7700,,,0,"Fix creating triggers with the same pattern, wf and wf-input

Closes-bug: #1383146

Change-Id: I1885f329f25e1424ed2b76325e007d7b5a028e93
",git fetch https://review.opendev.org/openstack/mistral refs/changes/47/137647/1 && git format-patch -1 --stdout FETCH_HEAD,"['mistral/tests/unit/api/v2/test_cron_triggers.py', 'mistral/api/controllers/v2/cron_trigger.py']",2,480f0f3515a8c8e52e7a5e1b4e851bf7b1d0dadd,bug/1383146,"from mistral import exceptions as exc same_triggers = db_api.get_cron_triggers( workflow_name=values['workflow_name'], pattern=values['pattern'], workflow_input=values.get('workflow_input') ) if same_triggers: raise exc.DBDuplicateEntry( ""Trigger already exists but has another "" ""name: %s"" % same_triggers ) ",,25,0
openstack%2Fpython-mistralclient~master~Ia0d8f727e6875f6c39551dd84eab1ad9422ad820,openstack/python-mistralclient,master,Ia0d8f727e6875f6c39551dd84eab1ad9422ad820,Add new functional CLI tests,MERGED,2014-12-02 12:48:47.000000000,2014-12-08 05:06:07.000000000,2014-12-08 05:06:07.000000000,"[{'_account_id': 3}, {'_account_id': 7700}, {'_account_id': 8731}]","[{'number': 1, 'created': '2014-12-02 12:48:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-mistralclient/commit/e03ce902051042b383caf90ad792fbebb5fe5143', 'message': 'Add new functional CLI tests\n\nChange-Id: Ia0d8f727e6875f6c39551dd84eab1ad9422ad820\nCloses-Bug: #1398383\n'}, {'number': 2, 'created': '2014-12-02 13:20:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-mistralclient/commit/89fca613a40f6e2fa99e7f875df26ee3821af77f', 'message': 'Add new functional CLI tests\n\nChange-Id: Ia0d8f727e6875f6c39551dd84eab1ad9422ad820\nCloses-Bug: #1398383\n'}, {'number': 3, 'created': '2014-12-02 14:06:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-mistralclient/commit/d385560375c226b80377c7f399888edbb54a76e1', 'message': 'Add new functional CLI tests\n\nChange-Id: Ia0d8f727e6875f6c39551dd84eab1ad9422ad820\nCloses-Bug: #1398383\n'}, {'number': 4, 'created': '2014-12-04 08:14:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-mistralclient/commit/eb2466862a9e5c198bf6e09a68c04220259fa5cf', 'message': 'Add new functional CLI tests\n\nChange-Id: Ia0d8f727e6875f6c39551dd84eab1ad9422ad820\nCloses-Bug: #1398383\n'}, {'number': 5, 'created': '2014-12-04 09:48:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-mistralclient/commit/0f5d6a68920f0256b83ad46b2265de01c7381880', 'message': 'Add new functional CLI tests\n\nChange-Id: Ia0d8f727e6875f6c39551dd84eab1ad9422ad820\nCloses-Bug: #1398383\n'}, {'number': 6, 'created': '2014-12-04 11:39:24.000000000', 'files': ['mistralclient/tests/functional/cli/v2/base_v2.py', 'functionaltests/resources/v2/wf_v2.yaml', 'mistralclient/tests/functional/cli/v2/cli_tests_v2.py'], 'web_link': 'https://opendev.org/openstack/python-mistralclient/commit/71000659f06704a04d81c6fbab5ea938af1992be', 'message': 'Add new functional CLI tests\n\nChange-Id: Ia0d8f727e6875f6c39551dd84eab1ad9422ad820\nCloses-Bug: #1398383\n'}]",2,138345,71000659f06704a04d81c6fbab5ea938af1992be,19,3,6,8592,,,0,"Add new functional CLI tests

Change-Id: Ia0d8f727e6875f6c39551dd84eab1ad9422ad820
Closes-Bug: #1398383
",git fetch https://review.opendev.org/openstack/python-mistralclient refs/changes/45/138345/4 && git format-patch -1 --stdout FETCH_HEAD,"['functionaltests/resources/v2/wf_v2.yaml', 'mistralclient/tests/functional/cli/v2/cli_tests_v2.py']",2,e03ce902051042b383caf90ad792fbebb5fe5143,," def test_workbook_create_with_tags(self): wb = self.workbook_create(self.wb_with_tags_def) tags = self.get_value_of_field(wb, 'Tags') self.assertIn('tag', tags) 'workbook-update', params=self.wb_with_tags_def) init_wfs = self.mistral_admin( wf_name = [wf['Name'] for wf in init_wfs] self.assertTableStruct(init_wfs, ['Name', 'Created at', 'Updated at']) self.assertIn(wf_name[0], [workflow['Name'] for workflow in wfs]) for wf in wfs: self.mistral_admin('workflow-delete', params=wf['Name']) for wf in wf_name: self.assertNotIn(wf, [workflow['Name'] for workflow in wfs]) def test_create_wf_with_tags(self): init_wfs = self.workflow_create(self.wf_def) wf_name = init_wfs[1]['Name'] self.assertTableStruct(init_wfs, ['Name', 'Created at', 'Updated at', 'Tags']) created_wf_info = self.get_item_info( get_from=init_wfs, get_by='Name', value=wf_name) self.assertEqual('tag', created_wf_info['Tags']) created_wf_info = self.get_item_info( get_from=wf, get_by='Name', value=wf_name) updated_wf_info = self.get_item_info( get_from=upd_wf, get_by='Name', value=wf_name) self.assertEqual(created_wf_info['Created at'].split(""."")[0], updated_wf_info['Created at']) self.assertNotEqual(created_wf_info['Updated at'], updated_wf_info['Updated at']) wfs = self.workflow_create(self.wf_def) self.direct_wf = wfs[0] self.reverse_wf = wfs[1] self.create_file('input', '{\n ""farewell"": ""Bye""\n}\n') self.create_file('task_name', '{\n ""task_name"": ""goodbye""\n}\n') 'execution-create', params=self.direct_wf['Name']) self.assertEqual(self.direct_wf['Name'], wf) def test_execution_create_with_input_and_start_task(self): execution = self.execution_create( ""%s input task_name"" % self.reverse_wf['Name']) exec_id = self.get_value_of_field(execution, 'ID') execution = self.mistral_admin( 'execution-get', params=exec_id) exec_state = self.get_value_of_field(execution, 'State') self.assertEqual('SUCCESS', exec_state) def test_execution_update(self): execution = self.execution_create(self.direct_wf['Name']) execution = self.execution_create(self.direct_wf['Name']) self.assertEqual(self.direct_wf['Name'], wf) execution = self.execution_create(self.direct_wf['Name']) execution = self.execution_create(self.direct_wf['Name']) self.mistral('cron-trigger-delete', params=tr_name)class TaskCLITests(base_v2.MistralClientTestBase): """"""Test suite checks commands to work with tasks."""""" def setUp(self): super(TaskCLITests, self).setUp() wfs = self.workflow_create(self.wf_def) self.direct_wf = wfs[0] self.reverse_wf = wfs[1] self.create_file('input', '{\n ""farewell"": ""Bye""\n}\n') self.create_file('task_name', '{\n ""task_name"": ""goodbye""\n}\n') def test_task_get(self): execution = self.execution_create(self.direct_wf['Name']) exec_id = self.get_value_of_field(execution, 'ID') tasks = self.mistral_admin('task-list') created_task_id = tasks[-1]['ID'] fetched_task = self.mistral_admin('task-get', params=created_task_id) fetched_task_id = self.get_value_of_field(fetched_task, 'ID') task_execution_id = self.get_value_of_field(fetched_task, 'Execution ID') self.assertEqual(created_task_id, fetched_task_id) self.assertEqual(exec_id, task_execution_id) acts = self.mistral_admin('action-update', params=self.act_def) def test_wb_update_wrong_path_to_def(self): def test_wb_update_nonexistant_wb(self): self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'workbook-update', params=self.wb_with_tags_def) def test_wb_create_empty_def(self): self.create_file('empty') self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'workbook-create', params='empty') def test_wb_update_empty_def(self): self.create_file('empty') self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'workbook-update', params='empty') def test_wb_create_invalid_def(self): self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'workbook-create', params=self.wf_def) def test_wb_update_invalid_def(self): self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'workbook-update', params=self.wf_def) def test_wb_update_without_def(self): self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'workbook-update') def test_wf_get_definition_missed_param(self): self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'workflow-get-definition') def test_wf_create_invalid_def(self): self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'workflow-create', params=self.wb_def) def test_wf_update_invalid_def(self): self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'workflow-update', params=self.wb_def) def test_wf_create_empty_def(self): self.create_file('empty') self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'workflow-create', params='empty') def test_wf_update_empty_def(self): self.create_file('empty') self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'workflow-update', params='empty') def test_ex_create_without_wf_name(self): self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'execution-create') def test_ex_create_reverse_wf_without_start_task(self): wf = self.workflow_create(self.wf_def) self.create_file('input', '{\n ""farewell"": ""Bye""\n}\n') self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'execution-create ', params=wf[1]['Name']) def test_ex_create_missed_input(self): self.create_file('empty') wf = self.workflow_create(self.wf_def) self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'execution-create empty', params=wf[1]['Name']) def test_ex_invalid_status_changing(self): wf = self.workflow_create(self.wf_def) execution = self.execution_create(params=wf[0]['Name']) exec_id = self.get_value_of_field(execution, 'ID') self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'execution-update', params='%s SUCCESS' % exec_id) def test_ex_delete_nonexistent_execution(self): self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'execution-delete', params='1a2b3c') def test_task_get_nonexistent_task(self): self.assertRaises(exceptions.CommandFailed, self.mistral_admin, 'task-get', params='nonexist')"," 'workbook-update', params='{0}'.format(self.wb_with_tags_def)) init_wf = self.mistral_admin( wf_name = init_wf[0]['Name'] self.assertTableStruct(init_wf, ['Name', 'Created at', 'Updated at']) self.assertIn(wf_name, [workflow['Name'] for workflow in wfs]) self.mistral_admin('workflow-delete', params=wf_name) self.assertNotIn(wf_name, [workflow['Name'] for workflow in wfs]) wf = self.workflow_create(self.wf_def) self.wf_name = wf[0]['Name'] wf_delay = self.workflow_create(self.wf_with_delay_def) self.wf_delay = wf_delay[0][""Name""] 'execution-create', params=self.wf_name) self.assertEqual(self.wf_name, wf) def test_execution_update(self): execution = self.execution_create(self.wf_delay) execution = self.execution_create(self.wf_name) self.assertEqual(self.wf_name, wf) execution = self.execution_create(self.wf_name) execution = self.execution_create(self.wf_name) self.mistral('cron-trigger-delete', params='{0}'.format(tr_name)) acts = self.mistral_admin( 'action-update', params='{0}'.format(self.act_def)) def test_wb_update_unexist_wb(self):",208,22
openstack%2Fopenstack-manuals~master~I5b369a51e3ef13312225e0d9550e6f8735b1c103,openstack/openstack-manuals,master,I5b369a51e3ef13312225e0d9550e6f8735b1c103,Updated the backup_swift_url default value in the Configuration Reference Guide,ABANDONED,2014-12-02 06:01:32.000000000,2014-12-08 05:02:14.000000000,,"[{'_account_id': 3}, {'_account_id': 6547}, {'_account_id': 7923}, {'_account_id': 10705}]","[{'number': 1, 'created': '2014-12-02 06:01:32.000000000', 'files': ['doc/common/tables/cinder-backups_swift.xml'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/f3f4a85f33ce0d2d173d7abc42ca5f07c336aafb', 'message': 'Updated the backup_swift_url default value in the Configuration Reference Guide\n\nChanged backup_swift_url = http://localhost:8080/v1/AUTH in the configuration options table\n\nChange-Id: I5b369a51e3ef13312225e0d9550e6f8735b1c103\nbackport: none\nCloses-Bug: #1390517\n'}]",0,138264,f3f4a85f33ce0d2d173d7abc42ca5f07c336aafb,6,4,1,10705,,,0,"Updated the backup_swift_url default value in the Configuration Reference Guide

Changed backup_swift_url = http://localhost:8080/v1/AUTH in the configuration options table

Change-Id: I5b369a51e3ef13312225e0d9550e6f8735b1c103
backport: none
Closes-Bug: #1390517
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/64/138264/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/common/tables/cinder-backups_swift.xml'],1,f3f4a85f33ce0d2d173d7abc42ca5f07c336aafb,swiftbackupdriver_configref/darren, <td>backup_swift_url = http://localhost:8080/v1/AUTH_</td>, <td>backup_swift_url = None</td>,1,1
openstack%2Fkeystone~master~Ifb19d09a1ed8e82aebcaea82783480176daa3536,openstack/keystone,master,Ifb19d09a1ed8e82aebcaea82783480176daa3536,make sample_data.sh account for the default options in keystone.conf,MERGED,2014-11-21 00:54:31.000000000,2014-12-08 03:05:27.000000000,2014-12-08 03:05:26.000000000,"[{'_account_id': 3}, {'_account_id': 1916}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 11022}]","[{'number': 1, 'created': '2014-11-21 00:54:31.000000000', 'files': ['tools/sample_data.sh'], 'web_link': 'https://opendev.org/openstack/keystone/commit/cdda91986bf5045bda0476f85425f35a0053bc57', 'message': 'make sample_data.sh account for the default options in keystone.conf\n\nCloses-Bug: 1394816\n\nChange-Id: Ifb19d09a1ed8e82aebcaea82783480176daa3536\n'}]",0,136199,cdda91986bf5045bda0476f85425f35a0053bc57,10,5,1,1916,,,0,"make sample_data.sh account for the default options in keystone.conf

Closes-Bug: 1394816

Change-Id: Ifb19d09a1ed8e82aebcaea82783480176daa3536
",git fetch https://review.opendev.org/openstack/keystone refs/changes/99/136199/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/sample_data.sh'],1,cdda91986bf5045bda0476f85425f35a0053bc57,bug/1394816," if [[ -z ""${CONFIG_SERVICE_TOKEN}"" ]]; then # default config options are commented out, so lets try those CONFIG_SERVICE_TOKEN=$(sed 's/[[:space:]]//g' $KEYSTONE_CONF | grep ^\#admin_token= | cut -d'=' -f2) fi if [[ -z ""${CONFIG_ADMIN_PORT}"" ]]; then # default config options are commented out, so lets try those CONFIG_ADMIN_PORT=$(sed 's/[[:space:]]//g' $KEYSTONE_CONF | grep ^\#admin_port= | cut -d'=' -f2) fi",,8,0
openstack%2Fkeystone~master~I9eb9885a2e3d3a743920818a733f299bf012bb5d,openstack/keystone,master,I9eb9885a2e3d3a743920818a733f299bf012bb5d,Move notification unit tests to unit test dir,MERGED,2014-11-11 21:53:42.000000000,2014-12-08 02:56:03.000000000,2014-12-08 02:56:02.000000000,"[{'_account_id': 3}, {'_account_id': 5046}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 8871}, {'_account_id': 11022}, {'_account_id': 13055}]","[{'number': 1, 'created': '2014-11-11 21:53:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/20c3e1454498844dd5e101767560c748dcffca04', 'message': 'Move notification unit tests to unit test dir\n\nThe test_notification.py module holds unit tests for the notification\nmodule and should live in the common unit test directory.\n\nChange-Id: I9eb9885a2e3d3a743920818a733f299bf012bb5d\n'}, {'number': 2, 'created': '2014-11-12 14:52:43.000000000', 'files': ['keystone/tests/unit/common/test_notifications.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/d8695848fac6a1eb4d9f0c3f824f9603629173ed', 'message': 'Move notification unit tests to unit test dir\n\nThe test_notification.py module holds unit tests for the notification\nmodule and should live in the common unit test directory.\n\nChange-Id: I9eb9885a2e3d3a743920818a733f299bf012bb5d\n'}]",0,133834,d8695848fac6a1eb4d9f0c3f824f9603629173ed,15,7,2,5046,,,0,"Move notification unit tests to unit test dir

The test_notification.py module holds unit tests for the notification
module and should live in the common unit test directory.

Change-Id: I9eb9885a2e3d3a743920818a733f299bf012bb5d
",git fetch https://review.opendev.org/openstack/keystone refs/changes/34/133834/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/tests/unit/common/test_notifications.py'],1,20c3e1454498844dd5e101767560c748dcffca04,add-notification-unit-tests,,,0,0
openstack%2Fkeystone~master~I86d4dd65f0eae8563723f6cbe02ecbc6962654e9,openstack/keystone,master,I86d4dd65f0eae8563723f6cbe02ecbc6962654e9,move matching id check in policy update into controller,ABANDONED,2014-10-31 03:14:35.000000000,2014-12-08 02:46:35.000000000,,"[{'_account_id': 3}, {'_account_id': 7725}, {'_account_id': 9101}, {'_account_id': 13055}, {'_account_id': 13063}]","[{'number': 1, 'created': '2014-10-31 03:14:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/7f4fbec969d7b6ccd6780b50a774c4194c3209d4', 'message': 'move matching id check in policy update into controller\n\nMake it consistent with other keystone objects.\n\nChange-Id: I86d4dd65f0eae8563723f6cbe02ecbc6962654e9\n'}, {'number': 2, 'created': '2014-10-31 06:30:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/1183b75dec35f3d15cf51bbc6e3555030fad52ff', 'message': 'move matching id check in policy update into controller\n\nMake it consistent with other keystone objects.\n\nChange-Id: I86d4dd65f0eae8563723f6cbe02ecbc6962654e9\n'}, {'number': 3, 'created': '2014-11-27 03:44:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/330ef87826fbb6df47592b5266789ef62761f6df', 'message': 'move matching id check in policy update into controller\n\nThere is _require_matching_id method to do this already, and the parameter check should be in controller layer too.\nMake it consistent with other keystone objects.\n\nChange-Id: I86d4dd65f0eae8563723f6cbe02ecbc6962654e9\n'}, {'number': 4, 'created': '2014-11-27 03:44:37.000000000', 'files': ['keystone/policy/controllers.py', 'keystone/policy/core.py', 'keystone/tests/test_backend.py', 'keystone/tests/test_backend_rules.py', 'keystone/tests/test_v3_policy.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/689eb8b3c520f2d66f78375e34a5bc05fc4b17d8', 'message': 'move matching id check in policy update into controller\n\nThere is _require_matching_id method to do this already,\nand the parameter check should be in controller layer too.\nMake it consistent with other keystone objects.\n\nChange-Id: I86d4dd65f0eae8563723f6cbe02ecbc6962654e9\n'}]",8,132152,689eb8b3c520f2d66f78375e34a5bc05fc4b17d8,16,5,4,9101,,,0,"move matching id check in policy update into controller

There is _require_matching_id method to do this already,
and the parameter check should be in controller layer too.
Make it consistent with other keystone objects.

Change-Id: I86d4dd65f0eae8563723f6cbe02ecbc6962654e9
",git fetch https://review.opendev.org/openstack/keystone refs/changes/52/132152/4 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/policy/controllers.py', 'keystone/policy/core.py', 'keystone/tests/test_backend.py', 'keystone/tests/test_v3_policy.py']",4,7f4fbec969d7b6ccd6780b50a774c4194c3209d4,udpate_policy,"from keystone import exception def test_update_policy_id_fails(self): policy = self.new_policy_ref() self.patch( '/policies/%(policy_id)s' % { 'policy_id': self.policy_id}, body={'policy': policy}, expected_status=exception.ValidationError.code) ",,27,12
openstack%2Fpython-keystoneclient~master~I8cfc85011b0eef5ee77ee8d1b01b4bc58ee23023,openstack/python-keystoneclient,master,I8cfc85011b0eef5ee77ee8d1b01b4bc58ee23023,remove the value convert that no logger needed,ABANDONED,2014-11-03 03:58:12.000000000,2014-12-08 02:45:58.000000000,,"[{'_account_id': 3}, {'_account_id': 5046}, {'_account_id': 7191}, {'_account_id': 9101}, {'_account_id': 11022}, {'_account_id': 13478}]","[{'number': 1, 'created': '2014-11-03 03:58:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/75e3a2b46e0a568430764793730073c2abaeda5f', 'message': 'remove the value convert that no logger needed\n\nThe bug #1267530 has been fixed, so the Flase->0 convert is no\nlogger needed.\n\nChange-Id: I8cfc85011b0eef5ee77ee8d1b01b4bc58ee23023\n'}, {'number': 2, 'created': '2014-11-25 01:03:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/f2cda52f5ba1286863a836d6a018cad64f5524c9', 'message': 'remove the value convert that no logger needed\n\nThe bug #1267530 has been fixed, so the Flase->0 convert is no\nlogger needed.\n\nRelated-Bug: #1267530\nChange-Id: I8cfc85011b0eef5ee77ee8d1b01b4bc58ee23023\n'}, {'number': 3, 'created': '2014-11-25 09:22:26.000000000', 'files': ['keystoneclient/tests/v3/test_domains.py', 'keystoneclient/v3/domains.py'], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/46c7f47f46e7d05e7914729c26fd20c915c4bbb2', 'message': 'remove the value convert that no logger needed\n\nThe bug #1267530 has been fixed, so the False->0 convert is no\nlogger needed.\n\nRelated-Bug: #1267530\nChange-Id: I8cfc85011b0eef5ee77ee8d1b01b4bc58ee23023\n'}]",2,132500,46c7f47f46e7d05e7914729c26fd20c915c4bbb2,15,6,3,9101,,,0,"remove the value convert that no logger needed

The bug #1267530 has been fixed, so the False->0 convert is no
logger needed.

Related-Bug: #1267530
Change-Id: I8cfc85011b0eef5ee77ee8d1b01b4bc58ee23023
",git fetch https://review.opendev.org/openstack/python-keystoneclient refs/changes/00/132500/3 && git format-patch -1 --stdout FETCH_HEAD,"['keystoneclient/tests/v3/test_domains.py', 'keystoneclient/v3/domains.py']",2,75e3a2b46e0a568430764793730073c2abaeda5f,bug/1267530,, # Ref bug #1267530 we have to pass 0 for False to get the expected # results on all keystone versions if kwargs.get('enabled') is False: kwargs['enabled'] = 0,1,6
openstack%2Fnova~master~I9b45e5108106573f2da191beaf6ee98582940a71,openstack/nova,master,I9b45e5108106573f2da191beaf6ee98582940a71,Fix driver attach volume error,ABANDONED,2014-11-21 10:33:33.000000000,2014-12-08 02:43:57.000000000,,"[{'_account_id': 3}, {'_account_id': 1779}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 11531}]","[{'number': 1, 'created': '2014-11-21 10:33:33.000000000', 'files': ['nova/tests/unit/compute/test_compute.py', 'nova/compute/manager.py', 'nova/virt/block_device.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/14e1770bf54417f7cb2bdfc8d550cecc07fafd75', 'message': ""Fix driver attach volume error\n\nI attach volume on VM, there is a exception occured when call Cinder's\nattach_volume, Cinder is shown that the volume is not mounted, but I login\nin to the VM and found that the volume was mounted on the VM.\nI think that Nova need to detach volume when the exception occurs,\nbecause the Nova driver has mounted successful before call Cinder's\nattach_volume.\n\nChange-Id: I9b45e5108106573f2da191beaf6ee98582940a71\nCloses-Bug: #1340709\n""}]",0,136289,14e1770bf54417f7cb2bdfc8d550cecc07fafd75,10,8,1,11531,,,0,"Fix driver attach volume error

I attach volume on VM, there is a exception occured when call Cinder's
attach_volume, Cinder is shown that the volume is not mounted, but I login
in to the VM and found that the volume was mounted on the VM.
I think that Nova need to detach volume when the exception occurs,
because the Nova driver has mounted successful before call Cinder's
attach_volume.

Change-Id: I9b45e5108106573f2da191beaf6ee98582940a71
Closes-Bug: #1340709
",git fetch https://review.opendev.org/openstack/nova refs/changes/89/136289/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/compute/test_compute.py', 'nova/compute/manager.py', 'nova/virt/block_device.py']",3,14e1770bf54417f7cb2bdfc8d550cecc07fafd75,bug/1340709," try: volume_api.attach(context, volume_id, instance['uuid'], self['mount_device'], mode=mode) except Exception: with excutils.save_and_reraise_exception(): if do_driver_attach: virt_driver.detach_volume(connection_info, instance, self['mount_device']) volume_api.terminate_connection(context, volume_id, connector)"," volume_api.attach(context, volume_id, instance['uuid'], self['mount_device'], mode=mode)",16,8
openstack%2Fkeystone~master~I5060c30cd03b3154838db9e6f8fee94110751745,openstack/keystone,master,I5060c30cd03b3154838db9e6f8fee94110751745,Typo in policy call,MERGED,2014-12-06 04:17:55.000000000,2014-12-08 02:38:02.000000000,2014-12-08 02:38:01.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 5707}, {'_account_id': 6482}, {'_account_id': 9142}, {'_account_id': 11022}]","[{'number': 1, 'created': '2014-12-06 04:17:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/50a189108ff5fe0aa4f4a9e010568b888bd62263', 'message': 'Typo in policy call\n\nCloses-Bug 1399857\n\nChange-Id: I5060c30cd03b3154838db9e6f8fee94110751745\n'}, {'number': 2, 'created': '2014-12-06 10:27:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/082ced7f08ba82d0027a9f8a4f68e3e2f7d885ff', 'message': 'Typo in policy call\n\nCloses-Bug: 1399857\n\nChange-Id: I5060c30cd03b3154838db9e6f8fee94110751745\n'}, {'number': 3, 'created': '2014-12-06 10:54:00.000000000', 'files': ['keystone/contrib/endpoint_policy/controllers.py', 'keystone/tests/test_v3_endpoint_policy.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/b22b85837673e1cdeb082755585877ff5b322798', 'message': 'Typo in policy call\n\nThe callback notification to endpoint-policy upon deletion of a policy\nhad a typo in it. This patch fixes this, and adds a test to ensure this\nnotification is working as designed. The other three notifications\nalready have tests, for some reason this one had been missed out.\n\nCloses-Bug: 1399857\n\nChange-Id: I5060c30cd03b3154838db9e6f8fee94110751745\n'}]",2,139780,b22b85837673e1cdeb082755585877ff5b322798,17,6,3,2218,,,0,"Typo in policy call

The callback notification to endpoint-policy upon deletion of a policy
had a typo in it. This patch fixes this, and adds a test to ensure this
notification is working as designed. The other three notifications
already have tests, for some reason this one had been missed out.

Closes-Bug: 1399857

Change-Id: I5060c30cd03b3154838db9e6f8fee94110751745
",git fetch https://review.opendev.org/openstack/keystone refs/changes/80/139780/3 && git format-patch -1 --stdout FETCH_HEAD,['keystone/contrib/endpoint_policy/controllers.py'],1,50a189108ff5fe0aa4f4a9e010568b888bd62263,bug/1399857, self.endpoint_policy_api.delete_association_by_policy(, self.endpoint_policy_api.delete_association_by_polcy(,1,1
openstack%2Fheat~master~I856300b896a55bbd33cdc8b465a2bb4a43ea2b39,openstack/heat,master,I856300b896a55bbd33cdc8b465a2bb4a43ea2b39,Use template directly instead of stack.tmpl,MERGED,2014-12-05 07:20:13.000000000,2014-12-08 02:35:53.000000000,2014-12-08 02:35:52.000000000,"[{'_account_id': 3}, {'_account_id': 3098}, {'_account_id': 4715}, {'_account_id': 13323}]","[{'number': 1, 'created': '2014-12-05 07:20:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/7550cb0d1053604254923efb308c73495c2fcfa5', 'message': 'Use template directly instead of stack.tmpl\n\nUse template.Template directly instead of stack.tmpl.Template.\n\nChange-Id: I856300b896a55bbd33cdc8b465a2bb4a43ea2b39\n'}, {'number': 2, 'created': '2014-12-05 08:17:28.000000000', 'files': ['heat/tests/test_engine_service.py', 'heat/tests/test_swiftsignal.py', 'heat/engine/stack_resource.py', 'heat/tests/test_stack_resource.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/1962b73c82f1782c52baaf30a74afbd1af523ad1', 'message': 'Use template directly instead of stack.tmpl\n\nUse template.Template directly instead of stack.tmpl.Template.\n\nChange-Id: I856300b896a55bbd33cdc8b465a2bb4a43ea2b39\n'}]",0,139548,1962b73c82f1782c52baaf30a74afbd1af523ad1,11,4,2,8289,,,0,"Use template directly instead of stack.tmpl

Use template.Template directly instead of stack.tmpl.Template.

Change-Id: I856300b896a55bbd33cdc8b465a2bb4a43ea2b39
",git fetch https://review.opendev.org/openstack/heat refs/changes/48/139548/2 && git format-patch -1 --stdout FETCH_HEAD,"['heat/tests/test_engine_service.py', 'heat/tests/test_swiftsignal.py', 'heat/engine/stack_resource.py', 'heat/tests/test_stack_resource.py']",4,7550cb0d1053604254923efb308c73495c2fcfa5,use-template-instead-stack.tmpl, t = templatem.Template(, t = parser.tmpl.Template(,6,5
openstack%2Fkeystonemiddleware~master~Ib624b893f6d545223ea09714124752c750651818,openstack/keystonemiddleware,master,Ib624b893f6d545223ea09714124752c750651818,fix strange logic in verify_token method,ABANDONED,2014-10-30 08:04:23.000000000,2014-12-08 01:48:50.000000000,,"[{'_account_id': 3}, {'_account_id': 6486}]","[{'number': 1, 'created': '2014-10-30 08:04:23.000000000', 'files': ['keystonemiddleware/auth_token.py'], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/df3c0271786583d546349e54014f7efe93c364e3', 'message': 'fix strange logic in verify_token method\n\nWhen we catch NotFound, Unauthorized, HttpError exception we just\nreturn None rather than raise InvalidToken exception. Instead, we\nraise InvalidToken exception via the parameter check in\n_get_token_expiration method. The verify_token method itself should\nraise InvalidToken exception when token authorization failed.\n\nChange-Id: Ib624b893f6d545223ea09714124752c750651818\n'}]",0,131962,df3c0271786583d546349e54014f7efe93c364e3,4,2,1,9101,,,0,"fix strange logic in verify_token method

When we catch NotFound, Unauthorized, HttpError exception we just
return None rather than raise InvalidToken exception. Instead, we
raise InvalidToken exception via the parameter check in
_get_token_expiration method. The verify_token method itself should
raise InvalidToken exception when token authorization failed.

Change-Id: Ib624b893f6d545223ea09714124752c750651818
",git fetch https://review.opendev.org/openstack/keystonemiddleware refs/changes/62/131962/1 && git format-patch -1 --stdout FETCH_HEAD,['keystonemiddleware/auth_token.py'],1,df3c0271786583d546349e54014f7efe93c364e3,improve, raise InvalidToken('Token authorization failed'), :raise ServiceError: if unable to authenticate token raise InvalidToken(),1,2
openstack%2Fheat~master~I47bc858e6f0d0b92a103cf40fdf47e1c8d5c89c5,openstack/heat,master,I47bc858e6f0d0b92a103cf40fdf47e1c8d5c89c5,Remove i18n import from loguserdata.py,MERGED,2014-12-05 13:26:50.000000000,2014-12-08 01:37:29.000000000,2014-12-05 16:19:47.000000000,"[{'_account_id': 3}, {'_account_id': 4571}, {'_account_id': 6577}, {'_account_id': 9542}, {'_account_id': 13009}]","[{'number': 1, 'created': '2014-12-05 13:26:50.000000000', 'files': ['heat/cloudinit/loguserdata.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/014bf9d6112af6ac062156d3b8032a8f08e12ba3', 'message': 'Remove i18n import from loguserdata.py\n\nChange-Id: I47bc858e6f0d0b92a103cf40fdf47e1c8d5c89c5\nCloses-Bug: #1399642\n'}]",0,139623,014bf9d6112af6ac062156d3b8032a8f08e12ba3,10,5,1,13323,,,0,"Remove i18n import from loguserdata.py

Change-Id: I47bc858e6f0d0b92a103cf40fdf47e1c8d5c89c5
Closes-Bug: #1399642
",git fetch https://review.opendev.org/openstack/heat refs/changes/23/139623/1 && git format-patch -1 --stdout FETCH_HEAD,['heat/cloudinit/loguserdata.py'],1,014bf9d6112af6ac062156d3b8032a8f08e12ba3,bug/1399642," LOG.error('Userdata empty or not executable: %s', ex) LOG.error('OS error running userdata: %s', ex) LOG.error('Unknown error running userdata: %s', ex) LOG.error('Unable to log provisioning, need a newer version of ' 'cloud-init') LOG.info('Provision began: %s', datetime.datetime.now()) LOG.info('Provision done: %s', datetime.datetime.now()) LOG.error('Provision failed with exit code %s', code)","from heat.common.i18n import _LE from heat.common.i18n import _LI LOG.error(_LE('Userdata empty or not executable: %s'), ex) LOG.error(_LE('OS error running userdata: %s'), ex) LOG.error(_LE('Unknown error running userdata: %s'), ex) LOG.error(_LE('Unable to log provisioning, need a newer version ' 'of cloud-init')) LOG.info(_LI('Provision began: %s'), datetime.datetime.now()) LOG.info(_LI('Provision done: %s'), datetime.datetime.now()) LOG.error(_LE('Provision failed with exit code %s'), code)",8,11
openstack%2Fdevstack~stable%2Ficehouse~I3b509312846d29a5d14607d5eb561eb9545b28a5,openstack/devstack,stable/icehouse,I3b509312846d29a5d14607d5eb561eb9545b28a5,Actually enable new feature of container-sync,MERGED,2014-11-26 11:57:16.000000000,2014-12-08 01:34:17.000000000,2014-12-08 01:34:15.000000000,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 7118}, {'_account_id': 8859}]","[{'number': 1, 'created': '2014-11-26 11:57:16.000000000', 'files': ['lib/swift'], 'web_link': 'https://opendev.org/openstack/devstack/commit/81dcbbdca842cbf4094755ea432671d75416d05a', 'message': 'Actually enable new feature of container-sync\n\nI modified devstack to run newer version of container-sync in\ncommit d03915f, and cherry-picked to stable/icehouse.\nHowever, $SWIFT_SERVICE_PROTOCOL variable is introduced after\nstable/icehouse (commit 18d4778), therefore my cherry-pick patch did\nnot enable container-sync feature.\n\nThis patch replaces this variable with string value ""http"", so now\nicehouse swift deployed by devstack works appropriately.\n\nChange-Id: I3b509312846d29a5d14607d5eb561eb9545b28a5\nCloses-Bug: 1378646\n'}]",0,137342,81dcbbdca842cbf4094755ea432671d75416d05a,8,4,1,8859,,,0,"Actually enable new feature of container-sync

I modified devstack to run newer version of container-sync in
commit d03915f, and cherry-picked to stable/icehouse.
However, $SWIFT_SERVICE_PROTOCOL variable is introduced after
stable/icehouse (commit 18d4778), therefore my cherry-pick patch did
not enable container-sync feature.

This patch replaces this variable with string value ""http"", so now
icehouse swift deployed by devstack works appropriately.

Change-Id: I3b509312846d29a5d14607d5eb561eb9545b28a5
Closes-Bug: 1378646
",git fetch https://review.opendev.org/openstack/devstack refs/changes/42/137342/1 && git format-patch -1 --stdout FETCH_HEAD,['lib/swift'],1,81dcbbdca842cbf4094755ea432671d75416d05a,bug/1378646," iniset ${csyncfile} realm1 cluster_name1 ""http://$SERVICE_HOST:8080/v1/"""," iniset ${csyncfile} realm1 cluster_name1 ""$SWIFT_SERVICE_PROTOCOL://$SERVICE_HOST:8080/v1/""",1,1
openstack%2Fheat~master~I842eb89ac42de8d525d2092ff9d02e70daa8a981,openstack/heat,master,I842eb89ac42de8d525d2092ff9d02e70daa8a981,Expose resource attributes in the engine,MERGED,2014-11-17 23:11:57.000000000,2014-12-08 00:55:10.000000000,2014-12-08 00:55:08.000000000,"[{'_account_id': 3}, {'_account_id': 4715}, {'_account_id': 7230}, {'_account_id': 7253}, {'_account_id': 7256}, {'_account_id': 7404}, {'_account_id': 8289}, {'_account_id': 9189}]","[{'number': 1, 'created': '2014-11-17 23:11:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/62829630c4e321c1636f14285051f28c101df5ee', 'message': 'Show resource attributes when viewing resource\n\nThis changes the engine to allow the user to be able to view resource\nattributes after creating a stack, even if such attributes were not\nincluded in the template outputs.\n\nImplements: blueprint detailed-resource-show\nChange-Id: I842eb89ac42de8d525d2092ff9d02e70daa8a981\n'}, {'number': 2, 'created': '2014-11-18 14:37:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/72cfadbf5817c38540ce522fd98f01b1f6d4791c', 'message': 'Show resource attributes when viewing resource\n\nThis changes the engine to allow the user to be able to view resource\nattributes after creating a stack, even if such attributes were not\nincluded in the template outputs.\n\nImplements: blueprint detailed-resource-show\nChange-Id: I842eb89ac42de8d525d2092ff9d02e70daa8a981\n'}, {'number': 3, 'created': '2014-11-20 17:42:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/2294e91cde6ac299121080b3680ef6e4f203e201', 'message': 'Show resource attributes when viewing resource\n\nThis changes the engine to allow the user to be able to view resource\nattributes after creating a stack, even if such attributes were not\nincluded in the template outputs.\n\nImplements: blueprint detailed-resource-show\nChange-Id: I842eb89ac42de8d525d2092ff9d02e70daa8a981\n'}, {'number': 4, 'created': '2014-12-04 17:52:41.000000000', 'files': ['heat/tests/test_engine_service.py', 'heat/engine/api.py', 'heat/engine/service.py', 'heat/tests/test_engine_api_utils.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/6d8a5cb35cec81b23a71e8c6fa692f8e7b4dca1c', 'message': 'Expose resource attributes in the engine\n\nThis changes the engine to allow the user to be able to view resource\nattributes after creating a stack, even if such attributes were not\nincluded in the template outputs.\n\nImplements: blueprint detailed-resource-show\nChange-Id: I842eb89ac42de8d525d2092ff9d02e70daa8a981\n'}]",3,135110,6d8a5cb35cec81b23a71e8c6fa692f8e7b4dca1c,25,8,4,9189,,,0,"Expose resource attributes in the engine

This changes the engine to allow the user to be able to view resource
attributes after creating a stack, even if such attributes were not
included in the template outputs.

Implements: blueprint detailed-resource-show
Change-Id: I842eb89ac42de8d525d2092ff9d02e70daa8a981
",git fetch https://review.opendev.org/openstack/heat refs/changes/10/135110/4 && git format-patch -1 --stdout FETCH_HEAD,"['heat/tests/test_engine_service.py', 'heat/engine/api.py', 'heat/engine/service.py', 'heat/tests/test_engine_api_utils.py']",4,62829630c4e321c1636f14285051f28c101df5ee,bp/detailed-resource-show," rpc_api.RES_REQUIRED_BY, )) resource_details_keys = resource_keys.union(set(( rpc_api.RES_DESCRIPTION, rpc_api.RES_METADATA, rpc_api.RES_SCHEMA_ATTRIBUTES, ))) @mock.patch.object(api, 'format_resource_attributes') def test_format_stack_resource_with_attributes(self, mock_format_attrs): mock_format_attrs.return_value = 'formatted_resource_attrs' res = self.stack['generic1'] formatted = api.format_stack_resource(res, True, with_attr=['a', 'b']) formatted_attrs = formatted[rpc_api.RES_SCHEMA_ATTRIBUTES] self.assertEqual('formatted_resource_attrs', formatted_attrs) def test_format_resource_attributes(self): res = self.stack['generic1'] formatted_attributes = api.format_resource_attributes(res) self.assertEqual(2, len(formatted_attributes)) self.assertIn('foo', formatted_attributes) self.assertIn('Foo', formatted_attributes) def test_format_resource_attributes_show_attribute(self): res = mock.Mock() res.attributes = {'a': 'a_value', 'show': {'b': 'b_value'}} formatted_attributes = api.format_resource_attributes(res) self.assertIn('b', formatted_attributes) self.assertNotIn('a', formatted_attributes) def test_format_resource_attributes_force_attributes(self): res = self.stack['generic1'] force_attrs = ['a1', 'a2'] formatted_attributes = api.format_resource_attributes(res, force_attrs) self.assertEqual(4, len(formatted_attributes)) self.assertIn('foo', formatted_attributes) self.assertIn('Foo', formatted_attributes) self.assertIn('a1', formatted_attributes) self.assertIn('a2', formatted_attributes) "," rpc_api.RES_REQUIRED_BY)) resource_details_keys = resource_keys.union(set( (rpc_api.RES_DESCRIPTION, rpc_api.RES_METADATA)))",71,7
openstack%2Fnova-specs~master~I5d580c98d98d6899ce46b892d64b21c8dd8abe69,openstack/nova-specs,master,I5d580c98d98d6899ce46b892d64b21c8dd8abe69,Adds RequestSpec object model,MERGED,2014-10-10 18:10:02.000000000,2014-12-07 23:39:00.000000000,2014-12-07 23:39:00.000000000,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 782}, {'_account_id': 1063}, {'_account_id': 1981}, {'_account_id': 2271}, {'_account_id': 4393}, {'_account_id': 5441}, {'_account_id': 5511}, {'_account_id': 5754}, {'_account_id': 6828}, {'_account_id': 7166}, {'_account_id': 7664}]","[{'number': 1, 'created': '2014-10-10 18:10:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/7501844038e431b4ad0c5781f9612d293051f331', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 2, 'created': '2014-10-13 16:36:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/ef10815c4f75eb2a9985680570ac381ea3720519', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 3, 'created': '2014-10-13 16:38:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/c2aeac25ce3b0fd50fc535897c6c1f5fc0f612f7', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 4, 'created': '2014-10-13 16:43:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/8c4395df9360cd7b06e29b8dade6e5e26dcee054', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 5, 'created': '2014-10-14 00:36:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/6059f1009366aea292ecde653f3cfc4a564af3cc', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 6, 'created': '2014-11-11 09:03:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/377bca4472fe6033450eb97be0e05af02d9e3c3c', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 7, 'created': '2014-11-11 14:10:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/27fb69f160cb1c74f4480ca5a10e5e4db2ea08f7', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 8, 'created': '2014-11-12 15:34:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/08426fbb3369313b3b5b9f9b18b0fba9ce7a0528', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 9, 'created': '2014-11-17 15:41:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/8ee195d7947bd62baa254d77cef1de303cc9f02c', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 10, 'created': '2014-11-19 10:30:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/a3408713cf7e9b68fe2b20b3ead5830e2782bdec', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nCo-Authored-By: Sylvain Bauza <sbauza@redhat.com>\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 11, 'created': '2014-11-20 15:12:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/21e89680981cb25dca3eb69718856fbe7081ca8c', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nCo-Authored-By: Sylvain Bauza <sbauza@redhat.com>\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 12, 'created': '2014-11-21 21:54:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/df703bda9837ce169445d26613aca8244161ea14', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nCo-Authored-By: Sylvain Bauza <sbauza@redhat.com>\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 13, 'created': '2014-11-26 17:06:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/b1d965365fd21f9e47346cd92b6b632ed7c861f9', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nCo-Authored-By: Sylvain Bauza <sbauza@redhat.com>\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 14, 'created': '2014-12-07 20:15:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/b968ffef5c1a64a40252b6bc21d1ea78fae93929', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nCo-Authored-By: Sylvain Bauza <sbauza@redhat.com>\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}, {'number': 15, 'created': '2014-12-07 23:16:28.000000000', 'files': ['specs/kilo/approved/request-spec-object.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/cd7ebb8877ae7309b69f05feee2b1e7c13f3c297', 'message': 'Adds RequestSpec object model\n\nSpecification for adding an object model that encapsulates the request\nto launch one or more instances in the cloud.\n\nCo-Authored-By: Sylvain Bauza <sbauza@redhat.com>\n\nChange-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69\nBlueprint: request-spec-object\n'}]",37,127610,cd7ebb8877ae7309b69f05feee2b1e7c13f3c297,70,13,15,7,,,0,"Adds RequestSpec object model

Specification for adding an object model that encapsulates the request
to launch one or more instances in the cloud.

Co-Authored-By: Sylvain Bauza <sbauza@redhat.com>

Change-Id: I5d580c98d98d6899ce46b892d64b21c8dd8abe69
Blueprint: request-spec-object
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/10/127610/5 && git format-patch -1 --stdout FETCH_HEAD,['specs/kilo/request-spec-object.rst'],1,7501844038e431b4ad0c5781f9612d293051f331,bp/request-spec-object,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================= Create RequestSpec Object ========================= Include the URL of your launchpad blueprint: https://blueprints.launchpad.net/nova/+spec/request-spec-object Add a structured, documented object that represents a specification for launching multiple instances in a cloud. Problem description =================== The main interface into the scheduler, the `select_destinations()` method, accepts a `request_spec` parameter that is a nested dict. This nested dict is constructed in `nova.scheduler.utils.build_request_spec()`, however the structure of the request spec is not documented anywhere and the filters in the scheduler seem to take a laisse faire approach to querying the object during scheduling as well as modifying the `request_spec` object during loops of the `nova.scheduler.host_manager.HostStateManager.get_filtered_hosts()` method, which calls the filter object's `host_passes` object, supplying a `filter_properties` parameter, which itself has a key called `request_spec` that contains the aforementioned nested dict. This situation makes it very difficult to understand exactly what is going on in the scheduler, and cleaning up this parameter in the scheduler interface is a pre-requisite to making a properly-versioned and properly-documented interface in preparation for a split-out of the scheduler code. Use Cases ---------- Nova contributors wish to extend the functionality of the scheduler and intend to break the scheduler out into the Gantt project. In order to do this effectively, the internal interfaces around the resource tracker and the scheduler must be cleaned up to use structured objects. Project Priority ----------------- Falls under the technical debt reduction category. Proposed change =============== A new class called `RequestSpec` will be created that models a request to launch multiple virtual machine instances. The first version of the `RequestSpec` object will simply be an objectified version of the current dictionary parameter. The scheduler will construct this `RequestSpec` object from the `request_spec` dictionary itself. The existing `nova.scheduler.utils.build_request_spec` method will be removed in favor of a factory method on `nova.objects.request_spec.RequestSpec` that will construct a `RequestSpec` from the existing key/value pairs in the `request_spec` parameter supplied to `select_destinations`. Alternatives ------------ None. Data model impact ----------------- None. REST API impact --------------- None. Security impact --------------- None. Notifications impact -------------------- None. Other end user impact --------------------- None. Performance Impact ------------------ None. Other deployer impact --------------------- None. Developer impact ---------------- None, besides making the scheduler call interfaces gradually easier to read and understand. Implementation ============== Assignee(s) ----------- Primary assignee: jaypipes Implementation ============== The `request_spec` dictionary is currently constructed by the nova-conductor when it calls the `nova.scheduler.utils.build_request_spec()` function, which looks like this: .. code:: python def build_request_spec(ctxt, image, instances, instance_type=None): """"""Build a request_spec for the scheduler. The request_spec assumes that all instances to be scheduled are the same type. """""" instance = instances[0] if isinstance(instance, obj_base.NovaObject): instance = obj_base.obj_to_primitive(instance) if instance_type is None: instance_type = flavors.extract_flavor(instance) # NOTE(comstud): This is a bit ugly, but will get cleaned up when # we're passing an InstanceType internal object. extra_specs = db.flavor_extra_specs_get(ctxt, instance_type['flavorid']) instance_type['extra_specs'] = extra_specs request_spec = { 'image': image or {}, 'instance_properties': instance, 'instance_type': instance_type, 'num_instances': len(instances), # NOTE(alaski): This should be removed as logic moves from the # scheduler to conductor. Provides backwards compatibility now. 'instance_uuids': [inst['uuid'] for inst in instances]} return jsonutils.to_primitive(request_spec) A possible first version of a class interface for the `RequestSpec` class would look like this, in order to be as close to a straight conversion from the nested dict's keys to object attribute notation: .. code:: python class RequestSpec(base.NovaObject): """"""Models the request to launch one or more instances in the cloud."""""" VERSION = '1.0' fields = { 'image': fields.DictOfStringsField(nullable=False), 'instance_properties': fields.DictOfStringsField(nullable=False), 'instance_type': fields.ObjectField('Flavor', nullable=True), 'num_instances': fields.IntegerField(nullable=False, default=1), 'instance_uuids': fields.ListOfStringsField(nullable=True) } This blueprint does not change the `select_destinations` scheduler RPC API method, so we would construct a `nova.objects.request_spec.RequestSpec` object *on the nova-scheduler side*, from the `request_spec` dictionary key in the `FilterScheduler._schedule()` method, like so: .. code:: python def _schedule(self, context, request_spec, filter_properties): """"""Returns a list of hosts that meet the required specs, ordered by their fitness. """""" elevated = context.elevated() request_spec = objects.RequestSpec.from_dict(request_spec) instance_properties = request_spec.instance_properties instance_type = request_spec.instance_type instance_uuids = request_spec.instance_uuids update_group_hosts = self._setup_instance_group(context, filter_properties) config_options = self._get_configuration_options() filter_properties.update({'context': context, 'request_spec': request_spec, 'config_options': config_options, 'instance_type': instance_type}) self.populate_filter_properties(request_spec, filter_properties) # Find our local list of acceptable hosts by repeatedly # filtering and weighing our options. Each time we choose a # host, we virtually consume resources on it so subsequent # selections can adjust accordingly. # Note: remember, we are using an iterator here. So only # traverse this list once. This can bite you if the hosts # are being scanned in a filter or weighing function. hosts = self._get_all_host_states(elevated) selected_hosts = [] if instance_uuids: num_instances = len(instance_uuids) else: num_instances = request_spec.num_instances for num in xrange(num_instances): # Filter local hosts based on requirements ... hosts = self.host_manager.get_filtered_hosts(hosts, filter_properties, index=num) The final steps of implementing this blueprint would be to change all of the scheduler filters to access the original properties via object notation instead of dict-notation. Assignee(s) ----------- Primary assignee: jaypipes Work Items ---------- - Add request spec class to `nova/scheduler/request_spec.py` w/ unit tests - Add a factory classmethod on `nova.scheduler.RequestSpec` that constructs a `RequestSpec` object from the *existing* set of instance type extra_specs, scheduler_hints, flavor and image objects that are supplied to the `nova.scheduler.utils.build_request_spec` function. - Modify `FilterScheduler._schedule()` to construct a `RequestSpec` object from the supplied nested `request_spec` dictionary - Convert all filter classes to operate against the `RequestSpec` object instead the nested `request_spec` dictionary. - Add developer reference documentation for what the request spec models. Dependencies ============ None. Testing ======= New unit tests for the request spec objects will be added. The existing unit tests of the scheduler filters will be modified to access the `RequestSpec` object in the `filter_properties` dictionary. Documentation Impact ==================== Update any developer reference material that might be referencing the old dictionary accesses. References ========== This blueprint is part of an overall effort to clean up, version, and stabilize the interfaces between the nova-api, nova-scheduler, nova-conductor and nova-compute daemons that involve scheduling and resource decisions. Blueprints involved in this effort include: - `detach-service-from-computenode` - `resource-objects` - `request-spec-object` <-- this blueprint - `sched-select-destinations-use-request-spec-object` - `placement-spec-object` - `condition-objects` - `sched-placement-spec-use-resource-objects` - `sched-placement-spec-use-condition-objects` - `sched-get-placement-claims` ",,288,0
openstack%2Fneutron-specs~master~Ibf20960104660268bb28fd0083ad8dd3b0cd38c8,openstack/neutron-specs,master,Ibf20960104660268bb28fd0083ad8dd3b0cd38c8,Report HA router master,MERGED,2014-10-15 11:36:24.000000000,2014-12-07 23:15:17.000000000,2014-12-07 23:15:16.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 261}, {'_account_id': 333}, {'_account_id': 748}, {'_account_id': 2035}, {'_account_id': 2592}, {'_account_id': 2888}, {'_account_id': 7141}, {'_account_id': 7448}, {'_account_id': 7921}, {'_account_id': 8788}, {'_account_id': 8873}, {'_account_id': 10870}, {'_account_id': 12444}]","[{'number': 1, 'created': '2014-10-15 11:36:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/0750a52879059a0c8b76f898520b98cbc125cb92', 'message': 'Report HA router master\n\nChange-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8\n'}, {'number': 2, 'created': '2014-10-15 11:42:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/21d8a645460cbbc9131caef1652082b1ffa22594', 'message': 'Report HA router master\n\nChange-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8\n'}, {'number': 3, 'created': '2014-10-15 13:01:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/35a3e4651365998f9508e17d0e088d6229e4c269', 'message': 'Report HA router master\n\nChange-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8\n'}, {'number': 4, 'created': '2014-11-11 14:34:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/2b16040ec420e93d88a2def930175836b988705f', 'message': 'Report HA router master\n\nChange-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8\n'}, {'number': 5, 'created': '2014-11-11 14:36:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/6465f324e6acde7d3d8a6b3efa7a0a4722858a76', 'message': 'Report HA router master\n\nChange-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8\n'}, {'number': 6, 'created': '2014-11-11 18:01:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/e7658c8644d05488f05c9a039678b4f10a07187c', 'message': 'Report HA router master\n\nChange-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8\n'}, {'number': 7, 'created': '2014-11-16 12:03:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/8746787caf8034a8f346595bfadb0d3f2e4c19d4', 'message': 'Report HA router master\n\nChange-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8\n'}, {'number': 8, 'created': '2014-11-18 13:18:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/78b5097e9e3fee36dbca050ed6f589d9413d1ff3', 'message': 'Report HA router master\n\nChange-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8\n'}, {'number': 9, 'created': '2014-11-18 13:19:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/97e01b403266aefe75d826acb95c27ec2b17b863', 'message': 'Report HA router master\n\nChange-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8\n'}, {'number': 10, 'created': '2014-11-26 12:21:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/a7cd2525bcd77bb806496b8125a374424ecc1b1e', 'message': 'Report HA router master\n\nChange-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8\n'}, {'number': 11, 'created': '2014-11-26 12:23:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/63c9feae2870be0cf1e019fa91e55acc043922f6', 'message': 'Report HA router master\n\nChange-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8\n'}, {'number': 12, 'created': '2014-11-26 14:04:45.000000000', 'files': ['specs/kilo/report-ha-router-master.rst'], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/ad9d2f67d609dfe0752a62cc649c5485157b6756', 'message': 'Report HA router master\n\nChange-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8\n'}]",72,128613,ad9d2f67d609dfe0752a62cc649c5485157b6756,48,15,12,8873,,,0,"Report HA router master

Change-Id: Ibf20960104660268bb28fd0083ad8dd3b0cd38c8
",git fetch https://review.opendev.org/openstack/neutron-specs refs/changes/13/128613/11 && git format-patch -1 --stdout FETCH_HEAD,['specs/kilo/report-ha-router-master.rst'],1,0750a52879059a0c8b76f898520b98cbc125cb92,bp/report-ha-router-master,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ======================= Report HA Router Master ======================= https://blueprints.launchpad.net/neutron/+spec/report_ha_router_master Highly available routers is a new functionality that was merged in the l3-high-availability blueprint. HA routers are scheduled on multiple L3 agents however the cloud operator has no way of knowing where the active instance is. Problem Description =================== A cloud operator can know what L3 agents are hosting a router, but not where the active instance is. Legacy routers may be manually moved from one agent to another. With HA routers, the equivalent is moving the active instance, but that is not currently possible. The first step is to know where the active instance, which will be addressed in this blueprint, however determing the location of the active instance is out of scope and will be addressed in the future. The operator might want to perform node maintenance which is assisted by manually moving routers from the node. Likewise the operator might want to see the state of routers after a failover (Did the active instance actually failover?). Proposed Change =============== l3-agent-list-hosting-router <router_id> Currently shows all L3 agents hosting the router. It will now also show the HA state (Active or standby) of said router on every agent. :: +-----------+------+----------------+-------+----------+ | id | host | admin_state_up | alive | ha_state | +-----------+------+----------------+-------+----------+ | 534c4b37- | net1 | True | :-) | active | | da2730c6- | net2 | True | :-) | standby | +-----------+------+----------------+-------+----------+ Keepalived doesn't support a way to query the current VRRP state. The only way to know then is to use notifier scripts. These scripts are executed when a state transition occurs, and receive the new state (Master, backup, fault). Every time we reconfigure keepalived (When the router is created or updated) we tell it to execute a Python script (That is maintained as part of the repository). The script will: 1) Write the new state to a file in $state_path/ha_confs/router_id/state 2) Start the metadata proxy if the transition was to master, or shut it down if the transition was to backup or fault. 3) Notify the agent that a transition has occurred via a unix domain socket. The reason that 1 & 2 will happen in the script and not in the agent after it receives the notification is that we want to execute steps 1 & 2 even if the agent is down. The L3 agent will batch these state change notifications over a period of T seconds. When T seconds have passed and no new notifications have arrived it will send a RPC message to the server with a map of router ID to VRRP state on that specific agent. Every time the agent starts it gets a list of routers scheduled on the agent. The agent will now loop through said routers, collect their HA states from disk and update the server. This is to catch any state changes that occured if and when an agent is down. The RPC message send will be retried indefinitely in case the management network is temporarily down, or the agent is disconnected from it. The server will then persist this information following the RPC message: The tables are already set up for this. Each router has an entry in the HA bindings table per agent it is scheduled to, and the record contains the VRRP state on that specific agent. No DB migration will be necessary. Data Model Impact ----------------- The data model already supports the new functionality. The HA state of every router to agent binding is persisted in the L3HARouterAgentPortBinding table. It is currently unused. REST API Impact --------------- l3-agent-list-hosting-router will now return an extra column that can be 'active' or 'standby' or HA routers, or None for other types of routers. Security Impact --------------- None. Notifications Impact -------------------- None. Other End User Impact --------------------- python-neutronclient will support the new ha_state column. It will show 'active' or 'standby' when a proper response is received. '-' will be displayed if None is received by an old server or for non-HA routers. Performance Impact ------------------ Assuming two L3 agents and 1,000 routers hosted on each, a failover from node 1 to node 2 should induce only a single RPC call from node 2 to the server, and a single DB access. Other Deployer Impact --------------------- None. Developer Impact ---------------- None. Community Impact ---------------- This is a natural continuation of the L3 HA work implemented during the Juno cycle. Alternatives ------------ Instead of neutron-keepalived-state-change notifying the agent via a unix domain socket, the agent could poll for the state of all HA routers every T seconds. It would then diff the new states against a cached copy and notify the server of any changes. One could argue that this is simpler to implement and maintain, but is less performant. Implementation ============== Assignee(s) ----------- Primary assignee: Assaf Muller <amuller> Work Items ---------- * Current keepalived notifier bash scripts are generated in-line. These will now be a Python script maintained as part of the repository. The script will be available as neutron-keepalived-state-change and will be invoked by keepalived. * At first the script will replicate the existing behavior of the bash scripts: Write the new state to disk and start up or shut down the metadata proxy. * The script must also notify the agent of the state change via a unix domain socket. * The RPC message that updates HA routers states will be implemented (It currently actually already exists but cannot be used without changing its format). * The agent will batch up state change notifications in to a single RPC message. The Nova notifier mechanism batches notifications and the code will be reused. * The API must expose the new ha_state column. * The L3 agent must report HA states after it receives its routers from the server. Dependencies ============ None. Testing ======= Tempest Tests ------------- L3 HA cannot be tested in Tempest without multi-node support. L3 HA is the first candidate to be tested when in-tree integration tests are introduced via the integration-tests blueprint. Functional Tests ---------------- The L3 agent already has functional testing in place. Two new tests will be added: 1) When a state change occurs, that the notification arrives at the agent. 2) When multiple state changes occur, that the RPC call is sent to the server with the expected parameters. API Tests --------- The RPC and DB methods will be tested with unit tests. Documentation Impact ==================== The changes to the API and CLI require documentation. User Documentation ------------------ The CLI client documentation must be updated. Developer Documentation ----------------------- The Neutron API change must be documented. References ========== * Topic branch: https://review.openstack.org/#/q/branch:master+topic:bug/1365453,n,z * https://blueprints.launchpad.net/neutron/+spec/l3-high-availability * https://bugs.launchpad.net/neutron/+bug/1365453 ",,231,0
openstack%2Fneutron-specs~master~Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba,openstack/neutron-specs,master,Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba,allow setting ip address of floating ip,MERGED,2014-06-16 10:41:51.000000000,2014-12-07 23:14:53.000000000,2014-12-07 23:14:52.000000000,"[{'_account_id': 3}, {'_account_id': 261}, {'_account_id': 333}, {'_account_id': 841}, {'_account_id': 1935}, {'_account_id': 2592}, {'_account_id': 3217}, {'_account_id': 5948}, {'_account_id': 6854}, {'_account_id': 7203}, {'_account_id': 7448}, {'_account_id': 9200}, {'_account_id': 10064}, {'_account_id': 12697}]","[{'number': 1, 'created': '2014-06-16 10:41:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/0735c7fb6d7c9a53691cc1d65ee03c0e202cce7b', 'message': 'change to allow setting ip address of floating ip\n\nblueprint allow-specific-floating-ip-address\n\nChange-Id: Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba\n'}, {'number': 2, 'created': '2014-06-30 07:25:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/ea38485683ccf60bc3d3961bf2d41bf0f15f42c7', 'message': 'change to allow setting ip address of floating ip\n\nblueprint allow-specific-floating-ip-address\n\nChange-Id: Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba\n'}, {'number': 3, 'created': '2014-06-30 23:58:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/dc3557184bbf2993c071f48fa8966016eab50b05', 'message': 'change to allow setting ip address of floating ip\n\nblueprint allow-specific-floating-ip-address\n\nChange-Id: Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba\n'}, {'number': 4, 'created': '2014-07-15 03:58:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/e1e800cbc8e5094a63faa97d3619078c7cd92617', 'message': 'change to allow setting ip address of floating ip\n\nblueprint allow-specific-floating-ip-address\n\nChange-Id: Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba\n'}, {'number': 5, 'created': '2014-07-15 04:01:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/85e118a9c1d0ab970f39423f2d6f277aa2af1661', 'message': 'change to allow setting ip address of floating ip\n\nblueprint allow-specific-floating-ip-address\n\nChange-Id: Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba\n'}, {'number': 6, 'created': '2014-10-07 10:52:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/1288c99528a61e4147394b9402c985c034480012', 'message': 'change to allow setting ip address of floating ip\n\nblueprint allow-specific-floating-ip-address\n\nChange-Id: Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba\n'}, {'number': 7, 'created': '2014-11-13 00:10:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/d5c4c92ffd74d8595e0e2286ad786e1cdd99125e', 'message': 'change to allow setting ip address of floating ip\n\nblueprint allow-specific-floating-ip-address\n\nDocImpact\nAPIImpact\nChange-Id: Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba\n'}, {'number': 8, 'created': '2014-11-13 00:16:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/cb88d628bb2c0ed57f003bdd01472146d38eda29', 'message': 'change to allow setting ip address of floating ip\n\nblueprint allow-specific-floating-ip-address\n\nDocImpact\nAPIImpact\nChange-Id: Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba\n'}, {'number': 9, 'created': '2014-11-26 00:02:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/1214836d6e772ee5618ec009345d12a96fdc22f1', 'message': 'allow setting ip address of floating ip\n\nblueprint allow-specific-floating-ip-address\n\nDocImpact\nAPIImpact\nChange-Id: Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba\n'}, {'number': 10, 'created': '2014-11-26 00:15:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/19ff6f8a6354a58dca8332839872c0800f53ee7e', 'message': 'allow setting ip address of floating ip\n\nblueprint allow-specific-floating-ip-address\n\nDocImpact\nAPIImpact\nChange-Id: Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba\n'}, {'number': 11, 'created': '2014-11-26 11:03:39.000000000', 'files': ['specs/kilo/allow-specific-floating-ip-address.rst'], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/eca6dd91e3297983277ac8dfae81b0ad0eed9f7c', 'message': 'allow setting ip address of floating ip\n\nblueprint allow-specific-floating-ip-address\n\nDocImpact\nAPIImpact\nChange-Id: Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba\n'}]",58,100214,eca6dd91e3297983277ac8dfae81b0ad0eed9f7c,75,14,11,7203,,,0,"allow setting ip address of floating ip

blueprint allow-specific-floating-ip-address

DocImpact
APIImpact
Change-Id: Ie56bba78d4f3b92eb4f3f71f62899a51b19857ba
",git fetch https://review.opendev.org/openstack/neutron-specs refs/changes/14/100214/10 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/allow-specific-floating-ip-address.rst'],1,0735c7fb6d7c9a53691cc1d65ee03c0e202cce7b,bp/allow-specific-floating-ip-address,"================================== Allow specific Floating IP Address ================================== Launchpad blueprint: https://blueprints.launchpad.net/neutron/+spec/allow-specific-floating-ip-address Problem description =================== IP Address of floating ip is auto decided. In some case, user want to specify the IP Address of floating ip. Proposed change =============== Adding the floating ip address as argument to the API for creating floating ip. When network has some subnets, subnet id is needed. Therefore, add subnet id to the API as the same as floating ip address. The parameters are limited by ""policy.json"". In default, non-admin user cannot use the parameters. Admin can use it. (""rule:admin_only"") Alternatives ------------ None Data model impact ----------------- None REST API impact --------------- floating_ip_address and floating_subnet_id will be added to POST /v2.0/floatingips Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- None Performance Impact ------------------ None Other deployer impact --------------------- None Developer impact ---------------- None Implementation ============== Assignee(s) ----------- Primary assignee: fujioka-yuuichi-d Work Items ---------- 1. Add argument to API Dependencies ============ None Testing ======= * all patterns that parameter is passed/not passed. * check policy whether user is admin. Documentation Impact ==================== New arguments are added to API, it should be documented. References ========== None ",,107,0
openstack%2Fpuppet-manila~master~Idcb4d258bd6dc18d95e3bef767df48e8897cf548,openstack/puppet-manila,master,Idcb4d258bd6dc18d95e3bef767df48e8897cf548,Remove enabled_share_backends from manifest,ABANDONED,2014-11-27 07:02:55.000000000,2014-12-07 22:53:22.000000000,,"[{'_account_id': 3}, {'_account_id': 11878}]","[{'number': 1, 'created': '2014-11-27 07:02:55.000000000', 'files': ['manifests/backend/netapp.pp'], 'web_link': 'https://opendev.org/openstack/puppet-manila/commit/006160b0b31fb67d9a8a28d5c4b655f449890a34', 'message': 'Remove enabled_share_backends from manifest\n\nChange-Id: Idcb4d258bd6dc18d95e3bef767df48e8897cf548\n'}]",0,137535,006160b0b31fb67d9a8a28d5c4b655f449890a34,5,2,1,13084,,,0,"Remove enabled_share_backends from manifest

Change-Id: Idcb4d258bd6dc18d95e3bef767df48e8897cf548
",git fetch https://review.opendev.org/openstack/puppet-manila refs/changes/35/137535/1 && git format-patch -1 --stdout FETCH_HEAD,['manifests/backend/netapp.pp'],1,006160b0b31fb67d9a8a28d5c4b655f449890a34,,, 'DEFAULT/enabled_share_backends': value => $share_backend_name;,0,1
openstack%2Fswift~feature%2Fec~I663a9c92dc2132126c1c17396731ade35d4bc050,openstack/swift,feature/ec,I663a9c92dc2132126c1c17396731ade35d4bc050,Merge master to feature/ec,MERGED,2014-12-07 18:19:25.000000000,2014-12-07 20:50:33.000000000,2014-12-07 20:50:31.000000000,"[{'_account_id': 3}, {'_account_id': 7479}, {'_account_id': 13052}]","[{'number': 1, 'created': '2014-12-07 18:19:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/132f8b3169cd0b5ba094736b16fbc75ccc11551e', 'message': 'Merge master to feature/ec\n\nChange-Id: I663a9c92dc2132126c1c17396731ade35d4bc050\n'}]",0,139870,132f8b3169cd0b5ba094736b16fbc75ccc11551e,7,3,1,7479,,,0,"Merge master to feature/ec

Change-Id: I663a9c92dc2132126c1c17396731ade35d4bc050
",git fetch https://review.opendev.org/openstack/swift refs/changes/70/139870/1 && git format-patch -1 --stdout FETCH_HEAD,[],0,132f8b3169cd0b5ba094736b16fbc75ccc11551e,ec_merge,,,0,0
openstack%2Fpython-manilaclient~master~Ib50f0ff10da76e53a9b4545a1007e5425d39fd94,openstack/python-manilaclient,master,Ib50f0ff10da76e53a9b4545a1007e5425d39fd94,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:51:30.000000000,2014-12-07 20:25:37.000000000,2014-12-07 20:25:36.000000000,"[{'_account_id': 3}, {'_account_id': 6491}, {'_account_id': 8851}]","[{'number': 1, 'created': '2014-12-05 03:51:30.000000000', 'files': ['doc/source/index.rst', 'README.rst'], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/8f135629cdd3879f07cae208543d9e8ee82ddb5d', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Ib50f0ff10da76e53a9b4545a1007e5425d39fd94\n'}]",0,139379,8f135629cdd3879f07cae208543d9e8ee82ddb5d,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Ib50f0ff10da76e53a9b4545a1007e5425d39fd94
",git fetch https://review.opendev.org/openstack/python-manilaclient refs/changes/79/139379/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/index.rst', 'README.rst']",2,8f135629cdd3879f07cae208543d9e8ee82ddb5d,infra-manual,.. _Gerrit: http://docs.openstack.org/infra/manual/developers.html#development-workflow,.. _Gerrit: http://wiki.openstack.org/GerritWorkflow,2,2
openstack%2Fneutron~master~I8239e73eb959648178aa94535d15d77a817c89f5,openstack/neutron,master,I8239e73eb959648178aa94535d15d77a817c89f5,Update i18n translation for IBM plugin log msg's,MERGED,2014-11-25 03:18:43.000000000,2014-12-07 20:01:43.000000000,2014-12-07 20:01:42.000000000,"[{'_account_id': 3}, {'_account_id': 704}, {'_account_id': 1653}, {'_account_id': 2592}, {'_account_id': 4656}, {'_account_id': 5170}, {'_account_id': 6072}, {'_account_id': 6854}, {'_account_id': 7183}, {'_account_id': 7448}, {'_account_id': 8124}, {'_account_id': 8645}, {'_account_id': 8873}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10294}, {'_account_id': 10980}, {'_account_id': 11126}, {'_account_id': 12040}, {'_account_id': 13051}]","[{'number': 1, 'created': '2014-11-25 03:18:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9f751d50eae59b46bc29baaa4171e6249bb3c36a', 'message': ""Update i18n translation for IBM plugin log msg's\n\nAll the log messages now have the required hints. In addition to this\ndebug messages are not translated.\n\nThis is done for the plugins/ibm\n\nChange-Id: I8239e73eb959648178aa94535d15d77a817c89f5\n""}, {'number': 2, 'created': '2014-11-25 15:40:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/7823fd852ff0fef95cbd560f9078bcffe0ccc0f7', 'message': ""Update i18n translation for IBM plugin log msg's\n\nAll the log messages now have the required hints. In addition to this\ndebug messages are not translated.\n\nThis is done for the plugins/ibm\n\nChange-Id: I8239e73eb959648178aa94535d15d77a817c89f5\n""}, {'number': 3, 'created': '2014-11-26 02:07:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/424274fa2d377a55a702d53411c1e70e89785516', 'message': ""Update i18n translation for IBM plugin log msg's\n\nAll the log messages now have the required hints. In addition to this\ndebug messages are not translated.\n\nThis is done for the plugins/ibm\n\nChange-Id: I8239e73eb959648178aa94535d15d77a817c89f5\n""}, {'number': 4, 'created': '2014-12-02 13:25:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/29ad7900bde8467459caf121aca298a7417f47ac', 'message': ""Update i18n translation for IBM plugin log msg's\n\nAll the log messages now have the required hints. In addition to this\ndebug messages are not translated.\n\nThis is done for the plugins/ibm\n\nChange-Id: I8239e73eb959648178aa94535d15d77a817c89f5\n""}, {'number': 5, 'created': '2014-12-03 07:37:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e5b3e6027f652411d0e0f40936019c87c3cd3640', 'message': ""Update i18n translation for IBM plugin log msg's\n\nAll the log messages now have the required hints. In addition to this\ndebug messages are not translated.\n\nThis is done for the plugins/ibm\n\nChange-Id: I8239e73eb959648178aa94535d15d77a817c89f5\n""}, {'number': 6, 'created': '2014-12-05 06:01:07.000000000', 'files': ['neutron/hacking/checks.py', 'neutron/plugins/ibm/sdnve_api_fake.py', 'neutron/plugins/ibm/agent/sdnve_neutron_agent.py', 'neutron/plugins/ibm/sdnve_api.py', 'neutron/plugins/ibm/sdnve_neutron_plugin.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/f266d474b769cd204e7ee3c5f464dabe6ff795d1', 'message': ""Update i18n translation for IBM plugin log msg's\n\nAll the log messages now have the required hints. In addition to this\ndebug messages are not translated.\n\nThis is done for the plugins/ibm\n\nChange-Id: I8239e73eb959648178aa94535d15d77a817c89f5\n""}]",2,136955,f266d474b769cd204e7ee3c5f464dabe6ff795d1,140,31,6,7183,,,0,"Update i18n translation for IBM plugin log msg's

All the log messages now have the required hints. In addition to this
debug messages are not translated.

This is done for the plugins/ibm

Change-Id: I8239e73eb959648178aa94535d15d77a817c89f5
",git fetch https://review.opendev.org/openstack/neutron refs/changes/55/136955/4 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/hacking/checks.py', 'neutron/plugins/ibm/sdnve_api_fake.py', 'neutron/plugins/ibm/agent/sdnve_neutron_agent.py', 'neutron/plugins/ibm/sdnve_api.py', 'neutron/plugins/ibm/sdnve_neutron_plugin.py']",5,9f751d50eae59b46bc29baaa4171e6249bb3c36a,debug-ibm,"from neutron.openstack.common.gettextutils import _LE, _LI, _LW LOG.info(_LI(""Set a new controller if needed."")) LOG.info(_LI(""Set the controller to a new controller: %s""), LOG.debug(""Create network in progress: %r"", network) LOG.debug(""Created network: %s"", net['id']) LOG.debug(""Update network in progress: %r"", network) LOG.debug(""Delete network in progress: %s"", id) _LE(""Delete net failed after deleting the network in DB: %s""), LOG.debug(""Get network in progress: %s"", id) LOG.debug(""Get networks in progress"") LOG.debug(""Create port in progress: %r"", port) LOG.debug(""Create port does not have tenant id info"") ""Create port does not have tenant id info; "" ""obtained is: %s"", LOG.debug(""Created port: %s"", port.get('id', 'id not found')) LOG.debug(""Update port in progress: %r"", port) LOG.debug(""Delete port in progress: %s"", id) _LE(""Delete port operation failed in SDN-VE "" ""after deleting the port from DB: %s""), res) LOG.debug(""Create subnet in progress: %r"", subnet) LOG.debug(""Subnet created: %s"", new_subnet['id']) LOG.debug(""Update subnet in progress: %r"", subnet) LOG.debug(""Delete subnet in progress: %s"", id) LOG.error(_LE(""Delete subnet operation failed in SDN-VE after "" ""deleting the subnet from DB: %s""), res) LOG.debug(""Create router in progress: %r"", router) LOG.warning(_LW('Ignoring admin_state_up=False for router=%r. ' 'Overriding with True'), router) LOG.debug(""Router created: %r"", new_router) LOG.debug(""Update router in progress: id=%(id)s "" ""router=%(router)r"", LOG.debug(""Delete router in progress: %s"", id) _LE(""Delete router operation failed in SDN-VE after "" ""deleting the router in DB: %s""), res) LOG.debug(""Add router interface in progress: "" ""router_id=%(router_id)s "" ""interface_info=%(interface_info)r"", ""SdnvePluginV2.add_router_interface called. Port info: %s"", LOG.debug(""Added router interface: %r"", new_interface) LOG.debug(""Add router interface only called: "" ""router_id=%(router_id)s "" ""interface_info=%(interface_info)r"", LOG.error(_LE(""SdnvePluginV2._add_router_interface_only: "" ""failed to add the interface in the roll back."" "" of a remove_router_interface operation"")) LOG.debug(""Remove router interface in progress: "" ""router_id=%(router_id)s "" ""interface_info=%(interface_info)r"", LOG.debug(""SdnvePluginV2.remove_router_interface port: %s"", ""SdnvePluginV2.remove_router_interface subnet_id: %s"", LOG.debug(""Create floatingip in progress: %r"", LOG.debug(""Created floatingip : %r"", new_floatingip) LOG.debug(""Update floatingip in progress: %r"", floatingip) LOG.debug(""Delete floatingip in progress: %s"", id) LOG.error(_LE(""Delete floatingip failed in SDN-VE: %s""), res)"," LOG.info(_(""Set a new controller if needed."")) LOG.info(_(""Set the controller to a new controller: %s""), LOG.debug(_(""Create network in progress: %r""), network) LOG.debug(_(""Created network: %s""), net['id']) LOG.debug(_(""Update network in progress: %r""), network) LOG.debug(_(""Delete network in progress: %s""), id) _(""Delete net failed after deleting the network in DB: %s""), LOG.debug(_(""Get network in progress: %s""), id) LOG.debug(_(""Get networks in progress"")) LOG.debug(_(""Create port in progress: %r""), port) LOG.debug(_(""Create port does not have tenant id info"")) _(""Create port does not have tenant id info; "" ""obtained is: %s""), LOG.debug(_(""Created port: %s""), port.get('id', 'id not found')) LOG.debug(_(""Update port in progress: %r""), port) LOG.debug(_(""Delete port in progress: %s""), id) _(""Delete port operation failed in SDN-VE "" ""after deleting the port from DB: %s""), res) LOG.debug(_(""Create subnet in progress: %r""), subnet) LOG.debug(_(""Subnet created: %s""), new_subnet['id']) LOG.debug(_(""Update subnet in progress: %r""), subnet) LOG.debug(_(""Delete subnet in progress: %s""), id) LOG.error(_(""Delete subnet operation failed in SDN-VE after "" ""deleting the subnet from DB: %s""), res) LOG.debug(_(""Create router in progress: %r""), router) LOG.warning(_('Ignoring admin_state_up=False for router=%r. ' 'Overriding with True'), router) LOG.debug(_(""Router created: %r""), new_router) LOG.debug(_(""Update router in progress: id=%(id)s "" ""router=%(router)r""), LOG.debug(_(""Delete router in progress: %s""), id) _(""Delete router operation failed in SDN-VE after "" ""deleting the router in DB: %s""), res) LOG.debug(_(""Add router interface in progress: "" ""router_id=%(router_id)s "" ""interface_info=%(interface_info)r""), _(""SdnvePluginV2.add_router_interface called. Port info: %s""), LOG.debug(_(""Added router interface: %r""), new_interface) LOG.debug(_(""Add router interface only called: "" ""router_id=%(router_id)s "" ""interface_info=%(interface_info)r""), LOG.error(_(""SdnvePluginV2._add_router_interface_only: "" ""failed to add the interface in the roll back."" "" of a remove_router_interface operation"")) LOG.debug(_(""Remove router interface in progress: "" ""router_id=%(router_id)s "" ""interface_info=%(interface_info)r""), LOG.debug(_(""SdnvePluginV2.remove_router_interface port: %s""), _(""SdnvePluginV2.remove_router_interface subnet_id: %s""), LOG.debug(_(""Create floatingip in progress: %r""), LOG.debug(_(""Created floatingip : %r""), new_floatingip) LOG.debug(_(""Update floatingip in progress: %r""), floatingip) LOG.debug(_(""Delete floatingip in progress: %s""), id) LOG.error(_(""Delete floatingip failed in SDN-VE: %s""), res)",103,98
openstack%2Fneutron~master~I485eb4dde0b109bb19b17c6376ead45dec06ae99,openstack/neutron,master,I485eb4dde0b109bb19b17c6376ead45dec06ae99,Imported Translations from Transifex,MERGED,2014-12-06 06:06:27.000000000,2014-12-07 19:56:54.000000000,2014-12-07 19:56:53.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 2592}, {'_account_id': 5170}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10980}, {'_account_id': 12040}, {'_account_id': 13051}, {'_account_id': 14208}, {'_account_id': 14212}, {'_account_id': 14214}]","[{'number': 1, 'created': '2014-12-06 06:06:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/cfb57fd35d101e9990b3ca7d27723d53949a3932', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I485eb4dde0b109bb19b17c6376ead45dec06ae99\n'}, {'number': 2, 'created': '2014-12-07 06:07:49.000000000', 'files': ['neutron/locale/neutron-log-error.pot', 'neutron/locale/pt_BR/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/ja/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/de/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/zh_CN/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/es/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/neutron-log-warning.pot', 'neutron/locale/fr/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/neutron-log-info.pot', 'neutron/locale/neutron.pot', 'neutron/locale/ko_KR/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/zh_TW/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/it/LC_MESSAGES/neutron-log-info.po'], 'web_link': 'https://opendev.org/openstack/neutron/commit/c2c257188af9cad09165bcdf27aab173d378d119', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I485eb4dde0b109bb19b17c6376ead45dec06ae99\n'}]",0,139785,c2c257188af9cad09165bcdf27aab173d378d119,48,22,2,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: I485eb4dde0b109bb19b17c6376ead45dec06ae99
",git fetch https://review.opendev.org/openstack/neutron refs/changes/85/139785/2 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/locale/neutron-log-error.pot', 'neutron/locale/pt_BR/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/ja/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/de/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/zh_CN/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/es/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/neutron-log-warning.pot', 'neutron/locale/fr/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/neutron-log-info.pot', 'neutron/locale/neutron.pot', 'neutron/locale/ko_KR/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/zh_TW/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/it/LC_MESSAGES/neutron-log-info.po']",13,cfb57fd35d101e9990b3ca7d27723d53949a3932,transifex/translations,"""POT-Creation-Date: 2014-12-06 06:05+0000\n"" ""PO-Revision-Date: 2014-12-05 22:51+0000\n""#: neutron/plugins/nec/agent/nec_neutron_agent.py:141#: neutron/plugins/ml2/db.py:61#: neutron/plugins/nec/nec_plugin.py:279 #, python-format msgid ""deactivate_port(): OFC port for port=%s is already removed."" msgstr """" #: neutron/plugins/nec/nec_router.py:334 #, python-format msgid ""Enabled router drivers: %s"" msgstr """" #: neutron/plugins/nec/agent/nec_neutron_agent.py:53 #, python-format msgid ""Update ports: added=%(added)s, removed=%(removed)s"" msgstr ""Aggiornamento porte: aggiunta=%(added)s, rimossa=%(removed)s"" #: neutron/plugins/nec/common/ofc_client.py:108 #, python-format msgid ""Specified resource %s does not exist on OFC "" msgstr """" #: neutron/plugins/nec/common/ofc_client.py:140 #, python-format msgid ""Waiting for %s seconds due to OFC Service_Unavailable."" msgstr """" #: neutron/plugins/nec/drivers/__init__.py:37 #, python-format msgid ""Loading OFC driver: %s"" msgstr ""Caricamento driver OFC: %s"" ","""POT-Creation-Date: 2014-12-05 06:09+0000\n"" ""PO-Revision-Date: 2014-12-05 00:25+0000\n""#: neutron/plugins/ml2/db.py:63",477,411
openstack%2Fcinder~master~Icb6522852d427dea3fbd1ec4b4cc94a184d3669d,openstack/cinder,master,Icb6522852d427dea3fbd1ec4b4cc94a184d3669d,Allow HostState to handle empty capabilities,MERGED,2014-12-05 10:19:02.000000000,2014-12-07 19:44:08.000000000,2014-12-07 19:44:07.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 6491}, {'_account_id': 8247}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-05 10:19:02.000000000', 'files': ['cinder/tests/scheduler/test_host_manager.py', 'cinder/scheduler/host_manager.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/6ac5054ceb398b55a03d311ff868e5c93a65dcbb', 'message': 'Allow HostState to handle empty capabilities\n\nIn some cases, where the capabilities reported from driver is\nemtpy, scheduler would raise exception thus unable to correctly\nhandle scheduling request even when other backend candidates are\nable to fulfill the request.  This change does some modifications\nto HostState so that it is able to handle empty capabilities.\n\nChange-Id: Icb6522852d427dea3fbd1ec4b4cc94a184d3669d\nFixes-bug: #1398875\n'}]",0,139584,6ac5054ceb398b55a03d311ff868e5c93a65dcbb,15,11,1,2759,,,0,"Allow HostState to handle empty capabilities

In some cases, where the capabilities reported from driver is
emtpy, scheduler would raise exception thus unable to correctly
handle scheduling request even when other backend candidates are
able to fulfill the request.  This change does some modifications
to HostState so that it is able to handle empty capabilities.

Change-Id: Icb6522852d427dea3fbd1ec4b4cc94a184d3669d
Fixes-bug: #1398875
",git fetch https://review.opendev.org/openstack/cinder refs/changes/84/139584/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/scheduler/test_host_manager.py', 'cinder/scheduler/host_manager.py']",2,6ac5054ceb398b55a03d311ff868e5c93a65dcbb,bug/1398875," self.total_capacity_gb = capability.get('total_capacity_gb', 0) self.free_capacity_gb = capability.get('free_capacity_gb', 0) self.reserved_percentage = capability.get('reserved_percentage', 0)", self.total_capacity_gb = capability['total_capacity_gb'] self.free_capacity_gb = capability['free_capacity_gb'] self.reserved_percentage = capability['reserved_percentage'],17,3
openstack%2Fcinder~master~Idb06b37c803d94bb33b0df9d19ced8b330def2ad,openstack/cinder,master,Idb06b37c803d94bb33b0df9d19ced8b330def2ad,Improve unit tests for cinder/volume/utils.py,MERGED,2014-12-04 15:51:36.000000000,2014-12-07 19:39:29.000000000,2014-12-07 19:39:28.000000000,"[{'_account_id': 3}, {'_account_id': 1736}, {'_account_id': 2243}, {'_account_id': 6491}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}]","[{'number': 1, 'created': '2014-12-04 15:51:36.000000000', 'files': ['cinder/tests/test_volume_utils.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/5e10c4e58e70e8187bef5ef1274d97571646b1a1', 'message': 'Improve unit tests for cinder/volume/utils.py\n\nThis commit increases the unit test coverage of cinder/volume/utils.py.\nIt also replaces tests using mox or stubout.\n\nChange-Id: Idb06b37c803d94bb33b0df9d19ced8b330def2ad\n'}]",0,139099,5e10c4e58e70e8187bef5ef1274d97571646b1a1,14,10,1,7219,,,0,"Improve unit tests for cinder/volume/utils.py

This commit increases the unit test coverage of cinder/volume/utils.py.
It also replaces tests using mox or stubout.

Change-Id: Idb06b37c803d94bb33b0df9d19ced8b330def2ad
",git fetch https://review.opendev.org/openstack/cinder refs/changes/99/139099/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/tests/test_volume_utils.py'],1,5e10c4e58e70e8187bef5ef1274d97571646b1a1,test_vol_utils,"class NotifyUsageTestCase(test.TestCase): @mock.patch('cinder.volume.utils._usage_from_volume') @mock.patch('cinder.volume.utils.CONF') @mock.patch('cinder.volume.utils.rpc') def test_notify_about_volume_usage(self, mock_rpc, mock_conf, mock_usage): mock_conf.host = 'host1' output = volume_utils.notify_about_volume_usage(mock.sentinel.context, mock.sentinel.volume, 'test_suffix') self.assertIsNone(output) mock_usage.assert_called_once_with(mock.sentinel.context, mock.sentinel.volume) mock_rpc.get_notifier.assert_called_once_with('volume', 'host1') mock_rpc.get_notifier.return_value.info.assert_called_once_with( mock.sentinel.context, 'volume.test_suffix', mock_usage.return_value) @mock.patch('cinder.volume.utils._usage_from_volume') @mock.patch('cinder.volume.utils.CONF') @mock.patch('cinder.volume.utils.rpc') def test_notify_about_volume_usage_with_kwargs(self, mock_rpc, mock_conf, mock_usage): mock_conf.host = 'host1' output = volume_utils.notify_about_volume_usage( mock.sentinel.context, mock.sentinel.volume, 'test_suffix', extra_usage_info={'a': 'b', 'c': 'd'}, host='host2') self.assertIsNone(output) mock_usage.assert_called_once_with(mock.sentinel.context, mock.sentinel.volume, a='b', c='d') mock_rpc.get_notifier.assert_called_once_with('volume', 'host2') mock_rpc.get_notifier.return_value.info.assert_called_once_with( mock.sentinel.context, 'volume.test_suffix', mock_usage.return_value) @mock.patch('cinder.volume.utils._usage_from_volume') @mock.patch('cinder.volume.utils.CONF') @mock.patch('cinder.volume.utils.rpc') def test_notify_about_replication_usage(self, mock_rpc, mock_conf, mock_usage): mock_conf.host = 'host1' output = volume_utils.notify_about_replication_usage( mock.sentinel.context, mock.sentinel.volume, 'test_suffix') self.assertIsNone(output) mock_usage.assert_called_once_with(mock.sentinel.context, mock.sentinel.volume) mock_rpc.get_notifier.assert_called_once_with('replication', 'host1') mock_rpc.get_notifier.return_value.info.assert_called_once_with( mock.sentinel.context, 'replication.test_suffix', mock_usage.return_value) @mock.patch('cinder.volume.utils._usage_from_volume') @mock.patch('cinder.volume.utils.CONF') @mock.patch('cinder.volume.utils.rpc') def test_notify_about_replication_usage_with_kwargs(self, mock_rpc, mock_conf, mock_usage): mock_conf.host = 'host1' output = volume_utils.notify_about_replication_usage( mock.sentinel.context, mock.sentinel.volume, 'test_suffix', extra_usage_info={'a': 'b', 'c': 'd'}, host='host2') self.assertIsNone(output) mock_usage.assert_called_once_with(mock.sentinel.context, mock.sentinel.volume, a='b', c='d') mock_rpc.get_notifier.assert_called_once_with('replication', 'host2') mock_rpc.get_notifier.return_value.info.assert_called_once_with( mock.sentinel.context, 'replication.test_suffix', mock_usage.return_value) @mock.patch('cinder.volume.utils._usage_from_volume') @mock.patch('cinder.volume.utils.CONF') @mock.patch('cinder.volume.utils.rpc') def test_notify_about_replication_error(self, mock_rpc, mock_conf, mock_usage): mock_conf.host = 'host1' output = volume_utils.notify_about_replication_error( mock.sentinel.context, mock.sentinel.volume, 'test_suffix') self.assertIsNone(output) mock_usage.assert_called_once_with(mock.sentinel.context, mock.sentinel.volume) mock_rpc.get_notifier.assert_called_once_with('replication', 'host1') mock_rpc.get_notifier.return_value.error.assert_called_once_with( mock.sentinel.context, 'replication.test_suffix', mock_usage.return_value) @mock.patch('cinder.volume.utils._usage_from_volume') @mock.patch('cinder.volume.utils.CONF') @mock.patch('cinder.volume.utils.rpc') def test_notify_about_replication_error_with_kwargs(self, mock_rpc, mock_conf, mock_usage): mock_conf.host = 'host1' output = volume_utils.notify_about_replication_error( mock.sentinel.context, mock.sentinel.volume, 'test_suffix', extra_error_info={'a': 'b', 'c': 'd'}, host='host2') self.assertIsNone(output) mock_usage.assert_called_once_with(mock.sentinel.context, mock.sentinel.volume, a='b', c='d') mock_rpc.get_notifier.assert_called_once_with('replication', 'host2') mock_rpc.get_notifier.return_value.error.assert_called_once_with( mock.sentinel.context, 'replication.test_suffix', mock_usage.return_value) @mock.patch('cinder.volume.utils._usage_from_snapshot') @mock.patch('cinder.volume.utils.CONF') @mock.patch('cinder.volume.utils.rpc') def test_notify_about_snapshot_usage(self, mock_rpc, mock_conf, mock_usage): mock_conf.host = 'host1' output = volume_utils.notify_about_snapshot_usage( mock.sentinel.context, mock.sentinel.snapshot, 'test_suffix') self.assertIsNone(output) mock_usage.assert_called_once_with(mock.sentinel.context, mock.sentinel.snapshot) mock_rpc.get_notifier.assert_called_once_with('snapshot', 'host1') mock_rpc.get_notifier.return_value.info.assert_called_once_with( mock.sentinel.context, 'snapshot.test_suffix', mock_usage.return_value) @mock.patch('cinder.volume.utils._usage_from_snapshot') @mock.patch('cinder.volume.utils.CONF') @mock.patch('cinder.volume.utils.rpc') def test_notify_about_snapshot_usage_with_kwargs(self, mock_rpc, mock_conf, mock_usage): mock_conf.host = 'host1' output = volume_utils.notify_about_snapshot_usage( mock.sentinel.context, mock.sentinel.snapshot, 'test_suffix', extra_usage_info={'a': 'b', 'c': 'd'}, host='host2') self.assertIsNone(output) mock_usage.assert_called_once_with(mock.sentinel.context, mock.sentinel.snapshot, a='b', c='d') mock_rpc.get_notifier.assert_called_once_with('snapshot', 'host2') mock_rpc.get_notifier.return_value.info.assert_called_once_with( mock.sentinel.context, 'snapshot.test_suffix', mock_usage.return_value) @mock.patch('cinder.volume.utils._usage_from_consistencygroup') @mock.patch('cinder.volume.utils.CONF') @mock.patch('cinder.volume.utils.rpc') def test_notify_about_consistencygroup_usage(self, mock_rpc, mock_conf, mock_usage): mock_conf.host = 'host1' output = volume_utils.notify_about_consistencygroup_usage( mock.sentinel.context, mock.sentinel.consistencygroup, 'test_suffix') self.assertIsNone(output) mock_usage.assert_called_once_with(mock.sentinel.context, mock.sentinel.consistencygroup) mock_rpc.get_notifier.assert_called_once_with('consistencygroup', 'host1') mock_rpc.get_notifier.return_value.info.assert_called_once_with( mock.sentinel.context, 'consistencygroup.test_suffix', mock_usage.return_value) @mock.patch('cinder.volume.utils._usage_from_consistencygroup') @mock.patch('cinder.volume.utils.CONF') @mock.patch('cinder.volume.utils.rpc') def test_notify_about_consistencygroup_usage_with_kwargs(self, mock_rpc, mock_conf, mock_usage): mock_conf.host = 'host1' output = volume_utils.notify_about_consistencygroup_usage( mock.sentinel.context, mock.sentinel.consistencygroup, 'test_suffix', extra_usage_info={'a': 'b', 'c': 'd'}, host='host2') self.assertIsNone(output) mock_usage.assert_called_once_with(mock.sentinel.context, mock.sentinel.consistencygroup, a='b', c='d') mock_rpc.get_notifier.assert_called_once_with('consistencygroup', 'host2') mock_rpc.get_notifier.return_value.info.assert_called_once_with( mock.sentinel.context, 'consistencygroup.test_suffix', mock_usage.return_value) @mock.patch('cinder.volume.utils._usage_from_cgsnapshot') @mock.patch('cinder.volume.utils.CONF') @mock.patch('cinder.volume.utils.rpc') def test_notify_about_cgsnapshot_usage(self, mock_rpc, mock_conf, mock_usage): mock_conf.host = 'host1' output = volume_utils.notify_about_cgsnapshot_usage( mock.sentinel.context, mock.sentinel.cgsnapshot, 'test_suffix') self.assertIsNone(output) mock_usage.assert_called_once_with(mock.sentinel.context, mock.sentinel.cgsnapshot) mock_rpc.get_notifier.assert_called_once_with('cgsnapshot', 'host1') mock_rpc.get_notifier.return_value.info.assert_called_once_with( mock.sentinel.context, 'cgsnapshot.test_suffix', mock_usage.return_value) @mock.patch('cinder.volume.utils._usage_from_cgsnapshot') @mock.patch('cinder.volume.utils.CONF') @mock.patch('cinder.volume.utils.rpc') def test_notify_about_cgsnapshot_usage_with_kwargs(self, mock_rpc, mock_conf, mock_usage): mock_conf.host = 'host1' output = volume_utils.notify_about_cgsnapshot_usage( mock.sentinel.context, mock.sentinel.cgsnapshot, 'test_suffix', extra_usage_info={'a': 'b', 'c': 'd'}, host='host2') self.assertIsNone(output) mock_usage.assert_called_once_with(mock.sentinel.context, mock.sentinel.cgsnapshot, a='b', c='d') mock_rpc.get_notifier.assert_called_once_with('cgsnapshot', 'host2') mock_rpc.get_notifier.return_value.info.assert_called_once_with( mock.sentinel.context, 'cgsnapshot.test_suffix', mock_usage.return_value)class OdirectSupportTestCase(test.TestCase): @mock.patch('cinder.utils.execute') def test_check_for_odirect_support(self, mock_exec): output = volume_utils.check_for_odirect_support('/dev/abc', '/dev/def') self.assertTrue(output) mock_exec.assert_called_once_with('dd', 'count=0', 'if=/dev/abc', 'of=/dev/def', 'oflag=direct', run_as_root=True) mock_exec.reset_mock() output = volume_utils.check_for_odirect_support('/dev/abc', '/dev/def', 'iflag=direct') self.assertTrue(output) mock_exec.assert_called_once_with('dd', 'count=0', 'if=/dev/abc', 'of=/dev/def', 'iflag=direct', run_as_root=True) @mock.patch('cinder.utils.execute', side_effect=processutils.ProcessExecutionError) def test_check_for_odirect_support_error(self, mock_exec): output = volume_utils.check_for_odirect_support('/dev/abc', '/dev/def') self.assertFalse(output) mock_exec.assert_called_once_with('dd', 'count=0', 'if=/dev/abc', 'of=/dev/def', 'oflag=direct', run_as_root=True) class ClearVolumeTestCase(test.TestCase): @mock.patch('cinder.volume.utils.copy_volume', return_value=None) @mock.patch('cinder.volume.utils.CONF') def test_clear_volume_conf(self, mock_conf, mock_copy): mock_conf.volume_clear = 'zero' mock_conf.volume_clear_size = 0 mock_conf.volume_dd_blocksize = '1M' mock_conf.volume_clear_ionice = '-c3' output = volume_utils.clear_volume(1024, 'volume_path') self.assertIsNone(output) mock_copy.assert_called_once_with('/dev/zero', 'volume_path', 1024, '1M', sync=True, execute=utils.execute, ionice='-c3') @mock.patch('cinder.volume.utils.copy_volume', return_value=None) @mock.patch('cinder.volume.utils.CONF') def test_clear_volume_args(self, mock_conf, mock_copy): mock_conf.volume_clear = 'shred' mock_conf.volume_clear_size = 0 mock_conf.volume_dd_blocksize = '1M' mock_conf.volume_clear_ionice = '-c3' output = volume_utils.clear_volume(1024, 'volume_path', 'zero', 1, '-c0') self.assertIsNone(output) mock_copy.assert_called_once_with('/dev/zero', 'volume_path', 1, '1M', sync=True, execute=utils.execute, ionice='-c0') @mock.patch('cinder.utils.execute') @mock.patch('cinder.volume.utils.CONF') def test_clear_volume_shred(self, mock_conf, mock_exec): mock_conf.volume_clear = 'shred' mock_conf.volume_clear_size = 1 mock_conf.volume_clear_ionice = None output = volume_utils.clear_volume(1024, 'volume_path') self.assertIsNone(output) mock_exec.assert_called_once_with( 'shred', '-n3', '-s1MiB', ""volume_path"", run_as_root=True) @mock.patch('cinder.utils.execute') @mock.patch('cinder.volume.utils.CONF') def test_clear_volume_shred_not_clear_size(self, mock_conf, mock_exec): mock_conf.volume_clear = 'shred' mock_conf.volume_clear_size = None mock_conf.volume_clear_ionice = None output = volume_utils.clear_volume(1024, 'volume_path') self.assertIsNone(output) mock_exec.assert_called_once_with( 'shred', '-n3', ""volume_path"", run_as_root=True) @mock.patch('cinder.volume.utils.CONF') def test_clear_volume_invalid_opt(self, mock_conf): mock_conf.volume_clear = 'non_existent_volume_clearer' mock_conf.volume_clear_size = 0 mock_conf.volume_clear_ionice = None @mock.patch('cinder.volume.utils.setup_blkio_cgroup', return_value=['cg_cmd']) @mock.patch('cinder.volume.utils._calculate_count', return_value=(1234, 5678)) @mock.patch('cinder.volume.utils.check_for_odirect_support', return_value=True) @mock.patch('cinder.utils.execute') @mock.patch('cinder.volume.utils.CONF') def test_copy_volume_dd_iflag_and_oflag(self, mock_conf, mock_exec, mock_support, mock_count, mock_cg): mock_conf.volume_copy_bps_limit = 10 output = volume_utils.copy_volume('/dev/zero', '/dev/null', 1024, 1, sync=True, execute=utils.execute, ionice=None) self.assertIsNone(output) mock_exec.assert_called_once_with('cg_cmd', 'dd', 'if=/dev/zero', 'of=/dev/null', 'count=5678', 'bs=1234', 'iflag=direct', 'oflag=direct', run_as_root=True) mock_exec.reset_mock() output = volume_utils.copy_volume('/dev/zero', '/dev/null', 1024, 1, sync=False, execute=utils.execute, ionice=None) self.assertIsNone(output) mock_exec.assert_called_once_with('cg_cmd', 'dd', 'if=/dev/zero', 'of=/dev/null', 'count=5678', 'bs=1234', 'iflag=direct', 'oflag=direct', run_as_root=True) @mock.patch('cinder.volume.utils.setup_blkio_cgroup', return_value=['cg_cmd']) @mock.patch('cinder.volume.utils._calculate_count', return_value=(1234, 5678)) @mock.patch('cinder.volume.utils.check_for_odirect_support', return_value=False) @mock.patch('cinder.utils.execute') @mock.patch('cinder.volume.utils.CONF') def test_copy_volume_dd_no_iflag_or_oflag(self, mock_conf, mock_exec, mock_support, mock_count, mock_cg): mock_conf.volume_copy_bps_limit = 10 output = volume_utils.copy_volume('/dev/zero', '/dev/null', 1024, 1, sync=True, execute=utils.execute, ionice=None) self.assertIsNone(output) mock_exec.assert_called_once_with('cg_cmd', 'dd', 'if=/dev/zero', 'of=/dev/null', 'count=5678', 'bs=1234', 'conv=fdatasync', run_as_root=True) mock_exec.reset_mock() output = volume_utils.copy_volume('/dev/zero', '/dev/null', 1024, 1, sync=False, execute=utils.execute, ionice=None) self.assertIsNone(output) mock_exec.assert_called_once_with('cg_cmd', 'dd', 'if=/dev/zero', 'of=/dev/null', 'count=5678', 'bs=1234', run_as_root=True) @mock.patch('cinder.volume.utils.setup_blkio_cgroup', return_value=None) @mock.patch('cinder.volume.utils._calculate_count', return_value=(1234, 5678)) @mock.patch('cinder.volume.utils.check_for_odirect_support', return_value=False) @mock.patch('cinder.utils.execute') @mock.patch('cinder.volume.utils.CONF') def test_copy_volume_dd_no_cgroup(self, mock_conf, mock_exec, mock_support, mock_count, mock_cg): mock_conf.volume_copy_bps_limit = 10 output = volume_utils.copy_volume('/dev/zero', '/dev/null', 1024, 1, sync=True, execute=utils.execute, ionice=None) self.assertIsNone(output) mock_exec.assert_called_once_with('dd', 'if=/dev/zero', 'of=/dev/null', 'count=5678', 'bs=1234', 'conv=fdatasync', run_as_root=True) @mock.patch('cinder.volume.utils.setup_blkio_cgroup', return_value=None) @mock.patch('cinder.volume.utils._calculate_count', return_value=(1234, 5678)) @mock.patch('cinder.volume.utils.check_for_odirect_support', return_value=False) @mock.patch('cinder.utils.execute') @mock.patch('cinder.volume.utils.CONF') def test_copy_volume_dd_with_ionice(self, mock_conf, mock_exec, mock_support, mock_count, mock_cg): mock_conf.volume_copy_bps_limit = 10 output = volume_utils.copy_volume('/dev/zero', '/dev/null', 1024, 1, sync=True, execute=utils.execute, ionice='-c3') self.assertIsNone(output) mock_exec.assert_called_once_with('ionice', '-c3', 'dd', 'if=/dev/zero', 'of=/dev/null', 'count=5678', 'bs=1234', 'conv=fdatasync', run_as_root=True) def test_bps_limit_zero(self): mock_exec = mock.Mock() output = volume_utils.setup_blkio_cgroup('src', 'dst', 0, execute=mock_exec) self.assertIsNone(output) self.assertFalse(mock_exec.called) @mock.patch('cinder.utils.get_blkdev_major_minor', side_effect=exception.Error) def test_get_blkdev_error(self, mock_get_blkdev): mock_exec = mock.Mock() output = volume_utils.setup_blkio_cgroup('src', 'dst', 1, execute=mock_exec) self.assertIsNone(output) mock_get_blkdev.assert_has_calls([mock.call('src'), mock.call('dst')]) self.assertFalse(mock_exec.called) @mock.patch('cinder.utils.get_blkdev_major_minor', side_effect=lambda x: x) @mock.patch('cinder.volume.utils.CONF') def test_cgcreate_fail(self, mock_conf, mock_get_blkdev): mock_conf.volume_copy_blkio_cgroup_name = 'test_group' mock_exec = mock.Mock() mock_exec.side_effect = processutils.ProcessExecutionError output = volume_utils.setup_blkio_cgroup('src', 'dst', 1, execute=mock_exec) self.assertIsNone(output) mock_get_blkdev.assert_has_calls([mock.call('src'), mock.call('dst')]) mock_exec.assert_called_once_with('cgcreate', '-g', 'blkio:test_group', run_as_root=True) @mock.patch('cinder.utils.get_blkdev_major_minor', side_effect=lambda x: x) @mock.patch('cinder.volume.utils.CONF') def test_cgset_fail(self, mock_conf, mock_get_blkdev): mock_conf.volume_copy_blkio_cgroup_name = 'test_group' mock_exec = mock.Mock() def cgset_exception(*args, **kwargs): if 'cgset' in args: raise processutils.ProcessExecutionError mock_exec.side_effect = cgset_exception output = volume_utils.setup_blkio_cgroup('src', 'dst', 1, execute=mock_exec) self.assertIsNone(output) mock_get_blkdev.assert_has_calls([mock.call('src'), mock.call('dst')]) mock_exec.assert_has_calls([ mock.call('cgcreate', '-g', 'blkio:test_group', run_as_root=True), mock.call('cgset', '-r', 'blkio.throttle.read_bps_device=src 1', 'test_group', run_as_root=True)]) @mock.patch('cinder.utils.get_blkdev_major_minor', side_effect=lambda x: x) @mock.patch('cinder.volume.utils.CONF') def test_setup_blkio_cgroup(self, mock_conf, mock_get_blkdev): mock_conf.volume_copy_blkio_cgroup_name = 'test_group' mock_exec = mock.Mock() output = volume_utils.setup_blkio_cgroup('src', 'dst', 1, execute=mock_exec) self.assertEqual(['cgexec', '-g', 'blkio:test_group'], output) mock_get_blkdev.assert_has_calls([mock.call('src'), mock.call('dst')]) mock_exec.assert_has_calls([ mock.call('cgcreate', '-g', 'blkio:test_group', run_as_root=True), mock.call('cgset', '-r', 'blkio.throttle.read_bps_device=src 1', 'test_group', run_as_root=True), mock.call('cgset', '-r', 'blkio.throttle.write_bps_device=dst 1', 'test_group', run_as_root=True)]) def test_null_safe_str(self): self.assertEqual('', volume_utils.null_safe_str(None)) self.assertEqual('', volume_utils.null_safe_str(False)) self.assertEqual('', volume_utils.null_safe_str(0)) self.assertEqual('', volume_utils.null_safe_str([])) self.assertEqual('', volume_utils.null_safe_str(())) self.assertEqual('', volume_utils.null_safe_str({})) self.assertEqual('', volume_utils.null_safe_str(set())) self.assertEqual('a', volume_utils.null_safe_str('a')) self.assertEqual('1', volume_utils.null_safe_str(1)) self.assertEqual('True', volume_utils.null_safe_str(True)) @mock.patch('cinder.utils.get_root_helper') @mock.patch('cinder.brick.local_dev.lvm.LVM.supports_thin_provisioning') def test_supports_thin_provisioning(self, mock_supports_thin, mock_helper): self.assertEqual(mock_supports_thin.return_value, volume_utils.supports_thin_provisioning()) mock_helper.assert_called_once_with() @mock.patch('cinder.utils.get_root_helper') @mock.patch('cinder.brick.local_dev.lvm.LVM.get_all_physical_volumes') def test_get_all_physical_volumes(self, mock_get_vols, mock_helper): self.assertEqual(mock_get_vols.return_value, volume_utils.get_all_physical_volumes()) mock_helper.assert_called_once_with() @mock.patch('cinder.utils.get_root_helper') @mock.patch('cinder.brick.local_dev.lvm.LVM.get_all_volume_groups') def test_get_all_volume_groups(self, mock_get_groups, mock_helper): self.assertEqual(mock_get_groups.return_value, volume_utils.get_all_volume_groups()) mock_helper.assert_called_once_with() self.assertTrue(any(c for c in password if c in '23456789')) self.assertTrue(any(c for c in password if c in 'abcdefghijkmnopqrstuvwxyz')) self.assertTrue(any(c for c in password if c in 'ABCDEFGHJKLMNPQRSTUVWXYZ')) self.assertEqual(16, len(password)) self.assertEqual(10, len(volume_utils.generate_password(10))) @mock.patch('cinder.volume.utils.generate_password') def test_generate_username(self, mock_gen_pass): output = volume_utils.generate_username() self.assertEqual(mock_gen_pass.return_value, output)","import os import re from oslo.utils import importutils from cinder import context from cinder import dbfrom cinder.tests import fake_notifierclass UsageInfoTestCase(test.TestCase): QUEUE_NAME = 'cinder-volume' HOSTNAME = 'my-host.com' HOSTIP = '10.0.0.1' BACKEND = 'test_backend' MULTI_AT_BACKEND = 'test_b@ckend' def setUp(self): super(UsageInfoTestCase, self).setUp() self.addCleanup(fake_notifier.reset) self.flags(host='fake', notification_driver=[""test""]) self.volume = importutils.import_object(CONF.volume_manager) self.user_id = 'fake' self.project_id = 'fake' self.snapshot_id = 'fake' self.volume_size = 0 self.context = context.RequestContext(self.user_id, self.project_id) def _create_volume(self, params=None): """"""Create a test volume."""""" params = params or {} vol = {} vol['snapshot_id'] = self.snapshot_id vol['user_id'] = self.user_id vol['project_id'] = self.project_id vol['host'] = CONF.host vol['availability_zone'] = CONF.storage_availability_zone vol['status'] = ""creating"" vol['attach_status'] = ""detached"" vol['size'] = self.volume_size vol.update(params) return db.volume_create(self.context, vol)['id']class ClearVolumeTestCase(test.TestCase): def test_clear_volume(self): CONF.volume_clear = 'zero' CONF.volume_clear_size = 0 CONF.volume_dd_blocksize = '1M' CONF.volume_clear_ionice = None self.mox.StubOutWithMock(volume_utils, 'copy_volume') volume_utils.copy_volume(""/dev/zero"", ""volume_path"", 1024, CONF.volume_dd_blocksize, sync=True, ionice=None, execute=utils.execute) self.mox.ReplayAll() volume_utils.clear_volume(1024, ""volume_path"") def test_clear_volume_zero(self): CONF.volume_clear = 'zero' CONF.volume_clear_size = 1 CONF.volume_clear_ionice = None self.mox.StubOutWithMock(volume_utils, 'copy_volume') volume_utils.copy_volume(""/dev/zero"", ""volume_path"", 1, CONF.volume_dd_blocksize, sync=True, ionice=None, execute=utils.execute) self.mox.ReplayAll() volume_utils.clear_volume(1024, ""volume_path"") def test_clear_volume_ionice(self): CONF.volume_clear = 'zero' CONF.volume_clear_size = 0 CONF.volume_dd_blocksize = '1M' CONF.volume_clear_ionice = '-c3' self.mox.StubOutWithMock(volume_utils, 'copy_volume') volume_utils.copy_volume(""/dev/zero"", ""volume_path"", 1024, CONF.volume_dd_blocksize, sync=True, ionice=CONF.volume_clear_ionice, execute=utils.execute) self.mox.ReplayAll() volume_utils.clear_volume(1024, ""volume_path"") def test_clear_volume_zero_ionice(self): CONF.volume_clear = 'zero' CONF.volume_clear_size = 1 CONF.volume_clear_ionice = '-c3' self.mox.StubOutWithMock(volume_utils, 'copy_volume') volume_utils.copy_volume(""/dev/zero"", ""volume_path"", 1, CONF.volume_dd_blocksize, sync=True, ionice=CONF.volume_clear_ionice, execute=utils.execute) self.mox.ReplayAll() volume_utils.clear_volume(1024, ""volume_path"") def test_clear_volume_shred(self): CONF.volume_clear = 'shred' CONF.volume_clear_size = 1 clear_cmd = ['shred', '-n3', '-s1MiB', ""volume_path""] self.mox.StubOutWithMock(utils, ""execute"") utils.execute(*clear_cmd, run_as_root=True) self.mox.ReplayAll() volume_utils.clear_volume(1024, ""volume_path"") def test_clear_volume_shred_not_clear_size(self): CONF.volume_clear = 'shred' CONF.volume_clear_size = None clear_cmd = ['shred', '-n3', ""volume_path""] self.mox.StubOutWithMock(utils, ""execute"") utils.execute(*clear_cmd, run_as_root=True) self.mox.ReplayAll() volume_utils.clear_volume(1024, ""volume_path"") def test_clear_volume_invalid_opt(self): CONF.volume_clear = 'non_existent_volume_clearer' CONF.volume_clear_size = 0 self.mox.StubOutWithMock(volume_utils, 'copy_volume') self.mox.ReplayAll() def test_clear_volume_lvm_snap(self): self.stubs.Set(os.path, 'exists', lambda x: True) CONF.volume_clear = 'zero' CONF.volume_clear_size = 0 uuid = '00000000-0000-0000-0000-90ed32cdeed3' name = 'snapshot-' + uuid mangle_name = '_' + re.sub(r'-', r'--', name) vol_path = '/dev/mapper/cinder--volumes-%s-cow' % mangle_name def fake_copy_volume(srcstr, deststr, size, blocksize, **kwargs): self.assertEqual(deststr, vol_path) return True self.stubs.Set(volume_utils, 'copy_volume', fake_copy_volume) volume_utils.clear_volume(123, vol_path) def test_copy_volume_dd_iflag_and_oflag(self): def fake_utils_execute(*cmd, **kwargs): if 'if=/dev/zero' in cmd and 'iflag=direct' in cmd: raise processutils.ProcessExecutionError() if 'of=/dev/null' in cmd and 'oflag=direct' in cmd: raise processutils.ProcessExecutionError() if 'iflag=direct' in cmd and 'oflag=direct' in cmd: raise exception.InvalidInput(message='iflag/oflag error') def fake_check_odirect(src, dest, flags='blah'): return False self.stubs.Set(volume_utils, 'check_for_odirect_support', fake_check_odirect) volume_utils.copy_volume('/dev/zero', '/dev/null', 1024, CONF.volume_dd_blocksize, sync=True, ionice=None, execute=fake_utils_execute) @mock.patch.object(utils, 'get_blkdev_major_minor') def test_setup_blkio_cgroup(self, mock_major_minor): def fake_get_blkdev_major_minor(path): return {'src_volume': ""253:0"", 'dst_volume': ""253:1""}[path] mock_major_minor.side_effect = fake_get_blkdev_major_minor self.exec_cnt = 0 def fake_utils_execute(*cmd, **kwargs): exec_cmds = [('cgcreate', '-g', 'blkio:' + CONF.volume_copy_blkio_cgroup_name), ('cgset', '-r', 'blkio.throttle.read_bps_device=253:0 1024', CONF.volume_copy_blkio_cgroup_name), ('cgset', '-r', 'blkio.throttle.write_bps_device=253:1 1024', CONF.volume_copy_blkio_cgroup_name)] self.assertEqual(exec_cmds[self.exec_cnt], cmd) self.exec_cnt += 1 cmd = volume_utils.setup_blkio_cgroup('src_volume', 'dst_volume', 1024, execute=fake_utils_execute) self.assertEqual(['cgexec', '-g', 'blkio:' + CONF.volume_copy_blkio_cgroup_name], cmd) self.assertTrue([c for c in password if c in '0123456789']) self.assertTrue([c for c in password if c in 'abcdefghijklmnopqrstuvwxyz']) self.assertTrue([c for c in password if c in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'])",523,166
openstack%2Fheat~master~I3050d7c88196d2426c7e4821b1e504d2f67ecedd,openstack/heat,master,I3050d7c88196d2426c7e4821b1e504d2f67ecedd,Updated from global requirements,MERGED,2014-12-04 21:28:01.000000000,2014-12-07 19:37:10.000000000,2014-12-07 19:37:08.000000000,"[{'_account_id': 3}, {'_account_id': 4571}]","[{'number': 1, 'created': '2014-12-04 21:28:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/2a8799b78b5b125682de9a73163bfb020bbabe1a', 'message': 'Updated from global requirements\n\nChange-Id: I3050d7c88196d2426c7e4821b1e504d2f67ecedd\n'}, {'number': 2, 'created': '2014-12-04 22:36:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/fa88beb9f50f3fd3be0c55e532e979f0c7e821dc', 'message': 'Updated from global requirements\n\nChange-Id: I3050d7c88196d2426c7e4821b1e504d2f67ecedd\n'}, {'number': 3, 'created': '2014-12-05 16:33:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/8d574ca1bc0eedcaa602debbf58667c9b4e054ac', 'message': 'Updated from global requirements\n\nChange-Id: I3050d7c88196d2426c7e4821b1e504d2f67ecedd\n'}, {'number': 4, 'created': '2014-12-06 00:00:01.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/heat/commit/0442b77d5321a61b6b3d95937659e55975dbd036', 'message': 'Updated from global requirements\n\nChange-Id: I3050d7c88196d2426c7e4821b1e504d2f67ecedd\n'}]",0,139205,0442b77d5321a61b6b3d95937659e55975dbd036,16,2,4,11131,,,0,"Updated from global requirements

Change-Id: I3050d7c88196d2426c7e4821b1e504d2f67ecedd
",git fetch https://review.opendev.org/openstack/heat refs/changes/05/139205/4 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,2a8799b78b5b125682de9a73163bfb020bbabe1a,openstack/requirements,python-saharaclient>=0.7.6,python-saharaclient>=0.7.5,1,1
openstack%2Fceilometer~master~Id60e6a0391a1159f558a7eda06a649786e72299b,openstack/ceilometer,master,Id60e6a0391a1159f558a7eda06a649786e72299b,Update error messages in event.storage.base,ABANDONED,2014-11-06 01:35:26.000000000,2014-12-07 19:31:42.000000000,,"[{'_account_id': 3}, {'_account_id': 6537}, {'_account_id': 6676}, {'_account_id': 10987}, {'_account_id': 13189}]","[{'number': 1, 'created': '2014-11-06 01:35:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/7b0de0f5bbd43ae6f644444504d7ea33ffc870f1', 'message': ""Update error messages in event.storage.base\n\nWe raise NotImplementedError('Events not implemented.') when event,\nevent_type, trait and trait_type related methods are not implemented,\nthat is not every precise.\n\nThis patch updates those error messages to be more precise.\n\nChange-Id: Id60e6a0391a1159f558a7eda06a649786e72299b\nCloses-Bug: #1389921\n""}, {'number': 2, 'created': '2014-11-06 08:54:51.000000000', 'files': ['ceilometer/event/storage/base.py'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/9bb3dff21c5ddf761f24515f50a530456b450b22', 'message': ""Update error messages in event.storage.base\n\nWe raise NotImplementedError('Events not implemented.') when event,\nevent_type, trait and trait_type related methods are not implemented,\nthat is not very precise.\n\nThis patch updates those error messages to be more precise.\n\nChange-Id: Id60e6a0391a1159f558a7eda06a649786e72299b\nCloses-Bug: #1389921\n""}]",6,132932,9bb3dff21c5ddf761f24515f50a530456b450b22,14,5,2,6676,,,0,"Update error messages in event.storage.base

We raise NotImplementedError('Events not implemented.') when event,
event_type, trait and trait_type related methods are not implemented,
that is not very precise.

This patch updates those error messages to be more precise.

Change-Id: Id60e6a0391a1159f558a7eda06a649786e72299b
Closes-Bug: #1389921
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/32/132932/2 && git format-patch -1 --stdout FETCH_HEAD,['ceilometer/event/storage/base.py'],1,7b0de0f5bbd43ae6f644444504d7ea33ffc870f1,bug/1389921, raise ceilometer.NotImplementedError('Event types not implemented.') raise ceilometer.NotImplementedError('Trait types not implemented.') raise ceilometer.NotImplementedError('Traits not implemented.'), raise ceilometer.NotImplementedError('Events not implemented.') raise ceilometer.NotImplementedError('Events not implemented.') raise ceilometer.NotImplementedError('Events not implemented.'),3,3
openstack%2Frally~master~I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0,openstack/rally,master,I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0,Add scenario for booting vm with security groups,MERGED,2014-10-17 14:37:12.000000000,2014-12-07 19:11:10.000000000,2014-12-07 19:11:09.000000000,"[{'_account_id': 3}, {'_account_id': 6172}, {'_account_id': 7369}, {'_account_id': 8507}, {'_account_id': 9545}, {'_account_id': 9601}, {'_account_id': 14135}]","[{'number': 1, 'created': '2014-10-17 14:37:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/23b32729571e6d79f1ce5f8f284e7b3b2d0c8427', 'message': 'Do not merge: functional test for rally verify --set image\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 2, 'created': '2014-10-17 14:39:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d1a5378d26630ec65f443ab63f92efd7957d4803', 'message': 'Do not merge: functional test for rally verify --set image\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 3, 'created': '2014-11-26 15:48:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/aabb14f635ca1171f442e1ef901cf6270942a600', 'message': 'DO NOT MERGE: try to debug secgroups in env with neutron\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 4, 'created': '2014-11-26 15:52:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/b5ccdb9198bf3af653e59663314e81e23b0d36a8', 'message': 'DO NOT MERGE: try to debug secgroups in env with neutron\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 5, 'created': '2014-12-01 14:35:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/d42db059a2cf9523d98322fb03d61ad64580a3b1', 'message': 'DO NOT MERGE: try to boot vm with secgroups; env: neutron\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 6, 'created': '2014-12-01 15:04:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/9684930d9fe15b1593be54a29604d4aba051e3da', 'message': 'DO NOT MERGE: try to boot vm with secgroups; env: neutron\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 7, 'created': '2014-12-01 15:23:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ae02918c9610dab2f58fbf2957de180d3963785d', 'message': 'DO NOT MERGE: try to boot vm with secgroups; env: neutron\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 8, 'created': '2014-12-01 17:17:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/3228e010619f3acf4db8530c111fb05fede792f1', 'message': 'Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 9, 'created': '2014-12-01 17:46:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/421c0197c81c5634eed44315f852b034b7e25829', 'message': 'Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 10, 'created': '2014-12-01 19:31:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/198734370334d319436d432bd5f900741ae6bb74', 'message': 'DO NOT MERGE: Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 11, 'created': '2014-12-01 19:56:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0573a4975c172cfc69f4c01b9507f808da9708b8', 'message': 'DO NOT MERGE: Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 12, 'created': '2014-12-01 20:47:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/e4b49c50c8c3607ebf1a5b7cd8368f8b5a8e99c3', 'message': 'DO NOT MERGE: Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 13, 'created': '2014-12-01 21:17:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/416cd0a26241f4e46516e5736e7d5400ca99ae1c', 'message': 'DO NOT MERGE: Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 14, 'created': '2014-12-01 22:02:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/f78ec6b57d412bc93fa2140d609db1ace34565e3', 'message': 'DO NOT MERGE: Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 15, 'created': '2014-12-01 22:36:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a590b0e7ccd9aef04ecd32ed1f62e1521388a8b9', 'message': 'DO NOT MERGE: Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 16, 'created': '2014-12-02 08:49:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/17b8c01cc6c1120ebee4455749b69f62fb7a2836', 'message': 'DO NOT MERGE: Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 17, 'created': '2014-12-02 09:27:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/5a9cd67e690f0d0d4f51c08468d8df4da0743bc8', 'message': 'DO NOT MERGE: Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 18, 'created': '2014-12-02 10:30:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/0966b2d1b98a1e3bcb0c1f4c4effdf20a8721108', 'message': 'DO NOT MERGE: Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 19, 'created': '2014-12-02 11:26:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/267fc6f4b76f0bbd0f5200127b143ec4fdf1ca7b', 'message': 'DO NOT MERGE: Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 20, 'created': '2014-12-02 11:40:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/40eb33c42b562315538efd40731802d5ecffe8c9', 'message': 'DO NOT MERGE: Add new scenario for nova security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 21, 'created': '2014-12-02 13:38:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/2d7fff3631038fd0675575f49b7d0c7a2fc3621f', 'message': 'Add scenario for booting vm with security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 22, 'created': '2014-12-02 13:44:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/4d2216acd920b6f7b79494e320b8fe569490bb8c', 'message': 'Add scenario for booting vm with security groups\n\n- boot-and-delete-server-with-secgroups\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 23, 'created': '2014-12-02 23:30:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/be56a821bc64a1139f94337fe8fceb43c9f7dfe6', 'message': 'Add scenario for booting vm with security groups\n\nName of new scenario:\n  boot-and-delete-server-with-secgroups\nIts plan:\n  - [neutron-specific]: create network with 1 subnet(required to boot vm\n    with security groups)\n  - create N security groups with M rules per group\n  - boot a VM with created security groups\n  - get list of attached security groups to server\n  - delete server\n  - delete all security groups\n  - check that all groups were attached to server\n  - [neutron-specific]: delete created network\n\nAlso, action ""nova.create_%s_rules_in_every_of_%s_security_group""\nwas renamed to ""nova.create_%s_rules"", because original name is too long\nand doesn\'t clearly display the number of created rules.\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 24, 'created': '2014-12-03 13:46:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/fd02a3c054a5faca5ced62705064d2b54456d42d', 'message': 'Add scenario for booting vm with security groups\n\nName of new scenario:\n  boot-and-delete-server-with-secgroups\nIts plan:\n  - create N security groups with M rules per group\n  - [neutron-specific]: create network with 1 subnet(required to boot vm\n    with security groups)\n  - create N security groups with M rules per group\n  - boot a VM with created security groups\n  - get list of attached security groups to server\n  - delete server\n  - delete all security groups\n  - [neutron-specific]: delete created network\n  - check that all groups were attached to server\n\nAlso, action ""nova.create_%s_rules_in_every_of_%s_security_group""\nwas renamed to ""nova.create_%s_rules"", because original name is too long\nand doesn\'t clearly display the number of created rules.\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 25, 'created': '2014-12-03 14:46:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a20be1da8e2b7338a94b071ab7e4d89a782aa60d', 'message': 'Add scenario for booting vm with security groups\n\nName of new scenario:\n  boot-and-delete-server-with-secgroups\nIts plan:\n  - create N security groups with M rules per group\n  - [neutron-specific]: create network with 1 subnet(required to boot vm\n    with security groups)\n  - create N security groups with M rules per group\n  - boot a VM with created security groups\n  - get list of attached security groups to server\n  - delete server\n  - delete all security groups\n  - [neutron-specific]: delete created network\n  - check that all groups were attached to server\n\nAlso, action ""nova.create_%s_rules_in_every_of_%s_security_group""\nwas renamed to ""nova.create_%s_rules"", because original name is too long\nand doesn\'t clearly display the number of created rules.\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 26, 'created': '2014-12-03 15:08:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/a8e2ddfcd4d0d2471f50ca3a61bee2322eb61795', 'message': 'Add scenario for booting vm with security groups\n\nName of new scenario:\n  boot-and-delete-server-with-secgroups\nIts plan:\n  - create N security groups with M rules per group\n  - [neutron-specific]: create network with 1 subnet(required to boot vm\n    with security groups)\n  - create N security groups with M rules per group\n  - boot a VM with created security groups\n  - get list of attached security groups to server\n  - delete server\n  - delete all security groups\n  - [neutron-specific]: delete created network\n  - check that all groups were attached to server\n\nAlso, action ""nova.create_%s_rules_in_every_of_%s_security_group""\nwas renamed to ""nova.create_%s_rules"", because original name is too long\nand doesn\'t clearly display the number of created rules.\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 27, 'created': '2014-12-05 11:30:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/c2b41c694fbb8356e1848812f9001694da04be0f', 'message': 'Add scenario for booting vm with security groups\n\nName of new scenario:\n  boot-and-delete-server-with-secgroups\nIts plan:\n  - create N security groups with M rules per group\n  - [neutron-specific]: create network with 1 subnet(required to boot vm\n    with security groups)\n  - boot a VM with created security groups\n  - get list of attached security groups to server\n  - delete server\n  - delete all security groups\n  - [neutron-specific]: delete created network\n  - check that all groups were attached to server\n\nAlso, action ""nova.create_%s_rules_in_every_of_%s_security_group""\nwas renamed to ""nova.create_%s_rules"", because original name is too long\nand doesn\'t clearly display the number of created rules.\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}, {'number': 28, 'created': '2014-12-05 15:33:54.000000000', 'files': ['tests/functional/test_cli_info.py', 'rally/benchmark/scenarios/nova/security_group.py', 'rally/benchmark/scenarios/nova/utils.py', 'tests/unit/benchmark/scenarios/nova/test_security_group.py', 'doc/samples/tasks/scenarios/nova/boot-and-delete-server-with-secgroups.json', 'doc/samples/tasks/scenarios/nova/boot-and-delete-server-with-secgroups.yaml', 'rally-jobs/rally-neutron.yaml', 'tests/unit/benchmark/scenarios/nova/test_utils.py', 'rally-jobs/rally.yaml'], 'web_link': 'https://opendev.org/openstack/rally/commit/492181c8220089e4f7fecd2a5844ca64dec4802e', 'message': 'Add scenario for booting vm with security groups\n\nName of new scenario:\n  boot-and-delete-server-with-secgroups\nIts plan:\n  - create N security groups with M rules per group\n  - [neutron-specific]: create network with 1 subnet(required to boot vm\n    with security groups)\n  - boot a VM with created security groups\n  - get list of attached security groups to server\n  - delete server\n  - delete all security groups\n  - [neutron-specific]: delete created network\n  - check that all groups were attached to server\n\nAlso, action ""nova.create_%s_rules_in_every_of_%s_security_group""\nwas renamed to ""nova.create_%s_rules"", because original name is too long\nand doesn\'t clearly display the number of created rules.\n\nChange-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0\n'}]",18,129282,492181c8220089e4f7fecd2a5844ca64dec4802e,123,7,28,9545,,,0,"Add scenario for booting vm with security groups

Name of new scenario:
  boot-and-delete-server-with-secgroups
Its plan:
  - create N security groups with M rules per group
  - [neutron-specific]: create network with 1 subnet(required to boot vm
    with security groups)
  - boot a VM with created security groups
  - get list of attached security groups to server
  - delete server
  - delete all security groups
  - [neutron-specific]: delete created network
  - check that all groups were attached to server

Also, action ""nova.create_%s_rules_in_every_of_%s_security_group""
was renamed to ""nova.create_%s_rules"", because original name is too long
and doesn't clearly display the number of created rules.

Change-Id: I0c4d30ebf03195a5bcd68e87a6ba1d7756fc7fc0
",git fetch https://review.opendev.org/openstack/rally refs/changes/82/129282/18 && git format-patch -1 --stdout FETCH_HEAD,['tests/functional/test_cli_verify.py'],1,23b32729571e6d79f1ce5f8f284e7b3b2d0c8427,secgroups,"# Copyright 2014: Mirantis Inc. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import unittest from tests.functional import utils class CliUtilsTestCase(unittest.TestCase): def setUp(self): super(CliUtilsTestCase, self).setUp() self.rally = utils.Rally() def test_image_set(self): raise KeyError(self.rally(""verify start --set image"")) ",,28,0
openstack%2Fmonasca-ui~master~Ic75e8bf29689a2aff751f5fdb340f74cba5a88b4,openstack/monasca-ui,master,Ic75e8bf29689a2aff751f5fdb340f74cba5a88b4,Ensure that dimensions do not extend beyond field,MERGED,2014-12-04 20:09:41.000000000,2014-12-07 19:03:50.000000000,2014-12-07 19:03:50.000000000,"[{'_account_id': 3}, {'_account_id': 6230}]","[{'number': 1, 'created': '2014-12-04 20:09:41.000000000', 'files': ['monitoring/static/monitoring/js/controllers.js', 'monitoring/static/monitoring/js/app.js', 'monitoring/alarmdefs/templates/alarmdefs/expression_field.html'], 'web_link': 'https://opendev.org/openstack/monasca-ui/commit/e2d290fb23eb59bdd2fcd7a71daa905b297b770f', 'message': 'Ensure that dimensions do not extend beyond field\n\nChange-Id: Ic75e8bf29689a2aff751f5fdb340f74cba5a88b4\n'}]",0,139173,e2d290fb23eb59bdd2fcd7a71daa905b297b770f,10,2,1,6230,,,0,"Ensure that dimensions do not extend beyond field

Change-Id: Ic75e8bf29689a2aff751f5fdb340f74cba5a88b4
",git fetch https://review.opendev.org/openstack/monasca-ui refs/changes/73/139173/1 && git format-patch -1 --stdout FETCH_HEAD,"['monitoring/static/monitoring/js/controllers.js', 'monitoring/static/monitoring/js/app.js', 'monitoring/alarmdefs/templates/alarmdefs/expression_field.html']",3,e2d290fb23eb59bdd2fcd7a71daa905b297b770f,," <td ng-repeat=""dim in dimnames"" style=""white-space:normal"">{$metric[dim] | spacedim $}</td>"," <td ng-repeat=""dim in dimnames"">{$metric[dim]$}</td>",12,2
openstack%2Ftraining-guides~master~I7a8a3877aa56aae5ab2e7b9f5ea278ea22d8043b,openstack/training-guides,master,I7a8a3877aa56aae5ab2e7b9f5ea278ea22d8043b,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:52:36.000000000,2014-12-07 18:31:08.000000000,2014-12-07 18:31:07.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 7007}]","[{'number': 1, 'created': '2014-12-05 03:52:36.000000000', 'files': ['doc/training-guides/sources/cinder/development.environment.xml', 'doc/training-guides/locale/training-guides.pot', 'doc/training-guides/common/editing-code.xml', 'doc/training-guides/under-contruction-notice.xml', 'specs/README.rst', 'doc/training-guides/bk_preface.xml', 'doc/upstream-training/12-howtocontribute.rst', 'specs/juno/developer-training-guide-how-to-participate-classroom.rst', 'labs/HACKING.rst', 'doc/training-guides/developer-guide/ch_developer-apis-day-two-lab.xml'], 'web_link': 'https://opendev.org/openstack/training-guides/commit/05792a6c4d8a89549ff4529b2ba7fd2ccaba0c96', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I7a8a3877aa56aae5ab2e7b9f5ea278ea22d8043b\n'}]",0,139397,05792a6c4d8a89549ff4529b2ba7fd2ccaba0c96,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I7a8a3877aa56aae5ab2e7b9f5ea278ea22d8043b
",git fetch https://review.opendev.org/openstack/training-guides refs/changes/97/139397/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/training-guides/sources/cinder/development.environment.xml', 'doc/training-guides/common/editing-code.xml', 'doc/training-guides/locale/training-guides.pot', 'doc/training-guides/under-contruction-notice.xml', 'specs/README.rst', 'doc/training-guides/bk_preface.xml', 'doc/upstream-training/12-howtocontribute.rst', 'specs/juno/developer-training-guide-how-to-participate-classroom.rst', 'labs/HACKING.rst', 'doc/training-guides/developer-guide/ch_developer-apis-day-two-lab.xml']",10,05792a6c4d8a89549ff4529b2ba7fd2ccaba0c96,infra-manual," <link xlink:href=""http://docs.openstack.org/infra/manual/developers.html#development-workflow"">Gerrit Basics</link>"," <link xlink:href=""https://wiki.openstack.org/wiki/Gerrit_Workflow"">Gerrit Basics</link>",18,18
openstack%2Fdevstack~master~I0b073359fda8b4531cae7b8946eb52a561c82857,openstack/devstack,master,I0b073359fda8b4531cae7b8946eb52a561c82857,"libvirt plugin ""parallels"" support",MERGED,2014-11-26 11:39:04.000000000,2014-12-07 18:17:47.000000000,2014-12-07 18:17:46.000000000,"[{'_account_id': 1}, {'_account_id': 3}, {'_account_id': 866}, {'_account_id': 970}, {'_account_id': 1653}, {'_account_id': 2750}, {'_account_id': 5196}, {'_account_id': 7118}, {'_account_id': 10068}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-26 11:39:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/6d5f3dc5418be217f8ef82de3da36357d926d211', 'message': 'libvirt plugin ""parallels"" requires extra customization of nova.conf and glance-api.conf\n\nWe\'re in the process of establishing the CI testing with\nParallels Cloud Server plugin for libvirt.\nCurrently we use local clone of devstack in our infrastructure,\nbut we want to switch to upstream\n\nnova.conf should have ""images_type"" and ""connection_uri""\ndefined, and glance-api.conf to have ""ploop"" in disk formats\n\nChange-Id: I0b073359fda8b4531cae7b8946eb52a561c82857\n'}, {'number': 2, 'created': '2014-11-26 11:42:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/bb9046f480b0bd7257d1f66f8e368bb869cbf076', 'message': 'libvirt plugin ""parallels"" support\n\nWe\'re in the process of establishing the CI testing with\nParallels Cloud Server plugin for libvirt.\nCurrently we use local clone of devstack in our infrastructure,\nbut we want to switch to upstream\n\nrequires extra customization of nova.conf and glance-api.conf:\nnova.conf should have ""images_type"" and ""connection_uri""\ndefined, and glance-api.conf to have ""ploop"" in disk formats\n\nChange-Id: I0b073359fda8b4531cae7b8946eb52a561c82857\n'}, {'number': 3, 'created': '2014-11-26 15:06:54.000000000', 'files': ['lib/glance', 'lib/nova_plugins/hypervisor-libvirt'], 'web_link': 'https://opendev.org/openstack/devstack/commit/19354585e16513f5ee590c90620b7fae603b6a78', 'message': 'libvirt plugin ""parallels"" support\n\nWe\'re in the process of establishing the CI testing with\nParallels Cloud Server plugin for libvirt.\nCurrently we use local clone of devstack in our infrastructure,\nbut we want to switch to upstream\n\nrequires extra customization of nova.conf and glance-api.conf:\nnova.conf should have ""images_type"" and ""connection_uri""\ndefined, and glance-api.conf to have ""ploop"" in disk formats\n\nImplements: blueprint pcs-support\nChange-Id: I0b073359fda8b4531cae7b8946eb52a561c82857\n'}]",0,137337,19354585e16513f5ee590c90620b7fae603b6a78,13,10,3,13431,,,0,"libvirt plugin ""parallels"" support

We're in the process of establishing the CI testing with
Parallels Cloud Server plugin for libvirt.
Currently we use local clone of devstack in our infrastructure,
but we want to switch to upstream

requires extra customization of nova.conf and glance-api.conf:
nova.conf should have ""images_type"" and ""connection_uri""
defined, and glance-api.conf to have ""ploop"" in disk formats

Implements: blueprint pcs-support
Change-Id: I0b073359fda8b4531cae7b8946eb52a561c82857
",git fetch https://review.opendev.org/openstack/devstack refs/changes/37/137337/1 && git format-patch -1 --stdout FETCH_HEAD,"['lib/glance', 'lib/nova_plugins/hypervisor-libvirt']",2,6d5f3dc5418be217f8ef82de3da36357d926d211,bp/pcs-support," if [[ ""$LIBVIRT_TYPE"" = ""parallels"" ]]; then iniset $NOVA_CONF libvirt connection_uri ""parallels+unix:///system"" iniset $NOVA_CONF libvirt images_type ""ploop"" fi",,8,0
openstack%2Fdevstack~master~Ibcceabf2c76aaeeb8902a670557cc0093943a3e4,openstack/devstack,master,Ibcceabf2c76aaeeb8902a670557cc0093943a3e4,Explicitly pass in SLAPPASS when setting up LDAP,MERGED,2014-09-29 04:00:48.000000000,2014-12-07 18:17:39.000000000,2014-12-07 18:17:38.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 2750}, {'_account_id': 4978}, {'_account_id': 7715}, {'_account_id': 9009}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-29 04:00:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/fa6acadda9099258f1789d10cbfc61fafbace2c8', 'message': 'Fix typo in SLAPPASS variable when setting up LDAP\n\nThe substitution function expects it to be uppercase.\n\nChange-Id: Ibcceabf2c76aaeeb8902a670557cc0093943a3e4\nPartial-Bug: #1373750\n'}, {'number': 2, 'created': '2014-11-21 15:10:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/39d96c64b9ff7357fbacd2158594b793b0dc2074', 'message': 'Explicitly pass in SLAPPASS when setting up LDAP\n\nChange-Id: Ibcceabf2c76aaeeb8902a670557cc0093943a3e4\nCloses-Bug: #1373750\n'}, {'number': 3, 'created': '2014-11-26 14:37:39.000000000', 'files': ['lib/ldap'], 'web_link': 'https://opendev.org/openstack/devstack/commit/a3d60c80d9f39ee6a0410dcdd5c778d0b7511c9d', 'message': 'Explicitly pass in SLAPPASS when setting up LDAP\n\nChange-Id: Ibcceabf2c76aaeeb8902a670557cc0093943a3e4\nCloses-Bug: #1373750\n'}]",2,124658,a3d60c80d9f39ee6a0410dcdd5c778d0b7511c9d,21,7,3,4978,,,0,"Explicitly pass in SLAPPASS when setting up LDAP

Change-Id: Ibcceabf2c76aaeeb8902a670557cc0093943a3e4
Closes-Bug: #1373750
",git fetch https://review.opendev.org/openstack/devstack refs/changes/58/124658/1 && git format-patch -1 --stdout FETCH_HEAD,['lib/ldap'],1,fa6acadda9099258f1789d10cbfc61fafbace2c8,bug/1373750," local SLAPPASS=$(slappasswd -s $LDAP_PASSWORD) printf ""LDAP secret is $SLAPPASS\n"""," local slappass=$(slappasswd -s $LDAP_PASSWORD) printf ""LDAP secret is $slappass\n""",2,2
openstack%2Fdevstack~master~I20501fec140998b91c9ddfd84b7b10168624430a,openstack/devstack,master,I20501fec140998b91c9ddfd84b7b10168624430a,remove nova baremetal driver support,MERGED,2014-12-05 13:31:47.000000000,2014-12-07 18:17:30.000000000,2014-12-07 18:17:29.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 1420}, {'_account_id': 2750}, {'_account_id': 9008}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-12-05 13:31:47.000000000', 'files': ['clean.sh', 'doc/source/index.rst', 'lib/neutron', 'lib/nova', 'lib/baremetal', 'lib/nova_plugins/hypervisor-baremetal', 'unstack.sh', 'stackrc', 'stack.sh'], 'web_link': 'https://opendev.org/openstack/devstack/commit/2f8e08b5728f4272b415b1c0aab8ff62eae29b06', 'message': 'remove nova baremetal driver support\n\nThis has been deleted from the nova tree, we should purge it from\ndevstack, as it will not work any more.\n\nChange-Id: I20501fec140998b91c9ddfd84b7b10168624430a\n'}]",0,139625,2f8e08b5728f4272b415b1c0aab8ff62eae29b06,12,6,1,2750,,,0,"remove nova baremetal driver support

This has been deleted from the nova tree, we should purge it from
devstack, as it will not work any more.

Change-Id: I20501fec140998b91c9ddfd84b7b10168624430a
",git fetch https://review.opendev.org/openstack/devstack refs/changes/25/139625/1 && git format-patch -1 --stdout FETCH_HEAD,"['clean.sh', 'doc/source/index.rst', 'lib/neutron', 'lib/baremetal', 'lib/nova', 'lib/nova_plugins/hypervisor-baremetal', 'unstack.sh', 'stackrc', 'stack.sh']",9,2f8e08b5728f4272b415b1c0aab8ff62eae29b06,baremetal," echo_summary ""Uploading images"" # Option to upload legacy ami-tty, which works with xenserver if [[ -n ""$UPLOAD_LEGACY_TTY"" ]]; then IMAGE_URLS=""${IMAGE_URLS:+${IMAGE_URLS},}https://github.com/downloads/citrix-openstack/warehouse/tty.tgz"" for image_url in ${IMAGE_URLS//,/ }; do upload_image $image_url $TOKEN done","source $TOP_DIR/lib/baremetal# Extra things to prepare nova for baremetal, before nova starts if is_service_enabled nova && is_baremetal; then echo_summary ""Preparing for nova baremetal"" prepare_baremetal_toolchain configure_baremetal_nova_dirs fi if is_baremetal; then echo_summary ""Creating and uploading baremetal images"" # build and upload separate deploy kernel & ramdisk upload_baremetal_deploy $TOKEN # upload images, separating out the kernel & ramdisk for PXE boot for image_url in ${IMAGE_URLS//,/ }; do upload_baremetal_image $image_url $TOKEN done else echo_summary ""Uploading images"" # Option to upload legacy ami-tty, which works with xenserver if [[ -n ""$UPLOAD_LEGACY_TTY"" ]]; then IMAGE_URLS=""${IMAGE_URLS:+${IMAGE_URLS},}https://github.com/downloads/citrix-openstack/warehouse/tty.tgz"" fi for image_url in ${IMAGE_URLS//,/ }; do upload_image $image_url $TOKEN done# If we are running nova with baremetal driver, there are a few # last-mile configuration bits to attend to, which must happen # after n-api and n-sch have started. # Also, creating the baremetal flavor must happen after images # are loaded into glance, though just knowing the IDs is sufficient here if is_service_enabled nova && is_baremetal; then # create special flavor for baremetal if we know what images to associate [[ -n ""$BM_DEPLOY_KERNEL_ID"" ]] && [[ -n ""$BM_DEPLOY_RAMDISK_ID"" ]] && \ create_baremetal_flavor $BM_DEPLOY_KERNEL_ID $BM_DEPLOY_RAMDISK_ID # otherwise user can manually add it later by calling nova-baremetal-manage [[ -n ""$BM_FIRST_MAC"" ]] && add_baremetal_node if [[ ""$BM_DNSMASQ_FROM_NOVA_NETWORK"" = ""False"" ]]; then # NOTE: we do this here to ensure that our copy of dnsmasq is running sudo pkill dnsmasq || true sudo dnsmasq --conf-file= --port=0 --enable-tftp --tftp-root=/tftpboot \ --dhcp-boot=pxelinux.0 --bind-interfaces --pid-file=/var/run/dnsmasq.pid \ --interface=$BM_DNSMASQ_IFACE --dhcp-range=$BM_DNSMASQ_RANGE \ ${BM_DNSMASQ_DNS:+--dhcp-option=option:dns-server,$BM_DNSMASQ_DNS} fi # ensure callback daemon is running sudo pkill nova-baremetal-deploy-helper || true run_process baremetal ""nova-baremetal-deploy-helper"" fi ",11,611
openstack%2Fcinder~master~I8f8b1d782549d72e1ff0e42ae36fc271f7eeea04,openstack/cinder,master,I8f8b1d782549d72e1ff0e42ae36fc271f7eeea04,Updated from global requirements,MERGED,2014-12-05 23:59:28.000000000,2014-12-07 17:16:19.000000000,2014-12-07 17:16:17.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-05 23:59:28.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/cinder/commit/78dbe1957b29a9939645b6c308e9b7021f34a5e0', 'message': 'Updated from global requirements\n\nChange-Id: I8f8b1d782549d72e1ff0e42ae36fc271f7eeea04\n'}]",0,139760,78dbe1957b29a9939645b6c308e9b7021f34a5e0,12,8,1,11131,,,0,"Updated from global requirements

Change-Id: I8f8b1d782549d72e1ff0e42ae36fc271f7eeea04
",git fetch https://review.opendev.org/openstack/cinder refs/changes/60/139760/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,78dbe1957b29a9939645b6c308e9b7021f34a5e0,openstack/requirements,oslo.concurrency>=0.3.0 # Apache-2.0,oslo.concurrency>=0.1.0 # Apache-2.0,1,1
openstack%2Ftooz~master~I3a5ba47b42e7984a35900ff430f5f1e7dce53d70,openstack/tooz,master,I3a5ba47b42e7984a35900ff430f5f1e7dce53d70,Fix .gitreview after rename/transfer,MERGED,2014-12-06 16:27:21.000000000,2014-12-07 16:51:30.000000000,2014-12-07 16:51:30.000000000,"[{'_account_id': 3}, {'_account_id': 1669}, {'_account_id': 6786}]","[{'number': 1, 'created': '2014-12-06 16:27:21.000000000', 'files': ['.gitreview'], 'web_link': 'https://opendev.org/openstack/tooz/commit/e9f51e813b706a7990c2df831e29f9ecf85d6c0c', 'message': 'Fix .gitreview after rename/transfer\n\nChange-Id: I3a5ba47b42e7984a35900ff430f5f1e7dce53d70\n'}]",0,139813,e9f51e813b706a7990c2df831e29f9ecf85d6c0c,10,3,1,6786,,,0,"Fix .gitreview after rename/transfer

Change-Id: I3a5ba47b42e7984a35900ff430f5f1e7dce53d70
",git fetch https://review.opendev.org/openstack/tooz refs/changes/13/139813/1 && git format-patch -1 --stdout FETCH_HEAD,['.gitreview'],1,e9f51e813b706a7990c2df831e29f9ecf85d6c0c,,project=openstack/tooz.git,project=stackforge/tooz.git,1,1
openstack%2Fcinder~stable%2Ficehouse~I5e6eaaaf6bed3e139ff476ecf9510ebe214a83f9,openstack/cinder,stable/icehouse,I5e6eaaaf6bed3e139ff476ecf9510ebe214a83f9,Ensure rbd connect exception is properly caught,MERGED,2014-10-31 19:52:10.000000000,2014-12-07 16:50:36.000000000,2014-12-07 16:50:35.000000000,"[{'_account_id': 3}, {'_account_id': 979}, {'_account_id': 2243}, {'_account_id': 6542}, {'_account_id': 6737}, {'_account_id': 7198}, {'_account_id': 7878}, {'_account_id': 10503}, {'_account_id': 10621}, {'_account_id': 11811}]","[{'number': 1, 'created': '2014-10-31 19:52:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/8c0634f5376948562c64285f9d4f1c275d93f24e', 'message': 'Update rbd client connect() to accept optional timeout parameter\n\nThe Rados client specifies an infinite timeout by default.\n\nThis patch adds an optional timeout parameter to pass to the client.\n\nThis fix has already been merged into openstack/cinder\n\ncherry-picked from stable/icehouse I5e6eaaaf6bed3e139ff476ecf9510ebe214a83f9\nSee also https://review.openstack.org/#/c/103424/\n\nChange-Id: I5e6eaaaf6bed3e139ff476ecf9510ebe214a83f9\n'}, {'number': 2, 'created': '2014-11-03 15:42:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c860ce024edcff9727a399714d04aea0424adb2c', 'message': 'Update rbd client connect() to accept optional timeout parameter\n\nThe Rados client specifies an infinite timeout by default.\n\nThis patch adds an optional timeout parameter to pass to the client.\n\nThis fix has already been merged into openstack/cinder\n\nSee also https://review.openstack.org/#/c/103424/\n\nChange-Id: I5e6eaaaf6bed3e139ff476ecf9510ebe214a83f9\n'}, {'number': 3, 'created': '2014-11-04 16:57:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a2029f43e5e395df2096570d825e4e5f7f3ad6b1', 'message': 'Update rbd client connect() to accept optional timeout parameter\n\nThe Rados client specifies an infinite timeout by default.\n\nThis patch adds an optional timeout parameter to pass to the client.\n\nThis fix has already been merged into openstack/cinder\n\nSee also https://review.openstack.org/#/c/103424/\n\nChange-Id: I5e6eaaaf6bed3e139ff476ecf9510ebe214a83f9\n'}, {'number': 4, 'created': '2014-11-13 18:55:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/49691f68058d4d97153cffd2be7ca51893330d21', 'message': ""Add config option for rados connect timeout.\n\nConflicts:\n\tcinder/volume/drivers/rbd.py\n\tetc/cinder/cinder.conf.sample\n\nDocImpact: new config option 'rados_connect_timout'\n(cherry picked from commit 02e4afbd8108ca26af09ccaebb8b1c8b72ea3f3f)\nChange-Id: I5e6eaaaf6bed3e139ff476ecf9510ebe214a83f9\n""}, {'number': 5, 'created': '2014-11-13 19:18:59.000000000', 'files': ['cinder/volume/drivers/rbd.py', 'etc/cinder/cinder.conf.sample', 'cinder/tests/test_rbd.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/4a14db7a7477ef85d8cb57109cdc17763fba2917', 'message': ""Ensure rbd connect exception is properly caught\n\nIf the rbd driver fails to connect to Ceph the exception\nwas not being properly caught resulting in the volume\nremaining in the 'creating' state until the corresponding\ntask eventually times out (on top of the time it took\nfor the connect to fail).\n\nAlso added config option for rados connect timeout.\n\nConflicts:\n    cinder/volume/drivers/rbd.py\n    etc/cinder/cinder.conf.sample\n\nDocImpact: new config option 'rados_connect_timout'\n(cherry picked from commit 02e4afbd8108ca26af09ccaebb8b1c8b72ea3f3f)\nChange-Id: I5e6eaaaf6bed3e139ff476ecf9510ebe214a83f9\n""}]",4,132336,4a14db7a7477ef85d8cb57109cdc17763fba2917,26,10,5,6542,,,0,"Ensure rbd connect exception is properly caught

If the rbd driver fails to connect to Ceph the exception
was not being properly caught resulting in the volume
remaining in the 'creating' state until the corresponding
task eventually times out (on top of the time it took
for the connect to fail).

Also added config option for rados connect timeout.

Conflicts:
    cinder/volume/drivers/rbd.py
    etc/cinder/cinder.conf.sample

DocImpact: new config option 'rados_connect_timout'
(cherry picked from commit 02e4afbd8108ca26af09ccaebb8b1c8b72ea3f3f)
Change-Id: I5e6eaaaf6bed3e139ff476ecf9510ebe214a83f9
",git fetch https://review.opendev.org/openstack/cinder refs/changes/36/132336/5 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/rbd.py', 'etc/cinder/cinder.conf.sample', 'cinder/tests/test_rbd.py']",3,8c0634f5376948562c64285f9d4f1c275d93f24e,, self.cfg.rados_connect_timeout = -1 # Expect no timeout if default is used self.mock_rados.Rados.connect.assert_called_once_with() # With timeout self.cfg.rados_connect_timeout = 1 self.mock_rados.Rados.connect.reset_mock() self.driver._connect_to_rados() self.mock_rados.Rados.connect.assert_called_once_with(timeout=1) ,,26,2
openstack%2Fcinder~master~I0a04f82a843ce795ec7518b58a3677ce3f2a314d,openstack/cinder,master,I0a04f82a843ce795ec7518b58a3677ce3f2a314d,Remove the cinder.conf.sample file,MERGED,2014-12-05 04:06:45.000000000,2014-12-07 16:31:30.000000000,2014-12-07 16:31:29.000000000,"[{'_account_id': 3}, {'_account_id': 1736}, {'_account_id': 2243}, {'_account_id': 2759}, {'_account_id': 7198}, {'_account_id': 8247}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12561}, {'_account_id': 12779}, {'_account_id': 12780}, {'_account_id': 13013}]","[{'number': 1, 'created': '2014-12-05 04:06:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/ce27207dd907c86a28dc20802f1840195c0c675b', 'message': 'Remove the cinder.conf.sample file\n\nWe removed the conf_uptodate checks however we didn\'t\nremove the cinder.conf.sample file, but we should.\n\nNow that we\'re not checking it, it\'s guaranteed that it will\nmost certainly be ""wrong"".  So, this patch removes it, we\'ll\nupdate the README.conf.sample file if/when we have an auto\npublished version available.\n\nChange-Id: I0a04f82a843ce795ec7518b58a3677ce3f2a314d\n'}, {'number': 2, 'created': '2014-12-05 14:34:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/0600d9bd682b82e884eccafff36e0422a3279556', 'message': 'Remove the cinder.conf.sample file\n\nWe removed the conf_uptodate checks however we didn\'t\nremove the cinder.conf.sample file, but we should.\n\nNow that we\'re not checking it, it\'s guaranteed that it will\nmost certainly be ""wrong"".  So, this patch removes it, we\'ll\nupdate the README.conf.sample file if/when we have an auto\npublished version available.\n\nChange-Id: I0a04f82a843ce795ec7518b58a3677ce3f2a314d\n'}, {'number': 3, 'created': '2014-12-07 15:11:15.000000000', 'files': ['etc/cinder/cinder.conf.sample', 'etc/cinder/README-cinder.conf.sample'], 'web_link': 'https://opendev.org/openstack/cinder/commit/62a72a7574b18f74e8fceab700c2e831f6ca8961', 'message': 'Remove the cinder.conf.sample file\n\nWe removed the conf_uptodate checks however we didn\'t\nremove the cinder.conf.sample file, but we should.\n\nNow that we\'re not checking it, it\'s guaranteed that it will\nmost certainly be ""wrong"".  So, this patch removes it, we\'ll\nupdate the README.conf.sample file if/when we have an auto\npublished version available.\n\nChange-Id: I0a04f82a843ce795ec7518b58a3677ce3f2a314d\n'}]",4,139511,62a72a7574b18f74e8fceab700c2e831f6ca8961,34,16,3,2243,,,0,"Remove the cinder.conf.sample file

We removed the conf_uptodate checks however we didn't
remove the cinder.conf.sample file, but we should.

Now that we're not checking it, it's guaranteed that it will
most certainly be ""wrong"".  So, this patch removes it, we'll
update the README.conf.sample file if/when we have an auto
published version available.

Change-Id: I0a04f82a843ce795ec7518b58a3677ce3f2a314d
",git fetch https://review.opendev.org/openstack/cinder refs/changes/11/139511/2 && git format-patch -1 --stdout FETCH_HEAD,"['etc/cinder/README-cinder.conf.sample', 'etc/cinder/cinder.conf.sample']",2,ce27207dd907c86a28dc20802f1840195c0c675b,remove_sample_conf,,"[DEFAULT] # # Options defined in oslo.messaging # # Use durable queues in AMQP. (boolean value) # Deprecated group/name - [DEFAULT]/rabbit_durable_queues #amqp_durable_queues=false # Auto-delete queues in AMQP. (boolean value) #amqp_auto_delete=false # Size of RPC connection pool. (integer value) #rpc_conn_pool_size=30 # Qpid broker hostname. (string value) #qpid_hostname=localhost # Qpid broker port. (integer value) #qpid_port=5672 # Qpid HA cluster host:port pairs. (list value) #qpid_hosts=$qpid_hostname:$qpid_port # Username for Qpid connection. (string value) #qpid_username= # Password for Qpid connection. (string value) #qpid_password= # Space separated list of SASL mechanisms to use for auth. # (string value) #qpid_sasl_mechanisms= # Seconds between connection keepalive heartbeats. (integer # value) #qpid_heartbeat=60 # Transport to use, either 'tcp' or 'ssl'. (string value) #qpid_protocol=tcp # Whether to disable the Nagle algorithm. (boolean value) #qpid_tcp_nodelay=true # The number of prefetched messages held by receiver. (integer # value) #qpid_receiver_capacity=1 # The qpid topology version to use. Version 1 is what was # originally used by impl_qpid. Version 2 includes some # backwards-incompatible changes that allow broker federation # to work. Users should update to version 2 when they are # able to take everything down, as it requires a clean break. # (integer value) #qpid_topology_version=1 # SSL version to use (valid only if SSL enabled). valid values # are TLSv1 and SSLv23. SSLv2 and SSLv3 may be available on # some distributions. (string value) #kombu_ssl_version= # SSL key file (valid only if SSL enabled). (string value) #kombu_ssl_keyfile= # SSL cert file (valid only if SSL enabled). (string value) #kombu_ssl_certfile= # SSL certification authority file (valid only if SSL # enabled). (string value) #kombu_ssl_ca_certs= # How long to wait before reconnecting in response to an AMQP # consumer cancel notification. (floating point value) #kombu_reconnect_delay=1.0 # The RabbitMQ broker address where a single node is used. # (string value) #rabbit_host=localhost # The RabbitMQ broker port where a single node is used. # (integer value) #rabbit_port=5672 # RabbitMQ HA cluster host:port pairs. (list value) #rabbit_hosts=$rabbit_host:$rabbit_port # Connect over SSL for RabbitMQ. (boolean value) #rabbit_use_ssl=false # The RabbitMQ userid. (string value) #rabbit_userid=guest # The RabbitMQ password. (string value) #rabbit_password=guest # The RabbitMQ login method. (string value) #rabbit_login_method=AMQPLAIN # The RabbitMQ virtual host. (string value) #rabbit_virtual_host=/ # How frequently to retry connecting with RabbitMQ. (integer # value) #rabbit_retry_interval=1 # How long to backoff for between retries when connecting to # RabbitMQ. (integer value) #rabbit_retry_backoff=2 # Maximum number of RabbitMQ connection retries. Default is 0 # (infinite retry count). (integer value) #rabbit_max_retries=0 # Use HA queues in RabbitMQ (x-ha-policy: all). If you change # this option, you must wipe the RabbitMQ database. (boolean # value) #rabbit_ha_queues=false # ZeroMQ bind address. Should be a wildcard (*), an ethernet # interface, or IP. The ""host"" option should point or resolve # to this address. (string value) #rpc_zmq_bind_address=* # MatchMaker driver. (string value) #rpc_zmq_matchmaker=oslo.messaging._drivers.matchmaker.MatchMakerLocalhost # ZeroMQ receiver listening port. (integer value) #rpc_zmq_port=9501 # Number of ZeroMQ contexts, defaults to 1. (integer value) #rpc_zmq_contexts=1 # Maximum number of ingress messages to locally buffer per # topic. Default is unlimited. (integer value) #rpc_zmq_topic_backlog=<None> # Directory for holding IPC sockets. (string value) #rpc_zmq_ipc_dir=/var/run/openstack # Name of this node. Must be a valid hostname, FQDN, or IP # address. Must match ""host"" option, if running Nova. (string # value) #rpc_zmq_host=cinder # Seconds to wait before a cast expires (TTL). Only supported # by impl_zmq. (integer value) #rpc_cast_timeout=30 # Heartbeat frequency. (integer value) #matchmaker_heartbeat_freq=300 # Heartbeat time-to-live. (integer value) #matchmaker_heartbeat_ttl=600 # Size of RPC greenthread pool. (integer value) #rpc_thread_pool_size=64 # Driver or drivers to handle sending notifications. (multi # valued) #notification_driver= # AMQP topic used for OpenStack notifications. (list value) # Deprecated group/name - [rpc_notifier2]/topics #notification_topics=notifications # Seconds to wait for a response from a call. (integer value) #rpc_response_timeout=60 # A URL representing the messaging driver to use and its full # configuration. If not set, we fall back to the rpc_backend # option and driver specific configuration. (string value) #transport_url=<None> # The messaging driver to use, defaults to rabbit. Other # drivers include qpid and zmq. (string value) #rpc_backend=rabbit # The default exchange under which topics are scoped. May be # overridden by an exchange name specified in the # transport_url option. (string value) #control_exchange=openstack # # Options defined in cinder.exception # # Make exception message format errors fatal. (boolean value) #fatal_exception_format_errors=false # # Options defined in cinder.quota # # Number of volumes allowed per project (integer value) #quota_volumes=10 # Number of volume snapshots allowed per project (integer # value) #quota_snapshots=10 # Number of consistencygroups allowed per project (integer # value) #quota_consistencygroups=10 # Total amount of storage, in gigabytes, allowed for volumes # and snapshots per project (integer value) #quota_gigabytes=1000 # Number of volume backups allowed per project (integer value) #quota_backups=10 # Total amount of storage, in gigabytes, allowed for backups # per project (integer value) #quota_backup_gigabytes=1000 # Number of seconds until a reservation expires (integer # value) #reservation_expire=86400 # Count of reservations until usage is refreshed (integer # value) #until_refresh=0 # Number of seconds between subsequent usage refreshes # (integer value) #max_age=0 # Default driver to use for quota checks (string value) #quota_driver=cinder.quota.DbQuotaDriver # Enables or disables use of default quota class with default # quota. (boolean value) #use_default_quota_class=true # # Options defined in cinder.service # # Interval, in seconds, between nodes reporting state to # datastore (integer value) #report_interval=10 # Interval, in seconds, between running periodic tasks # (integer value) #periodic_interval=60 # Range, in seconds, to randomly delay when starting the # periodic task scheduler to reduce stampeding. (Disable by # setting to 0) (integer value) #periodic_fuzzy_delay=60 # IP address on which OpenStack Volume API listens (string # value) #osapi_volume_listen=0.0.0.0 # Port on which OpenStack Volume API listens (integer value) #osapi_volume_listen_port=8776 # Number of workers for OpenStack Volume API service. The # default is equal to the number of CPUs available. (integer # value) #osapi_volume_workers=<None> # # Options defined in cinder.ssh_utils # # Option to enable strict host key checking. When set to # ""True"" Cinder will only connect to systems with a host key # present in the configured ""ssh_hosts_key_file"". When set to # ""False"" the host key will be saved upon first connection and # used for subsequent connections. Default=False (boolean # value) #strict_ssh_host_key_policy=false # File containing SSH host keys for the systems with which # Cinder needs to communicate. OPTIONAL: # Default=$state_path/ssh_known_hosts (string value) #ssh_hosts_key_file=$state_path/ssh_known_hosts # # Options defined in cinder.test # # File name of clean sqlite db (string value) #sqlite_clean_db=clean.sqlite # # Options defined in cinder.wsgi # # Maximum line size of message headers to be accepted. # max_header_line may need to be increased when using large # tokens (typically those generated by the Keystone v3 API # with big service catalogs). (integer value) #max_header_line=16384 # Timeout for client connections' socket operations. If an # incoming connection is idle for this number of seconds it # will be closed. A value of '0' means wait forever. (integer # value) #client_socket_timeout=900 # If False, closes the client socket connection explicitly. # Setting it to True to maintain backward compatibility. # Recommended setting is set it to False. (boolean value) #wsgi_keep_alive=true # Sets the value of TCP_KEEPALIVE (True/False) for each server # socket. (boolean value) #tcp_keepalive=true # Sets the value of TCP_KEEPIDLE in seconds for each server # socket. Not supported on OS X. (integer value) #tcp_keepidle=600 # Sets the value of TCP_KEEPINTVL in seconds for each server # socket. Not supported on OS X. (integer value) #tcp_keepalive_interval=<None> # Sets the value of TCP_KEEPCNT for each server socket. Not # supported on OS X. (integer value) #tcp_keepalive_count=<None> # CA certificate file to use to verify connecting clients # (string value) #ssl_ca_file=<None> # Certificate file to use when starting the server securely # (string value) #ssl_cert_file=<None> # Private key file to use when starting the server securely # (string value) #ssl_key_file=<None> # # Options defined in cinder.api.common # # The maximum number of items that a collection resource # returns in a single response (integer value) #osapi_max_limit=1000 # Base URL that will be presented to users in links to the # OpenStack Volume API (string value) # Deprecated group/name - [DEFAULT]/osapi_compute_link_prefix #osapi_volume_base_URL=<None> # # Options defined in cinder.api.middleware.auth # # Treat X-Forwarded-For as the canonical remote address. Only # enable this if you have a sanitizing proxy. (boolean value) #use_forwarded_for=false # # Options defined in cinder.api.middleware.sizelimit # # Max size for body of a request (integer value) #osapi_max_request_body_size=114688 # # Options defined in cinder.backup.driver # # Backup metadata version to be used when backing up volume # metadata. If this number is bumped, make sure the service # doing the restore supports the new version. (integer value) #backup_metadata_version=1 # # Options defined in cinder.backup.drivers.ceph # # Ceph configuration file to use. (string value) #backup_ceph_conf=/etc/ceph/ceph.conf # The Ceph user to connect with. Default here is to use the # same user as for Cinder volumes. If not using cephx this # should be set to None. (string value) #backup_ceph_user=cinder # The chunk size, in bytes, that a backup is broken into # before transfer to the Ceph object store. (integer value) #backup_ceph_chunk_size=134217728 # The Ceph pool where volume backups are stored. (string # value) #backup_ceph_pool=backups # RBD stripe unit to use when creating a backup image. # (integer value) #backup_ceph_stripe_unit=0 # RBD stripe count to use when creating a backup image. # (integer value) #backup_ceph_stripe_count=0 # If True, always discard excess bytes when restoring volumes # i.e. pad with zeroes. (boolean value) #restore_discard_excess_bytes=true # # Options defined in cinder.backup.drivers.swift # # The URL of the Swift endpoint (string value) #backup_swift_url=<None> # Info to match when looking for swift in the service catalog. # Format is: separated values of the form: # <service_type>:<service_name>:<endpoint_type> - Only used if # backup_swift_url is unset (string value) #swift_catalog_info=object-store:swift:publicURL # Swift authentication mechanism (string value) #backup_swift_auth=per_user # Swift authentication version. Specify ""1"" for auth 1.0, or # ""2"" for auth 2.0 (string value) #backup_swift_auth_version=1 # Swift tenant/account name. Required when connecting to an # auth 2.0 system (string value) #backup_swift_tenant=<None> # Swift user name (string value) #backup_swift_user=<None> # Swift key for authentication (string value) #backup_swift_key=<None> # The default Swift container to use (string value) #backup_swift_container=volumebackups # The size in bytes of Swift backup objects (integer value) #backup_swift_object_size=52428800 # The number of retries to make for Swift operations (integer # value) #backup_swift_retry_attempts=3 # The backoff time in seconds between Swift retries (integer # value) #backup_swift_retry_backoff=2 # Compression algorithm (None to disable) (string value) #backup_compression_algorithm=zlib # # Options defined in cinder.backup.drivers.tsm # # Volume prefix for the backup id when backing up to TSM # (string value) #backup_tsm_volume_prefix=backup # TSM password for the running username (string value) #backup_tsm_password=password # Enable or Disable compression for backups (boolean value) #backup_tsm_compression=true # # Options defined in cinder.backup.manager # # Driver to use for backups. (string value) # Deprecated group/name - [DEFAULT]/backup_service #backup_driver=cinder.backup.drivers.swift # # Options defined in cinder.common.config # # File name for the paste.deploy config for cinder-api (string # value) #api_paste_config=api-paste.ini # Top-level directory for maintaining cinder's state (string # value) # Deprecated group/name - [DEFAULT]/pybasedir #state_path=/var/lib/cinder # IP address of this host (string value) #my_ip=10.0.0.1 # Default glance host name or IP (string value) #glance_host=$my_ip # Default glance port (integer value) #glance_port=9292 # A list of the glance API servers available to cinder # ([hostname|ip]:port) (list value) #glance_api_servers=$glance_host:$glance_port # Version of the glance API to use (integer value) #glance_api_version=1 # Number retries when downloading an image from glance # (integer value) #glance_num_retries=0 # Allow to perform insecure SSL (https) requests to glance # (boolean value) #glance_api_insecure=false # Enables or disables negotiation of SSL layer compression. In # some cases disabling compression can improve data # throughput, such as when high network bandwidth is available # and you use compressed image formats like qcow2. (boolean # value) #glance_api_ssl_compression=false # Location of ca certificates file to use for glance client # requests. (string value) #glance_ca_certificates_file=<None> # http/https timeout value for glance operations. If no value # (None) is supplied here, the glanceclient default value is # used. (integer value) #glance_request_timeout=<None> # The topic that scheduler nodes listen on (string value) #scheduler_topic=cinder-scheduler # The topic that volume nodes listen on (string value) #volume_topic=cinder-volume # The topic that volume backup nodes listen on (string value) #backup_topic=cinder-backup # DEPRECATED: Deploy v1 of the Cinder API. (boolean value) #enable_v1_api=true # Deploy v2 of the Cinder API. (boolean value) #enable_v2_api=true # Enables or disables rate limit of the API. (boolean value) #api_rate_limit=true # Specify list of extensions to load when using # osapi_volume_extension option with # cinder.api.contrib.select_extensions (list value) #osapi_volume_ext_list= # osapi volume extension to load (multi valued) #osapi_volume_extension=cinder.api.contrib.standard_extensions # Full class name for the Manager for volume (string value) #volume_manager=cinder.volume.manager.VolumeManager # Full class name for the Manager for volume backup (string # value) #backup_manager=cinder.backup.manager.BackupManager # Full class name for the Manager for scheduler (string value) #scheduler_manager=cinder.scheduler.manager.SchedulerManager # Name of this node. This can be an opaque identifier. It is # not necessarily a host name, FQDN, or IP address. (string # value) #host=cinder # Availability zone of this node (string value) #storage_availability_zone=nova # Default availability zone for new volumes. If not set, the # storage_availability_zone option value is used as the # default for new volumes. (string value) #default_availability_zone=<None> # Default volume type to use (string value) #default_volume_type=<None> # Time period for which to generate volume usages. The options # are hour, day, month, or year. (string value) #volume_usage_audit_period=month # Path to the rootwrap configuration file to use for running # commands as root (string value) #rootwrap_config=/etc/cinder/rootwrap.conf # Enable monkey patching (boolean value) #monkey_patch=false # List of modules/decorators to monkey patch (list value) #monkey_patch_modules= # Maximum time since last check-in for a service to be # considered up (integer value) #service_down_time=60 # The full class name of the volume API class to use (string # value) #volume_api_class=cinder.volume.api.API # The full class name of the volume backup API class (string # value) #backup_api_class=cinder.backup.api.API # The strategy to use for auth. Supports noauth, keystone, and # deprecated. (string value) #auth_strategy=noauth # A list of backend names to use. These backend names should # be backed by a unique [CONFIG] group with its options (list # value) #enabled_backends=<None> # Whether snapshots count against GigaByte quota (boolean # value) #no_snapshot_gb_quota=false # The full class name of the volume transfer API class (string # value) #transfer_api_class=cinder.transfer.api.API # The full class name of the volume replication API class # (string value) #replication_api_class=cinder.replication.api.API # The full class name of the consistencygroup API class # (string value) #consistencygroup_api_class=cinder.consistencygroup.api.API # # Options defined in cinder.compute # # The full class name of the compute API class to use (string # value) #compute_api_class=cinder.compute.nova.API # # Options defined in cinder.compute.nova # # Match this value when searching for nova in the service # catalog. Format is: separated values of the form: # <service_type>:<service_name>:<endpoint_type> (string value) #nova_catalog_info=compute:nova:publicURL # Same as nova_catalog_info, but for admin endpoint. (string # value) #nova_catalog_admin_info=compute:nova:adminURL # Override service catalog lookup with template for nova # endpoint e.g. http://localhost:8774/v2/%(project_id)s # (string value) #nova_endpoint_template=<None> # Same as nova_endpoint_template, but for admin endpoint. # (string value) #nova_endpoint_admin_template=<None> # Region name of this node (string value) #os_region_name=<None> # Location of ca certificates file to use for nova client # requests. (string value) #nova_ca_certificates_file=<None> # Allow to perform insecure SSL requests to nova (boolean # value) #nova_api_insecure=false # # Options defined in cinder.db.api # # The backend to use for db (string value) #db_backend=sqlalchemy # Services to be added to the available pool on create # (boolean value) #enable_new_services=true # Template string to be used to generate volume names (string # value) #volume_name_template=volume-%s # Template string to be used to generate snapshot names # (string value) #snapshot_name_template=snapshot-%s # Template string to be used to generate backup names (string # value) #backup_name_template=backup-%s # # Options defined in cinder.db.base # # Driver to use for database access (string value) #db_driver=cinder.db # # Options defined in cinder.image.glance # # Default core properties of image (list value) #glance_core_properties=checksum,container_format,disk_format,image_name,image_id,min_disk,min_ram,name,size # A list of url schemes that can be downloaded directly via # the direct_url. Currently supported schemes: [file]. (list # value) #allowed_direct_url_schemes= # # Options defined in cinder.image.image_utils # # Directory used for temporary storage during image conversion # (string value) #image_conversion_dir=$state_path/conversion # # Options defined in cinder.openstack.common.eventlet_backdoor # # Enable eventlet backdoor. Acceptable values are 0, <port>, # and <start>:<end>, where 0 results in listening on a random # tcp port number; <port> results in listening on the # specified port number (and not enabling backdoor if that # port is in use); and <start>:<end> results in listening on # the smallest unused port number within the specified range # of port numbers. The chosen port is displayed in the # service's log file. (string value) #backdoor_port=<None> # # Options defined in cinder.openstack.common.log # # Print debugging output (set logging level to DEBUG instead # of default WARNING level). (boolean value) #debug=false # Print more verbose output (set logging level to INFO instead # of default WARNING level). (boolean value) #verbose=false # Log output to standard error. (boolean value) #use_stderr=true # Format string to use for log messages with context. (string # value) #logging_context_format_string=%(asctime)s.%(msecs)03d %(process)d %(levelname)s %(name)s [%(request_id)s %(user_identity)s] %(instance)s%(message)s # Format string to use for log messages without context. # (string value) #logging_default_format_string=%(asctime)s.%(msecs)03d %(process)d %(levelname)s %(name)s [-] %(instance)s%(message)s # Data to append to log format when level is DEBUG. (string # value) #logging_debug_format_suffix=%(funcName)s %(pathname)s:%(lineno)d # Prefix each line of exception output with this format. # (string value) #logging_exception_prefix=%(asctime)s.%(msecs)03d %(process)d TRACE %(name)s %(instance)s # List of logger=LEVEL pairs. (list value) #default_log_levels=amqp=WARN,amqplib=WARN,boto=WARN,qpid=WARN,sqlalchemy=WARN,suds=INFO,oslo.messaging=INFO,iso8601=WARN,requests.packages.urllib3.connectionpool=WARN,urllib3.connectionpool=WARN,websocket=WARN,keystonemiddleware=WARN,routes.middleware=WARN,stevedore=WARN # Enables or disables publication of error events. (boolean # value) #publish_errors=false # Enables or disables fatal status of deprecations. (boolean # value) #fatal_deprecations=false # The format for an instance that is passed with the log # message. (string value) #instance_format=""[instance: %(uuid)s] "" # The format for an instance UUID that is passed with the log # message. (string value) #instance_uuid_format=""[instance: %(uuid)s] "" # The name of a logging configuration file. This file is # appended to any existing logging configuration files. For # details about logging configuration files, see the Python # logging module documentation. (string value) # Deprecated group/name - [DEFAULT]/log_config #log_config_append=<None> # DEPRECATED. A logging.Formatter log message format string # which may use any of the available logging.LogRecord # attributes. This option is deprecated. Please use # logging_context_format_string and # logging_default_format_string instead. (string value) #log_format=<None> # Format string for %%(asctime)s in log records. Default: # %(default)s . (string value) #log_date_format=%Y-%m-%d %H:%M:%S # (Optional) Name of log file to output to. If no default is # set, logging will go to stdout. (string value) # Deprecated group/name - [DEFAULT]/logfile #log_file=<None> # (Optional) The base directory used for relative --log-file # paths. (string value) # Deprecated group/name - [DEFAULT]/logdir #log_dir=<None> # Use syslog for logging. Existing syslog format is DEPRECATED # during I, and will change in J to honor RFC5424. (boolean # value) #use_syslog=false # (Optional) Enables or disables syslog rfc5424 format for # logging. If enabled, prefixes the MSG part of the syslog # message with APP-NAME (RFC5424). The format without the APP- # NAME is deprecated in I, and will be removed in J. (boolean # value) #use_syslog_rfc_format=false # Syslog facility to receive log lines. (string value) #syslog_log_facility=LOG_USER # # Options defined in cinder.openstack.common.periodic_task # # Some periodic tasks can be run in a separate process. Should # we run them here? (boolean value) #run_external_periodic_tasks=true # # Options defined in cinder.openstack.common.policy # # The JSON file that defines policies. (string value) #policy_file=policy.json # Default rule. Enforced when a requested rule is not found. # (string value) #policy_default_rule=default # Directories where policy configuration files are stored. # They can be relative to any directory in the search path # defined by the config_dir option, or absolute paths. The # file defined by policy_file must exist for these directories # to be searched. (multi valued) #policy_dirs=policy.d # # Options defined in cinder.scheduler.driver # # The scheduler host manager class to use (string value) #scheduler_host_manager=cinder.scheduler.host_manager.HostManager # Maximum number of attempts to schedule an volume (integer # value) #scheduler_max_attempts=3 # # Options defined in cinder.scheduler.host_manager # # Which filter class names to use for filtering hosts when not # specified in the request. (list value) #scheduler_default_filters=AvailabilityZoneFilter,CapacityFilter,CapabilitiesFilter # Which weigher class names to use for weighing hosts. (list # value) #scheduler_default_weighers=CapacityWeigher # # Options defined in cinder.scheduler.manager # # Default scheduler driver to use (string value) #scheduler_driver=cinder.scheduler.filter_scheduler.FilterScheduler # # Options defined in cinder.scheduler.scheduler_options # # Absolute path to scheduler configuration JSON file. (string # value) #scheduler_json_config_location= # # Options defined in cinder.scheduler.simple # # This configure option has been deprecated along with the # SimpleScheduler. New scheduler is able to gather capacity # information for each host, thus setting the maximum number # of volume gigabytes for host is no longer needed. It's safe # to remove this configure from cinder.conf. (integer value) #max_gigabytes=10000 # # Options defined in cinder.scheduler.weights.capacity # # Multiplier used for weighing volume capacity. Negative # numbers mean to stack vs spread. (floating point value) #capacity_weight_multiplier=1.0 # Multiplier used for weighing volume capacity. Negative # numbers mean to stack vs spread. (floating point value) #allocated_capacity_weight_multiplier=-1.0 # # Options defined in cinder.scheduler.weights.volume_number # # Multiplier used for weighing volume number. Negative numbers # mean to spread vs stack. (floating point value) #volume_number_multiplier=-1.0 # # Options defined in cinder.transfer.api # # The number of characters in the salt. (integer value) #volume_transfer_salt_length=8 # The number of characters in the autogenerated auth key. # (integer value) #volume_transfer_key_length=16 # # Options defined in cinder.volume.api # # Cache volume availability zones in memory for the provided # duration in seconds (integer value) #az_cache_duration=3600 # Create volume from snapshot at the host where snapshot # resides (boolean value) #snapshot_same_host=true # Ensure that the new volumes are the same AZ as snapshot or # source volume (boolean value) #cloned_volume_same_az=true # # Options defined in cinder.volume.driver # # The maximum number of times to rescan iSER targetto find # volume (integer value) #num_iser_scan_tries=3 # The maximum number of iSER target IDs per host (integer # value) #iser_num_targets=100 # Prefix for iSER volumes (string value) #iser_target_prefix=iqn.2010-10.org.iser.openstack: # The IP address that the iSER daemon is listening on (string # value) #iser_ip_address=$my_ip # The port that the iSER daemon is listening on (integer # value) #iser_port=3260 # The name of the iSER target user-land tool to use (string # value) #iser_helper=tgtadm # Number of times to attempt to run flakey shell commands # (integer value) #num_shell_tries=3 # The percentage of backend capacity is reserved (integer # value) #reserved_percentage=0 # The maximum number of iSCSI target IDs per host (integer # value) #iscsi_num_targets=100 # Prefix for iSCSI volumes (string value) #iscsi_target_prefix=iqn.2010-10.org.openstack: # The IP address that the iSCSI daemon is listening on (string # value) #iscsi_ip_address=$my_ip # The port that the iSCSI daemon is listening on (integer # value) #iscsi_port=3260 # The maximum number of times to rescan targets to find volume # (integer value) # Deprecated group/name - [DEFAULT]/num_iscsi_scan_tries #num_volume_device_scan_tries=3 # The backend name for a given driver implementation (string # value) #volume_backend_name=<None> # Do we attach/detach volumes in cinder using multipath for # volume to image and image to volume transfers? (boolean # value) #use_multipath_for_image_xfer=false # Method used to wipe old volumes (valid options are: none, # zero, shred) (string value) #volume_clear=zero # Size in MiB to wipe at start of old volumes. 0 => all # (integer value) #volume_clear_size=0 # The flag to pass to ionice to alter the i/o priority of the # process used to zero a volume after deletion, for example # ""-c3"" for idle only priority. (string value) #volume_clear_ionice=<None> # iSCSI target user-land tool to use. tgtadm is default, use # lioadm for LIO iSCSI support, iseradm for the ISER protocol, # or fake for testing. (string value) #iscsi_helper=tgtadm # Volume configuration file storage directory (string value) #volumes_dir=$state_path/volumes # IET configuration file (string value) #iet_conf=/etc/iet/ietd.conf # Comma-separated list of initiator IQNs allowed to connect to # the iSCSI target. (From Nova compute nodes.) (string value) #lio_initiator_iqns= # Sets the behavior of the iSCSI target to either perform # blockio or fileio optionally, auto can be set and Cinder # will autodetect type of backing device (string value) #iscsi_iotype=fileio # The default block size used when copying/clearing volumes # (string value) #volume_dd_blocksize=1M # The blkio cgroup name to be used to limit bandwidth of # volume copy (string value) #volume_copy_blkio_cgroup_name=cinder-volume-copy # The upper limit of bandwidth of volume copy. 0 => unlimited # (integer value) #volume_copy_bps_limit=0 # Sets the behavior of the iSCSI target to either perform # write-back(on) or write-through(off). This parameter is # valid if iscsi_helper is set to tgtadm or iseradm. (string # value) #iscsi_write_cache=on # The path to the client certificate key for verification, if # the driver supports it. (string value) #driver_client_cert_key=<None> # The path to the client certificate for verification, if the # driver supports it. (string value) #driver_client_cert=<None> # Tell driver to use SSL for connection to backend storage if # the driver supports it. (boolean value) #driver_use_ssl=false # # Options defined in cinder.volume.drivers.block_device # # List of all available devices (list value) #available_devices= # # Options defined in cinder.volume.drivers.coraid # # IP address of Coraid ESM (string value) #coraid_esm_address= # User name to connect to Coraid ESM (string value) #coraid_user=admin # Name of group on Coraid ESM to which coraid_user belongs # (must have admin privilege) (string value) #coraid_group=admin # Password to connect to Coraid ESM (string value) #coraid_password=password # Volume Type key name to store ESM Repository Name (string # value) #coraid_repository_key=coraid_repository # ESM Repository Name to use if not specified in Volume Type # keys (string value) #coraid_default_repository=<None> # # Options defined in cinder.volume.drivers.datera # # Datera API token. (string value) #datera_api_token=<None> # Datera API port. (string value) #datera_api_port=7717 # Datera API version. (string value) #datera_api_version=1 # Number of replicas to create of an inode. (string value) #datera_num_replicas=3 # # Options defined in cinder.volume.drivers.emc.emc_vmax_common # # use this file for cinder emc plugin config data (string # value) #cinder_emc_config_file=/etc/cinder/cinder_emc_config.xml # # Options defined in cinder.volume.drivers.emc.emc_vnx_cli # # VNX authentication scope type. (string value) #storage_vnx_authentication_type=global # Directory path that contains the VNX security file. Make # sure the security file is generated first. (string value) #storage_vnx_security_file_dir=<None> # Naviseccli Path. (string value) #naviseccli_path= # Storage pool name. (string value) #storage_vnx_pool_name=<None> # VNX secondary SP IP Address. (string value) #san_secondary_ip=<None> # Default timeout for CLI operations in minutes. For example, # LUN migration is a typical long running operation, which # depends on the LUN size and the load of the array. An upper # bound in the specific deployment can be set to avoid # unnecessary long wait. By default, it is 365 days long. # (integer value) #default_timeout=525600 # Default max number of LUNs in a storage group. By default, # the value is 255. (integer value) #max_luns_per_storage_group=255 # To destroy storage group when the last LUN is removed from # it. By default, the value is False. (boolean value) #destroy_empty_storage_group=false # Mapping between hostname and its iSCSI initiator IP # addresses. (string value) #iscsi_initiators= # Automatically register initiators. By default, the value is # False. (boolean value) #initiator_auto_registration=false # # Options defined in cinder.volume.drivers.eqlx # # Group name to use for creating volumes (string value) #eqlx_group_name=group-0 # Timeout for the Group Manager cli command execution (integer # value) #eqlx_cli_timeout=30 # Maximum retry count for reconnection (integer value) #eqlx_cli_max_retries=5 # Use CHAP authentication for targets? (boolean value) #eqlx_use_chap=false # Existing CHAP account name (string value) #eqlx_chap_login=admin # Password for specified CHAP account name (string value) #eqlx_chap_password=password # Pool in which volumes will be created (string value) #eqlx_pool=default # # Options defined in cinder.volume.drivers.fujitsu_eternus_dx_common # # The configuration file for the Cinder SMI-S driver (string # value) #cinder_smis_config_file=/etc/cinder/cinder_fujitsu_eternus_dx.xml # # Options defined in cinder.volume.drivers.fusionio.ioControl # # amount of time wait for iSCSI target to come online (integer # value) #fusionio_iocontrol_targetdelay=5 # number of retries for GET operations (integer value) #fusionio_iocontrol_retry=3 # verify the array certificate on each transaction (boolean # value) #fusionio_iocontrol_verify_cert=true # # Options defined in cinder.volume.drivers.glusterfs # # File with the list of available gluster shares (string # value) #glusterfs_shares_config=/etc/cinder/glusterfs_shares # Create volumes as sparsed files which take no space.If set # to False volume is created as regular file.In such case # volume creation takes a lot of time. (boolean value) #glusterfs_sparsed_volumes=true # Create volumes as QCOW2 files rather than raw files. # (boolean value) #glusterfs_qcow2_volumes=false # Base dir containing mount points for gluster shares. (string # value) #glusterfs_mount_point_base=$state_path/mnt # # Options defined in cinder.volume.drivers.hds.hds # # The configuration file for the Cinder HDS driver for HUS # (string value) #hds_cinder_config_file=/opt/hds/hus/cinder_hus_conf.xml # # Options defined in cinder.volume.drivers.hds.iscsi # # Configuration file for HDS iSCSI cinder plugin (string # value) #hds_hnas_iscsi_config_file=/opt/hds/hnas/cinder_iscsi_conf.xml # # Options defined in cinder.volume.drivers.hds.nfs # # Configuration file for HDS NFS cinder plugin (string value) #hds_hnas_nfs_config_file=/opt/hds/hnas/cinder_nfs_conf.xml # # Options defined in cinder.volume.drivers.hitachi.hbsd_common # # Serial number of storage system (string value) #hitachi_serial_number=<None> # Name of an array unit (string value) #hitachi_unit_name=<None> # Pool ID of storage system (integer value) #hitachi_pool_id=<None> # Thin pool ID of storage system (integer value) #hitachi_thin_pool_id=<None> # Range of logical device of storage system (string value) #hitachi_ldev_range=<None> # Default copy method of storage system (string value) #hitachi_default_copy_method=FULL # Copy speed of storage system (integer value) #hitachi_copy_speed=3 # Interval to check copy (integer value) #hitachi_copy_check_interval=3 # Interval to check copy asynchronously (integer value) #hitachi_async_copy_check_interval=10 # Control port names for HostGroup or iSCSI Target (string # value) #hitachi_target_ports=<None> # Range of group number (string value) #hitachi_group_range=<None> # Request for creating HostGroup or iSCSI Target (boolean # value) #hitachi_group_request=false # # Options defined in cinder.volume.drivers.hitachi.hbsd_fc # # Request for FC Zone creating HostGroup (boolean value) #hitachi_zoning_request=false # # Options defined in cinder.volume.drivers.hitachi.hbsd_horcm # # Instance numbers for HORCM (string value) #hitachi_horcm_numbers=200,201 # Username of storage system for HORCM (string value) #hitachi_horcm_user=<None> # Password of storage system for HORCM (string value) #hitachi_horcm_password=<None> # Add to HORCM configuration (boolean value) #hitachi_horcm_add_conf=true # # Options defined in cinder.volume.drivers.hitachi.hbsd_iscsi # # Add CHAP user (boolean value) #hitachi_add_chap_user=false # iSCSI authentication method (string value) #hitachi_auth_method=<None> # iSCSI authentication username (string value) #hitachi_auth_user=HBSD-CHAP-user # iSCSI authentication password (string value) #hitachi_auth_password=HBSD-CHAP-password # # Options defined in cinder.volume.drivers.huawei # # The configuration file for the Cinder Huawei driver (string # value) #cinder_huawei_conf_file=/etc/cinder/cinder_huawei_conf.xml # # Options defined in cinder.volume.drivers.ibm.flashsystem # # Connection protocol should be FC. (string value) #flashsystem_connection_protocol=FC # Connect with multipath (FC only). (boolean value) #flashsystem_multipath_enabled=false # Allows vdisk to multi host mapping. (boolean value) #flashsystem_multihostmap_enabled=true # # Options defined in cinder.volume.drivers.ibm.gpfs # # Specifies the path of the GPFS directory where Block Storage # volume and snapshot files are stored. (string value) #gpfs_mount_point_base=<None> # Specifies the path of the Image service repository in GPFS. # Leave undefined if not storing images in GPFS. (string # value) #gpfs_images_dir=<None> # Specifies the type of image copy to be used. Set this when # the Image service repository also uses GPFS so that image # files can be transferred efficiently from the Image service # to the Block Storage service. There are two valid values: # ""copy"" specifies that a full copy of the image is made; # ""copy_on_write"" specifies that copy-on-write optimization # strategy is used and unmodified blocks of the image file are # shared efficiently. (string value) #gpfs_images_share_mode=<None> # Specifies an upper limit on the number of indirections # required to reach a specific block due to snapshots or # clones. A lengthy chain of copy-on-write snapshots or # clones can have a negative impact on performance, but # improves space utilization. 0 indicates unlimited clone # depth. (integer value) #gpfs_max_clone_depth=0 # Specifies that volumes are created as sparse files which # initially consume no space. If set to False, the volume is # created as a fully allocated file, in which case, creation # may take a significantly longer time. (boolean value) #gpfs_sparse_volumes=true # Specifies the storage pool that volumes are assigned to. By # default, the system storage pool is used. (string value) #gpfs_storage_pool=system # # Options defined in cinder.volume.drivers.ibm.ibmnas # # IP address or Hostname of NAS system. (string value) #nas_ip= # User name to connect to NAS system. (string value) #nas_login=admin # Password to connect to NAS system. (string value) #nas_password= # SSH port to use to connect to NAS system. (integer value) #nas_ssh_port=22 # Filename of private key to use for SSH authentication. # (string value) #nas_private_key= # Allow network-attached storage systems to operate in a # secure environment where root level access is not permitted. # If set to False, access is as the root user and insecure. If # set to True, access is not as root. If set to auto, a check # is done to determine if this is a new installation: True is # used if so, otherwise False. Default is auto. (string value) #nas_secure_file_operations=auto # Set more secure file permissions on network-attached storage # volume files to restrict broad other/world access. If set to # False, volumes are created with open permissions. If set to # True, volumes are created with permissions for the cinder # user and group (660). If set to auto, a check is done to # determine if this is a new installation: True is used if so, # otherwise False. Default is auto. (string value) #nas_secure_file_permissions=auto # IBMNAS platform type to be used as backend storage; valid # values are - v7ku : for using IBM Storwize V7000 Unified, # sonas : for using IBM Scale Out NAS, gpfs-nas : for using # NFS based IBM GPFS deployments. (string value) #ibmnas_platform_type=v7ku # # Options defined in cinder.volume.drivers.ibm.storwize_svc # # Storage system storage pool for volumes (string value) #storwize_svc_volpool_name=volpool # Storage system space-efficiency parameter for volumes # (percentage) (integer value) #storwize_svc_vol_rsize=2 # Storage system threshold for volume capacity warnings # (percentage) (integer value) #storwize_svc_vol_warning=0 # Storage system autoexpand parameter for volumes (True/False) # (boolean value) #storwize_svc_vol_autoexpand=true # Storage system grain size parameter for volumes # (32/64/128/256) (integer value) #storwize_svc_vol_grainsize=256 # Storage system compression option for volumes (boolean # value) #storwize_svc_vol_compression=false # Enable Easy Tier for volumes (boolean value) #storwize_svc_vol_easytier=true # The I/O group in which to allocate volumes (integer value) #storwize_svc_vol_iogrp=0 # Maximum number of seconds to wait for FlashCopy to be # prepared. Maximum value is 600 seconds (10 minutes) (integer # value) #storwize_svc_flashcopy_timeout=120 # Connection protocol (iSCSI/FC) (string value) #storwize_svc_connection_protocol=iSCSI # Configure CHAP authentication for iSCSI connections # (Default: Enabled) (boolean value) #storwize_svc_iscsi_chap_enabled=true # Connect with multipath (FC only; iSCSI multipath is # controlled by Nova) (boolean value) #storwize_svc_multipath_enabled=false # Allows vdisk to multi host mapping (boolean value) #storwize_svc_multihostmap_enabled=true # Indicate whether svc driver is compatible for NPIV setup. If # it is compatible, it will allow no wwpns being returned on # get_conn_fc_wwpns during initialize_connection (boolean # value) #storwize_svc_npiv_compatibility_mode=false # Allow tenants to specify QOS on create (boolean value) #storwize_svc_allow_tenant_qos=false # If operating in stretched cluster mode, specify the name of # the pool in which mirrored copies are stored.Example: # ""pool2"" (string value) #storwize_svc_stretched_cluster_partner=<None> # # Options defined in cinder.volume.drivers.ibm.xiv_ds8k # # Proxy driver that connects to the IBM Storage Array (string # value) #xiv_ds8k_proxy=xiv_ds8k_openstack.nova_proxy.XIVDS8KNovaProxy # Connection type to the IBM Storage Array # (fibre_channel|iscsi) (string value) #xiv_ds8k_connection_type=iscsi # CHAP authentication mode, effective only for iscsi # (disabled|enabled) (string value) #xiv_chap=disabled # # Options defined in cinder.volume.drivers.lvm # # Name for the VG that will contain exported volumes (string # value) #volume_group=cinder-volumes # If >0, create LVs with multiple mirrors. Note that this # requires lvm_mirrors + 2 PVs with available space (integer # value) #lvm_mirrors=0 # Type of LVM volumes to deploy; (default or thin) (string # value) #lvm_type=default # # Options defined in cinder.volume.drivers.netapp.options # # The vFiler unit on which provisioning of block storage # volumes will be done. This option is only used by the driver # when connecting to an instance with a storage family of Data # ONTAP operating in 7-Mode. Only use this option when # utilizing the MultiStore feature on the NetApp storage # system. (string value) #netapp_vfiler=<None> # Administrative user account name used to access the storage # system or proxy server. (string value) #netapp_login=<None> # Password for the administrative user account specified in # the netapp_login option. (string value) #netapp_password=<None> # This option specifies the virtual storage server (Vserver) # name on the storage cluster on which provisioning of block # storage volumes should occur. If using the NFS storage # protocol, this parameter is mandatory for storage service # catalog support (utilized by Cinder volume type extra_specs # support). If this option is specified, the exports belonging # to the Vserver will only be used for provisioning in the # future. Block storage volumes on exports not belonging to # the Vserver specified by this option will continue to # function normally. (string value) #netapp_vserver=<None> # The hostname (or IP address) for the storage system or proxy # server. (string value) #netapp_server_hostname=<None> # The TCP port to use for communication with the storage # system or proxy server. If not specified, Data ONTAP drivers # will use 80 for HTTP and 443 for HTTPS; E-Series will use # 8080 for HTTP and 8443 for HTTPS. (integer value) #netapp_server_port=<None> # This option is used to specify the path to the E-Series # proxy application on a proxy server. The value is combined # with the value of the netapp_transport_type, # netapp_server_hostname, and netapp_server_port options to # create the URL used by the driver to connect to the proxy # application. (string value) #netapp_webservice_path=/devmgr/v2 # This option is only utilized when the storage family is # configured to eseries. This option is used to restrict # provisioning to the specified controllers. Specify the value # of this option to be a comma separated list of controller # hostnames or IP addresses to be used for provisioning. # (string value) #netapp_controller_ips=<None> # Password for the NetApp E-Series storage array. (string # value) #netapp_sa_password=<None> # This option is used to restrict provisioning to the # specified storage pools. Only dynamic disk pools are # currently supported. Specify the value of this option to be # a comma separated list of disk pool names to be used for # provisioning. (string value) #netapp_storage_pools=<None> # This option is used to define how the controllers in the # E-Series storage array will work with the particular # operating system on the hosts that are connected to it. # (string value) #netapp_eseries_host_type=linux_dm_mp # If the percentage of available space for an NFS share has # dropped below the value specified by this option, the NFS # image cache will be cleaned. (integer value) #thres_avl_size_perc_start=20 # When the percentage of available space on an NFS share has # reached the percentage specified by this option, the driver # will stop clearing files from the NFS image cache that have # not been accessed in the last M minutes, where M is the # value of the expiry_thres_minutes configuration option. # (integer value) #thres_avl_size_perc_stop=60 # This option specifies the threshold for last access time for # images in the NFS image cache. When a cache cleaning cycle # begins, images in the cache that have not been accessed in # the last M minutes, where M is the value of this parameter, # will be deleted from the cache to create free space on the # NFS share. (integer value) #expiry_thres_minutes=720 # This option specifies the path of the NetApp copy offload # tool binary. Ensure that the binary has execute permissions # set which allow the effective user of the cinder-volume # process to execute the file. (string value) #netapp_copyoffload_tool_path=<None> # The quantity to be multiplied by the requested volume size # to ensure enough space is available on the virtual storage # server (Vserver) to fulfill the volume creation request. # (floating point value) #netapp_size_multiplier=1.2 # This option is only utilized when the storage protocol is # configured to use iSCSI. This option is used to restrict # provisioning to the specified controller volumes. Specify # the value of this option to be a comma separated list of # NetApp controller volume names to be used for provisioning. # (string value) #netapp_volume_list=<None> # The storage family type used on the storage system; valid # values are ontap_7mode for using Data ONTAP operating in # 7-Mode, ontap_cluster for using clustered Data ONTAP, or # eseries for using E-Series. (string value) #netapp_storage_family=ontap_cluster # The storage protocol to be used on the data path with the # storage system; valid values are iscsi or nfs. (string # value) #netapp_storage_protocol=<None> # The transport protocol used when communicating with the # storage system or proxy server. Valid values are http or # https. (string value) #netapp_transport_type=http # # Options defined in cinder.volume.drivers.nexenta.options # # IP address of Nexenta SA (string value) #nexenta_host= # HTTP port to connect to Nexenta REST API server (integer # value) #nexenta_rest_port=2000 # Use http or https for REST connection (default auto) (string # value) #nexenta_rest_protocol=auto # User name to connect to Nexenta SA (string value) #nexenta_user=admin # Password to connect to Nexenta SA (string value) #nexenta_password=nexenta # Nexenta target portal port (integer value) #nexenta_iscsi_target_portal_port=3260 # SA Pool that holds all volumes (string value) #nexenta_volume=cinder # IQN prefix for iSCSI targets (string value) #nexenta_target_prefix=iqn.1986-03.com.sun:02:cinder- # Prefix for iSCSI target groups on SA (string value) #nexenta_target_group_prefix=cinder/ # File with the list of available nfs shares (string value) #nexenta_shares_config=/etc/cinder/nfs_shares # Base directory that contains NFS share mount points (string # value) #nexenta_mount_point_base=$state_path/mnt # Enables or disables the creation of volumes as sparsed files # that take no space. If disabled (False), volume is created # as a regular file, which takes a long time. (boolean value) #nexenta_sparsed_volumes=true # Default compression value for new ZFS folders. (string # value) #nexenta_volume_compression=on # If set True cache NexentaStor appliance volroot option # value. (boolean value) #nexenta_nms_cache_volroot=true # Enable stream compression, level 1..9. 1 - gives best speed; # 9 - gives best compression. (integer value) #nexenta_rrmgr_compression=0 # TCP Buffer size in KiloBytes. (integer value) #nexenta_rrmgr_tcp_buf_size=4096 # Number of TCP connections. (integer value) #nexenta_rrmgr_connections=2 # Block size for volumes (default=blank means 8KB) (string # value) #nexenta_blocksize= # Enables or disables the creation of sparse volumes (boolean # value) #nexenta_sparse=false # # Options defined in cinder.volume.drivers.nfs # # File with the list of available nfs shares (string value) #nfs_shares_config=/etc/cinder/nfs_shares # Create volumes as sparsed files which take no space.If set # to False volume is created as regular file.In such case # volume creation takes a lot of time. (boolean value) #nfs_sparsed_volumes=true # Percent of ACTUAL usage of the underlying volume before no # new volumes can be allocated to the volume destination. # (floating point value) #nfs_used_ratio=0.95 # This will compare the allocated to available space on the # volume destination. If the ratio exceeds this number, the # destination will no longer be valid. (floating point value) #nfs_oversub_ratio=1.0 # Base dir containing mount points for nfs shares. (string # value) #nfs_mount_point_base=$state_path/mnt # Mount options passed to the nfs client. See section of the # nfs man page for details. (string value) #nfs_mount_options=<None> # # Options defined in cinder.volume.drivers.nimble # # Nimble Controller pool name (string value) #nimble_pool_name=default # Nimble Subnet Label (string value) #nimble_subnet_label=* # # Options defined in cinder.volume.drivers.prophetstor.options # # DPL pool uuid in which DPL volumes are stored. (string # value) #dpl_pool= # DPL port number. (integer value) #dpl_port=8357 # # Options defined in cinder.volume.drivers.pure # # REST API authorization token. (string value) #pure_api_token=<None> # # Options defined in cinder.volume.drivers.rbd # # The RADOS pool where rbd volumes are stored (string value) #rbd_pool=rbd # The RADOS client name for accessing rbd volumes - only set # when using cephx authentication (string value) #rbd_user=<None> # Path to the ceph configuration file (string value) #rbd_ceph_conf= # Flatten volumes created from snapshots to remove dependency # from volume to snapshot (boolean value) #rbd_flatten_volume_from_snapshot=false # The libvirt uuid of the secret for the rbd_user volumes # (string value) #rbd_secret_uuid=<None> # Directory where temporary image files are stored when the # volume driver does not write them directly to the volume. # (string value) #volume_tmp_dir=<None> # Maximum number of nested volume clones that are taken before # a flatten occurs. Set to 0 to disable cloning. (integer # value) #rbd_max_clone_depth=5 # Volumes will be chunked into objects of this size (in # megabytes). (integer value) #rbd_store_chunk_size=4 # Timeout value (in seconds) used when connecting to ceph # cluster. If value < 0, no timeout is set and default # librados value is used. (integer value) #rados_connect_timeout=-1 # # Options defined in cinder.volume.drivers.remotefs # # IP address or Hostname of NAS system. (string value) #nas_ip= # User name to connect to NAS system. (string value) #nas_login=admin # Password to connect to NAS system. (string value) #nas_password= # SSH port to use to connect to NAS system. (integer value) #nas_ssh_port=22 # Filename of private key to use for SSH authentication. # (string value) #nas_private_key= # Allow network-attached storage systems to operate in a # secure environment where root level access is not permitted. # If set to False, access is as the root user and insecure. If # set to True, access is not as root. If set to auto, a check # is done to determine if this is a new installation: True is # used if so, otherwise False. Default is auto. (string value) #nas_secure_file_operations=auto # Set more secure file permissions on network-attached storage # volume files to restrict broad other/world access. If set to # False, volumes are created with open permissions. If set to # True, volumes are created with permissions for the cinder # user and group (660). If set to auto, a check is done to # determine if this is a new installation: True is used if so, # otherwise False. Default is auto. (string value) #nas_secure_file_permissions=auto # # Options defined in cinder.volume.drivers.san.hp.hp_3par_common # # 3PAR WSAPI Server Url like https://<3par ip>:8080/api/v1 # (string value) #hp3par_api_url= # 3PAR Super user username (string value) #hp3par_username= # 3PAR Super user password (string value) #hp3par_password= # List of the CPG(s) to use for volume creation (list value) #hp3par_cpg=OpenStack # The CPG to use for Snapshots for volumes. If empty the # userCPG will be used. (string value) #hp3par_cpg_snap= # The time in hours to retain a snapshot. You can't delete it # before this expires. (string value) #hp3par_snapshot_retention= # The time in hours when a snapshot expires and is deleted. # This must be larger than expiration (string value) #hp3par_snapshot_expiration= # Enable HTTP debugging to 3PAR (boolean value) #hp3par_debug=false # List of target iSCSI addresses to use. (list value) #hp3par_iscsi_ips= # Enable CHAP authentication for iSCSI connections. (boolean # value) #hp3par_iscsi_chap_enabled=false # # Options defined in cinder.volume.drivers.san.hp.hp_lefthand_rest_proxy # # HP LeftHand WSAPI Server Url like https://<LeftHand # ip>:8081/lhos (string value) #hplefthand_api_url=<None> # HP LeftHand Super user username (string value) #hplefthand_username=<None> # HP LeftHand Super user password (string value) #hplefthand_password=<None> # HP LeftHand cluster name (string value) #hplefthand_clustername=<None> # Configure CHAP authentication for iSCSI connections # (Default: Disabled) (boolean value) #hplefthand_iscsi_chap_enabled=false # Enable HTTP debugging to LeftHand (boolean value) #hplefthand_debug=false # # Options defined in cinder.volume.drivers.san.hp.hp_msa_common # # The VDisk to use for volume creation. (string value) #msa_vdisk=OpenStack # # Options defined in cinder.volume.drivers.san.san # # Use thin provisioning for SAN volumes? (boolean value) #san_thin_provision=true # IP address of SAN controller (string value) #san_ip= # Username for SAN controller (string value) #san_login=admin # Password for SAN controller (string value) #san_password= # Filename of private key to use for SSH authentication # (string value) #san_private_key= # Cluster name to use for creating volumes (string value) #san_clustername= # SSH port to use with SAN (integer value) #san_ssh_port=22 # Execute commands locally instead of over SSH; use if the # volume service is running on the SAN device (boolean value) #san_is_local=false # SSH connection timeout in seconds (integer value) #ssh_conn_timeout=30 # Minimum ssh connections in the pool (integer value) #ssh_min_pool_conn=1 # Maximum ssh connections in the pool (integer value) #ssh_max_pool_conn=5 # # Options defined in cinder.volume.drivers.san.solaris # # The ZFS path under which to create zvols for volumes. # (string value) #san_zfs_volume_base=rpool/ # # Options defined in cinder.volume.drivers.scality # # Path or URL to Scality SOFS configuration file (string # value) #scality_sofs_config=<None> # Base dir where Scality SOFS shall be mounted (string value) #scality_sofs_mount_point=$state_path/scality # Path from Scality SOFS root to volume dir (string value) #scality_sofs_volume_dir=cinder/volumes # # Options defined in cinder.volume.drivers.smbfs # # File with the list of available smbfs shares. (string value) #smbfs_shares_config=/etc/cinder/smbfs_shares # Default format that will be used when creating volumes if no # volume format is specified. Can be set to: raw, qcow2, vhd # or vhdx. (string value) #smbfs_default_volume_format=qcow2 # Create volumes as sparsed files which take no space rather # than regular files when using raw format, in which case # volume creation takes lot of time. (boolean value) #smbfs_sparsed_volumes=true # Percent of ACTUAL usage of the underlying volume before no # new volumes can be allocated to the volume destination. # (floating point value) #smbfs_used_ratio=0.95 # This will compare the allocated to available space on the # volume destination. If the ratio exceeds this number, the # destination will no longer be valid. (floating point value) #smbfs_oversub_ratio=1.0 # Base dir containing mount points for smbfs shares. (string # value) #smbfs_mount_point_base=$state_path/mnt # Mount options passed to the smbfs client. See mount.cifs man # page for details. (string value) #smbfs_mount_options=noperm,file_mode=0775,dir_mode=0775 # # Options defined in cinder.volume.drivers.solidfire # # Set 512 byte emulation on volume creation; (boolean value) #sf_emulate_512=true # Allow tenants to specify QOS on create (boolean value) #sf_allow_tenant_qos=false # Create SolidFire accounts with this prefix. Any string can # be used here, but the string ""hostname"" is special and will # create a prefix using the cinder node hostsname (previous # default behavior). The default is NO prefix. (string value) #sf_account_prefix=<None> # SolidFire API port. Useful if the device api is behind a # proxy on a different port. (integer value) #sf_api_port=443 # # Options defined in cinder.volume.drivers.vmware.vmdk # # IP address for connecting to VMware ESX/VC server. (string # value) #vmware_host_ip=<None> # Username for authenticating with VMware ESX/VC server. # (string value) #vmware_host_username=<None> # Password for authenticating with VMware ESX/VC server. # (string value) #vmware_host_password=<None> # Optional VIM service WSDL Location e.g # http://<server>/vimService.wsdl. Optional over-ride to # default location for bug work-arounds. (string value) #vmware_wsdl_location=<None> # Number of times VMware ESX/VC server API must be retried # upon connection related issues. (integer value) #vmware_api_retry_count=10 # The interval (in seconds) for polling remote tasks invoked # on VMware ESX/VC server. (floating point value) #vmware_task_poll_interval=0.5 # Name for the folder in the VC datacenter that will contain # cinder volumes. (string value) #vmware_volume_folder=cinder-volumes # Timeout in seconds for VMDK volume transfer between Cinder # and Glance. (integer value) #vmware_image_transfer_timeout_secs=7200 # Max number of objects to be retrieved per batch. Query # results will be obtained in batches from the server and not # in one shot. Server may still limit the count to something # less than the configured value. (integer value) #vmware_max_objects_retrieval=100 # Optional string specifying the VMware VC server version. The # driver attempts to retrieve the version from VMware VC # server. Set this configuration only if you want to override # the VC server version. (string value) #vmware_host_version=<None> # Directory where virtual disks are stored during volume # backup and restore. (string value) #vmware_tmp_dir=/tmp # # Options defined in cinder.volume.drivers.windows.windows # # Path to store VHD backed volumes (string value) #windows_iscsi_lun_path=C:\iSCSIVirtualDisks # # Options defined in cinder.volume.drivers.xio # # Default storage pool for volumes. (integer value) #ise_storage_pool=1 # Raid level for ISE volumes. (integer value) #ise_raid=1 # Number of retries (per port) when establishing connection to # ISE management port. (integer value) #ise_connection_retries=5 # Interval (secs) between retries. (integer value) #ise_retry_interval=1 # Number on retries to get completion status after issuing a # command to ISE. (integer value) #ise_completion_retries=30 # # Options defined in cinder.volume.drivers.zadara # # Management IP of Zadara VPSA (string value) #zadara_vpsa_ip=<None> # Zadara VPSA port number (string value) #zadara_vpsa_port=<None> # Use SSL connection (boolean value) #zadara_vpsa_use_ssl=false # User name for the VPSA (string value) #zadara_user=<None> # Password for the VPSA (string value) #zadara_password=<None> # Name of VPSA storage pool for volumes (string value) #zadara_vpsa_poolname=<None> # Default thin provisioning policy for volumes (boolean value) #zadara_vol_thin=true # Default encryption policy for volumes (boolean value) #zadara_vol_encrypt=false # Default template for VPSA volume names (string value) #zadara_vol_name_template=OS_%s # Automatically detach from servers on volume delete (boolean # value) #zadara_vpsa_auto_detach_on_delete=true # Don't halt on deletion of non-existing volumes (boolean # value) #zadara_vpsa_allow_nonexistent_delete=true # # Options defined in cinder.volume.drivers.zfssa.zfssaiscsi # # Storage pool name. (string value) #zfssa_pool=<None> # Project name. (string value) #zfssa_project=<None> # Block size: 512, 1k, 2k, 4k, 8k, 16k, 32k, 64k, 128k. # (string value) #zfssa_lun_volblocksize=8k # Flag to enable sparse (thin-provisioned): True, False. # (boolean value) #zfssa_lun_sparse=false # Data compression-off, lzjb, gzip-2, gzip, gzip-9. (string # value) #zfssa_lun_compression= # Synchronous write bias-latency, throughput. (string value) #zfssa_lun_logbias= # iSCSI initiator group. (string value) #zfssa_initiator_group= # iSCSI initiator IQNs. (comma separated) (string value) #zfssa_initiator= # iSCSI initiator CHAP user. (string value) #zfssa_initiator_user= # iSCSI initiator CHAP password. (string value) #zfssa_initiator_password= # iSCSI target group name. (string value) #zfssa_target_group=tgt-grp # iSCSI target CHAP user. (string value) #zfssa_target_user= # iSCSI target CHAP password. (string value) #zfssa_target_password= # iSCSI target portal (Data-IP:Port, w.x.y.z:3260). (string # value) #zfssa_target_portal=<None> # Network interfaces of iSCSI targets. (comma separated) # (string value) #zfssa_target_interfaces=<None> # REST connection timeout. (seconds) (integer value) #zfssa_rest_timeout=<None> # # Options defined in cinder.volume.manager # # Driver to use for volume creation (string value) #volume_driver=cinder.volume.drivers.lvm.LVMISCSIDriver # Timeout for creating the volume to migrate to when # performing volume migration (seconds) (integer value) #migration_create_volume_timeout_secs=300 # Offload pending volume delete during volume service startup # (boolean value) #volume_service_inithost_offload=false # FC Zoning mode configured (string value) #zoning_mode=none # User defined capabilities, a JSON formatted string # specifying key/value pairs. (string value) #extra_capabilities={} [BRCD_FABRIC_EXAMPLE] # # Options defined in cinder.zonemanager.drivers.brocade.brcd_fabric_opts # # Management IP of fabric (string value) #fc_fabric_address= # Fabric user ID (string value) #fc_fabric_user= # Password for user (string value) #fc_fabric_password= # Connecting port (integer value) #fc_fabric_port=22 # overridden zoning policy (string value) #zoning_policy=initiator-target # overridden zoning activation state (boolean value) #zone_activate=true # overridden zone name prefix (string value) #zone_name_prefix=<None> # Principal switch WWN of the fabric (string value) #principal_switch_wwn=<None> [CISCO_FABRIC_EXAMPLE] # # Options defined in cinder.zonemanager.drivers.cisco.cisco_fabric_opts # # Management IP of fabric (string value) #cisco_fc_fabric_address= # Fabric user ID (string value) #cisco_fc_fabric_user= # Password for user (string value) #cisco_fc_fabric_password= # Connecting port (integer value) #cisco_fc_fabric_port=22 # overridden zoning policy (string value) #cisco_zoning_policy=initiator-target # overridden zoning activation state (boolean value) #cisco_zone_activate=true # overridden zone name prefix (string value) #cisco_zone_name_prefix=<None> # VSAN of the Fabric (string value) #cisco_zoning_vsan=<None> [database] # # Options defined in oslo.db # # The file name to use with SQLite. (string value) #sqlite_db=oslo.sqlite # If True, SQLite uses synchronous mode. (boolean value) #sqlite_synchronous=true # The back end to use for the database. (string value) # Deprecated group/name - [DEFAULT]/db_backend #backend=sqlalchemy # The SQLAlchemy connection string to use to connect to the # database. (string value) # Deprecated group/name - [DEFAULT]/sql_connection # Deprecated group/name - [DATABASE]/sql_connection # Deprecated group/name - [sql]/connection #connection=<None> # The SQLAlchemy connection string to use to connect to the # slave database. (string value) #slave_connection=<None> # The SQL mode to be used for MySQL sessions. This option, # including the default, overrides any server-set SQL mode. To # use whatever SQL mode is set by the server configuration, # set this to no value. Example: mysql_sql_mode= (string # value) #mysql_sql_mode=TRADITIONAL # Timeout before idle SQL connections are reaped. (integer # value) # Deprecated group/name - [DEFAULT]/sql_idle_timeout # Deprecated group/name - [DATABASE]/sql_idle_timeout # Deprecated group/name - [sql]/idle_timeout #idle_timeout=3600 # Minimum number of SQL connections to keep open in a pool. # (integer value) # Deprecated group/name - [DEFAULT]/sql_min_pool_size # Deprecated group/name - [DATABASE]/sql_min_pool_size #min_pool_size=1 # Maximum number of SQL connections to keep open in a pool. # (integer value) # Deprecated group/name - [DEFAULT]/sql_max_pool_size # Deprecated group/name - [DATABASE]/sql_max_pool_size #max_pool_size=<None> # Maximum number of database connection retries during # startup. Set to -1 to specify an infinite retry count. # (integer value) # Deprecated group/name - [DEFAULT]/sql_max_retries # Deprecated group/name - [DATABASE]/sql_max_retries #max_retries=10 # Interval between retries of opening a SQL connection. # (integer value) # Deprecated group/name - [DEFAULT]/sql_retry_interval # Deprecated group/name - [DATABASE]/reconnect_interval #retry_interval=10 # If set, use this value for max_overflow with SQLAlchemy. # (integer value) # Deprecated group/name - [DEFAULT]/sql_max_overflow # Deprecated group/name - [DATABASE]/sqlalchemy_max_overflow #max_overflow=<None> # Verbosity of SQL debugging information: 0=None, # 100=Everything. (integer value) # Deprecated group/name - [DEFAULT]/sql_connection_debug #connection_debug=0 # Add Python stack traces to SQL as comment strings. (boolean # value) # Deprecated group/name - [DEFAULT]/sql_connection_trace #connection_trace=false # If set, use this value for pool_timeout with SQLAlchemy. # (integer value) # Deprecated group/name - [DATABASE]/sqlalchemy_pool_timeout #pool_timeout=<None> # Enable the experimental use of database reconnect on # connection lost. (boolean value) #use_db_reconnect=false # Seconds between database connection retries. (integer value) #db_retry_interval=1 # If True, increases the interval between database connection # retries up to db_max_retry_interval. (boolean value) #db_inc_retry_interval=true # If db_inc_retry_interval is set, the maximum seconds between # database connection retries. (integer value) #db_max_retry_interval=10 # Maximum database connection retries before error is raised. # Set to -1 to specify an infinite retry count. (integer # value) #db_max_retries=20 # # Options defined in oslo.db.concurrency # # Enable the experimental use of thread pooling for all DB API # calls (boolean value) # Deprecated group/name - [DEFAULT]/dbapi_use_tpool #use_tpool=false [fc-zone-manager] # # Options defined in cinder.zonemanager.drivers.brocade.brcd_fc_zone_driver # # Southbound connector for zoning operation (string value) #brcd_sb_connector=cinder.zonemanager.drivers.brocade.brcd_fc_zone_client_cli.BrcdFCZoneClientCLI # # Options defined in cinder.zonemanager.drivers.cisco.cisco_fc_zone_driver # # Southbound connector for zoning operation (string value) #cisco_sb_connector=cinder.zonemanager.drivers.cisco.cisco_fc_zone_client_cli.CiscoFCZoneClientCLI # # Options defined in cinder.zonemanager.fc_zone_manager # # FC Zone Driver responsible for zone management (string # value) #zone_driver=cinder.zonemanager.drivers.brocade.brcd_fc_zone_driver.BrcdFCZoneDriver # Zoning policy configured by user (string value) #zoning_policy=initiator-target # Comma separated list of fibre channel fabric names. This # list of names is used to retrieve other SAN credentials for # connecting to each SAN fabric (string value) #fc_fabric_names=<None> # FC San Lookup Service (string value) #fc_san_lookup_service=cinder.zonemanager.drivers.brocade.brcd_fc_san_lookup_service.BrcdFCSanLookupService [keymgr] # # Options defined in cinder.keymgr # # The full class name of the key manager API class (string # value) #api_class=cinder.keymgr.conf_key_mgr.ConfKeyManager # # Options defined in cinder.keymgr.conf_key_mgr # # Fixed key returned by key manager, specified in hex (string # value) #fixed_key=<None> # # Options defined in cinder.keymgr.key_mgr # # Authentication url for encryption service. (string value) #encryption_auth_url=http://localhost:5000/v3 # Url for encryption service. (string value) #encryption_api_url=http://localhost:9311/v1 [keystone_authtoken] # # Options defined in keystonemiddleware.auth_token # # Prefix to prepend at the beginning of the path. Deprecated, # use identity_uri. (string value) #auth_admin_prefix= # Host providing the admin Identity API endpoint. Deprecated, # use identity_uri. (string value) #auth_host=127.0.0.1 # Port of the admin Identity API endpoint. Deprecated, use # identity_uri. (integer value) #auth_port=35357 # Protocol of the admin Identity API endpoint (http or https). # Deprecated, use identity_uri. (string value) #auth_protocol=https # Complete public Identity API endpoint (string value) #auth_uri=<None> # Complete admin Identity API endpoint. This should specify # the unversioned root endpoint e.g. https://localhost:35357/ # (string value) #identity_uri=<None> # API version of the admin Identity API endpoint (string # value) #auth_version=<None> # Do not handle authorization requests within the middleware, # but delegate the authorization decision to downstream WSGI # components (boolean value) #delay_auth_decision=false # Request timeout value for communicating with Identity API # server. (boolean value) #http_connect_timeout=<None> # How many times are we trying to reconnect when communicating # with Identity API Server. (integer value) #http_request_max_retries=3 # This option is deprecated and may be removed in a future # release. Single shared secret with the Keystone # configuration used for bootstrapping a Keystone # installation, or otherwise bypassing the normal # authentication process. This option should not be used, use # `admin_user` and `admin_password` instead. (string value) #admin_token=<None> # Keystone account username (string value) #admin_user=<None> # Keystone account password (string value) #admin_password=<None> # Keystone service account tenant name to validate user tokens # (string value) #admin_tenant_name=admin # Env key for the swift cache (string value) #cache=<None> # Required if Keystone server requires client certificate # (string value) #certfile=<None> # Required if Keystone server requires client certificate # (string value) #keyfile=<None> # A PEM encoded Certificate Authority to use when verifying # HTTPs connections. Defaults to system CAs. (string value) #cafile=<None> # Verify HTTPS connections. (boolean value) #insecure=false # Directory used to cache files related to PKI tokens (string # value) #signing_dir=<None> # Optionally specify a list of memcached server(s) to use for # caching. If left undefined, tokens will instead be cached # in-process. (list value) # Deprecated group/name - [DEFAULT]/memcache_servers #memcached_servers=<None> # In order to prevent excessive effort spent validating # tokens, the middleware caches previously-seen tokens for a # configurable duration (in seconds). Set to -1 to disable # caching completely. (integer value) #token_cache_time=300 # Determines the frequency at which the list of revoked tokens # is retrieved from the Identity service (in seconds). A high # number of revocation events combined with a low cache # duration may significantly reduce performance. (integer # value) #revocation_cache_time=10 # (optional) if defined, indicate whether token data should be # authenticated or authenticated and encrypted. Acceptable # values are MAC or ENCRYPT. If MAC, token data is # authenticated (with HMAC) in the cache. If ENCRYPT, token # data is encrypted and authenticated in the cache. If the # value is not one of these options or empty, auth_token will # raise an exception on initialization. (string value) #memcache_security_strategy=<None> # (optional, mandatory if memcache_security_strategy is # defined) this string is used for key derivation. (string # value) #memcache_secret_key=<None> # (optional) number of seconds memcached server is considered # dead before it is tried again. (integer value) #memcache_pool_dead_retry=300 # (optional) max total number of open connections to every # memcached server. (integer value) #memcache_pool_maxsize=10 # (optional) socket timeout in seconds for communicating with # a memcache server. (integer value) #memcache_pool_socket_timeout=3 # (optional) number of seconds a connection to memcached is # held unused in the pool before it is closed. (integer value) #memcache_pool_unused_timeout=60 # (optional) number of seconds that an operation will wait to # get a memcache client connection from the pool. (integer # value) #memcache_pool_conn_get_timeout=10 # (optional) use the advanced (eventlet safe) memcache client # pool. The advanced pool will only work under python 2.x. # (boolean value) #memcache_use_advanced_pool=false # (optional) indicate whether to set the X-Service-Catalog # header. If False, middleware will not ask for service # catalog on token validation and will not set the X-Service- # Catalog header. (boolean value) #include_service_catalog=true # Used to control the use and type of token binding. Can be # set to: ""disabled"" to not check token binding. ""permissive"" # (default) to validate binding information if the bind type # is of a form known to the server and ignore it if not. # ""strict"" like ""permissive"" but if the bind type is unknown # the token will be rejected. ""required"" any form of token # binding is needed to be allowed. Finally the name of a # binding method that must be present in tokens. (string # value) #enforce_token_bind=permissive # If true, the revocation list will be checked for cached # tokens. This requires that PKI tokens are configured on the # Keystone server. (boolean value) #check_revocations_for_cached=false # Hash algorithms to use for hashing PKI tokens. This may be a # single algorithm or multiple. The algorithms are those # supported by Python standard hashlib.new(). The hashes will # be tried in the order given, so put the preferred one first # for performance. The result of the first hash will be stored # in the cache. This will typically be set to multiple values # only while migrating from a less secure algorithm to a more # secure one. Once all the old tokens are expired this option # should be set to a single value for better performance. # (list value) #hash_algorithms=md5 [matchmaker_redis] # # Options defined in oslo.messaging # # Host to locate redis. (string value) #host=127.0.0.1 # Use this port to connect to redis host. (integer value) #port=6379 # Password for Redis server (optional). (string value) #password=<None> [matchmaker_ring] # # Options defined in oslo.messaging # # Matchmaker ring file (JSON). (string value) # Deprecated group/name - [DEFAULT]/matchmaker_ringfile #ringfile=/etc/oslo/matchmaker_ring.json [oslo_messaging_amqp] # # Options defined in oslo.messaging # # address prefix used when sending to a specific server # (string value) #server_request_prefix=exclusive # address prefix used when broadcasting to all servers (string # value) #broadcast_prefix=broadcast # address prefix when sending to any server in group (string # value) #group_request_prefix=unicast # Name for the AMQP container (string value) #container_name=<None> # Timeout for inactive connections (in seconds) (integer # value) #idle_timeout=0 # Debug: dump AMQP frames to stdout (boolean value) #trace=false # CA certificate PEM file for verifing server certificate # (string value) #ssl_ca_file= # Identifying certificate PEM file to present to clients # (string value) #ssl_cert_file= # Private key PEM file used to sign cert_file certificate # (string value) #ssl_key_file= # Password for decrypting ssl_key_file (if encrypted) (string # value) #ssl_key_password=<None> # Accept clients using either SSL or plain TCP (boolean value) #allow_insecure_clients=false [profiler] # # Options defined in cinder.service # # If False fully disable profiling feature. (boolean value) #profiler_enabled=false # If False doesn't trace SQL requests. (boolean value) #trace_sqlalchemy=false ",5,2879
openstack%2Fmagnum~master~Id66b606cd11e43a8ef7b8c46f805b37c22c571a8,openstack/magnum,master,Id66b606cd11e43a8ef7b8c46f805b37c22c571a8,Update db migration for pod,MERGED,2014-12-07 10:11:00.000000000,2014-12-07 15:56:28.000000000,2014-12-07 15:56:28.000000000,"[{'_account_id': 3}, {'_account_id': 2834}]","[{'number': 1, 'created': '2014-12-07 10:11:00.000000000', 'files': ['magnum/db/sqlalchemy/alembic/versions/2581ebaf0cb2_initial_migration.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/4bb16aaa904cd78ca6fd4c9d0237d9e562cece7b', 'message': 'Update db migration for pod\n\nPatch Ib765d1c3ed8a6c06136233abd393fb1efae71e41 add two fields\nfor pods but db migration was not updated.\n\nChange-Id: Id66b606cd11e43a8ef7b8c46f805b37c22c571a8\n'}]",0,139855,4bb16aaa904cd78ca6fd4c9d0237d9e562cece7b,6,2,1,7494,,,0,"Update db migration for pod

Patch Ib765d1c3ed8a6c06136233abd393fb1efae71e41 add two fields
for pods but db migration was not updated.

Change-Id: Id66b606cd11e43a8ef7b8c46f805b37c22c571a8
",git fetch https://review.opendev.org/openstack/magnum refs/changes/55/139855/1 && git format-patch -1 --stdout FETCH_HEAD,['magnum/db/sqlalchemy/alembic/versions/2581ebaf0cb2_initial_migration.py'],1,4bb16aaa904cd78ca6fd4c9d0237d9e562cece7b,master," sa.Column('name', sa.String(length=255), nullable=True), sa.Column('desc', sa.String(length=255), nullable=True),",,2,0
openstack%2Fcinder~master~Iba24fc0a36e01e1d163b398bf32c41fbb9103861,openstack/cinder,master,Iba24fc0a36e01e1d163b398bf32c41fbb9103861,Fix for typo in Purity Host create/delete methods in PureISCSIDriver,MERGED,2014-12-05 23:46:58.000000000,2014-12-07 15:41:41.000000000,2014-12-07 15:41:40.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 6491}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12780}, {'_account_id': 12924}]","[{'number': 1, 'created': '2014-12-05 23:46:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/3ce9ed69faaa2f6960adec6827e1f57b471ec78c', 'message': 'Fix for typo in Purity Host create/delete methods in PureISCSIDriver\n\nWhen refactoring the string manipulation methods there was a typo \nintroduced to the methods to create/delete hosts in purity. The REST \nAPI url was incorrectly pointing to /host/<host_name>/volume instead of \n/host/<host_name>\n\nThis fixes the url and adds in unit tests to prevent this type of \nregression in the future.\n\nChange-Id: Iba24fc0a36e01e1d163b398bf32c41fbb9103861\nCloses-Bug: 1399820\n'}, {'number': 2, 'created': '2014-12-07 01:39:36.000000000', 'files': ['cinder/volume/drivers/pure.py', 'cinder/tests/test_pure.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/ce23f238fb8ce89b35d7da22ef06f5a61cbfba47', 'message': 'Fix for typo in Purity Host create/delete methods in PureISCSIDriver\n\nWhen refactoring the string manipulation methods there was a typo\nintroduced in:\n\nhttps://git.openstack.org/cgit/openstack/cinder/commit/?\nid=1ebe1bddd0a02e9efaa05c8ec9d71a299b6877e1\n\nThat change modified the methods to create/delete hosts in purity. The\nREST API url was incorrectly pointing to /host/<host_name>/volume\ninstead of /host/<host_name>\n\nThis commit fixes the url and adds in unit tests to prevent this type of\nregression in the future.\n\nChange-Id: Iba24fc0a36e01e1d163b398bf32c41fbb9103861\nCloses-Bug: 1399820'}]",4,139756,ce23f238fb8ce89b35d7da22ef06f5a61cbfba47,23,11,2,12924,,,0,"Fix for typo in Purity Host create/delete methods in PureISCSIDriver

When refactoring the string manipulation methods there was a typo
introduced in:

https://git.openstack.org/cgit/openstack/cinder/commit/?
id=1ebe1bddd0a02e9efaa05c8ec9d71a299b6877e1

That change modified the methods to create/delete hosts in purity. The
REST API url was incorrectly pointing to /host/<host_name>/volume
instead of /host/<host_name>

This commit fixes the url and adds in unit tests to prevent this type of
regression in the future.

Change-Id: Iba24fc0a36e01e1d163b398bf32c41fbb9103861
Closes-Bug: 1399820",git fetch https://review.opendev.org/openstack/cinder refs/changes/56/139756/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/pure.py', 'cinder/tests/test_pure.py']",2,3ce9ed69faaa2f6960adec6827e1f57b471ec78c,bug/1399820," def test_create_host(self, mock_req): mock_req.return_value = self.result host_name = ""host1"" params = {'iqnlist': ['iqn1']} result = self.array.create_host(host_name, iqnlist=['iqn1']) self.assertEqual(result, self.result) mock_req.assert_called_with(self.array, ""POST"", ""host/"" + host_name, params) self.assert_error_propagates([mock_req], self.array.create_host, host_name, iqnlist=['iqn1']) def test_delete_host(self, mock_req): mock_req.return_value = self.result host_name = ""host1"" result = self.array.delete_host(host_name) self.assertEqual(result, self.result) mock_req.assert_called_with(self.array, ""DELETE"", ""host/"" + host_name) self.assert_error_propagates([mock_req], self.array.delete_host, host_name) ",,22,2
openstack%2Fcinder~master~I49161e9fc5a8f2749ee6097fa5a136b78dfcf3ab,openstack/cinder,master,I49161e9fc5a8f2749ee6097fa5a136b78dfcf3ab,VMware: Add missing storage profile requirement,MERGED,2014-12-02 07:06:06.000000000,2014-12-07 15:34:39.000000000,2014-12-07 15:34:38.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 2759}, {'_account_id': 4355}, {'_account_id': 7198}, {'_account_id': 9008}, {'_account_id': 9171}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-02 07:06:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/52092ee4b41868b95185c6ea7122e064d42e2e89', 'message': ""VMware: Add missing storage profile requirement\n\nIf a storage profile is part of a volume type, it should be used as a\nrequirement for datastore selection-- only datastores which satisfy the\nstorage profile should be used for backing VM creation. Currently in\nretype API, if storage profiles in old and new volume types are same,\nit is not passed as a requirement. Hence, the backing VM corresponding\nto the volume might end up in a datastore which doesn't satisfy the\nstorage profile. This patch fixes the problem.\n\nCloses-Bug: #1398293\nChange-Id: I49161e9fc5a8f2749ee6097fa5a136b78dfcf3ab\n""}, {'number': 2, 'created': '2014-12-03 07:21:18.000000000', 'files': ['cinder/volume/drivers/vmware/vmdk.py', 'cinder/tests/test_vmware_vmdk.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/b8b3590dbd4554c5d49d54252e52c8ec448a8464', 'message': ""VMware: Add missing storage profile requirement\n\nIf a storage profile is part of a volume type, it should be used as a\nrequirement for datastore selection-- only datastores which satisfy the\nstorage profile should be used for backing VM creation. Currently in\nretype API, if storage profiles in old and new volume types are same,\nit is not passed as a requirement. Hence, the backing VM corresponding\nto the volume might end up in a datastore which doesn't satisfy the\nstorage profile. This patch fixes the problem.\n\nCloses-Bug: #1398293\nChange-Id: I49161e9fc5a8f2749ee6097fa5a136b78dfcf3ab\n""}]",4,138271,b8b3590dbd4554c5d49d54252e52c8ec448a8464,26,14,2,9171,,,0,"VMware: Add missing storage profile requirement

If a storage profile is part of a volume type, it should be used as a
requirement for datastore selection-- only datastores which satisfy the
storage profile should be used for backing VM creation. Currently in
retype API, if storage profiles in old and new volume types are same,
it is not passed as a requirement. Hence, the backing VM corresponding
to the volume might end up in a datastore which doesn't satisfy the
storage profile. This patch fixes the problem.

Closes-Bug: #1398293
Change-Id: I49161e9fc5a8f2749ee6097fa5a136b78dfcf3ab
",git fetch https://review.opendev.org/openstack/cinder refs/changes/71/138271/2 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/vmware/vmdk.py', 'cinder/tests/test_vmware_vmdk.py']",2,52092ee4b41868b95185c6ea7122e064d42e2e89,retype_same_profile,"# TODO(vbala) Split test methods handling multiple cases into multiple methods, # each handling a specific case. # Modify the previous case with no profile change. get_volume_type_extra_specs.side_effect = [vmdk.THICK_VMDK_TYPE, vmdk.THIN_VMDK_TYPE, 'gold-1', 'gold-1'] ds_sel.select_datastore.reset_mock() vops.relocate_backing.reset_mock() vops.move_backing_to_folder.reset_mock() vops.change_backing_profile.reset_mock() self.assertTrue(self._driver.retype(context, vol, new_type, diff, host)) exp_req = {hub.DatastoreSelector.HARD_ANTI_AFFINITY_DS: [ds_value], hub.DatastoreSelector.PROFILE_NAME: 'gold-1', hub.DatastoreSelector.SIZE_BYTES: units.Gi} ds_sel.select_datastore.assert_called_once_with(exp_req) vops.relocate_backing.assert_called_once_with( backing, candidate_ds, rp, host, vmdk.THIN_VMDK_TYPE) vops.move_backing_to_folder.assert_called_once_with(backing, folder) self.assertFalse(vops.change_backing_profile.called) ",,29,5
openstack%2Fneutron~master~I40351d62b5e26be59715d550bc7ff88a7eb2649d,openstack/neutron,master,I40351d62b5e26be59715d550bc7ff88a7eb2649d,Keepalived python script now notifies agent via domain socket,ABANDONED,2014-10-03 14:00:03.000000000,2014-12-07 15:30:16.000000000,,"[{'_account_id': 3}, {'_account_id': 2035}, {'_account_id': 5170}, {'_account_id': 6659}, {'_account_id': 7448}, {'_account_id': 8645}, {'_account_id': 8873}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 12444}]","[{'number': 1, 'created': '2014-10-03 14:00:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/46bd88d1a07827a85e6f66937304d0bd79b5d641', 'message': 'Keepalived python script now notifies agent via domain socket\n\nTo-Do:\n* Figure out why workers > 0 fails the functional test\n* Is it readonable to start the L3 agent unix domain socket\n  listener on a greenthread in the L3 agent process? How\n  does this scale when a node fails and another node\n  receives 1000 concurrent notifications?\n\nRelated-Bug: #1365453\nCloses-Bug: #1367705\nChange-Id: I40351d62b5e26be59715d550bc7ff88a7eb2649d\n'}, {'number': 2, 'created': '2014-10-03 14:09:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/d7fa7b3f465c6fb787b7177dd85830e23b2c8487', 'message': 'Keepalived python script now notifies agent via domain socket\n\nTo-Do:\n* Figure out why workers > 0 fails the functional test\n* Is it readonable to start the L3 agent unix domain socket\n  listener on a greenthread in the L3 agent process? How\n  does this scale when a node fails and another node\n  receives 1000 concurrent notifications?\n\nRelated-Bug: #1365453\nCloses-Bug: #1367705\nChange-Id: I40351d62b5e26be59715d550bc7ff88a7eb2649d\n'}, {'number': 3, 'created': '2014-10-05 16:08:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/d353af201f5f94c924b2d719450bfb8bb4114ac2', 'message': 'Keepalived python script now notifies agent via domain socket\n\nTo-Do:\n* Figure out why workers > 0 fails the functional test\n* Is it readonable to start the L3 agent unix domain socket\n  listener on a greenthread in the L3 agent process? How\n  does this scale when a node fails and another node\n  receives 1000 concurrent notifications?\n\nRelated-Bug: #1365453\nCloses-Bug: #1367705\nChange-Id: I40351d62b5e26be59715d550bc7ff88a7eb2649d\n'}, {'number': 4, 'created': '2014-10-05 16:48:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9cb62c1eb30a51e191ce88eb816a09a191fa25f3', 'message': 'Keepalived python script now notifies agent via domain socket\n\nTo-Do:\n* Figure out why workers > 0 fails the functional test\n* Is it reasonable to start the L3 agent unix domain socket\n  listener on a greenthread in the L3 agent process? How\n  does this scale when a node fails and another node\n  receives 1000 concurrent notifications? How does this impact\n  the L3 agent CPU usage?\n\nRelated-Bug: #1365453\nCloses-Bug: #1367705\nChange-Id: I40351d62b5e26be59715d550bc7ff88a7eb2649d\n'}, {'number': 5, 'created': '2014-10-05 18:58:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/a44ebd52e044656e8d6cf816ed301c590b68875d', 'message': 'Keepalived python script now notifies agent via domain socket\n\nTo-Do:\n* Figure out why workers > 0 fails the functional test\n* Is it reasonable to start the L3 agent unix domain socket\n  listener on a greenthread in the L3 agent process? How\n  does this scale when a node fails and another node\n  receives 1000 concurrent notifications? How does this impact\n  the L3 agent CPU usage?\n\nRelated-Bug: #1365453\nCloses-Bug: #1367705\nChange-Id: I40351d62b5e26be59715d550bc7ff88a7eb2649d\n'}, {'number': 6, 'created': '2014-10-12 16:54:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/b09420860b267ea7c2afc95756c24bab2de29e7e', 'message': 'Keepalived python script now notifies agent via domain socket\n\nTo-Do:\n* Figure out why workers > 0 fails the functional test but works\n  outside of testing.\n\nRelated-Bug: #1365453\nCloses-Bug: #1367705\nChange-Id: I40351d62b5e26be59715d550bc7ff88a7eb2649d\n'}, {'number': 7, 'created': '2014-10-12 17:04:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/759d171e57230de783dbb8da3601faa6aeda5c5c', 'message': 'Keepalived python script now notifies agent via domain socket\n\nRelated-Bug: #1365453\nCloses-Bug: #1367705\nChange-Id: I40351d62b5e26be59715d550bc7ff88a7eb2649d\n'}, {'number': 8, 'created': '2014-10-13 16:56:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/4ff1b17ad605042b9b63011e25bc3ee00f10f1d5', 'message': 'Keepalived python script now notifies agent via domain socket\n\nRelated-Bug: #1365453\nCloses-Bug: #1367705\nChange-Id: I40351d62b5e26be59715d550bc7ff88a7eb2649d\n'}, {'number': 9, 'created': '2014-10-13 17:11:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e424781140735b1c6a6c925f70f485ac44e3cbf0', 'message': 'Keepalived python script now notifies agent via domain socket\n\nneutron-keepalived-state-change is invoked by keepalived whenever\na state change occurs. That script now notifies the L3 agent that\nthe event occured via a unix domain socket. At first, the agent\nwill simply log that the event occured. In the future it will\nreport state changes to the server via RPC.\n\nRelated-Bug: #1365453\nCloses-Bug: #1367705\nChange-Id: I40351d62b5e26be59715d550bc7ff88a7eb2649d\n'}, {'number': 10, 'created': '2014-11-19 15:16:12.000000000', 'files': ['etc/l3_agent.ini', 'neutron/agent/metadata/agent.py', 'neutron/agent/metadata/namespace_proxy.py', 'neutron/agent/l3_agent.py', 'neutron/agent/l3_ha_agent.py', 'neutron/cmd/keepalived_state_change.py', 'neutron/tests/functional/agent/test_l3_agent.py', 'neutron/agent/linux/utils.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/e9096955477d54bf11085a124cf1281bef4e9807', 'message': 'Keepalived python script now notifies agent via domain socket\n\nneutron-keepalived-state-change is invoked by keepalived whenever\na state change occurs. That script now notifies the L3 agent that\nthe event occured via a unix domain socket. At first, the agent\nwill simply log that the event occured. In the future it will\nreport state changes to the server via RPC.\n\nRelated-Bug: #1365453\nCloses-Bug: #1367705\nPartially-Implements: blueprint report-ha-router-master\nChange-Id: I40351d62b5e26be59715d550bc7ff88a7eb2649d\n'}]",15,125973,e9096955477d54bf11085a124cf1281bef4e9807,169,26,10,8873,,,0,"Keepalived python script now notifies agent via domain socket

neutron-keepalived-state-change is invoked by keepalived whenever
a state change occurs. That script now notifies the L3 agent that
the event occured via a unix domain socket. At first, the agent
will simply log that the event occured. In the future it will
report state changes to the server via RPC.

Related-Bug: #1365453
Closes-Bug: #1367705
Partially-Implements: blueprint report-ha-router-master
Change-Id: I40351d62b5e26be59715d550bc7ff88a7eb2649d
",git fetch https://review.opendev.org/openstack/neutron refs/changes/73/125973/6 && git format-patch -1 --stdout FETCH_HEAD,"['etc/l3_agent.ini', 'neutron/agent/metadata/agent.py', 'neutron/agent/metadata/namespace_proxy.py', 'neutron/agent/l3_ha_agent.py', 'neutron/cmd/keepalived_state_change.py', 'neutron/tests/functional/agent/test_l3_agent.py', 'neutron/agent/linux/utils.py']",7,46bd88d1a07827a85e6f66937304d0bd79b5d641,bp/report-ha-router-master," def prepare_unix_domain_socket(path): dirname = os.path.dirname(path) if os.path.isdir(dirname): try: os.unlink(path) except OSError: with excutils.save_and_reraise_exception() as ctxt: if not os.path.exists(path): ctxt.reraise = False else: os.makedirs(dirname, 0o755)",,144,16
openstack%2Fcinder~master~I32a45c4b4a77c5fab68ac39e66326e53743e16b3,openstack/cinder,master,I32a45c4b4a77c5fab68ac39e66326e53743e16b3,remove config file update check from tests,ABANDONED,2014-12-03 19:28:13.000000000,2014-12-07 14:19:16.000000000,,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 2750}, {'_account_id': 2759}, {'_account_id': 7198}, {'_account_id': 11811}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-03 19:28:13.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/cinder/commit/7476b0838755e42a4f99b7e12a201e8a8b980ddc', 'message': 'remove config file update check from tests\n\nThe config file update check can be broken by upstream oslo library\nreleases. Most other projects have removed this check from their\npipeline. Cinder probably should as well. This is currently blocking\nall cinder patches.\n\nChange-Id: I32a45c4b4a77c5fab68ac39e66326e53743e16b3\n'}]",0,138832,7476b0838755e42a4f99b7e12a201e8a8b980ddc,10,8,1,2750,,,0,"remove config file update check from tests

The config file update check can be broken by upstream oslo library
releases. Most other projects have removed this check from their
pipeline. Cinder probably should as well. This is currently blocking
all cinder patches.

Change-Id: I32a45c4b4a77c5fab68ac39e66326e53743e16b3
",git fetch https://review.opendev.org/openstack/cinder refs/changes/32/138832/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,7476b0838755e42a4f99b7e12a201e8a8b980ddc,rmchkuptodate,, {toxinidir}/tools/config/check_uptodate.sh,0,1
openstack%2Fneutron~stable%2Ficehouse~I8dca1fce9fbc83e75ba7e4ce948531427bf7e88b,openstack/neutron,stable/icehouse,I8dca1fce9fbc83e75ba7e4ce948531427bf7e88b,Fix enable_metadata_network flag,MERGED,2014-12-05 13:37:10.000000000,2014-12-07 13:00:39.000000000,2014-12-07 13:00:38.000000000,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 4395}, {'_account_id': 5170}, {'_account_id': 8655}, {'_account_id': 8873}, {'_account_id': 9656}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9846}, {'_account_id': 10119}, {'_account_id': 10153}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 14208}, {'_account_id': 14212}, {'_account_id': 14214}]","[{'number': 1, 'created': '2014-12-05 13:37:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/1da92bfe0e4233817b8a0a1fd705ef5d446e2308', 'message': ""Fix enable_metadata_network flag\n\nThe following patch: 9569b2fe broke the desired functionality of\nthe enable_metadata_network flag, by not allowing the metadata\nproxy to be spawn for 'metadata networks', which are used for\naccessing the metadata service when the logical router is not\nimplemented through the l3 agent.\n\nThis patch enables spawning of the metadata proxy for metadata\nnetworks when the appropriate flag is set to True.\n\nThe patch also adds rather pedant unit test coverage for the\nshould_enable_metadata method which previously had no unit test.\n\nCloses-bug: 1394020\n(cherry picked from commit c45842af38da322b93d1200451a4a254abfcaed1)\n\nConflicts:\n\tneutron/agent/dhcp_agent.py\n\tneutron/tests/unit/test_linux_dhcp.py\n\nChange-Id: I8dca1fce9fbc83e75ba7e4ce948531427bf7e88b\n""}, {'number': 2, 'created': '2014-12-05 18:43:35.000000000', 'files': ['neutron/agent/dhcp_agent.py', 'neutron/agent/linux/dhcp.py', 'neutron/tests/unit/test_linux_dhcp.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/950127127895ba90d0a19175804a3cd0777b3c78', 'message': ""Fix enable_metadata_network flag\n\nThe following patch: 9569b2fe broke the desired functionality of\nthe enable_metadata_network flag, by not allowing the metadata\nproxy to be spawn for 'metadata networks', which are used for\naccessing the metadata service when the logical router is not\nimplemented through the l3 agent.\n\nThis patch enables spawning of the metadata proxy for metadata\nnetworks when the appropriate flag is set to True.\n\nThe patch also adds rather pedant unit test coverage for the\nshould_enable_metadata method which previously had no unit test.\n\nCloses-bug: 1394020\n(cherry picked from commit c45842af38da322b93d1200451a4a254abfcaed1)\n\nConflicts:\n\tneutron/agent/dhcp_agent.py\n\tneutron/tests/unit/test_linux_dhcp.py\n\nChange-Id: I8dca1fce9fbc83e75ba7e4ce948531427bf7e88b\n""}]",2,139628,950127127895ba90d0a19175804a3cd0777b3c78,38,18,2,8655,,,0,"Fix enable_metadata_network flag

The following patch: 9569b2fe broke the desired functionality of
the enable_metadata_network flag, by not allowing the metadata
proxy to be spawn for 'metadata networks', which are used for
accessing the metadata service when the logical router is not
implemented through the l3 agent.

This patch enables spawning of the metadata proxy for metadata
networks when the appropriate flag is set to True.

The patch also adds rather pedant unit test coverage for the
should_enable_metadata method which previously had no unit test.

Closes-bug: 1394020
(cherry picked from commit c45842af38da322b93d1200451a4a254abfcaed1)

Conflicts:
	neutron/agent/dhcp_agent.py
	neutron/tests/unit/test_linux_dhcp.py

Change-Id: I8dca1fce9fbc83e75ba7e4ce948531427bf7e88b
",git fetch https://review.opendev.org/openstack/neutron refs/changes/28/139628/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/agent/dhcp_agent.py', 'neutron/agent/linux/dhcp.py', 'neutron/tests/unit/test_linux_dhcp.py']",3,1da92bfe0e4233817b8a0a1fd705ef5d446e2308,bug/1394020," def __init__(self, ip_address='192.168.0.1'): self.fixed_ips = [FakeIPAllocation( ip_address, 'dddddddd-dddd-dddd-dddd-dddddddddddd')]class FakeV4MetadataSubnet: id = 'dddddddd-dddd-dddd-dddd-dddddddddddd' ip_version = 4 cidr = '169.254.169.254/30' gateway_ip = '169.254.169.253' enable_dhcp = True host_routes = [] dns_nameservers = [] class FakeV4MetadataNetwork: id = 'cccccccc-cccc-cccc-cccc-cccccccccccc' subnets = [FakeV4MetadataSubnet()] ports = [FakeRouterPort(ip_address='169.254.169.253')] config.register_use_namespaces_opts_helper(self.conf) self.conf.register_opt(cfg.BoolOpt('enable_metadata_network', default=False)) def test_should_enable_metadata_namespaces_disabled_returns_false(self): self.conf.set_override('use_namespaces', False) self.assertFalse(dhcp.Dnsmasq.should_enable_metadata(self.conf, mock.ANY)) def test_should_enable_metadata_isolated_network_returns_true(self): self.assertTrue(dhcp.Dnsmasq.should_enable_metadata( self.conf, FakeV4NetworkNoRouter())) def test_should_enable_metadata_non_isolated_network_returns_false(self): self.assertFalse(dhcp.Dnsmasq.should_enable_metadata( self.conf, FakeV4MetadataNetwork())) def test_should_enable_metadata_isolated_meta_disabled_returns_false(self): self.conf.set_override('enable_isolated_metadata', False) self.assertFalse(dhcp.Dnsmasq.should_enable_metadata(self.conf, mock.ANY)) def test_should_enable_metadata_with_metadata_network_returns_true(self): self.conf.set_override('enable_metadata_network', True) self.assertTrue(dhcp.Dnsmasq.should_enable_metadata( self.conf, FakeV4MetadataNetwork()))"," fixed_ips = [FakeIPAllocation('192.168.0.1', 'dddddddd-dddd-dddd-dddd-dddddddddddd')] def __init__(self): self.conf.use_namespaces = True",67,10
openstack%2Fdesignate~master~I24ff7efd78d5a66fc6e6231720f673863505251d,openstack/designate,master,I24ff7efd78d5a66fc6e6231720f673863505251d,Updated from global requirements,MERGED,2014-12-05 23:59:35.000000000,2014-12-07 11:32:24.000000000,2014-12-07 11:32:23.000000000,"[{'_account_id': 3}, {'_account_id': 741}, {'_account_id': 8094}]","[{'number': 1, 'created': '2014-12-05 23:59:35.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/designate/commit/e12ba594f94bd534b072e4e6ef999e09571b4225', 'message': 'Updated from global requirements\n\nChange-Id: I24ff7efd78d5a66fc6e6231720f673863505251d\n'}]",0,139761,e12ba594f94bd534b072e4e6ef999e09571b4225,7,3,1,11131,,,0,"Updated from global requirements

Change-Id: I24ff7efd78d5a66fc6e6231720f673863505251d
",git fetch https://review.opendev.org/openstack/designate refs/changes/61/139761/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,e12ba594f94bd534b072e4e6ef999e09571b4225,openstack/requirements,oslo.concurrency>=0.3.0 # Apache-2.0,oslo.concurrency>=0.1.0 # Apache-2.0,1,1
openstack%2Fheat~master~If6f7772fdaed5762fbeded3fb1c61e1242925df8,openstack/heat,master,If6f7772fdaed5762fbeded3fb1c61e1242925df8,Enable metadata when create server groups,ABANDONED,2014-07-29 09:41:31.000000000,2014-12-07 10:42:18.000000000,,"[{'_account_id': 3}, {'_account_id': 7239}, {'_account_id': 7494}, {'_account_id': 8289}]","[{'number': 1, 'created': '2014-07-29 09:41:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/76f4334c85ad3e33b637a99ec8c52640b18655d4', 'message': 'Enable metadata when create server groups\n\ninstance_group object already support instance group metadata but\nheat resource do not support this.\n\nThis patch was enabling metadata support when creating instance\ngroup as heat resource.\n\nChange-Id: If6f7772fdaed5762fbeded3fb1c61e1242925df8\nPartial-Bug: #1348447\n'}, {'number': 2, 'created': '2014-07-29 13:01:10.000000000', 'files': ['heat/engine/resources/nova_servergroup.py', 'heat/tests/test_nova_servergroup.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/6cadbdce4fb2a1ab0ab7b91191962619df5f0e0a', 'message': 'Enable metadata when create server groups\n\ninstance_group object already support instance group metadata but\nheat resource do not support this.\n\nThis patch was enabling metadata support when creating instance\ngroup as heat resource.\n\nChange-Id: If6f7772fdaed5762fbeded3fb1c61e1242925df8\nPartial-Bug: #1348447\n'}]",3,110248,6cadbdce4fb2a1ab0ab7b91191962619df5f0e0a,10,4,2,7494,,,0,"Enable metadata when create server groups

instance_group object already support instance group metadata but
heat resource do not support this.

This patch was enabling metadata support when creating instance
group as heat resource.

Change-Id: If6f7772fdaed5762fbeded3fb1c61e1242925df8
Partial-Bug: #1348447
",git fetch https://review.opendev.org/openstack/heat refs/changes/48/110248/1 && git format-patch -1 --stdout FETCH_HEAD,"['heat/engine/resources/nova_servergroup.py', 'heat/tests/test_nova_servergroup.py']",2,76f4334c85ad3e33b637a99ec8c52640b18655d4,bug/1348447," ""policies"": [""anti-affinity""], ""metadetails"": {""foo1"": ""bar1"", ""foo2"": ""bar2""}, 'metadetails': {'foo1': 'bar1', 'foo2': 'bar2'},"," ""policies"": [""anti-affinity""]",13,4
openstack%2Fpython-novaclient~master~If3dc05fb3e16b48888abec08360fb408fbcac8d8,openstack/python-novaclient,master,If3dc05fb3e16b48888abec08360fb408fbcac8d8,Enable metadata when create server groups,ABANDONED,2014-07-29 04:30:22.000000000,2014-12-07 10:42:12.000000000,,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 1849}, {'_account_id': 7494}, {'_account_id': 9545}]","[{'number': 1, 'created': '2014-07-29 04:30:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-novaclient/commit/832b9350901bf003b11637eb6ae89b7ef1c90733', 'message': 'Enable metadata when create server groups\n\ninstance_group object already support instance group metadata but\nnova client do not support this.\n\nThis patch was enabling metadata support when creating instance group.\n\nChange-Id: If3dc05fb3e16b48888abec08360fb408fbcac8d8\nPartial-Bug: #1348447\n'}, {'number': 2, 'created': '2014-07-29 09:15:38.000000000', 'files': ['novaclient/tests/v1_1/test_shell.py', 'novaclient/v1_1/shell.py'], 'web_link': 'https://opendev.org/openstack/python-novaclient/commit/21af57b5e8403b3747c09169628f079dca73f6ad', 'message': 'Enable metadata when create server groups\n\ninstance_group object already support instance group metadata but\nnova client do not support this.\n\nThis patch was enabling metadata support when creating instance group.\n\nChange-Id: If3dc05fb3e16b48888abec08360fb408fbcac8d8\nPartial-Bug: #1348447\n'}]",2,110202,21af57b5e8403b3747c09169628f079dca73f6ad,12,5,2,7494,,,0,"Enable metadata when create server groups

instance_group object already support instance group metadata but
nova client do not support this.

This patch was enabling metadata support when creating instance group.

Change-Id: If3dc05fb3e16b48888abec08360fb408fbcac8d8
Partial-Bug: #1348447
",git fetch https://review.opendev.org/openstack/python-novaclient refs/changes/02/110202/1 && git format-patch -1 --stdout FETCH_HEAD,['novaclient/v1_1/shell.py'],1,832b9350901bf003b11637eb6ae89b7ef1c90733,bug/1348447,"@utils.arg('--meta-data', metavar=""key1=value1[,key2=value2...]"", action='append', default=[], help=_(""Meta data for server group."")) meta = {} for kv_str in args.meta_data[0].split("",""): err_msg = (_(""Invalid metadata '%s'."") % args.meta_data[0]) try: k, v = kv_str.split(""="", 1) meta[k] = v except ValueError: raise exceptions.CommandError(err_msg) 'policies': args.policy, 'metadetails': meta}", 'policies': args.policy},15,1
openstack%2Ftaskflow~master~Iae5919fb0f79ffbf9e1784723dc8da1cefdb9f27,openstack/taskflow,master,Iae5919fb0f79ffbf9e1784723dc8da1cefdb9f27,Correctly identify stack level in ``_extract_engine``,MERGED,2014-12-07 08:08:38.000000000,2014-12-07 10:19:58.000000000,2014-12-07 10:19:58.000000000,"[{'_account_id': 3}, {'_account_id': 1297}]","[{'number': 1, 'created': '2014-12-07 08:08:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/16a8cfe1a6ca52635d396e843a25b14a87b8453a', 'message': 'Correctly identify user stack in _extract_engine\n\nAdd on the needed frame evaluation logic to be able\nto correctly locate where users frames start and ours\nend so that we can correctly report the stack level in\nthe deprecation messages (which the warning module will\nuse to show the location of the users code that is using\nthe deprecated usage).\n\nChange-Id: Iae5919fb0f79ffbf9e1784723dc8da1cefdb9f27\n'}, {'number': 2, 'created': '2014-12-07 08:09:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/42b30be00504c5271c032461a102c687f68b1826', 'message': 'Correctly identify stacklevel in ``_extract_engine``\n\nAdd on the needed frame evaluation logic to be able\nto correctly locate where users frames start and ours\nend so that we can correctly report the stack level in\nthe deprecation messages (which the warning module will\nuse to show the location of the users code that is using\nthe deprecated usage).\n\nChange-Id: Iae5919fb0f79ffbf9e1784723dc8da1cefdb9f27\n'}, {'number': 3, 'created': '2014-12-07 08:17:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/69b8c7fe273197d5995265c7cdcef17e45e98891', 'message': 'Correctly identify stacklevel in ``_extract_engine``\n\nAdd on the needed frame evaluation logic to be able\nto correctly locate where users frames start and ours\nend so that we can correctly report the stack level in\nthe deprecation messages (which the warning module will\nuse to show the location of the users code that is using\nthe deprecated usage).\n\nChange-Id: Iae5919fb0f79ffbf9e1784723dc8da1cefdb9f27\n'}, {'number': 4, 'created': '2014-12-07 08:25:36.000000000', 'files': ['taskflow/engines/helpers.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/cd664bdd3b83e544feb3a80c75e4037f5da72d39', 'message': 'Correctly identify stack level in ``_extract_engine``\n\nAdd on the needed frame evaluation logic to be able\nto correctly locate where users frames start and ours\nend so that we can correctly report the stack level in\nthe deprecation messages (which the warning module will\nuse to show the location of the users code that is using\nthe deprecated usage).\n\nChange-Id: Iae5919fb0f79ffbf9e1784723dc8da1cefdb9f27\n'}]",0,139851,cd664bdd3b83e544feb3a80c75e4037f5da72d39,13,2,4,1297,,,0,"Correctly identify stack level in ``_extract_engine``

Add on the needed frame evaluation logic to be able
to correctly locate where users frames start and ours
end so that we can correctly report the stack level in
the deprecation messages (which the warning module will
use to show the location of the users code that is using
the deprecated usage).

Change-Id: Iae5919fb0f79ffbf9e1784723dc8da1cefdb9f27
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/51/139851/2 && git format-patch -1 --stdout FETCH_HEAD,['taskflow/engines/helpers.py'],1,16a8cfe1a6ca52635d396e843a25b14a87b8453a,,"import itertools import traceback# TODO(harlowja): only used during the deprecation cycle, remove it once # ``_extract_engine_compat`` is also gone... _FILE_NAMES = [__file__] if six.PY2: # Due to a bug in py2.x the __file__ may point to the pyc file & since # we are using the traceback module and that module only shows py files # we have to do a slight adjustment to ensure we match correctly... # # This is addressed in https://www.python.org/dev/peps/pep-3147/#file if __file__.endswith(""pyc""): _FILE_NAMES.append(__file__[0:-1]) _FILE_NAMES = tuple(_FILE_NAMES) def _extract_compat(**kwargs): options = {} kind = kwargs.pop('engine', None) engine_conf = kwargs.pop('engine_conf', None) if engine_conf is not None: if isinstance(engine_conf, six.string_types): kind = engine_conf else: options.update(engine_conf) kind = options.pop('engine', None) if not kind: kind = ENGINE_DEFAULT # See if it's a URI and if so, extract any further options... try: uri = misc.parse_uri(kind) except (TypeError, ValueError): pass else: kind = uri.scheme options = misc.merge_uri(uri, options.copy()) # Merge in any leftover **kwargs into the options, this makes it so # that the provided **kwargs override any URI or engine_conf specific # options. options.update(kwargs) return (kind, options) # Figure out where our code ends and the calling code begins (this is # needed since this code is called from two functions in this module, which # means the stack level will vary by one depending on that). finder = itertools.takewhile(lambda frame: frame[0] in _FILE_NAMES, reversed(traceback.extract_stack())) stacklevel = sum(1 for _frame in finder) decorator = deprecation.renamed_kwarg('engine_conf', 'engine', version=""0.6"", removal_version=""?"", # Three is added on since the # decorator adds three of its own # stack levels that we need to hop # out of... stacklevel=stacklevel + 3) return decorator(_extract_compat)(**kwargs)"," @deprecation.renamed_kwarg('engine_conf', 'engine', version=""0.6"", removal_version=""?"", # This is set to none since this function is called # from 2 other functions in this module, both of # which have different stack levels, possibly we # can fix this in the future... stacklevel=None) options = {} kind = kwargs.pop('engine', None) engine_conf = kwargs.pop('engine_conf', None) if engine_conf is not None: if isinstance(engine_conf, six.string_types): kind = engine_conf else: options.update(engine_conf) kind = options.pop('engine', None) if not kind: kind = ENGINE_DEFAULT # See if it's a URI and if so, extract any further options... try: uri = misc.parse_uri(kind) except (TypeError, ValueError): pass else: kind = uri.scheme options = misc.merge_uri(uri, options.copy()) # Merge in any leftover **kwargs into the options, this makes it so that # the provided **kwargs override any URI or engine_conf specific options. options.update(kwargs) return (kind, options)",55,29
openstack%2Fpython-cinderclient~master~I011f0ed1a4ab639f67db6cae580d978c0b44c1bb,openstack/python-cinderclient,master,I011f0ed1a4ab639f67db6cae580d978c0b44c1bb,List all the request items when the list is over osapi_max_limit,MERGED,2014-11-13 08:36:42.000000000,2014-12-07 06:56:18.000000000,2014-12-07 06:56:17.000000000,"[{'_account_id': 3}, {'_account_id': 25}, {'_account_id': 170}, {'_account_id': 1207}, {'_account_id': 1736}, {'_account_id': 2861}, {'_account_id': 6491}, {'_account_id': 7198}, {'_account_id': 11904}]","[{'number': 1, 'created': '2014-11-13 08:36:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/27f65787ca478f0535a074a8021e6d08b233ef43', 'message': 'List all the request items when the list is over osapi_max_limit\n\nConvert the function _list into a loop function, which can retrieve\nthe items from the next link till the limit or the end of items has\nbeen reached.\n\nChange-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb\ncloses-bug: #1342192\n'}, {'number': 2, 'created': '2014-11-13 09:25:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/28d88068f5a8695d52a5fba3fc32a2d19930fca3', 'message': 'List all the request items when the list is over osapi_max_limit\n\nConvert the function _list into a loop function, which can retrieve\nthe items from the next link till the limit or the end of items has\nbeen reached. This works for v2 only.\n\nSo far, only volume list in v2 support limit. The limit parameter\nwork for volume list in v2 only, but other list can extend it in\nfuture work.\n\nChange-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb\ncloses-bug: #1342192\n'}, {'number': 3, 'created': '2014-11-18 04:21:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/99e99148bc87b899401fd68ca9401aef864ba6f2', 'message': 'List all the request items when the list is over osapi_max_limit\n\nConvert the function _list into a loop function, which can retrieve\nthe items from the next link till the limit or the end of items has\nbeen reached. This works for v2 only.\n\nSo far, only volume list in v2 support limit. The limit parameter\nwork for volume list in v2 only, but other list can extend it in\nfuture work.\n\nChange-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb\ncloses-bug: #1342192\n'}, {'number': 4, 'created': '2014-11-18 05:40:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/4d2b73317618924129c9019b9c0db8ef86ea50d0', 'message': 'List all the request items when the list is over osapi_max_limit\n\nConvert the function _list into a loop function, which can retrieve\nthe items from the next link till the limit or the end of items has\nbeen reached. This works for v2 only.\n\nSo far, only volume list in v2 support limit. The limit parameter\nwork for volume list in v2 only, but other list can extend it in\nfuture work.\n\nChange-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb\ncloses-bug: #1342192\n'}, {'number': 5, 'created': '2014-11-19 06:33:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/19c471ada6eb378c034a8194e28bee3668aca430', 'message': 'List all the request items when the list is over osapi_max_limit\n\nConvert the function _list into a loop function, which can retrieve\nthe items from the next link till the limit or the end of items has\nbeen reached. This works for v2 only.\n\nSo far, only volume list in v2 support limit. The limit parameter\nwork for volume list in v2 only, but other list can extend it in\nfuture work.\n\nChange-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb\ncloses-bug: #1342192\n'}, {'number': 6, 'created': '2014-11-24 08:36:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/df3f90de2651040c9bec14147a730e383435a43a', 'message': 'List all the request items when the list is over osapi_max_limit\n\nConvert the function _list into a loop function, which can retrieve\nthe items from the next link till the limit or the end of items has\nbeen reached. This works for v2 only.\n\nSo far, only volume list in v2 support limit. The limit parameter\nwork for volume list in v2 only, but other list can extend it in\nfuture work.\n\nChange-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb\ncloses-bug: #1342192\n'}, {'number': 7, 'created': '2014-11-26 02:17:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/513ecf00c16f355f3429c3d533da0d8c9f822f91', 'message': 'List all the request items when the list is over osapi_max_limit\n\nConvert the function _list into a loop function, which can retrieve\nthe items from the next link till the limit or the end of items has\nbeen reached. This works for v2 only.\n\nSo far, only volume list in v2 support limit. The limit parameter\nwork for volume list in v2 only, but other list can extend it in\nfuture work.\n\nChange-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb\ncloses-bug: #1342192\n'}, {'number': 8, 'created': '2014-11-28 04:29:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/7b24c745cb74bb35b01823cbe3c8363393506640', 'message': 'List all the request items when the list is over osapi_max_limit\n\nConvert the function _list into a loop function, which can retrieve\nthe items from the next link till the limit or the end of items has\nbeen reached. This works for v2 only.\n\nSo far, only volume list in v2 support limit. The limit parameter\nwork for volume list in v2 only, but other list can extend it in\nfuture work.\n\nChange-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb\ncloses-bug: #1342192\n'}, {'number': 9, 'created': '2014-11-28 04:31:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/cf71576464227ded6322e6efc3415680e9f01182', 'message': 'List all the request items when the list is over osapi_max_limit\n\nConvert the function _list into a loop function, which can retrieve\nthe items from the next link till the limit or the end of items has\nbeen reached. This works for v2 only.\n\nSo far, only volume list in v2 support limit. The limit parameter\nwork for volume list in v2 only, but other list can extend it in\nfuture work.\n\nChange-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb\ncloses-bug: #1342192\n'}, {'number': 10, 'created': '2014-12-01 02:43:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/33d28f5da8bbfee54fd0a054ad9a7659826fb6d8', 'message': 'List all the request items when the list is over osapi_max_limit\n\nConvert the function _list into a loop function, which can retrieve\nthe items from the next link till the limit or the end of items has\nbeen reached. This works for v2 only.\n\nSo far, only volume list in v2 support limit. The limit parameter\nwork for volume list in v2 only, but other list can extend it in\nfuture work.\n\nChange-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb\ncloses-bug: #1342192\n'}, {'number': 11, 'created': '2014-12-03 06:04:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/15f6e9eafd9a3965ca121a9f82c5acdbfeab501e', 'message': 'List all the request items when the list is over osapi_max_limit\n\nConvert the function _list into a loop function, which can retrieve\nthe items from the next link till the limit or the end of items has\nbeen reached. This works for v2 only.\n\nSo far, only volume list in v2 support limit. The limit parameter\nwork for volume list in v2 only, but other list can extend it in\nfuture work.\n\nChange-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb\ncloses-bug: #1342192\n'}, {'number': 12, 'created': '2014-12-04 02:20:59.000000000', 'files': ['cinderclient/tests/v2/test_volumes.py', 'cinderclient/base.py', 'cinderclient/tests/v2/fakes.py', 'cinderclient/v2/volumes.py'], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/592099475c5c02d281abe036b0a4fa1d6a0a6473', 'message': 'List all the request items when the list is over osapi_max_limit\n\nConvert the function _list into a loop function, which can retrieve\nthe items from the next link till the limit or the end of items has\nbeen reached. This works for v2 only.\n\nSo far, only volume list in v2 support limit. The limit parameter\nwork for volume list in v2 only, but other list can extend it in\nfuture work.\n\nChange-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb\ncloses-bug: #1342192\n'}]",3,134133,592099475c5c02d281abe036b0a4fa1d6a0a6473,46,9,12,2861,,,0,"List all the request items when the list is over osapi_max_limit

Convert the function _list into a loop function, which can retrieve
the items from the next link till the limit or the end of items has
been reached. This works for v2 only.

So far, only volume list in v2 support limit. The limit parameter
work for volume list in v2 only, but other list can extend it in
future work.

Change-Id: I011f0ed1a4ab639f67db6cae580d978c0b44c1bb
closes-bug: #1342192
",git fetch https://review.opendev.org/openstack/python-cinderclient refs/changes/33/134133/6 && git format-patch -1 --stdout FETCH_HEAD,['cinderclient/base.py'],1,27f65787ca478f0535a074a8021e6d08b233ef43,Bug1342192," def _list(self, url, response_key, obj_class=None, body=None, limit=None, force_get_all=True, items=[]): items_new = [obj_class(self, res, loaded=True) for res in data if res] if limit: margin = limit - len(items) if margin <= len(items_new): # If the limit is reached, return the items. items.extend(items_new[:margin]) return items else: items.extend(items_new) else: items.extend(items_new) # It is possible that the length of the list we request is longer # than osapi_max_limit, so we have to retrieve multiple times to # get the complete list. # force_get_all is used to control if we get the complete list. # It is set to true to indicate a complete list. It is set to false # for the request to retrieve only once. next = None if force_get_all and 'volumes_links' in body: volumes_links = body['volumes_links'] if volumes_links: for volumes_link in volumes_links: if 'rel' in volumes_link and 'next' == volumes_link['rel']: next = volumes_link['href'] break if next: # As long as the 'next' link is not empty, keep request it # till there is no more items. items = self._list(next, response_key, obj_class, None, limit, force_get_all, items) return items"," def _list(self, url, response_key, obj_class=None, body=None): return [obj_class(self, res, loaded=True) for res in data if res]",35,5
openstack%2Fpython-cinderclient~master~I58e362f5a94e2c47a103d6901464df38110d6383,openstack/python-cinderclient,master,I58e362f5a94e2c47a103d6901464df38110d6383,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:44:31.000000000,2014-12-07 06:53:13.000000000,2014-12-07 06:53:12.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 6491}]","[{'number': 1, 'created': '2014-12-05 03:44:31.000000000', 'files': ['README.rst'], 'web_link': 'https://opendev.org/openstack/python-cinderclient/commit/6f66494eccdbd4716a0bae3bb64880714d709622', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I58e362f5a94e2c47a103d6901464df38110d6383\n'}]",0,139370,6f66494eccdbd4716a0bae3bb64880714d709622,9,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I58e362f5a94e2c47a103d6901464df38110d6383
",git fetch https://review.opendev.org/openstack/python-cinderclient refs/changes/70/139370/1 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,6f66494eccdbd4716a0bae3bb64880714d709622,infra-manual,.. _Gerrit: http://docs.openstack.org/infra/manual/developers.html#development-workflow,.. _Gerrit: http://wiki.openstack.org/GerritWorkflow,1,1
openstack%2Fopenstack-manuals~master~I2df17e8a35576ede5db0430b4cde181b4ac14854,openstack/openstack-manuals,master,I2df17e8a35576ede5db0430b4cde181b4ac14854,Imported Translations from Transifex,MERGED,2014-12-07 06:15:29.000000000,2014-12-07 06:36:15.000000000,2014-12-07 06:36:14.000000000,"[{'_account_id': 3}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-12-07 06:15:29.000000000', 'files': ['doc/config-reference/locale/config-reference.pot', 'doc/common/locale/common.pot', 'doc/common/locale/ja.po', 'doc/cli-reference/locale/cli-reference.pot', 'doc/common/locale/fr.po'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/4258867f0a8ce413aa2586ab1fc99e93f185696c', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I2df17e8a35576ede5db0430b4cde181b4ac14854\n'}]",0,139848,4258867f0a8ce413aa2586ab1fc99e93f185696c,6,2,1,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: I2df17e8a35576ede5db0430b4cde181b4ac14854
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/48/139848/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/config-reference/locale/config-reference.pot', 'doc/common/locale/common.pot', 'doc/common/locale/ja.po', 'doc/cli-reference/locale/cli-reference.pot', 'doc/common/locale/fr.po']",5,4258867f0a8ce413aa2586ab1fc99e93f185696c,transifex/translations,"""POT-Creation-Date: 2014-12-06 09:24+0000\n"" ""PO-Revision-Date: 2014-12-06 09:25+0000\n""#: ./doc/common/tables/ironic-redis.xml13(th)#: ./doc/common/tables/ironic-seamicro.xml13(th)#: ./doc/common/tables/ironic-rabbitmq.xml13(th)#: ./doc/common/tables/ironic-swift.xml13(th)#: ./doc/common/tables/ironic-agent.xml13(th)#: ./doc/common/tables/ironic-ca.xml13(th)#: ./doc/common/tables/ironic-rpc.xml13(th)#: ./doc/common/tables/ironic-amqp.xml13(th)#: ./doc/common/tables/ironic-disk_partitioner.xml13(th)#: ./doc/common/tables/ironic-neutron.xml13(th)#: ./doc/common/tables/ironic-auth_token.xml13(th)#: ./doc/common/tables/ironic-glance.xml13(th)#: ./doc/common/tables/ironic-ilo.xml13(th)#: ./doc/common/tables/ironic-qpid.xml13(th)#: ./doc/common/tables/ironic-ssh.xml13(th)#: ./doc/common/tables/ironic-api.xml13(th)#: ./doc/common/tables/ironic-database.xml13(th)#: ./doc/common/tables/ironic-ipmi.xml13(th)#: ./doc/common/tables/ironic-snmp.xml13(th)#: ./doc/common/tables/ironic-policy.xml13(th)#: ./doc/common/tables/ironic-rootwrap.xml13(th)#: ./doc/common/tables/ironic-common.xml13(th)#: ./doc/common/tables/ironic-zeromq.xml13(th)#: ./doc/common/tables/ironic-conductor.xml13(th)#: ./doc/common/tables/ironic-auth.xml13(th)#: ./doc/common/tables/ironic-pxe.xml13(th)#: ./doc/common/tables/ironic-debug.xml13(th) #: ./doc/common/tables/ironic-console.xml13(th)#: ./doc/common/tables/ironic-logging.xml13(th)#: ./doc/common/tables/ironic-dhcp.xml13(th)#: ./doc/common/tables/ironic-rabbitmq.xml18(th)#: ./doc/common/tables/ironic-rpc.xml18(th)#: ./doc/common/tables/ironic-amqp.xml18(th)#: ./doc/common/tables/ironic-qpid.xml18(th)#: ./doc/common/tables/ironic-policy.xml18(th)#: ./doc/common/tables/ironic-rootwrap.xml18(th)#: ./doc/common/tables/ironic-common.xml18(th)#: ./doc/common/tables/ironic-zeromq.xml18(th)#: ./doc/common/tables/ironic-auth.xml18(th)#: ./doc/common/tables/ironic-debug.xml18(th) #: ./doc/common/tables/ironic-logging.xml18(th)#: ./doc/common/tables/ironic-database.xml18(th)#: ./doc/common/tables/ironic-database.xml7(caption)#: ./doc/common/tables/ironic-redis.xml12(th)#: ./doc/common/tables/ironic-seamicro.xml12(th)#: ./doc/common/tables/ironic-rabbitmq.xml12(th)#: ./doc/common/tables/ironic-swift.xml12(th)#: ./doc/common/tables/ironic-agent.xml12(th)#: ./doc/common/tables/ironic-ca.xml12(th)#: ./doc/common/tables/ironic-rpc.xml12(th)#: ./doc/common/tables/ironic-amqp.xml12(th)#: ./doc/common/tables/ironic-disk_partitioner.xml12(th)#: ./doc/common/tables/ironic-neutron.xml12(th)#: ./doc/common/tables/ironic-auth_token.xml12(th)#: ./doc/common/tables/ironic-glance.xml12(th)#: ./doc/common/tables/ironic-ilo.xml12(th)#: ./doc/common/tables/ironic-qpid.xml12(th)#: ./doc/common/tables/ironic-ssh.xml12(th)#: ./doc/common/tables/ironic-api.xml12(th)#: ./doc/common/tables/ironic-database.xml12(th)#: ./doc/common/tables/ironic-ipmi.xml12(th)#: ./doc/common/tables/ironic-snmp.xml12(th)#: ./doc/common/tables/ironic-policy.xml12(th)#: ./doc/common/tables/ironic-rootwrap.xml12(th)#: ./doc/common/tables/ironic-common.xml12(th)#: ./doc/common/tables/ironic-zeromq.xml12(th)#: ./doc/common/tables/ironic-conductor.xml12(th)#: ./doc/common/tables/ironic-auth.xml12(th)#: ./doc/common/tables/ironic-pxe.xml12(th)#: ./doc/common/tables/ironic-debug.xml12(th) #: ./doc/common/tables/ironic-console.xml12(th)#: ./doc/common/tables/ironic-logging.xml12(th)#: ./doc/common/tables/ironic-dhcp.xml12(th)#: ./doc/common/tables/ironic-database.xml21(td)#: ./doc/common/tables/ironic-database.xml22(td)#: ./doc/common/tables/ironic-database.xml25(td)#: ./doc/common/tables/ironic-database.xml26(td)#: ./doc/common/tables/ironic-database.xml29(td)#: ./doc/common/tables/ironic-database.xml30(td)#: ./doc/common/tables/ironic-database.xml33(td)#: ./doc/common/tables/ironic-database.xml34(td)#: ./doc/common/tables/ironic-database.xml37(td)#: ./doc/common/tables/ironic-database.xml38(td)#: ./doc/common/tables/ironic-database.xml41(td)#: ./doc/common/tables/ironic-database.xml42(td)#: ./doc/common/tables/ironic-database.xml45(td)#: ./doc/common/tables/ironic-database.xml46(td)#: ./doc/common/tables/ironic-database.xml49(td)#: ./doc/common/tables/ironic-database.xml50(td)#: ./doc/common/tables/ironic-database.xml53(td)#: ./doc/common/tables/ironic-database.xml54(td)#: ./doc/common/tables/ironic-database.xml57(td)#: ./doc/common/tables/ironic-database.xml58(td)#: ./doc/common/tables/ironic-database.xml61(td)#: ./doc/common/tables/ironic-database.xml62(td)#: ./doc/common/tables/ironic-database.xml65(td)#: ./doc/common/tables/ironic-database.xml69(td)#: ./doc/common/tables/ironic-database.xml70(td)#: ./doc/common/tables/ironic-database.xml77(td)#: ./doc/common/tables/ironic-database.xml78(td)#: ./doc/common/tables/ironic-database.xml81(td)#: ./doc/common/tables/ironic-database.xml82(td)#: ./doc/common/tables/ironic-database.xml85(td)#: ./doc/common/tables/ironic-database.xml86(td)#: ./doc/common/tables/ironic-database.xml89(td)#: ./doc/common/tables/ironic-database.xml90(td)#: ./doc/common/tables/ironic-database.xml93(td)#: ./doc/common/tables/ironic-database.xml94(td)#: ./doc/common/tables/ironic-database.xml97(td)#: ./doc/common/tables/ironic-database.xml98(td)#: ./doc/common/tables/ironic-database.xml101(td)#: ./doc/common/tables/ironic-database.xml102(td)#: ./doc/common/tables/ironic-auth_token.xml105(td)#: ./doc/common/tables/ironic-rpc.xml7(caption)#: ./doc/common/tables/ironic-rpc.xml21(td)#: ./doc/common/tables/ironic-rpc.xml22(td)#: ./doc/common/tables/ironic-rpc.xml25(td)#: ./doc/common/tables/ironic-rpc.xml26(td)#: ./doc/common/tables/ironic-rpc.xml29(td)#: ./doc/common/tables/ironic-rpc.xml30(td)#: ./doc/common/tables/ironic-rpc.xml33(td)#: ./doc/common/tables/ironic-rpc.xml34(td)#: ./doc/common/tables/ironic-rpc.xml37(td)#: ./doc/common/tables/ironic-rpc.xml38(td)#: ./doc/common/tables/ironic-rpc.xml41(td)#: ./doc/common/tables/ironic-rpc.xml42(td)#: ./doc/common/tables/ironic-rpc.xml45(td)#: ./doc/common/tables/ironic-rpc.xml46(td)#: ./doc/common/tables/ironic-common.xml81(td)#: ./doc/common/tables/ironic-common.xml82(td)#: ./doc/common/tables/ironic-redis.xml7(caption) #: ./doc/common/tables/ceilometer-redis.xml7(caption) #: ./doc/common/tables/neutron-redis.xml7(caption) #: ./doc/common/tables/nova-redis.xml7(caption) #: ./doc/common/tables/sahara-redis.xml7(caption) #: ./doc/common/tables/cinder-redis.xml7(caption) #: ./doc/common/tables/glance-redis.xml7(caption) #: ./doc/common/tables/heat-redis.xml7(caption) #: ./doc/common/tables/trove-redis.xml7(caption) #: ./doc/common/tables/keystone-redis.xml7(caption) msgid ""Description of Redis configuration options"" msgstr """" #: ./doc/common/tables/ironic-redis.xml18(th) #: ./doc/common/tables/ceilometer-redis.xml18(th) #: ./doc/common/tables/neutron-redis.xml18(th) #: ./doc/common/tables/nova-redis.xml18(th) #: ./doc/common/tables/sahara-redis.xml18(th) #: ./doc/common/tables/cinder-redis.xml18(th) #: ./doc/common/tables/glance-redis.xml18(th) #: ./doc/common/tables/heat-redis.xml18(th) #: ./doc/common/tables/trove-redis.xml18(th) #: ./doc/common/tables/keystone-redis.xml18(th) msgid ""[matchmaker_redis]"" msgstr """" #: ./doc/common/tables/ironic-redis.xml21(td) #: ./doc/common/tables/ceilometer-redis.xml21(td) #: ./doc/common/tables/neutron-redis.xml21(td) #: ./doc/common/tables/nova-redis.xml21(td) #: ./doc/common/tables/sahara-redis.xml21(td) #: ./doc/common/tables/cinder-redis.xml21(td) #: ./doc/common/tables/glance-redis.xml21(td) #: ./doc/common/tables/heat-redis.xml21(td) #: ./doc/common/tables/trove-redis.xml21(td) #: ./doc/common/tables/neutron-nec.xml37(td) #: ./doc/common/tables/keystone-redis.xml21(td) msgid ""host = 127.0.0.1"" msgstr """" #: ./doc/common/tables/ironic-redis.xml22(td) #: ./doc/common/tables/ceilometer-redis.xml22(td) #: ./doc/common/tables/neutron-redis.xml22(td) #: ./doc/common/tables/nova-redis.xml22(td) #: ./doc/common/tables/sahara-redis.xml22(td) #: ./doc/common/tables/cinder-redis.xml22(td) #: ./doc/common/tables/glance-redis.xml22(td) #: ./doc/common/tables/heat-redis.xml22(td) #: ./doc/common/tables/keystone-redis.xml22(td) msgid ""(StrOpt) Host to locate redis."" msgstr """" #: ./doc/common/tables/ironic-redis.xml25(td) #: ./doc/common/tables/ceilometer-redis.xml25(td) #: ./doc/common/tables/neutron-redis.xml25(td) #: ./doc/common/tables/nova-redis.xml25(td) #: ./doc/common/tables/sahara-redis.xml25(td) #: ./doc/common/tables/keystone-ldap.xml101(td) #: ./doc/common/tables/cinder-redis.xml25(td) #: ./doc/common/tables/glance-redis.xml25(td) #: ./doc/common/tables/heat-redis.xml25(td) #: ./doc/common/tables/neutron-ml2_ncs.xml21(td) #: ./doc/common/tables/neutron-ml2_odl.xml21(td) #: ./doc/common/tables/trove-redis.xml25(td) #: ./doc/common/tables/keystone-redis.xml25(td) msgid ""password = None"" msgstr ""password = None"" #: ./doc/common/tables/ironic-redis.xml26(td) #: ./doc/common/tables/ceilometer-redis.xml26(td) #: ./doc/common/tables/neutron-redis.xml26(td) #: ./doc/common/tables/nova-redis.xml26(td) #: ./doc/common/tables/sahara-redis.xml26(td) #: ./doc/common/tables/cinder-redis.xml26(td) #: ./doc/common/tables/glance-redis.xml26(td) #: ./doc/common/tables/heat-redis.xml26(td) #: ./doc/common/tables/keystone-redis.xml26(td) msgid ""(StrOpt) Password for Redis server (optional)."" msgstr """" #: ./doc/common/tables/ironic-redis.xml29(td) #: ./doc/common/tables/ceilometer-redis.xml29(td) #: ./doc/common/tables/neutron-redis.xml29(td) #: ./doc/common/tables/nova-redis.xml29(td) #: ./doc/common/tables/sahara-redis.xml29(td) #: ./doc/common/tables/cinder-redis.xml29(td) #: ./doc/common/tables/glance-redis.xml29(td) #: ./doc/common/tables/heat-redis.xml29(td) #: ./doc/common/tables/trove-redis.xml29(td) #: ./doc/common/tables/keystone-redis.xml29(td) msgid ""port = 6379"" msgstr """" #: ./doc/common/tables/ironic-redis.xml30(td) #: ./doc/common/tables/ceilometer-redis.xml30(td) #: ./doc/common/tables/neutron-redis.xml30(td) #: ./doc/common/tables/nova-redis.xml30(td) #: ./doc/common/tables/sahara-redis.xml30(td) #: ./doc/common/tables/cinder-redis.xml30(td) #: ./doc/common/tables/glance-redis.xml30(td) #: ./doc/common/tables/heat-redis.xml30(td) #: ./doc/common/tables/trove-redis.xml30(td) #: ./doc/common/tables/keystone-redis.xml30(td) msgid ""(IntOpt) Use this port to connect to redis host."" msgstr """" #: ./doc/common/tables/ironic-redis.xml33(th) #: ./doc/common/tables/ceilometer-redis.xml33(th) #: ./doc/common/tables/neutron-redis.xml33(th) #: ./doc/common/tables/nova-redis.xml33(th) #: ./doc/common/tables/sahara-redis.xml33(th) #: ./doc/common/tables/cinder-redis.xml33(th) #: ./doc/common/tables/glance-redis.xml33(th) #: ./doc/common/tables/heat-redis.xml33(th) #: ./doc/common/tables/trove-redis.xml33(th) #: ./doc/common/tables/keystone-redis.xml33(th) msgid ""[matchmaker_ring]"" msgstr ""[matchmaker_ring]"" #: ./doc/common/tables/ironic-redis.xml36(td) #: ./doc/common/tables/ceilometer-redis.xml36(td) #: ./doc/common/tables/neutron-redis.xml36(td) #: ./doc/common/tables/nova-redis.xml36(td) #: ./doc/common/tables/sahara-redis.xml36(td) #: ./doc/common/tables/cinder-redis.xml36(td) #: ./doc/common/tables/glance-redis.xml36(td) #: ./doc/common/tables/heat-redis.xml36(td) #: ./doc/common/tables/trove-redis.xml36(td) #: ./doc/common/tables/keystone-redis.xml36(td) msgid ""ringfile = /etc/oslo/matchmaker_ring.json"" msgstr ""ringfile = /etc/oslo/matchmaker_ring.json"" #: ./doc/common/tables/ironic-redis.xml37(td) #: ./doc/common/tables/ceilometer-redis.xml37(td) #: ./doc/common/tables/neutron-redis.xml37(td) #: ./doc/common/tables/nova-redis.xml37(td) #: ./doc/common/tables/sahara-redis.xml37(td) #: ./doc/common/tables/cinder-redis.xml37(td) #: ./doc/common/tables/glance-redis.xml37(td) #: ./doc/common/tables/heat-redis.xml37(td) #: ./doc/common/tables/keystone-redis.xml37(td) msgid ""(StrOpt) Matchmaker ring file (JSON)."" msgstr """" #: ./doc/common/tables/ironic-auth.xml7(caption)#: ./doc/common/tables/ironic-policy.xml7(caption)#: ./doc/common/tables/ironic-policy.xml21(td)#: ./doc/common/tables/ironic-policy.xml25(td)#: ./doc/common/tables/ironic-auth_token.xml7(caption)#: ./doc/common/tables/ironic-auth_token.xml18(th)#: ./doc/common/tables/ironic-auth_token.xml21(td)#: ./doc/common/tables/ironic-auth_token.xml22(td)#: ./doc/common/tables/ironic-auth_token.xml25(td)#: ./doc/common/tables/ironic-auth_token.xml26(td)#: ./doc/common/tables/ironic-auth_token.xml29(td)#: ./doc/common/tables/ironic-auth_token.xml30(td)#: ./doc/common/tables/ironic-auth_token.xml33(td)#: ./doc/common/tables/ironic-auth_token.xml34(td)#: ./doc/common/tables/ironic-auth_token.xml37(td)#: ./doc/common/tables/ironic-auth_token.xml38(td)#: ./doc/common/tables/ironic-auth_token.xml41(td)#: ./doc/common/tables/ironic-auth_token.xml42(td)#: ./doc/common/tables/ironic-auth_token.xml45(td)#: ./doc/common/tables/ironic-auth_token.xml46(td)#: ./doc/common/tables/ironic-auth_token.xml49(td)#: ./doc/common/tables/ironic-auth_token.xml50(td)#: ./doc/common/tables/ironic-auth_token.xml53(td)#: ./doc/common/tables/ironic-auth_token.xml54(td)#: ./doc/common/tables/ironic-auth_token.xml57(td)#: ./doc/common/tables/ironic-auth_token.xml58(td)#: ./doc/common/tables/ironic-auth_token.xml61(td)#: ./doc/common/tables/ironic-auth_token.xml62(td)#: ./doc/common/tables/ironic-auth_token.xml65(td)#: ./doc/common/tables/ironic-auth_token.xml66(td)#: ./doc/common/tables/ironic-auth_token.xml69(td)#: ./doc/common/tables/ironic-auth_token.xml70(td) #: ./doc/common/tables/ironic-auth_token.xml110(td)#: ./doc/common/tables/ironic-auth_token.xml73(td)#: ./doc/common/tables/ironic-auth_token.xml74(td)#: ./doc/common/tables/ironic-auth_token.xml77(td)#: ./doc/common/tables/ironic-auth_token.xml78(td)#: ./doc/common/tables/ironic-auth_token.xml81(td)#: ./doc/common/tables/ironic-auth_token.xml82(td)#: ./doc/common/tables/ironic-auth_token.xml85(td)#: ./doc/common/tables/ironic-auth_token.xml86(td)#: ./doc/common/tables/ironic-auth_token.xml89(td)#: ./doc/common/tables/ironic-auth_token.xml90(td)#: ./doc/common/tables/ironic-auth_token.xml93(td)#: ./doc/common/tables/ironic-auth_token.xml94(td)#: ./doc/common/tables/ironic-auth_token.xml97(td)#: ./doc/common/tables/ironic-auth_token.xml98(td)#: ./doc/common/tables/ironic-auth_token.xml101(td)#: ./doc/common/tables/ironic-auth_token.xml102(td)#: ./doc/common/tables/ironic-auth_token.xml106(td)#: ./doc/common/tables/ironic-auth_token.xml109(td)#: ./doc/common/tables/ironic-auth_token.xml133(td)#: ./doc/common/tables/ironic-auth_token.xml134(td)#: ./doc/common/tables/ironic-auth_token.xml137(td)#: ./doc/common/tables/ironic-auth_token.xml138(td)#: ./doc/common/tables/ironic-auth_token.xml149(td)#: ./doc/common/tables/ironic-auth_token.xml150(td)#: ./doc/common/tables/ironic-auth_token.xml153(td)#: ./doc/common/tables/ironic-auth_token.xml154(td)#: ./doc/common/tables/ironic-auth_token.xml157(td)#: ./doc/common/tables/ironic-auth_token.xml158(td)#: ./doc/common/tables/ironic-seamicro.xml7(caption) msgid ""Description of SeaMicro configuration options"" msgstr """" #: ./doc/common/tables/ironic-seamicro.xml18(th) msgid ""[seamicro]"" msgstr """" #: ./doc/common/tables/ironic-seamicro.xml21(td) msgid ""action_timeout = 10"" msgstr """" #: ./doc/common/tables/ironic-seamicro.xml22(td) #: ./doc/common/tables/ironic-snmp.xml22(td) msgid ""(IntOpt) Seconds to wait for power action to be completed"" msgstr """" #: ./doc/common/tables/ironic-seamicro.xml25(td) msgid ""max_retry = 3"" msgstr """" #: ./doc/common/tables/ironic-seamicro.xml26(td) msgid ""(IntOpt) Maximum retries for SeaMicro operations"" msgstr """" #: ./doc/common/tables/ironic-ilo.xml25(td)#: ./doc/common/tables/ironic-amqp.xml7(caption)#: ./doc/common/tables/ironic-amqp.xml21(td)#: ./doc/common/tables/ironic-amqp.xml25(td)#: ./doc/common/tables/ironic-amqp.xml30(td)#: ./doc/common/tables/ironic-amqp.xml33(td)#: ./doc/common/tables/ironic-amqp.xml34(td)#: ./doc/common/tables/ironic-amqp.xml37(td)#: ./doc/common/tables/ironic-amqp.xml38(td)#: ./doc/common/tables/ironic-amqp.xml41(td)#: ./doc/common/tables/ironic-amqp.xml42(td)#: ./doc/common/tables/ironic-qpid.xml7(caption)#: ./doc/common/tables/ironic-qpid.xml21(td)#: ./doc/common/tables/ironic-qpid.xml22(td)#: ./doc/common/tables/ironic-qpid.xml25(td)#: ./doc/common/tables/ironic-qpid.xml26(td)#: ./doc/common/tables/ironic-qpid.xml29(td)#: ./doc/common/tables/ironic-qpid.xml30(td)#: ./doc/common/tables/ironic-qpid.xml33(td)#: ./doc/common/tables/ironic-qpid.xml34(td)#: ./doc/common/tables/ironic-qpid.xml37(td)#: ./doc/common/tables/ironic-qpid.xml38(td)#: ./doc/common/tables/ironic-qpid.xml41(td)#: ./doc/common/tables/ironic-qpid.xml42(td)#: ./doc/common/tables/ironic-qpid.xml45(td)#: ./doc/common/tables/ironic-qpid.xml46(td)#: ./doc/common/tables/ironic-qpid.xml49(td)#: ./doc/common/tables/ironic-qpid.xml50(td)#: ./doc/common/tables/ironic-qpid.xml53(td)#: ./doc/common/tables/ironic-qpid.xml54(td)#: ./doc/common/tables/ironic-qpid.xml57(td)#: ./doc/common/tables/ironic-qpid.xml58(td)#: ./doc/common/tables/ironic-qpid.xml61(td)#: ./doc/common/tables/ironic-qpid.xml62(td)#: ./doc/common/tables/ironic-ca.xml7(caption)#: ./doc/common/tables/ironic-common.xml7(caption)#: ./doc/common/tables/ironic-common.xml45(td)#: ./doc/common/tables/ironic-auth_token.xml145(td)#: ./doc/common/tables/ironic-common.xml57(td)#: ./doc/common/tables/ironic-common.xml58(td)#: ./doc/common/tables/ironic-auth_token.xml146(td)#: ./doc/common/tables/ironic-debug.xml7(caption) #: ./doc/common/tables/ironic-logging.xml7(caption)#: ./doc/common/tables/ironic-logging.xml21(td)#: ./doc/common/tables/ironic-logging.xml22(td)#: ./doc/common/tables/ironic-logging.xml26(td)#: ./doc/common/tables/ironic-common.xml29(td)#: ./doc/common/tables/ironic-common.xml30(td)#: ./doc/common/tables/ironic-logging.xml33(td)#: ./doc/common/tables/ironic-logging.xml34(td)#: ./doc/common/tables/ironic-logging.xml37(td)#: ./doc/common/tables/ironic-logging.xml38(td)#: ./doc/common/tables/ironic-logging.xml41(td)#: ./doc/common/tables/ironic-logging.xml42(td)#: ./doc/common/tables/ironic-logging.xml45(td)#: ./doc/common/tables/ironic-logging.xml46(td)#: ./doc/common/tables/ironic-logging.xml49(td)#: ./doc/common/tables/ironic-logging.xml50(td)#: ./doc/common/tables/ironic-logging.xml53(td)#: ./doc/common/tables/ironic-logging.xml54(td)#: ./doc/common/tables/ironic-logging.xml57(td)#: ./doc/common/tables/ironic-logging.xml58(td)#: ./doc/common/tables/ironic-logging.xml61(td)#: ./doc/common/tables/ironic-logging.xml62(td)#: ./doc/common/tables/ironic-logging.xml65(td)#: ./doc/common/tables/ironic-logging.xml66(td)#: ./doc/common/tables/ironic-logging.xml69(td)#: ./doc/common/tables/ironic-logging.xml70(td)#: ./doc/common/tables/ironic-logging.xml73(td)#: ./doc/common/tables/ironic-logging.xml74(td)#: ./doc/common/tables/ironic-logging.xml77(td)#: ./doc/common/tables/ironic-logging.xml78(td)#: ./doc/common/tables/ironic-logging.xml81(td)#: ./doc/common/tables/ironic-logging.xml82(td)#: ./doc/common/tables/ironic-logging.xml85(td)#: ./doc/common/tables/ironic-logging.xml86(td)#: ./doc/common/tables/ironic-rootwrap.xml29(td)#: ./doc/common/tables/ironic-logging.xml89(td)#: ./doc/common/tables/ironic-logging.xml90(td)#: ./doc/common/tables/ironic-logging.xml93(td)#: ./doc/common/tables/ironic-logging.xml94(td)#: ./doc/common/tables/ironic-logging.xml97(td)#: ./doc/common/tables/ironic-logging.xml98(td)#: ./doc/common/tables/ironic-common.xml69(td)#: ./doc/common/tables/ironic-common.xml70(td)#: ./doc/common/tables/ironic-zeromq.xml7(caption)#: ./doc/common/tables/ironic-zeromq.xml21(td)#: ./doc/common/tables/ironic-zeromq.xml22(td)#: ./doc/common/tables/ironic-zeromq.xml25(td)#: ./doc/common/tables/ironic-zeromq.xml26(td)#: ./doc/common/tables/ironic-zeromq.xml29(td)#: ./doc/common/tables/ironic-zeromq.xml30(td)#: ./doc/common/tables/ironic-zeromq.xml33(td)#: ./doc/common/tables/ironic-zeromq.xml34(td)#: ./doc/common/tables/ironic-zeromq.xml37(td)#: ./doc/common/tables/ironic-zeromq.xml38(td)#: ./doc/common/tables/ironic-zeromq.xml41(td)#: ./doc/common/tables/ironic-zeromq.xml42(td)#: ./doc/common/tables/ironic-zeromq.xml45(td)#: ./doc/common/tables/ironic-zeromq.xml46(td)#: ./doc/common/tables/ironic-rabbitmq.xml7(caption) #: ./doc/common/tables/nova-rabbitmq.xml7(caption) #: ./doc/common/tables/cinder-rabbitmq.xml7(caption) #: ./doc/common/tables/glance-rabbitmq.xml7(caption) #: ./doc/common/tables/heat-rabbitmq.xml7(caption) #: ./doc/common/tables/trove-rabbitmq.xml7(caption) #: ./doc/common/tables/keystone-rabbitmq.xml7(caption) #: ./doc/common/tables/sahara-rabbitmq.xml7(caption) #: ./doc/common/tables/neutron-rabbitmq.xml7(caption) #: ./doc/common/tables/ceilometer-rabbitmq.xml7(caption) msgid ""Description of RabbitMQ configuration options"" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml21(td) #: ./doc/common/tables/nova-rabbitmq.xml21(td) #: ./doc/common/tables/cinder-rabbitmq.xml21(td) #: ./doc/common/tables/glance-rabbitmq.xml21(td) #: ./doc/common/tables/heat-rabbitmq.xml21(td) #: ./doc/common/tables/keystone-rabbitmq.xml21(td) #: ./doc/common/tables/sahara-rabbitmq.xml21(td) #: ./doc/common/tables/neutron-rabbitmq.xml21(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml21(td) msgid ""kombu_reconnect_delay = 1.0"" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml22(td) #: ./doc/common/tables/nova-rabbitmq.xml22(td) #: ./doc/common/tables/cinder-rabbitmq.xml22(td) #: ./doc/common/tables/heat-conf-changes.xml37(td) #: ./doc/common/tables/neutron-conf-changes.xml81(td) #: ./doc/common/tables/glance-rabbitmq.xml22(td) #: ./doc/common/tables/heat-rabbitmq.xml22(td) #: ./doc/common/tables/keystone-rabbitmq.xml22(td) #: ./doc/common/tables/sahara-rabbitmq.xml22(td) #: ./doc/common/tables/neutron-rabbitmq.xml22(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml22(td) msgid """" ""(FloatOpt) How long to wait before reconnecting in response to an AMQP "" ""consumer cancel notification."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml25(td) #: ./doc/common/tables/nova-rabbitmq.xml25(td) #: ./doc/common/tables/cinder-rabbitmq.xml25(td) #: ./doc/common/tables/glance-rabbitmq.xml25(td) #: ./doc/common/tables/heat-rabbitmq.xml25(td) #: ./doc/common/tables/trove-rabbitmq.xml21(td) #: ./doc/common/tables/keystone-rabbitmq.xml25(td) #: ./doc/common/tables/sahara-rabbitmq.xml25(td) #: ./doc/common/tables/neutron-rabbitmq.xml25(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml25(td) msgid ""kombu_ssl_ca_certs ="" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml26(td) #: ./doc/common/tables/nova-rabbitmq.xml26(td) #: ./doc/common/tables/cinder-rabbitmq.xml26(td) #: ./doc/common/tables/glance-rabbitmq.xml26(td) #: ./doc/common/tables/heat-rabbitmq.xml26(td) #: ./doc/common/tables/keystone-rabbitmq.xml26(td) #: ./doc/common/tables/sahara-rabbitmq.xml26(td) #: ./doc/common/tables/neutron-rabbitmq.xml26(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml26(td) msgid ""(StrOpt) SSL certification authority file (valid only if SSL enabled)."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml29(td) #: ./doc/common/tables/nova-rabbitmq.xml29(td) #: ./doc/common/tables/cinder-rabbitmq.xml29(td) #: ./doc/common/tables/glance-rabbitmq.xml29(td) #: ./doc/common/tables/heat-rabbitmq.xml29(td) #: ./doc/common/tables/trove-rabbitmq.xml25(td) #: ./doc/common/tables/keystone-rabbitmq.xml29(td) #: ./doc/common/tables/sahara-rabbitmq.xml29(td) #: ./doc/common/tables/neutron-rabbitmq.xml29(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml29(td) msgid ""kombu_ssl_certfile ="" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml30(td) #: ./doc/common/tables/nova-rabbitmq.xml30(td) #: ./doc/common/tables/cinder-rabbitmq.xml30(td) #: ./doc/common/tables/glance-rabbitmq.xml30(td) #: ./doc/common/tables/heat-rabbitmq.xml30(td) #: ./doc/common/tables/keystone-rabbitmq.xml30(td) #: ./doc/common/tables/sahara-rabbitmq.xml30(td) #: ./doc/common/tables/neutron-rabbitmq.xml30(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml30(td) msgid ""(StrOpt) SSL cert file (valid only if SSL enabled)."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml33(td) #: ./doc/common/tables/nova-rabbitmq.xml33(td) #: ./doc/common/tables/cinder-rabbitmq.xml33(td) #: ./doc/common/tables/glance-rabbitmq.xml33(td) #: ./doc/common/tables/heat-rabbitmq.xml33(td) #: ./doc/common/tables/trove-rabbitmq.xml29(td) #: ./doc/common/tables/keystone-rabbitmq.xml33(td) #: ./doc/common/tables/sahara-rabbitmq.xml33(td) #: ./doc/common/tables/neutron-rabbitmq.xml33(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml33(td) msgid ""kombu_ssl_keyfile ="" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml34(td) #: ./doc/common/tables/nova-rabbitmq.xml34(td) #: ./doc/common/tables/cinder-rabbitmq.xml34(td) #: ./doc/common/tables/glance-rabbitmq.xml34(td) #: ./doc/common/tables/heat-rabbitmq.xml34(td) #: ./doc/common/tables/keystone-rabbitmq.xml34(td) #: ./doc/common/tables/sahara-rabbitmq.xml34(td) #: ./doc/common/tables/neutron-rabbitmq.xml34(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml34(td) msgid ""(StrOpt) SSL key file (valid only if SSL enabled)."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml37(td) #: ./doc/common/tables/nova-rabbitmq.xml37(td) #: ./doc/common/tables/cinder-rabbitmq.xml37(td) #: ./doc/common/tables/glance-rabbitmq.xml37(td) #: ./doc/common/tables/heat-rabbitmq.xml37(td) #: ./doc/common/tables/trove-rabbitmq.xml33(td) #: ./doc/common/tables/keystone-rabbitmq.xml37(td) #: ./doc/common/tables/sahara-rabbitmq.xml37(td) #: ./doc/common/tables/neutron-rabbitmq.xml37(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml37(td) msgid ""kombu_ssl_version ="" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml38(td) msgid """" ""(StrOpt) SSL version to use (valid only if SSL enabled). valid values are "" ""TLSv1 and SSLv23. SSLv2 and SSLv3 may be available on some distributions."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml41(td) #: ./doc/common/tables/nova-rabbitmq.xml41(td) #: ./doc/common/tables/cinder-rabbitmq.xml41(td) #: ./doc/common/tables/glance-rabbitmq.xml41(td) #: ./doc/common/tables/heat-rabbitmq.xml41(td) #: ./doc/common/tables/trove-rabbitmq.xml37(td) #: ./doc/common/tables/keystone-rabbitmq.xml41(td) #: ./doc/common/tables/sahara-rabbitmq.xml41(td) #: ./doc/common/tables/neutron-rabbitmq.xml41(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml41(td) msgid ""rabbit_ha_queues = False"" msgstr ""rabbit_ha_queues = False"" #: ./doc/common/tables/ironic-rabbitmq.xml42(td) #: ./doc/common/tables/nova-rabbitmq.xml42(td) #: ./doc/common/tables/cinder-rabbitmq.xml42(td) #: ./doc/common/tables/glance-rabbitmq.xml42(td) #: ./doc/common/tables/heat-rabbitmq.xml42(td) #: ./doc/common/tables/keystone-rabbitmq.xml42(td) #: ./doc/common/tables/sahara-rabbitmq.xml42(td) #: ./doc/common/tables/neutron-rabbitmq.xml42(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml42(td) msgid """" ""(BoolOpt) Use HA queues in RabbitMQ (x-ha-policy: all). If you change this "" ""option, you must wipe the RabbitMQ database."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml45(td) #: ./doc/common/tables/nova-rabbitmq.xml45(td) #: ./doc/common/tables/cinder-rabbitmq.xml45(td) #: ./doc/common/tables/glance-rabbitmq.xml45(td) #: ./doc/common/tables/heat-rabbitmq.xml45(td) #: ./doc/common/tables/trove-rabbitmq.xml41(td) #: ./doc/common/tables/keystone-rabbitmq.xml45(td) #: ./doc/common/tables/sahara-rabbitmq.xml45(td) #: ./doc/common/tables/neutron-rabbitmq.xml45(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml45(td) msgid ""rabbit_host = localhost"" msgstr ""rabbit_host = localhost"" #: ./doc/common/tables/ironic-rabbitmq.xml46(td) #: ./doc/common/tables/nova-rabbitmq.xml46(td) #: ./doc/common/tables/cinder-rabbitmq.xml46(td) #: ./doc/common/tables/glance-rabbitmq.xml46(td) #: ./doc/common/tables/heat-rabbitmq.xml46(td) #: ./doc/common/tables/keystone-rabbitmq.xml46(td) #: ./doc/common/tables/sahara-rabbitmq.xml46(td) #: ./doc/common/tables/neutron-rabbitmq.xml46(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml46(td) msgid ""(StrOpt) The RabbitMQ broker address where a single node is used."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml49(td) #: ./doc/common/tables/nova-rabbitmq.xml49(td) #: ./doc/common/tables/cinder-rabbitmq.xml49(td) #: ./doc/common/tables/glance-rabbitmq.xml49(td) #: ./doc/common/tables/heat-rabbitmq.xml49(td) #: ./doc/common/tables/trove-rabbitmq.xml45(td) #: ./doc/common/tables/keystone-rabbitmq.xml49(td) #: ./doc/common/tables/sahara-rabbitmq.xml49(td) #: ./doc/common/tables/neutron-rabbitmq.xml49(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml49(td) msgid ""rabbit_hosts = $rabbit_host:$rabbit_port"" msgstr ""rabbit_hosts = $rabbit_host:$rabbit_port"" #: ./doc/common/tables/ironic-rabbitmq.xml50(td) #: ./doc/common/tables/nova-rabbitmq.xml50(td) #: ./doc/common/tables/cinder-rabbitmq.xml50(td) #: ./doc/common/tables/glance-rabbitmq.xml50(td) #: ./doc/common/tables/heat-rabbitmq.xml50(td) #: ./doc/common/tables/keystone-rabbitmq.xml50(td) #: ./doc/common/tables/sahara-rabbitmq.xml50(td) #: ./doc/common/tables/neutron-rabbitmq.xml50(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml50(td) msgid ""(ListOpt) RabbitMQ HA cluster host:port pairs."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml53(td) #: ./doc/common/tables/nova-rabbitmq.xml53(td) #: ./doc/common/tables/cinder-rabbitmq.xml53(td) #: ./doc/common/tables/glance-rabbitmq.xml53(td) #: ./doc/common/tables/heat-rabbitmq.xml53(td) #: ./doc/common/tables/keystone-rabbitmq.xml53(td) #: ./doc/common/tables/sahara-rabbitmq.xml53(td) #: ./doc/common/tables/neutron-rabbitmq.xml53(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml53(td) msgid ""rabbit_login_method = AMQPLAIN"" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml54(td) msgid ""(StrOpt) The RabbitMQ login method."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml57(td) #: ./doc/common/tables/nova-rabbitmq.xml57(td) #: ./doc/common/tables/cinder-rabbitmq.xml57(td) #: ./doc/common/tables/glance-rabbitmq.xml57(td) #: ./doc/common/tables/heat-rabbitmq.xml57(td) #: ./doc/common/tables/trove-rabbitmq.xml49(td) #: ./doc/common/tables/keystone-rabbitmq.xml57(td) #: ./doc/common/tables/sahara-rabbitmq.xml57(td) #: ./doc/common/tables/neutron-rabbitmq.xml57(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml57(td) msgid ""rabbit_max_retries = 0"" msgstr ""rabbit_max_retries = 0"" #: ./doc/common/tables/ironic-rabbitmq.xml58(td) #: ./doc/common/tables/nova-rabbitmq.xml58(td) #: ./doc/common/tables/cinder-rabbitmq.xml58(td) #: ./doc/common/tables/glance-rabbitmq.xml58(td) #: ./doc/common/tables/heat-rabbitmq.xml58(td) #: ./doc/common/tables/keystone-rabbitmq.xml58(td) #: ./doc/common/tables/sahara-rabbitmq.xml58(td) #: ./doc/common/tables/neutron-rabbitmq.xml58(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml58(td) msgid """" ""(IntOpt) Maximum number of RabbitMQ connection retries. Default is 0 "" ""(infinite retry count)."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml61(td) #: ./doc/common/tables/nova-rabbitmq.xml61(td) #: ./doc/common/tables/cinder-rabbitmq.xml61(td) #: ./doc/common/tables/glance-rabbitmq.xml61(td) #: ./doc/common/tables/heat-rabbitmq.xml61(td) #: ./doc/common/tables/trove-rabbitmq.xml53(td) #: ./doc/common/tables/keystone-rabbitmq.xml61(td) #: ./doc/common/tables/sahara-rabbitmq.xml61(td) #: ./doc/common/tables/neutron-rabbitmq.xml61(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml61(td) msgid ""rabbit_password = guest"" msgstr ""rabbit_password = guest"" #: ./doc/common/tables/ironic-rabbitmq.xml62(td) #: ./doc/common/tables/nova-rabbitmq.xml62(td) #: ./doc/common/tables/cinder-rabbitmq.xml62(td) #: ./doc/common/tables/glance-rabbitmq.xml62(td) #: ./doc/common/tables/heat-rabbitmq.xml62(td) #: ./doc/common/tables/keystone-rabbitmq.xml62(td) #: ./doc/common/tables/sahara-rabbitmq.xml62(td) #: ./doc/common/tables/neutron-rabbitmq.xml62(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml62(td) msgid ""(StrOpt) The RabbitMQ password."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml65(td) #: ./doc/common/tables/nova-rabbitmq.xml65(td) #: ./doc/common/tables/cinder-rabbitmq.xml65(td) #: ./doc/common/tables/glance-rabbitmq.xml65(td) #: ./doc/common/tables/heat-rabbitmq.xml65(td) #: ./doc/common/tables/trove-rabbitmq.xml57(td) #: ./doc/common/tables/keystone-rabbitmq.xml65(td) #: ./doc/common/tables/sahara-rabbitmq.xml65(td) #: ./doc/common/tables/neutron-rabbitmq.xml65(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml65(td) msgid ""rabbit_port = 5672"" msgstr ""rabbit_port = 5672"" #: ./doc/common/tables/ironic-rabbitmq.xml66(td) #: ./doc/common/tables/nova-rabbitmq.xml66(td) #: ./doc/common/tables/cinder-rabbitmq.xml66(td) #: ./doc/common/tables/glance-rabbitmq.xml66(td) #: ./doc/common/tables/heat-rabbitmq.xml66(td) #: ./doc/common/tables/keystone-rabbitmq.xml66(td) #: ./doc/common/tables/sahara-rabbitmq.xml66(td) #: ./doc/common/tables/neutron-rabbitmq.xml66(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml66(td) msgid ""(IntOpt) The RabbitMQ broker port where a single node is used."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml69(td) #: ./doc/common/tables/nova-rabbitmq.xml69(td) #: ./doc/common/tables/cinder-rabbitmq.xml69(td) #: ./doc/common/tables/glance-rabbitmq.xml69(td) #: ./doc/common/tables/heat-rabbitmq.xml69(td) #: ./doc/common/tables/trove-rabbitmq.xml61(td) #: ./doc/common/tables/keystone-rabbitmq.xml69(td) #: ./doc/common/tables/sahara-rabbitmq.xml69(td) #: ./doc/common/tables/neutron-rabbitmq.xml69(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml69(td) msgid ""rabbit_retry_backoff = 2"" msgstr ""rabbit_retry_backoff = 2"" #: ./doc/common/tables/ironic-rabbitmq.xml70(td) #: ./doc/common/tables/nova-rabbitmq.xml70(td) #: ./doc/common/tables/cinder-rabbitmq.xml70(td) #: ./doc/common/tables/glance-rabbitmq.xml70(td) #: ./doc/common/tables/heat-rabbitmq.xml70(td) #: ./doc/common/tables/keystone-rabbitmq.xml70(td) #: ./doc/common/tables/sahara-rabbitmq.xml70(td) #: ./doc/common/tables/neutron-rabbitmq.xml70(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml70(td) msgid """" ""(IntOpt) How long to backoff for between retries when connecting to "" ""RabbitMQ."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml73(td) #: ./doc/common/tables/nova-rabbitmq.xml73(td) #: ./doc/common/tables/cinder-rabbitmq.xml73(td) #: ./doc/common/tables/glance-rabbitmq.xml73(td) #: ./doc/common/tables/heat-rabbitmq.xml73(td) #: ./doc/common/tables/trove-rabbitmq.xml65(td) #: ./doc/common/tables/keystone-rabbitmq.xml73(td) #: ./doc/common/tables/sahara-rabbitmq.xml73(td) #: ./doc/common/tables/neutron-rabbitmq.xml73(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml73(td) msgid ""rabbit_retry_interval = 1"" msgstr ""rabbit_retry_interval = 1"" #: ./doc/common/tables/ironic-rabbitmq.xml74(td) #: ./doc/common/tables/nova-rabbitmq.xml74(td) #: ./doc/common/tables/cinder-rabbitmq.xml74(td) #: ./doc/common/tables/glance-rabbitmq.xml74(td) #: ./doc/common/tables/heat-rabbitmq.xml74(td) #: ./doc/common/tables/keystone-rabbitmq.xml74(td) #: ./doc/common/tables/sahara-rabbitmq.xml74(td) #: ./doc/common/tables/neutron-rabbitmq.xml74(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml74(td) msgid ""(IntOpt) How frequently to retry connecting with RabbitMQ."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml77(td) #: ./doc/common/tables/nova-rabbitmq.xml77(td) #: ./doc/common/tables/cinder-rabbitmq.xml77(td) #: ./doc/common/tables/glance-rabbitmq.xml77(td) #: ./doc/common/tables/heat-rabbitmq.xml77(td) #: ./doc/common/tables/trove-rabbitmq.xml69(td) #: ./doc/common/tables/keystone-rabbitmq.xml77(td) #: ./doc/common/tables/sahara-rabbitmq.xml77(td) #: ./doc/common/tables/neutron-rabbitmq.xml77(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml77(td) msgid ""rabbit_use_ssl = False"" msgstr ""rabbit_use_ssl = False"" #: ./doc/common/tables/ironic-rabbitmq.xml78(td) #: ./doc/common/tables/nova-rabbitmq.xml78(td) #: ./doc/common/tables/cinder-rabbitmq.xml78(td) #: ./doc/common/tables/glance-rabbitmq.xml78(td) #: ./doc/common/tables/heat-rabbitmq.xml78(td) #: ./doc/common/tables/keystone-rabbitmq.xml78(td) #: ./doc/common/tables/sahara-rabbitmq.xml78(td) #: ./doc/common/tables/neutron-rabbitmq.xml78(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml78(td) msgid ""(BoolOpt) Connect over SSL for RabbitMQ."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml81(td) #: ./doc/common/tables/nova-rabbitmq.xml81(td) #: ./doc/common/tables/cinder-rabbitmq.xml81(td) #: ./doc/common/tables/glance-rabbitmq.xml81(td) #: ./doc/common/tables/heat-rabbitmq.xml81(td) #: ./doc/common/tables/trove-rabbitmq.xml73(td) #: ./doc/common/tables/keystone-rabbitmq.xml81(td) #: ./doc/common/tables/sahara-rabbitmq.xml81(td) #: ./doc/common/tables/neutron-rabbitmq.xml81(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml81(td) msgid ""rabbit_userid = guest"" msgstr ""rabbit_userid = guest"" #: ./doc/common/tables/ironic-rabbitmq.xml82(td) #: ./doc/common/tables/nova-rabbitmq.xml82(td) #: ./doc/common/tables/cinder-rabbitmq.xml82(td) #: ./doc/common/tables/glance-rabbitmq.xml82(td) #: ./doc/common/tables/heat-rabbitmq.xml82(td) #: ./doc/common/tables/keystone-rabbitmq.xml82(td) #: ./doc/common/tables/sahara-rabbitmq.xml82(td) #: ./doc/common/tables/neutron-rabbitmq.xml82(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml82(td) msgid ""(StrOpt) The RabbitMQ userid."" msgstr """" #: ./doc/common/tables/ironic-rabbitmq.xml85(td) #: ./doc/common/tables/nova-rabbitmq.xml85(td) #: ./doc/common/tables/cinder-rabbitmq.xml85(td) #: ./doc/common/tables/glance-rabbitmq.xml85(td) #: ./doc/common/tables/heat-rabbitmq.xml85(td) #: ./doc/common/tables/trove-rabbitmq.xml77(td) #: ./doc/common/tables/keystone-rabbitmq.xml85(td) #: ./doc/common/tables/sahara-rabbitmq.xml85(td) #: ./doc/common/tables/neutron-rabbitmq.xml85(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml85(td) msgid ""rabbit_virtual_host = /"" msgstr ""rabbit_virtual_host = /"" #: ./doc/common/tables/ironic-rabbitmq.xml86(td) #: ./doc/common/tables/nova-rabbitmq.xml86(td) #: ./doc/common/tables/cinder-rabbitmq.xml86(td) #: ./doc/common/tables/glance-rabbitmq.xml86(td) #: ./doc/common/tables/heat-rabbitmq.xml86(td) #: ./doc/common/tables/keystone-rabbitmq.xml86(td) #: ./doc/common/tables/sahara-rabbitmq.xml86(td) #: ./doc/common/tables/neutron-rabbitmq.xml86(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml86(td) msgid ""(StrOpt) The RabbitMQ virtual host."" msgstr """" #: ./doc/common/tables/ironic-api.xml7(caption)#: ./doc/common/tables/ironic-amqp.xml29(td)#: ./doc/common/tables/ironic-swift.xml7(caption) #: ./doc/common/tables/cinder-swift.xml7(caption) #: ./doc/common/tables/glance-swift.xml7(caption) #: ./doc/common/tables/ceilometer-swift.xml7(caption) #: ./doc/common/tables/trove-swift.xml7(caption) msgid ""Description of swift configuration options""#: ./doc/common/tables/ironic-swift.xml18(th) msgid ""[swift]""#: ./doc/common/tables/ironic-swift.xml21(td) msgid ""swift_max_retries = 2"" msgstr """" #: ./doc/common/tables/ironic-swift.xml22(td)""(IntOpt) Maximum number of times to retry a Swift request, before failing.""#: ./doc/common/tables/ironic-rootwrap.xml7(caption)#: ./doc/common/tables/ironic-rootwrap.xml22(td)#: ./doc/common/tables/ironic-rootwrap.xml25(td)#: ./doc/common/tables/ironic-rootwrap.xml26(td)#: ./doc/common/tables/ironic-rootwrap.xml30(td)#: ./doc/common/tables/ironic-rootwrap.xml33(td)#: ./doc/common/tables/ironic-rootwrap.xml37(td)#: ./doc/common/tables/ironic-rootwrap.xml38(td)#: ./doc/common/tables/ironic-agent.xml7(caption) #: ./doc/common/tables/neutron-agent.xml7(caption) msgid ""Description of agent configuration options"" msgstr """" #: ./doc/common/tables/ironic-agent.xml18(th) msgid ""[agent]"" msgstr """" #: ./doc/common/tables/ironic-agent.xml21(td) msgid ""agent_api_version = v1"" msgstr """" #: ./doc/common/tables/ironic-agent.xml22(td) msgid ""(StrOpt) API version to use for communicating with the ramdisk agent."" msgstr """" #: ./doc/common/tables/ironic-agent.xml25(td) msgid ""agent_pxe_append_params = nofb nomodeset vga=normal"" msgstr """" #: ./doc/common/tables/ironic-agent.xml26(td) #: ./doc/common/tables/ironic-pxe.xml62(td) msgid ""(StrOpt) Additional append parameters for baremetal PXE boot."" msgstr """" #: ./doc/common/tables/ironic-agent.xml29(td) msgid ""agent_pxe_bootfile_name = pxelinux.0"" msgstr """" #: ./doc/common/tables/ironic-agent.xml30(td) msgid ""(StrOpt) Neutron bootfile DHCP parameter."" msgstr """" #: ./doc/common/tables/ironic-agent.xml33(td) msgid """" ""agent_pxe_config_template = $pybasedir/drivers/modules/agent_config.template"" msgstr """" #: ./doc/common/tables/ironic-agent.xml34(td) #: ./doc/common/tables/ironic-pxe.xml70(td) msgid ""(StrOpt) Template file for PXE configuration."" msgstr """" #: ./doc/common/tables/ironic-agent.xml37(td) msgid ""heartbeat_timeout = 300"" msgstr """" #: ./doc/common/tables/ironic-agent.xml38(td) msgid ""(IntOpt) Maximum interval (in seconds) for agent heartbeats."" msgstr """" #: ./doc/common/tables/ironic-ca.xml18(th) #: ./doc/common/tables/ironic-amqp.xml45(th) msgid ""[oslo_messaging_amqp]"" msgstr """" #: ./doc/common/tables/ironic-ca.xml21(td) msgid ""ssl_ca_file ="" msgstr """" #: ./doc/common/tables/ironic-ca.xml22(td) msgid ""(StrOpt) CA certificate PEM file for verifing server certificate"" msgstr """" #: ./doc/common/tables/ironic-ca.xml25(td) msgid ""ssl_cert_file ="" msgstr """" #: ./doc/common/tables/ironic-ca.xml26(td) msgid ""(StrOpt) Identifying certificate PEM file to present to clients"" msgstr """" #: ./doc/common/tables/ironic-ca.xml29(td) msgid ""ssl_key_file ="" msgstr """" #: ./doc/common/tables/ironic-ca.xml30(td) msgid ""(StrOpt) Private key PEM file used to sign cert_file certificate"" msgstr """" #: ./doc/common/tables/ironic-ca.xml33(td) msgid ""ssl_key_password = None"" msgstr """" #: ./doc/common/tables/ironic-ca.xml34(td) msgid ""(StrOpt) Password for decrypting ssl_key_file (if encrypted)"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml22(td) msgid ""(BoolOpt) Auto-delete queues in AMQP."" msgstr """" #: ./doc/common/tables/ironic-amqp.xml26(td) msgid ""(BoolOpt) Use durable queues in AMQP."" msgstr """" #: ./doc/common/tables/ironic-amqp.xml48(td) msgid ""allow_insecure_clients = False"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml49(td) msgid ""(BoolOpt) Accept clients using either SSL or plain TCP"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml52(td) msgid ""broadcast_prefix = broadcast"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml53(td) msgid ""(StrOpt) address prefix used when broadcasting to all servers"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml56(td) msgid ""container_name = None"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml57(td) msgid ""(StrOpt) Name for the AMQP container"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml60(td) msgid ""group_request_prefix = unicast"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml61(td) msgid ""(StrOpt) address prefix when sending to any server in group"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml64(td) msgid ""idle_timeout = 0"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml65(td) msgid ""(IntOpt) Timeout for inactive connections (in seconds)"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml68(td) msgid ""server_request_prefix = exclusive"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml69(td) msgid ""(StrOpt) address prefix used when sending to a specific server"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml72(td) msgid ""trace = False"" msgstr """" #: ./doc/common/tables/ironic-amqp.xml73(td) msgid ""(BoolOpt) Debug: dump AMQP frames to stdout"" msgstr """" #: ./doc/common/tables/ironic-debug.xml21(td)#: ./doc/common/tables/ironic-debug.xml22(td)#: ./doc/common/tables/ironic-common.xml21(td)#: ./doc/common/tables/ironic-common.xml61(td)#: ./doc/common/tables/ironic-common.xml85(td)#: ./doc/common/tables/ironic-common.xml89(td)#: ./doc/common/tables/ironic-logging.xml29(td)#: ./doc/common/tables/ironic-logging.xml30(td)#: ./doc/common/tables/ironic-neutron.xml7(caption)#: ./doc/common/tables/ironic-neutron.xml18(th)#: ./doc/common/tables/ironic-neutron.xml21(td) #: ./doc/common/tables/ironic-glance.xml25(td)#: ./doc/common/tables/ironic-auth.xml21(td)#: ./doc/common/tables/ironic-neutron.xml29(td)#: ./doc/common/tables/ironic-disk_partitioner.xml7(caption) msgid ""Description of disk partitioner configuration options"" msgstr """" #: ./doc/common/tables/ironic-disk_partitioner.xml18(th) msgid ""[disk_partitioner]"" msgstr """" #: ./doc/common/tables/ironic-disk_partitioner.xml21(td) msgid ""check_device_interval = 1"" msgstr """" #: ./doc/common/tables/ironic-disk_partitioner.xml22(td) msgid """" ""(IntOpt) After Ironic has completed creating the partition table, it "" ""continues to check for activity on the attached iSCSI device status at this "" ""interval prior to copying the image to the node, in seconds"" msgstr """" #: ./doc/common/tables/ironic-disk_partitioner.xml25(td) msgid ""check_device_max_retries = 20"" msgstr """" #: ./doc/common/tables/ironic-disk_partitioner.xml26(td) msgid """" ""(IntOpt) The maximum number of times to check that the device is not "" ""accessed by another process. If the device is still busy after that, the "" ""disk partitioning will be treated as having failed."" msgstr """" #: ./doc/common/tables/ironic-neutron.xml22(td) msgid """" ""(StrOpt) Default authentication strategy to use when connecting to neutron. "" ""Can be either \""keystone\"" or \""noauth\"". Running neutron in noauth mode "" ""(related to but not affected by this setting) is insecure and should only be"" "" used for testing."" msgstr """" #: ./doc/common/tables/ironic-neutron.xml25(td) msgid ""url = http://$my_ip:9696"" msgstr """" #: ./doc/common/tables/ironic-neutron.xml26(td) msgid ""(StrOpt) URL for connecting to neutron."" msgstr """" #: ./doc/common/tables/ironic-neutron.xml30(td) msgid ""(IntOpt) Timeout value for connecting to neutron in seconds."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml7(caption)#: ./doc/common/tables/ironic-conductor.xml18(th)#: ./doc/common/tables/ironic-auth_token.xml113(td) msgid ""memcache_pool_conn_get_timeout = 10"" msgstr """" #: ./doc/common/tables/ironic-auth_token.xml114(td) msgid """" ""(IntOpt) (optional) number of seconds that an operation will wait to get a "" ""memcache client connection from the pool."" msgstr """" #: ./doc/common/tables/ironic-auth_token.xml117(td) msgid ""memcache_pool_dead_retry = 300"" msgstr """" #: ./doc/common/tables/ironic-auth_token.xml118(td) msgid """" ""(IntOpt) (optional) number of seconds memcached server is considered dead "" ""before it is tried again."" msgstr """" #: ./doc/common/tables/ironic-auth_token.xml121(td) #: ./doc/common/tables/keystone-cache.xml53(td) msgid ""memcache_pool_maxsize = 10"" msgstr """" #: ./doc/common/tables/ironic-auth_token.xml122(td) msgid """" ""(IntOpt) (optional) max total number of open connections to every memcached "" ""server."" msgstr """" #: ./doc/common/tables/ironic-auth_token.xml125(td) msgid ""memcache_pool_socket_timeout = 3"" msgstr """" #: ./doc/common/tables/ironic-auth_token.xml126(td) msgid """" ""(IntOpt) (optional) socket timeout in seconds for communicating with a "" ""memcache server."" msgstr """" #: ./doc/common/tables/ironic-auth_token.xml129(td) #: ./doc/common/tables/keystone-cache.xml57(td) msgid ""memcache_pool_unused_timeout = 60"" msgstr """" #: ./doc/common/tables/ironic-auth_token.xml130(td) msgid """" ""(IntOpt) (optional) number of seconds a connection to memcached is held "" ""unused in the pool before it is closed."" msgstr """" #: ./doc/common/tables/ironic-auth_token.xml141(td) msgid ""memcache_use_advanced_pool = False"" msgstr """" #: ./doc/common/tables/ironic-auth_token.xml142(td) msgid """" ""(BoolOpt) (optional) use the advanced (eventlet safe) memcache client pool. "" ""The advanced pool will only work under python 2.x."" msgstr """" #: ./doc/common/tables/ironic-glance.xml7(caption) #: ./doc/common/tables/nova-glance.xml7(caption) #: ./doc/common/tables/ceilometer-glance.xml7(caption) msgid ""Description of glance configuration options"" msgstr """" #: ./doc/common/tables/ironic-glance.xml18(th) #: ./doc/common/tables/nova-glance.xml25(th) msgid ""[glance]"" msgstr """" #: ./doc/common/tables/ironic-glance.xml21(td) #: ./doc/common/tables/nova-glance.xml28(td) #: ./doc/common/tables/cinder-images.xml21(td) msgid ""allowed_direct_url_schemes ="" msgstr """" #: ./doc/common/tables/ironic-glance.xml22(td) msgid """" ""(ListOpt) A list of URL schemes that can be downloaded directly via the "" ""direct_url. Currently supported schemes: [file]."" msgstr """" #: ./doc/common/tables/ironic-glance.xml26(td) #: ./doc/common/tables/ironic-glance.xml50(td) #: ./doc/common/tables/nova-glance.xml53(td) #: ./doc/common/tables/nova-conf-changes.xml125(td) msgid """" ""(StrOpt) Default protocol to use when connecting to glance. Set to https for"" "" SSL."" msgstr """" #: ./doc/common/tables/ironic-glance.xml29(td) #: ./doc/common/tables/cinder-images.xml25(td) msgid ""glance_api_insecure = False"" msgstr """" #: ./doc/common/tables/ironic-glance.xml30(td) msgid ""(BoolOpt) Allow to perform insecure SSL (https) requests to glance."" msgstr """" #: ./doc/common/tables/ironic-glance.xml33(td) msgid ""glance_api_servers = None"" msgstr """" #: ./doc/common/tables/ironic-glance.xml34(td) msgid """" ""(ListOpt) A list of the glance api servers available to ironic. Prefix with "" ""https:// for SSL-based glance API servers. Format is [hostname|IP]:port."" msgstr """" #: ./doc/common/tables/ironic-glance.xml37(td) #: ./doc/common/tables/cinder-images.xml49(td) msgid ""glance_host = $my_ip"" msgstr """" #: ./doc/common/tables/ironic-glance.xml38(td) msgid ""(StrOpt) Default glance hostname or IP address."" msgstr """" #: ./doc/common/tables/ironic-glance.xml41(td) #: ./doc/common/tables/cinder-images.xml53(td) msgid ""glance_num_retries = 0"" msgstr """" #: ./doc/common/tables/ironic-glance.xml42(td) msgid ""(IntOpt) Number of retries when downloading an image from glance."" msgstr """" #: ./doc/common/tables/ironic-glance.xml45(td) #: ./doc/common/tables/cinder-images.xml57(td) msgid ""glance_port = 9292"" msgstr """" #: ./doc/common/tables/ironic-glance.xml46(td) msgid ""(IntOpt) Default glance port."" msgstr """" #: ./doc/common/tables/ironic-glance.xml49(td) msgid ""glance_protocol = http"" msgstr """" #: ./doc/common/tables/ironic-glance.xml53(td) msgid ""swift_account = None"" msgstr """" #: ./doc/common/tables/ironic-glance.xml54(td) msgid """" ""(StrOpt) The account that Glance uses to communicate with Swift. The format "" ""is \""AUTH_uuid\"". \""uuid\"" is the UUID for the account configured in the "" ""glance-api.conf. Required for temporary URLs. For example: "" ""\""AUTH_a422b2-91f3-2f46-74b7-d7c9e8958f5d30\"". Swift temporary URL format: "" ""\""endpoint_url/api_version/account/container/object_id\"""" msgstr """" #: ./doc/common/tables/ironic-glance.xml57(td) msgid ""swift_api_version = v1"" msgstr """" #: ./doc/common/tables/ironic-glance.xml58(td) msgid """" ""(StrOpt) The Swift API version to create a temporary URL for. Defaults to "" ""\""v1\"". Swift temporary URL format: "" ""\""endpoint_url/api_version/account/container/object_id\"""" msgstr """" #: ./doc/common/tables/ironic-glance.xml61(td) msgid ""swift_container = glance"" msgstr """" #: ./doc/common/tables/ironic-glance.xml62(td) msgid """" ""(StrOpt) The Swift container Glance is configured to store its images in. "" ""Defaults to \""glance\"", which is the default in glance-api.conf. Swift "" ""temporary URL format: "" ""\""endpoint_url/api_version/account/container/object_id\"""" msgstr """" #: ./doc/common/tables/ironic-glance.xml65(td) msgid ""swift_endpoint_url = None"" msgstr """" #: ./doc/common/tables/ironic-glance.xml66(td) msgid """" ""(StrOpt) The \""endpoint\"" (scheme, hostname, optional port) for the Swift "" ""URL of the form \""endpoint_url/api_version/account/container/object_id\"". Do"" "" not include trailing \""/\"". For example, use \""https://swift.example.com\""."" "" Required for temporary URLs."" msgstr """" #: ./doc/common/tables/ironic-glance.xml69(td) msgid ""swift_temp_url_duration = 1200"" msgstr """" #: ./doc/common/tables/ironic-glance.xml70(td) msgid """" ""(IntOpt) The length of time in seconds that the temporary URL will be valid "" ""for. Defaults to 20 minutes. If some deploys get a 401 response code when "" ""trying to download from the temporary URL, try raising this duration."" msgstr """" #: ./doc/common/tables/ironic-glance.xml73(td) msgid ""swift_temp_url_key = None"" msgstr """" #: ./doc/common/tables/ironic-glance.xml74(td) msgid """" ""(StrOpt) The secret token given to Swift to allow temporary URL downloads. "" ""Required for temporary URLs."" msgstr """" #: ./doc/common/tables/ironic-ilo.xml7(caption) msgid ""Description of ILO configuration options"" msgstr """" #: ./doc/common/tables/ironic-ilo.xml18(th) msgid ""[ilo]"" msgstr """" #: ./doc/common/tables/ironic-ilo.xml21(td) msgid ""client_port = 443"" msgstr """" #: ./doc/common/tables/ironic-ilo.xml22(td) msgid ""(IntOpt) Port to be used for iLO operations"" msgstr """" #: ./doc/common/tables/ironic-ilo.xml26(td) msgid ""(IntOpt) Timeout (in seconds) for iLO operations"" msgstr """" #: ./doc/common/tables/ironic-ilo.xml29(td) msgid ""power_retry = 6"" msgstr """" #: ./doc/common/tables/ironic-ilo.xml30(td) msgid ""(IntOpt) Number of times a power operation needs to be retried"" msgstr """" #: ./doc/common/tables/ironic-ilo.xml33(td) msgid ""power_wait = 2"" msgstr """" #: ./doc/common/tables/ironic-ilo.xml34(td) msgid ""(IntOpt) Amount of time in seconds to wait in between power operations"" msgstr """" #: ./doc/common/tables/ironic-ilo.xml37(td) msgid ""swift_ilo_container = ironic_ilo_container"" msgstr """" #: ./doc/common/tables/ironic-ilo.xml38(td) msgid ""(StrOpt) The Swift iLO container to store data."" msgstr """" #: ./doc/common/tables/ironic-ilo.xml41(td) msgid ""swift_object_expiry_timeout = 900"" msgstr """" #: ./doc/common/tables/ironic-ilo.xml42(td) msgid ""(IntOpt) Amount of time in seconds for Swift objects to auto-expire."" msgstr """" #: ./doc/common/tables/ironic-api.xml18(th)#: ./doc/common/tables/ironic-ssh.xml7(caption) msgid ""Description of SSH configuration options"" msgstr """" #: ./doc/common/tables/ironic-ssh.xml18(th) msgid ""[ssh]"" msgstr """" #: ./doc/common/tables/ironic-ssh.xml21(td) msgid ""libvirt_uri = qemu:///system"" msgstr """" #: ./doc/common/tables/ironic-ssh.xml22(td) msgid ""(StrOpt) libvirt uri"" msgstr """" #: ./doc/common/tables/ironic-api.xml21(td) msgid ""host_ip = 0.0.0.0"" msgstr """" #: ./doc/common/tables/ironic-api.xml22(td) msgid ""(StrOpt) The listen IP for the Ironic API server."" msgstr """" #: ./doc/common/tables/ironic-api.xml25(td) msgid ""max_limit = 1000"" msgstr """" #: ./doc/common/tables/ironic-api.xml26(td) msgid """" ""(IntOpt) The maximum number of items returned in a single response from a "" ""collection resource."" msgstr """" #: ./doc/common/tables/ironic-api.xml29(td) msgid ""port = 6385"" msgstr """" #: ./doc/common/tables/ironic-api.xml30(td) msgid ""(IntOpt) The port for the Ironic API server."" msgstr """" #: ./doc/common/tables/ironic-rootwrap.xml34(td)#: ./doc/common/tables/ironic-database.xml66(td) msgid """" ""(IntOpt) Maximum number of database connection retries during startup. Set "" ""to -1 to specify an infinite retry count."" msgstr """" #: ./doc/common/tables/ironic-database.xml73(td) msgid ""mysql_engine = InnoDB"" msgstr """" #: ./doc/common/tables/ironic-database.xml74(td) msgid ""(StrOpt) MySQL engine to use."" msgstr """" #: ./doc/common/tables/ironic-ipmi.xml7(caption) #: ./doc/common/tables/ceilometer-ipmi.xml7(caption) msgid ""Description of IPMI configuration options"" msgstr """" #: ./doc/common/tables/ironic-ipmi.xml18(th) #: ./doc/common/tables/ceilometer-ipmi.xml18(th) msgid ""[ipmi]"" msgstr """" #: ./doc/common/tables/ironic-ipmi.xml21(td) msgid ""min_command_interval = 5"" msgstr """" #: ./doc/common/tables/ironic-ipmi.xml22(td) msgid """" ""(IntOpt) Minimum time, in seconds, between IPMI operations sent to a server."" "" There is a risk with some hardware that setting this too low may cause the "" ""BMC to crash. Recommended setting is 5 seconds."" msgstr """" #: ./doc/common/tables/ironic-ipmi.xml25(td) msgid ""retry_timeout = 60"" msgstr """" #: ./doc/common/tables/ironic-ipmi.xml26(td) msgid ""(IntOpt) Maximum time in seconds to retry IPMI operations."" msgstr """" #: ./doc/common/tables/ironic-snmp.xml7(caption) msgid ""Description of SNMP configuration options"" msgstr """" #: ./doc/common/tables/ironic-snmp.xml18(th) msgid ""[snmp]"" msgstr """" #: ./doc/common/tables/ironic-snmp.xml21(td) msgid ""power_timeout = 10"" msgstr """" #: ./doc/common/tables/ironic-policy.xml22(td) msgid ""(StrOpt) Rule checked when requested rule is not found."" msgstr """" #: ./doc/common/tables/ironic-policy.xml26(td) msgid ""(StrOpt) JSON file representing policy."" msgstr """" #: ./doc/common/tables/ironic-rootwrap.xml21(td) msgid ""filters_path = /etc/ironic/rootwrap.d,/usr/share/ironic/rootwrap"" msgstr """" #: ./doc/common/tables/ironic-common.xml22(td) msgid ""(StrOpt) Directory where ironic binaries are installed."" msgstr """" #: ./doc/common/tables/ironic-common.xml25(td) msgid ""enabled_drivers = pxe_ipmitool"" msgstr """" #: ./doc/common/tables/ironic-common.xml26(td) msgid """" ""(ListOpt) Specify the list of drivers to load during service initialization."" "" Missing drivers, or drivers which fail to initialize, will prevent the "" ""conductor service from starting. The option default is a recommended set of "" ""production-oriented drivers. A complete list of drivers present on your "" ""system may be found by enumerating the \""ironic.drivers\"" entrypoint. An "" ""example may be found in the developer documentation online."" msgstr """" #: ./doc/common/tables/ironic-common.xml33(td) #: ./doc/common/tables/nova-hypervisor.xml25(td) msgid ""force_raw_images = True"" msgstr """" #: ./doc/common/tables/ironic-common.xml34(td) msgid ""(BoolOpt) Force backing images to raw format."" msgstr """" #: ./doc/common/tables/ironic-common.xml37(td) msgid ""hash_distribution_replicas = 1"" msgstr """" #: ./doc/common/tables/ironic-common.xml38(td) msgid """" ""(IntOpt) [Experimental Feature] Number of hosts to map onto each hash "" ""partition. Setting this to more than one will cause additional conductor "" ""services to prepare deployment environments and potentially allow the Ironic"" "" cluster to recover more quickly if a conductor instance is terminated."" msgstr """" #: ./doc/common/tables/ironic-common.xml41(td) msgid ""hash_partition_exponent = 5"" msgstr """" #: ./doc/common/tables/ironic-common.xml42(td) msgid """" ""(IntOpt) Exponent to determine number of hash partitions to use when "" ""distributing load across conductors. Larger values will result in more even "" ""distribution of load and less load when rebalancing the ring, but more "" ""memory usage. Number of partitions per conductor is "" ""(2^hash_partition_exponent). This determines the granularity of rebalancing:"" "" given 10 hosts, and an exponent of the 2, there are 40 partitions in the "" ""ring.A few thousand partitions should make rebalancing smooth in most cases."" "" The default is suitable for up to a few hundred conductors. Too many "" ""partitions has a CPU impact."" msgstr """" #: ./doc/common/tables/ironic-common.xml46(td) msgid """" ""(StrOpt) Name of this node. This can be an opaque identifier. It is not "" ""necessarily a hostname, FQDN, or IP address. However, the node name must be "" ""valid within an AMQP key, and if using ZeroMQ, a valid hostname, FQDN, or IP"" "" address."" msgstr """" #: ./doc/common/tables/ironic-common.xml49(td) msgid ""isolinux_bin = /usr/lib/syslinux/isolinux.bin"" msgstr """" #: ./doc/common/tables/ironic-common.xml50(td) msgid ""(StrOpt) Path to isolinux binary file."" msgstr """" #: ./doc/common/tables/ironic-common.xml53(td) msgid ""isolinux_config_template = $pybasedir/common/isolinux_config.template"" msgstr """" #: ./doc/common/tables/ironic-common.xml54(td) msgid ""(StrOpt) Template file for isolinux configuration file."" msgstr """" #: ./doc/common/tables/ironic-common.xml62(td) msgid ""(StrOpt) IP address of this host."" msgstr """" #: ./doc/common/tables/ironic-common.xml65(td) msgid ""parallel_image_downloads = False"" msgstr """" #: ./doc/common/tables/ironic-common.xml66(td) msgid ""(BoolOpt) Run image downloads and raw format conversions in parallel."" msgstr """" #: ./doc/common/tables/ironic-common.xml73(td) msgid ""pybasedir = /usr/lib/python/site-packages/ironic/ironic"" msgstr """" #: ./doc/common/tables/ironic-common.xml74(td) msgid ""(StrOpt) Directory where the ironic python module is installed."" msgstr """" #: ./doc/common/tables/ironic-common.xml77(td) msgid ""rootwrap_config = /etc/ironic/rootwrap.conf"" msgstr """" #: ./doc/common/tables/ironic-common.xml78(td) msgid """" ""(StrOpt) Path to the rootwrap configuration file to use for running commands"" "" as root."" msgstr """" #: ./doc/common/tables/ironic-common.xml86(td) msgid ""(StrOpt) Top-level directory for maintaining ironic's state."" msgstr """" #: ./doc/common/tables/ironic-common.xml90(td) msgid ""(StrOpt) Explicitly specify the temporary working directory."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml21(td) msgid ""api_url = None"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml22(td) msgid """" ""(StrOpt) URL of Ironic API service. If not set ironic can get the current "" ""value from the keystone service catalog."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml25(td) msgid ""check_provision_state_interval = 60"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml26(td) msgid ""(IntOpt) Interval between checks of provision timeouts, in seconds."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml29(td) msgid ""deploy_callback_timeout = 1800"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml30(td) msgid """" ""(IntOpt) Timeout (seconds) for waiting callback from deploy ramdisk. 0 - "" ""unlimited."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml33(td) msgid ""force_power_state_during_sync = True"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml34(td) msgid """" ""(BoolOpt) During sync_power_state, should the hardware power state be set to"" "" the state recorded in the database (True) or should the database be updated"" "" based on the hardware state (False)."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml37(td) msgid ""heartbeat_interval = 10"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml38(td) msgid ""(IntOpt) Seconds between conductor heart beats."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml41(td) msgid ""heartbeat_timeout = 60"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml42(td) msgid """" ""(IntOpt) Maximum time (in seconds) since the last check-in of a conductor."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml45(td) msgid ""node_locked_retry_attempts = 3"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml46(td) msgid ""(IntOpt) Number of attempts to grab a node lock."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml49(td) msgid ""node_locked_retry_interval = 1"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml50(td) msgid ""(IntOpt) Seconds to sleep between node lock attempts."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml53(td) msgid ""periodic_max_workers = 8"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml54(td) msgid """" ""(IntOpt) Maximum number of worker threads that can be started simultaneously"" "" by a periodic task. Should be less than RPC thread pool size."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml57(td) msgid ""power_state_sync_max_retries = 3"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml58(td) msgid """" ""(IntOpt) During sync_power_state failures, limit the number of times Ironic "" ""should try syncing the hardware node power state with the node power state "" ""in DB"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml61(td) msgid ""send_sensor_data = False"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml62(td) msgid ""(BoolOpt) Enable sending sensor data message via the notification bus"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml65(td) msgid ""send_sensor_data_interval = 600"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml66(td) msgid """" ""(IntOpt) Seconds between conductor sending sensor data message to ceilometer"" "" via the notification bus."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml69(td) msgid ""send_sensor_data_types = ALL"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml70(td) msgid """" ""(ListOpt) List of comma separated metric types which need to be sent to "" ""Ceilometer. The default value, \""ALL\"", is a special value meaning send all "" ""the sensor data."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml73(td) msgid ""sync_local_state_interval = 180"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml74(td) msgid """" ""(IntOpt) When conductors join or leave the cluster, existing conductors may "" ""need to update any persistent local state as nodes are moved around the "" ""cluster. This option controls how often, in seconds, each conductor will "" ""check for nodes that it should \""take over\"". Set it to a negative value to "" ""disable the check entirely."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml77(td) msgid ""sync_power_state_interval = 60"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml78(td) msgid """" ""(IntOpt) Interval between syncing the node power state to the database, in "" ""seconds."" msgstr """" #: ./doc/common/tables/ironic-conductor.xml81(td) msgid ""workers_pool_size = 100"" msgstr """" #: ./doc/common/tables/ironic-conductor.xml82(td) msgid ""(IntOpt) The size of the workers greenthread pool."" msgstr """" #: ./doc/common/tables/ironic-auth.xml22(td) msgid ""(StrOpt) Method to use for authentication: noauth or keystone."" msgstr """" #: ./doc/common/tables/ironic-console.xml7(caption)#: ./doc/common/tables/ironic-pxe.xml7(caption) msgid ""Description of PXE configuration options"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml18(th) msgid ""[pxe]"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml21(td) msgid ""default_ephemeral_format = ext4"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml22(td) msgid """" ""(StrOpt) Default file system format for ephemeral partition, if one is "" ""created."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml25(td) msgid ""disk_devices = cciss/c0d0,sda,hda,vda"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml26(td) msgid ""(StrOpt) The disk devices to scan while doing the deploy."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml29(td) msgid ""http_root = /httpboot"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml30(td) msgid ""(StrOpt) Ironic compute node's HTTP root path."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml33(td) msgid ""http_url = None"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml34(td) msgid """" ""(StrOpt) Ironic compute node's HTTP server URL. Example: "" ""http://192.1.2.3:8080"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml37(td) msgid ""image_cache_size = 20480"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml38(td) msgid """" ""(IntOpt) Maximum size (in MiB) of cache for master images, including those "" ""in use."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml41(td) msgid ""image_cache_ttl = 10080"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml42(td) msgid ""(IntOpt) Maximum TTL (in minutes) for old master images in cache."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml45(td) msgid ""images_path = /var/lib/ironic/images/"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml46(td) msgid ""(StrOpt) Directory where images are stored on disk."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml49(td) msgid ""instance_master_path = /var/lib/ironic/master_images"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml50(td) msgid ""(StrOpt) Directory where master instance images are stored on disk."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml53(td) msgid ""ipxe_boot_script = $pybasedir/drivers/modules/boot.ipxe"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml54(td) msgid ""(StrOpt) The path to the main iPXE script file."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml57(td) msgid ""ipxe_enabled = False"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml58(td) msgid ""(BoolOpt) Enable iPXE boot."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml61(td) #: ./doc/common/tables/nova-baremetal.xml53(td) msgid ""pxe_append_params = nofb nomodeset vga=normal"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml65(td) #: ./doc/common/tables/nova-baremetal.xml57(td) msgid ""pxe_bootfile_name = pxelinux.0"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml66(td) msgid ""(StrOpt) Bootfile DHCP parameter."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml69(td) msgid ""pxe_config_template = $pybasedir/drivers/modules/pxe_config.template"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml73(td) msgid ""tftp_master_path = /tftpboot/master_images"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml74(td) msgid ""(StrOpt) Directory where master tftp images are stored on disk."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml77(td) #: ./doc/common/tables/nova-baremetal.xml89(td) msgid ""tftp_root = /tftpboot"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml78(td) msgid ""(StrOpt) Ironic compute node's tftp root path."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml81(td) msgid ""tftp_server = $my_ip"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml82(td) msgid ""(StrOpt) IP address of Ironic compute node's tftp server."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml85(td) msgid ""uefi_pxe_bootfile_name = elilo.efi"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml86(td) msgid ""(StrOpt) Bootfile DHCP parameter for UEFI boot mode."" msgstr """" #: ./doc/common/tables/ironic-pxe.xml89(td) msgid """" ""uefi_pxe_config_template = "" ""$pybasedir/drivers/modules/elilo_efi_pxe_config.template"" msgstr """" #: ./doc/common/tables/ironic-pxe.xml90(td) msgid ""(StrOpt) Template file for PXE configuration for UEFI boot loader."" msgstr """" #: ./doc/common/tables/ironic-console.xml18(th) msgid ""[console]"" msgstr """" #: ./doc/common/tables/ironic-console.xml21(td) msgid ""subprocess_checking_interval = 1"" msgstr """" #: ./doc/common/tables/ironic-console.xml22(td) msgid """" ""(IntOpt) Time interval (in seconds) for checking the status of console "" ""subprocess."" msgstr """" #: ./doc/common/tables/ironic-console.xml25(td) msgid ""subprocess_timeout = 10"" msgstr """" #: ./doc/common/tables/ironic-console.xml26(td) msgid """" ""(IntOpt) Time (in seconds) to wait for the console subprocess to start."" msgstr """" #: ./doc/common/tables/ironic-console.xml29(td) #: ./doc/common/tables/nova-baremetal.xml77(td) msgid ""terminal = shellinaboxd"" msgstr """" #: ./doc/common/tables/ironic-console.xml30(td) msgid ""(StrOpt) Path to serial console terminal program"" msgstr """" #: ./doc/common/tables/ironic-console.xml33(td) #: ./doc/common/tables/nova-baremetal.xml81(td) msgid ""terminal_cert_dir = None"" msgstr """" #: ./doc/common/tables/ironic-console.xml34(td) msgid """" ""(StrOpt) Directory containing the terminal SSL cert(PEM) for serial console "" ""access"" msgstr """" #: ./doc/common/tables/ironic-console.xml37(td) msgid ""terminal_pid_dir = None"" msgstr """" #: ./doc/common/tables/ironic-console.xml38(td) msgid """" ""(StrOpt) Directory for holding terminal pid files. If not specified, the "" ""temporary directory will be used."" msgstr """" #: ./doc/common/tables/ironic-logging.xml25(td) #: ./doc/common/tables/trove-logging.xml25(td) msgid """" ""default_log_levels = amqp=WARN, amqplib=WARN, boto=WARN, qpid=WARN, "" ""sqlalchemy=WARN, suds=INFO, oslo.messaging=INFO, iso8601=WARN, "" ""requests.packages.urllib3.connectionpool=WARN, urllib3.connectionpool=WARN, "" ""websocket=WARN"" msgstr """" #: ./doc/common/tables/ironic-dhcp.xml7(caption) msgid ""Description of DHCP configuration options"" msgstr """" #: ./doc/common/tables/ironic-dhcp.xml18(th) msgid ""[dhcp]"" msgstr """" #: ./doc/common/tables/ironic-dhcp.xml21(td) msgid ""dhcp_provider = neutron"" msgstr """" #: ./doc/common/tables/ironic-dhcp.xml22(td) msgid """" ""(StrOpt) DHCP provider to use. \""neutron\"" uses Neutron, and \""none\"" uses a"" "" no-op provider."" msgstr """" ","""POT-Creation-Date: 2014-12-04 23:29+0000\n"" ""PO-Revision-Date: 2014-12-04 21:11+0000\n""#: ./doc/common/tables/ceilometer-redis.xml7(caption) #: ./doc/common/tables/neutron-redis.xml7(caption) #: ./doc/common/tables/nova-redis.xml7(caption) #: ./doc/common/tables/sahara-redis.xml7(caption) #: ./doc/common/tables/cinder-redis.xml7(caption) #: ./doc/common/tables/glance-redis.xml7(caption) #: ./doc/common/tables/heat-redis.xml7(caption) #: ./doc/common/tables/trove-redis.xml7(caption) #: ./doc/common/tables/keystone-redis.xml7(caption) msgid ""Description of Redis configuration options"" msgstr """" #: ./doc/common/tables/ceilometer-redis.xml18(th) #: ./doc/common/tables/neutron-redis.xml18(th) #: ./doc/common/tables/nova-redis.xml18(th) #: ./doc/common/tables/sahara-redis.xml18(th) #: ./doc/common/tables/cinder-redis.xml18(th) #: ./doc/common/tables/glance-redis.xml18(th) #: ./doc/common/tables/heat-redis.xml18(th) #: ./doc/common/tables/trove-redis.xml18(th) #: ./doc/common/tables/keystone-redis.xml18(th) msgid ""[matchmaker_redis]"" msgstr """" #: ./doc/common/tables/ceilometer-redis.xml21(td) #: ./doc/common/tables/neutron-redis.xml21(td) #: ./doc/common/tables/nova-redis.xml21(td) #: ./doc/common/tables/sahara-redis.xml21(td) #: ./doc/common/tables/cinder-redis.xml21(td) #: ./doc/common/tables/glance-redis.xml21(td) #: ./doc/common/tables/heat-redis.xml21(td) #: ./doc/common/tables/trove-redis.xml21(td) #: ./doc/common/tables/neutron-nec.xml37(td) #: ./doc/common/tables/keystone-redis.xml21(td) msgid ""host = 127.0.0.1"" msgstr """" #: ./doc/common/tables/ceilometer-redis.xml22(td) #: ./doc/common/tables/neutron-redis.xml22(td) #: ./doc/common/tables/nova-redis.xml22(td) #: ./doc/common/tables/sahara-redis.xml22(td) #: ./doc/common/tables/cinder-redis.xml22(td) #: ./doc/common/tables/glance-redis.xml22(td) #: ./doc/common/tables/heat-redis.xml22(td) #: ./doc/common/tables/keystone-redis.xml22(td) msgid ""(StrOpt) Host to locate redis."" msgstr """" #: ./doc/common/tables/ceilometer-redis.xml25(td) #: ./doc/common/tables/neutron-redis.xml25(td) #: ./doc/common/tables/nova-redis.xml25(td) #: ./doc/common/tables/sahara-redis.xml25(td) #: ./doc/common/tables/keystone-ldap.xml101(td) #: ./doc/common/tables/cinder-redis.xml25(td) #: ./doc/common/tables/glance-redis.xml25(td) #: ./doc/common/tables/heat-redis.xml25(td) #: ./doc/common/tables/neutron-ml2_ncs.xml21(td) #: ./doc/common/tables/neutron-ml2_odl.xml21(td) #: ./doc/common/tables/trove-redis.xml25(td) #: ./doc/common/tables/keystone-redis.xml25(td) msgid ""password = None"" msgstr ""password = None"" #: ./doc/common/tables/ceilometer-redis.xml26(td) #: ./doc/common/tables/neutron-redis.xml26(td) #: ./doc/common/tables/nova-redis.xml26(td) #: ./doc/common/tables/sahara-redis.xml26(td) #: ./doc/common/tables/cinder-redis.xml26(td) #: ./doc/common/tables/glance-redis.xml26(td) #: ./doc/common/tables/heat-redis.xml26(td) #: ./doc/common/tables/keystone-redis.xml26(td) msgid ""(StrOpt) Password for Redis server (optional)."" msgstr """" #: ./doc/common/tables/ceilometer-redis.xml29(td) #: ./doc/common/tables/neutron-redis.xml29(td) #: ./doc/common/tables/nova-redis.xml29(td) #: ./doc/common/tables/sahara-redis.xml29(td) #: ./doc/common/tables/cinder-redis.xml29(td) #: ./doc/common/tables/glance-redis.xml29(td) #: ./doc/common/tables/heat-redis.xml29(td) #: ./doc/common/tables/trove-redis.xml29(td) #: ./doc/common/tables/keystone-redis.xml29(td) msgid ""port = 6379"" msgstr """" #: ./doc/common/tables/ceilometer-redis.xml30(td) #: ./doc/common/tables/neutron-redis.xml30(td) #: ./doc/common/tables/nova-redis.xml30(td) #: ./doc/common/tables/sahara-redis.xml30(td) #: ./doc/common/tables/cinder-redis.xml30(td) #: ./doc/common/tables/glance-redis.xml30(td) #: ./doc/common/tables/heat-redis.xml30(td) #: ./doc/common/tables/trove-redis.xml30(td) #: ./doc/common/tables/keystone-redis.xml30(td) msgid ""(IntOpt) Use this port to connect to redis host."" msgstr """" #: ./doc/common/tables/ceilometer-redis.xml33(th) #: ./doc/common/tables/neutron-redis.xml33(th) #: ./doc/common/tables/nova-redis.xml33(th) #: ./doc/common/tables/sahara-redis.xml33(th) #: ./doc/common/tables/cinder-redis.xml33(th) #: ./doc/common/tables/glance-redis.xml33(th) #: ./doc/common/tables/heat-redis.xml33(th) #: ./doc/common/tables/trove-redis.xml33(th) #: ./doc/common/tables/keystone-redis.xml33(th) msgid ""[matchmaker_ring]"" msgstr ""[matchmaker_ring]"" #: ./doc/common/tables/ceilometer-redis.xml36(td) #: ./doc/common/tables/neutron-redis.xml36(td) #: ./doc/common/tables/nova-redis.xml36(td) #: ./doc/common/tables/sahara-redis.xml36(td) #: ./doc/common/tables/cinder-redis.xml36(td) #: ./doc/common/tables/glance-redis.xml36(td) #: ./doc/common/tables/heat-redis.xml36(td) #: ./doc/common/tables/trove-redis.xml36(td) #: ./doc/common/tables/keystone-redis.xml36(td) msgid ""ringfile = /etc/oslo/matchmaker_ring.json"" msgstr ""ringfile = /etc/oslo/matchmaker_ring.json"" #: ./doc/common/tables/ceilometer-redis.xml37(td) #: ./doc/common/tables/neutron-redis.xml37(td) #: ./doc/common/tables/nova-redis.xml37(td) #: ./doc/common/tables/sahara-redis.xml37(td) #: ./doc/common/tables/cinder-redis.xml37(td) #: ./doc/common/tables/glance-redis.xml37(td) #: ./doc/common/tables/heat-redis.xml37(td) #: ./doc/common/tables/keystone-redis.xml37(td) msgid ""(StrOpt) Matchmaker ring file (JSON)."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml7(caption) #: ./doc/common/tables/cinder-rabbitmq.xml7(caption) #: ./doc/common/tables/glance-rabbitmq.xml7(caption) #: ./doc/common/tables/heat-rabbitmq.xml7(caption) #: ./doc/common/tables/trove-rabbitmq.xml7(caption) #: ./doc/common/tables/keystone-rabbitmq.xml7(caption) #: ./doc/common/tables/sahara-rabbitmq.xml7(caption) #: ./doc/common/tables/neutron-rabbitmq.xml7(caption) #: ./doc/common/tables/ceilometer-rabbitmq.xml7(caption) msgid ""Description of RabbitMQ configuration options""#: ./doc/common/tables/nova-rabbitmq.xml21(td) #: ./doc/common/tables/cinder-rabbitmq.xml21(td) #: ./doc/common/tables/glance-rabbitmq.xml21(td) #: ./doc/common/tables/heat-rabbitmq.xml21(td) #: ./doc/common/tables/keystone-rabbitmq.xml21(td) #: ./doc/common/tables/sahara-rabbitmq.xml21(td) #: ./doc/common/tables/neutron-rabbitmq.xml21(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml21(td) msgid ""kombu_reconnect_delay = 1.0""#: ./doc/common/tables/nova-rabbitmq.xml22(td) #: ./doc/common/tables/cinder-rabbitmq.xml22(td) #: ./doc/common/tables/heat-conf-changes.xml37(td) #: ./doc/common/tables/neutron-conf-changes.xml81(td) #: ./doc/common/tables/glance-rabbitmq.xml22(td) #: ./doc/common/tables/heat-rabbitmq.xml22(td) #: ./doc/common/tables/keystone-rabbitmq.xml22(td) #: ./doc/common/tables/sahara-rabbitmq.xml22(td) #: ./doc/common/tables/neutron-rabbitmq.xml22(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml22(td)""(FloatOpt) How long to wait before reconnecting in response to an AMQP "" ""consumer cancel notification."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml25(td) #: ./doc/common/tables/cinder-rabbitmq.xml25(td) #: ./doc/common/tables/glance-rabbitmq.xml25(td) #: ./doc/common/tables/heat-rabbitmq.xml25(td) #: ./doc/common/tables/trove-rabbitmq.xml21(td) #: ./doc/common/tables/keystone-rabbitmq.xml25(td) #: ./doc/common/tables/sahara-rabbitmq.xml25(td) #: ./doc/common/tables/neutron-rabbitmq.xml25(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml25(td) msgid ""kombu_ssl_ca_certs ="" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml26(td) #: ./doc/common/tables/cinder-rabbitmq.xml26(td) #: ./doc/common/tables/glance-rabbitmq.xml26(td) #: ./doc/common/tables/heat-rabbitmq.xml26(td) #: ./doc/common/tables/keystone-rabbitmq.xml26(td) #: ./doc/common/tables/sahara-rabbitmq.xml26(td) #: ./doc/common/tables/neutron-rabbitmq.xml26(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml26(td) msgid ""(StrOpt) SSL certification authority file (valid only if SSL enabled)."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml29(td) #: ./doc/common/tables/cinder-rabbitmq.xml29(td) #: ./doc/common/tables/glance-rabbitmq.xml29(td) #: ./doc/common/tables/heat-rabbitmq.xml29(td) #: ./doc/common/tables/trove-rabbitmq.xml25(td) #: ./doc/common/tables/keystone-rabbitmq.xml29(td) #: ./doc/common/tables/sahara-rabbitmq.xml29(td) #: ./doc/common/tables/neutron-rabbitmq.xml29(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml29(td) msgid ""kombu_ssl_certfile ="" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml30(td) #: ./doc/common/tables/cinder-rabbitmq.xml30(td) #: ./doc/common/tables/glance-rabbitmq.xml30(td) #: ./doc/common/tables/heat-rabbitmq.xml30(td) #: ./doc/common/tables/keystone-rabbitmq.xml30(td) #: ./doc/common/tables/sahara-rabbitmq.xml30(td) #: ./doc/common/tables/neutron-rabbitmq.xml30(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml30(td) msgid ""(StrOpt) SSL cert file (valid only if SSL enabled)."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml33(td) #: ./doc/common/tables/cinder-rabbitmq.xml33(td) #: ./doc/common/tables/glance-rabbitmq.xml33(td) #: ./doc/common/tables/heat-rabbitmq.xml33(td) #: ./doc/common/tables/trove-rabbitmq.xml29(td) #: ./doc/common/tables/keystone-rabbitmq.xml33(td) #: ./doc/common/tables/sahara-rabbitmq.xml33(td) #: ./doc/common/tables/neutron-rabbitmq.xml33(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml33(td) msgid ""kombu_ssl_keyfile ="" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml34(td) #: ./doc/common/tables/cinder-rabbitmq.xml34(td) #: ./doc/common/tables/glance-rabbitmq.xml34(td) #: ./doc/common/tables/heat-rabbitmq.xml34(td) #: ./doc/common/tables/keystone-rabbitmq.xml34(td) #: ./doc/common/tables/sahara-rabbitmq.xml34(td) #: ./doc/common/tables/neutron-rabbitmq.xml34(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml34(td) msgid ""(StrOpt) SSL key file (valid only if SSL enabled)."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml37(td) #: ./doc/common/tables/cinder-rabbitmq.xml37(td) #: ./doc/common/tables/glance-rabbitmq.xml37(td) #: ./doc/common/tables/heat-rabbitmq.xml37(td) #: ./doc/common/tables/trove-rabbitmq.xml33(td) #: ./doc/common/tables/keystone-rabbitmq.xml37(td) #: ./doc/common/tables/sahara-rabbitmq.xml37(td) #: ./doc/common/tables/neutron-rabbitmq.xml37(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml37(td) msgid ""kombu_ssl_version =""#: ./doc/common/tables/nova-rabbitmq.xml41(td) #: ./doc/common/tables/cinder-rabbitmq.xml41(td) #: ./doc/common/tables/glance-rabbitmq.xml41(td) #: ./doc/common/tables/heat-rabbitmq.xml41(td) #: ./doc/common/tables/trove-rabbitmq.xml37(td) #: ./doc/common/tables/keystone-rabbitmq.xml41(td) #: ./doc/common/tables/sahara-rabbitmq.xml41(td) #: ./doc/common/tables/neutron-rabbitmq.xml41(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml41(td) msgid ""rabbit_ha_queues = False"" msgstr ""rabbit_ha_queues = False"" #: ./doc/common/tables/nova-rabbitmq.xml42(td) #: ./doc/common/tables/cinder-rabbitmq.xml42(td) #: ./doc/common/tables/glance-rabbitmq.xml42(td) #: ./doc/common/tables/heat-rabbitmq.xml42(td) #: ./doc/common/tables/keystone-rabbitmq.xml42(td) #: ./doc/common/tables/sahara-rabbitmq.xml42(td) #: ./doc/common/tables/neutron-rabbitmq.xml42(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml42(td) msgid """" ""(BoolOpt) Use HA queues in RabbitMQ (x-ha-policy: all). If you change this "" ""option, you must wipe the RabbitMQ database."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml45(td) #: ./doc/common/tables/cinder-rabbitmq.xml45(td) #: ./doc/common/tables/glance-rabbitmq.xml45(td) #: ./doc/common/tables/heat-rabbitmq.xml45(td) #: ./doc/common/tables/trove-rabbitmq.xml41(td) #: ./doc/common/tables/keystone-rabbitmq.xml45(td) #: ./doc/common/tables/sahara-rabbitmq.xml45(td) #: ./doc/common/tables/neutron-rabbitmq.xml45(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml45(td) msgid ""rabbit_host = localhost"" msgstr ""rabbit_host = localhost"" #: ./doc/common/tables/nova-rabbitmq.xml46(td) #: ./doc/common/tables/cinder-rabbitmq.xml46(td) #: ./doc/common/tables/glance-rabbitmq.xml46(td) #: ./doc/common/tables/heat-rabbitmq.xml46(td) #: ./doc/common/tables/keystone-rabbitmq.xml46(td) #: ./doc/common/tables/sahara-rabbitmq.xml46(td) #: ./doc/common/tables/neutron-rabbitmq.xml46(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml46(td) msgid ""(StrOpt) The RabbitMQ broker address where a single node is used."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml49(td) #: ./doc/common/tables/cinder-rabbitmq.xml49(td) #: ./doc/common/tables/glance-rabbitmq.xml49(td) #: ./doc/common/tables/heat-rabbitmq.xml49(td) #: ./doc/common/tables/trove-rabbitmq.xml45(td) #: ./doc/common/tables/keystone-rabbitmq.xml49(td) #: ./doc/common/tables/sahara-rabbitmq.xml49(td) #: ./doc/common/tables/neutron-rabbitmq.xml49(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml49(td) msgid ""rabbit_hosts = $rabbit_host:$rabbit_port"" msgstr ""rabbit_hosts = $rabbit_host:$rabbit_port"" #: ./doc/common/tables/nova-rabbitmq.xml50(td) #: ./doc/common/tables/cinder-rabbitmq.xml50(td) #: ./doc/common/tables/glance-rabbitmq.xml50(td) #: ./doc/common/tables/heat-rabbitmq.xml50(td) #: ./doc/common/tables/keystone-rabbitmq.xml50(td) #: ./doc/common/tables/sahara-rabbitmq.xml50(td) #: ./doc/common/tables/neutron-rabbitmq.xml50(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml50(td) msgid ""(ListOpt) RabbitMQ HA cluster host:port pairs."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml53(td) #: ./doc/common/tables/cinder-rabbitmq.xml53(td) #: ./doc/common/tables/glance-rabbitmq.xml53(td) #: ./doc/common/tables/heat-rabbitmq.xml53(td) #: ./doc/common/tables/keystone-rabbitmq.xml53(td) #: ./doc/common/tables/sahara-rabbitmq.xml53(td) #: ./doc/common/tables/neutron-rabbitmq.xml53(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml53(td) msgid ""rabbit_login_method = AMQPLAIN"" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml57(td) #: ./doc/common/tables/cinder-rabbitmq.xml57(td) #: ./doc/common/tables/glance-rabbitmq.xml57(td) #: ./doc/common/tables/heat-rabbitmq.xml57(td) #: ./doc/common/tables/trove-rabbitmq.xml49(td) #: ./doc/common/tables/keystone-rabbitmq.xml57(td) #: ./doc/common/tables/sahara-rabbitmq.xml57(td) #: ./doc/common/tables/neutron-rabbitmq.xml57(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml57(td) msgid ""rabbit_max_retries = 0"" msgstr ""rabbit_max_retries = 0"" #: ./doc/common/tables/nova-rabbitmq.xml58(td) #: ./doc/common/tables/cinder-rabbitmq.xml58(td) #: ./doc/common/tables/glance-rabbitmq.xml58(td) #: ./doc/common/tables/heat-rabbitmq.xml58(td) #: ./doc/common/tables/keystone-rabbitmq.xml58(td) #: ./doc/common/tables/sahara-rabbitmq.xml58(td) #: ./doc/common/tables/neutron-rabbitmq.xml58(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml58(td) msgid """" ""(IntOpt) Maximum number of RabbitMQ connection retries. Default is 0 "" ""(infinite retry count)."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml61(td) #: ./doc/common/tables/cinder-rabbitmq.xml61(td) #: ./doc/common/tables/glance-rabbitmq.xml61(td) #: ./doc/common/tables/heat-rabbitmq.xml61(td) #: ./doc/common/tables/trove-rabbitmq.xml53(td) #: ./doc/common/tables/keystone-rabbitmq.xml61(td) #: ./doc/common/tables/sahara-rabbitmq.xml61(td) #: ./doc/common/tables/neutron-rabbitmq.xml61(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml61(td) msgid ""rabbit_password = guest"" msgstr ""rabbit_password = guest"" #: ./doc/common/tables/nova-rabbitmq.xml62(td) #: ./doc/common/tables/cinder-rabbitmq.xml62(td) #: ./doc/common/tables/glance-rabbitmq.xml62(td) #: ./doc/common/tables/heat-rabbitmq.xml62(td) #: ./doc/common/tables/keystone-rabbitmq.xml62(td) #: ./doc/common/tables/sahara-rabbitmq.xml62(td) #: ./doc/common/tables/neutron-rabbitmq.xml62(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml62(td) msgid ""(StrOpt) The RabbitMQ password."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml65(td) #: ./doc/common/tables/cinder-rabbitmq.xml65(td) #: ./doc/common/tables/glance-rabbitmq.xml65(td) #: ./doc/common/tables/heat-rabbitmq.xml65(td) #: ./doc/common/tables/trove-rabbitmq.xml57(td) #: ./doc/common/tables/keystone-rabbitmq.xml65(td) #: ./doc/common/tables/sahara-rabbitmq.xml65(td) #: ./doc/common/tables/neutron-rabbitmq.xml65(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml65(td) msgid ""rabbit_port = 5672"" msgstr ""rabbit_port = 5672"" #: ./doc/common/tables/nova-rabbitmq.xml66(td) #: ./doc/common/tables/cinder-rabbitmq.xml66(td) #: ./doc/common/tables/glance-rabbitmq.xml66(td) #: ./doc/common/tables/heat-rabbitmq.xml66(td) #: ./doc/common/tables/keystone-rabbitmq.xml66(td) #: ./doc/common/tables/sahara-rabbitmq.xml66(td) #: ./doc/common/tables/neutron-rabbitmq.xml66(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml66(td) msgid ""(IntOpt) The RabbitMQ broker port where a single node is used."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml69(td) #: ./doc/common/tables/cinder-rabbitmq.xml69(td) #: ./doc/common/tables/glance-rabbitmq.xml69(td) #: ./doc/common/tables/heat-rabbitmq.xml69(td) #: ./doc/common/tables/trove-rabbitmq.xml61(td) #: ./doc/common/tables/keystone-rabbitmq.xml69(td) #: ./doc/common/tables/sahara-rabbitmq.xml69(td) #: ./doc/common/tables/neutron-rabbitmq.xml69(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml69(td) msgid ""rabbit_retry_backoff = 2"" msgstr ""rabbit_retry_backoff = 2"" #: ./doc/common/tables/nova-rabbitmq.xml70(td) #: ./doc/common/tables/cinder-rabbitmq.xml70(td) #: ./doc/common/tables/glance-rabbitmq.xml70(td) #: ./doc/common/tables/heat-rabbitmq.xml70(td) #: ./doc/common/tables/keystone-rabbitmq.xml70(td) #: ./doc/common/tables/sahara-rabbitmq.xml70(td) #: ./doc/common/tables/neutron-rabbitmq.xml70(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml70(td) msgid """" ""(IntOpt) How long to backoff for between retries when connecting to "" ""RabbitMQ."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml73(td) #: ./doc/common/tables/cinder-rabbitmq.xml73(td) #: ./doc/common/tables/glance-rabbitmq.xml73(td) #: ./doc/common/tables/heat-rabbitmq.xml73(td) #: ./doc/common/tables/trove-rabbitmq.xml65(td) #: ./doc/common/tables/keystone-rabbitmq.xml73(td) #: ./doc/common/tables/sahara-rabbitmq.xml73(td) #: ./doc/common/tables/neutron-rabbitmq.xml73(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml73(td) msgid ""rabbit_retry_interval = 1"" msgstr ""rabbit_retry_interval = 1"" #: ./doc/common/tables/nova-rabbitmq.xml74(td) #: ./doc/common/tables/cinder-rabbitmq.xml74(td) #: ./doc/common/tables/glance-rabbitmq.xml74(td) #: ./doc/common/tables/heat-rabbitmq.xml74(td) #: ./doc/common/tables/keystone-rabbitmq.xml74(td) #: ./doc/common/tables/sahara-rabbitmq.xml74(td) #: ./doc/common/tables/neutron-rabbitmq.xml74(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml74(td) msgid ""(IntOpt) How frequently to retry connecting with RabbitMQ."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml77(td) #: ./doc/common/tables/cinder-rabbitmq.xml77(td) #: ./doc/common/tables/glance-rabbitmq.xml77(td) #: ./doc/common/tables/heat-rabbitmq.xml77(td) #: ./doc/common/tables/trove-rabbitmq.xml69(td) #: ./doc/common/tables/keystone-rabbitmq.xml77(td) #: ./doc/common/tables/sahara-rabbitmq.xml77(td) #: ./doc/common/tables/neutron-rabbitmq.xml77(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml77(td) msgid ""rabbit_use_ssl = False"" msgstr ""rabbit_use_ssl = False"" #: ./doc/common/tables/nova-rabbitmq.xml78(td) #: ./doc/common/tables/cinder-rabbitmq.xml78(td) #: ./doc/common/tables/glance-rabbitmq.xml78(td) #: ./doc/common/tables/heat-rabbitmq.xml78(td) #: ./doc/common/tables/keystone-rabbitmq.xml78(td) #: ./doc/common/tables/sahara-rabbitmq.xml78(td) #: ./doc/common/tables/neutron-rabbitmq.xml78(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml78(td) msgid ""(BoolOpt) Connect over SSL for RabbitMQ."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml81(td) #: ./doc/common/tables/cinder-rabbitmq.xml81(td) #: ./doc/common/tables/glance-rabbitmq.xml81(td) #: ./doc/common/tables/heat-rabbitmq.xml81(td) #: ./doc/common/tables/trove-rabbitmq.xml73(td) #: ./doc/common/tables/keystone-rabbitmq.xml81(td) #: ./doc/common/tables/sahara-rabbitmq.xml81(td) #: ./doc/common/tables/neutron-rabbitmq.xml81(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml81(td) msgid ""rabbit_userid = guest"" msgstr ""rabbit_userid = guest"" #: ./doc/common/tables/nova-rabbitmq.xml82(td) #: ./doc/common/tables/cinder-rabbitmq.xml82(td) #: ./doc/common/tables/glance-rabbitmq.xml82(td) #: ./doc/common/tables/heat-rabbitmq.xml82(td) #: ./doc/common/tables/keystone-rabbitmq.xml82(td) #: ./doc/common/tables/sahara-rabbitmq.xml82(td) #: ./doc/common/tables/neutron-rabbitmq.xml82(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml82(td) msgid ""(StrOpt) The RabbitMQ userid."" msgstr """" #: ./doc/common/tables/nova-rabbitmq.xml85(td) #: ./doc/common/tables/cinder-rabbitmq.xml85(td) #: ./doc/common/tables/glance-rabbitmq.xml85(td) #: ./doc/common/tables/heat-rabbitmq.xml85(td) #: ./doc/common/tables/trove-rabbitmq.xml77(td) #: ./doc/common/tables/keystone-rabbitmq.xml85(td) #: ./doc/common/tables/sahara-rabbitmq.xml85(td) #: ./doc/common/tables/neutron-rabbitmq.xml85(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml85(td) msgid ""rabbit_virtual_host = /"" msgstr ""rabbit_virtual_host = /"" #: ./doc/common/tables/nova-rabbitmq.xml86(td) #: ./doc/common/tables/cinder-rabbitmq.xml86(td) #: ./doc/common/tables/glance-rabbitmq.xml86(td) #: ./doc/common/tables/heat-rabbitmq.xml86(td) #: ./doc/common/tables/keystone-rabbitmq.xml86(td) #: ./doc/common/tables/sahara-rabbitmq.xml86(td) #: ./doc/common/tables/neutron-rabbitmq.xml86(td) #: ./doc/common/tables/ceilometer-rabbitmq.xml86(td) msgid ""(StrOpt) The RabbitMQ virtual host."" msgstr """" #: ./doc/common/tables/neutron-agent.xml7(caption) msgid ""Description of agent configuration options"" msgstr """" #: ./doc/common/tables/cinder-swift.xml7(caption) #: ./doc/common/tables/glance-swift.xml7(caption) #: ./doc/common/tables/ceilometer-swift.xml7(caption) #: ./doc/common/tables/trove-swift.xml7(caption) msgid ""Description of swift configuration options"" msgstr """" #: ./doc/common/tables/nova-glance.xml7(caption) #: ./doc/common/tables/ceilometer-glance.xml7(caption) msgid ""Description of glance configuration options"" msgstr """" #: ./doc/common/tables/nova-glance.xml25(th) msgid ""[glance]"" msgstr """" #: ./doc/common/tables/nova-glance.xml28(td) #: ./doc/common/tables/cinder-images.xml21(td) msgid ""allowed_direct_url_schemes ="" msgstr """" #: ./doc/common/tables/nova-glance.xml53(td) #: ./doc/common/tables/nova-conf-changes.xml125(td) msgid """" ""(StrOpt) Default protocol to use when connecting to glance. Set to https for"" "" SSL."" msgstr """" #: ./doc/common/tables/cinder-images.xml25(td) msgid ""glance_api_insecure = False"" msgstr """" #: ./doc/common/tables/cinder-images.xml49(td) msgid ""glance_host = $my_ip"" msgstr """" #: ./doc/common/tables/cinder-images.xml53(td) msgid ""glance_num_retries = 0"" msgstr """" #: ./doc/common/tables/cinder-images.xml57(td) msgid ""glance_port = 9292"" msgstr """" #: ./doc/common/tables/nova-baremetal.xml53(td) msgid ""pxe_append_params = nofb nomodeset vga=normal"" msgstr """" #: ./doc/common/tables/nova-baremetal.xml57(td) msgid ""pxe_bootfile_name = pxelinux.0"" msgstr """" #: ./doc/common/tables/nova-baremetal.xml77(td) msgid ""terminal = shellinaboxd"" msgstr """" #: ./doc/common/tables/nova-baremetal.xml81(td) msgid ""terminal_cert_dir = None"" msgstr """" #: ./doc/common/tables/nova-baremetal.xml89(td) msgid ""tftp_root = /tftpboot"" msgstr """" #: ./doc/common/tables/trove-logging.xml25(td) msgid """" ""default_log_levels = amqp=WARN, amqplib=WARN, boto=WARN, qpid=WARN, "" ""sqlalchemy=WARN, suds=INFO, oslo.messaging=INFO, iso8601=WARN, "" ""requests.packages.urllib3.connectionpool=WARN, urllib3.connectionpool=WARN, "" ""websocket=WARN"" msgstr """" #: ./doc/common/tables/nova-hypervisor.xml25(td) msgid ""force_raw_images = True"" msgstr """" #: ./doc/common/tables/keystone-cache.xml53(td) msgid ""memcache_pool_maxsize = 10"" msgstr """" #: ./doc/common/tables/keystone-cache.xml57(td) msgid ""memcache_pool_unused_timeout = 60"" msgstr """" #: ./doc/common/tables/ceilometer-ipmi.xml7(caption) msgid ""Description of IPMI configuration options"" msgstr """" #: ./doc/common/tables/ceilometer-ipmi.xml18(th) msgid ""[ipmi]"" msgstr """" ",5706,1976
openstack%2Fmanila~master~I50eb30cc1deae15457c2e5358be1eea310a943e2,openstack/manila,master,I50eb30cc1deae15457c2e5358be1eea310a943e2,Updated from global requirements,MERGED,2014-12-04 22:37:20.000000000,2014-12-07 06:36:00.000000000,2014-12-07 06:35:59.000000000,"[{'_account_id': 3}, {'_account_id': 6491}, {'_account_id': 8851}]","[{'number': 1, 'created': '2014-12-04 22:37:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/0e94125151205baf138755aba992565fd91a56b1', 'message': 'Updated from global requirements\n\nChange-Id: I50eb30cc1deae15457c2e5358be1eea310a943e2\n'}, {'number': 2, 'created': '2014-12-06 00:02:19.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/manila/commit/c6458ef6e01edfef66c710a1aa72e318e90a3628', 'message': 'Updated from global requirements\n\nChange-Id: I50eb30cc1deae15457c2e5358be1eea310a943e2\n'}]",0,139232,c6458ef6e01edfef66c710a1aa72e318e90a3628,10,3,2,11131,,,0,"Updated from global requirements

Change-Id: I50eb30cc1deae15457c2e5358be1eea310a943e2
",git fetch https://review.opendev.org/openstack/manila refs/changes/32/139232/2 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,0e94125151205baf138755aba992565fd91a56b1,openstack/requirements,"oslo.messaging>=1.4.0,!=1.5.0",oslo.messaging>=1.4.0,1,1
openstack%2Fmagnum~master~I5689d024f4ece93369498bd7f64cde0b8a59e292,openstack/magnum,master,I5689d024f4ece93369498bd7f64cde0b8a59e292,Add image_id and node_count to bay,MERGED,2014-12-05 01:00:00.000000000,2014-12-07 02:05:38.000000000,2014-12-07 02:05:37.000000000,"[{'_account_id': 3}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-12-05 01:00:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/4e9b90fdc4ab4ed9d4c21c4871e4ea9ddc13c298', 'message': 'Add image_id and node_count to bay\n\nThe idea of bay-create is that you create a grouping of machines that run\ncontainer software.  In order to do this, an image_id stored in glance is\nnecessary to pull the base image to modify for launching by either heat or\nIronic.  A node_count is needed as well, to identify how many ironic nodes\nyou want to launch.\n\nChange-Id: I5689d024f4ece93369498bd7f64cde0b8a59e292\n'}, {'number': 2, 'created': '2014-12-05 16:51:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/5d55c9394181e1712ba79820828779007ecdcc6e', 'message': 'Add image_id and node_count to bay\n\nThe idea of bay-create is that you create a grouping of machines that run\ncontainer software.  In order to do this, an image_id stored in glance is\nnecessary to pull the base image to modify for launching by either heat or\nIronic.  A node_count is needed as well, to identify how many ironic nodes\nyou want to launch.\n\nChange-Id: I5689d024f4ece93369498bd7f64cde0b8a59e292\n'}, {'number': 3, 'created': '2014-12-05 17:07:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/5669943f7133c06edfaa9daade9c10e36d12e3f2', 'message': 'Add image_id and node_count to bay\n\nThe idea of bay-create is that you create a grouping of machines that run\ncontainer software.  In order to do this, an image_id stored in glance is\nnecessary to pull the base image to modify for launching by either heat or\nIronic.  A node_count is needed as well, to identify how many ironic nodes\nyou want to launch.\n\nChange-Id: I5689d024f4ece93369498bd7f64cde0b8a59e292\n'}, {'number': 4, 'created': '2014-12-07 00:31:28.000000000', 'files': ['magnum/objects/bay.py', 'magnum/tests/test_functional.py', 'magnum/api/controllers/v1/bay.py', 'magnum/db/sqlalchemy/alembic/versions/2581ebaf0cb2_initial_migration.py', 'magnum/db/sqlalchemy/models.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/fc7c1aa39569e1315e249763c6f43e8d3d4ba647', 'message': 'Add image_id and node_count to bay\n\nThe idea of bay-create is that you create a grouping of machines that run\ncontainer software.  In order to do this, an image_id stored in glance is\nnecessary to pull the base image to modify for launching by either heat or\nIronic.  A node_count is needed as well, to identify how many ironic nodes\nyou want to launch.\n\nChange-Id: I5689d024f4ece93369498bd7f64cde0b8a59e292\n'}]",0,139293,fc7c1aa39569e1315e249763c6f43e8d3d4ba647,12,2,4,2834,,,0,"Add image_id and node_count to bay

The idea of bay-create is that you create a grouping of machines that run
container software.  In order to do this, an image_id stored in glance is
necessary to pull the base image to modify for launching by either heat or
Ironic.  A node_count is needed as well, to identify how many ironic nodes
you want to launch.

Change-Id: I5689d024f4ece93369498bd7f64cde0b8a59e292
",git fetch https://review.opendev.org/openstack/magnum refs/changes/93/139293/4 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/objects/bay.py', 'magnum/tests/test_functional.py', 'magnum/api/controllers/v1/bay.py', 'magnum/db/sqlalchemy/alembic/versions/2581ebaf0cb2_initial_migration.py', 'magnum/db/sqlalchemy/models.py']",5,4e9b90fdc4ab4ed9d4c21c4871e4ea9ddc13c298,, image_id = Column(String(255)) node_count = Column(Integer()),,21,3
openstack%2Fmagnum~master~If493eaee919e55b2e5f85f2411c0b0d813f21bde,openstack/magnum,master,If493eaee919e55b2e5f85f2411c0b0d813f21bde,Copy ironic/common files to magnum/common for RPC server,MERGED,2014-12-06 20:15:53.000000000,2014-12-07 02:05:22.000000000,2014-12-07 02:05:22.000000000,"[{'_account_id': 3}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-12-06 20:15:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/873d8df9da31f9fc7b256b3ae594d05a4d5a1f83', 'message': 'Copy ironic/common files to magnum/common for RPC server\n\nThese are straight copies of ironic/ironic/common files needed\nfor the RPC server.\n\nChange-Id: If493eaee919e55b2e5f85f2411c0b0d813f21bde\n'}, {'number': 2, 'created': '2014-12-06 21:20:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/c78d27fa9ecf82ee225af1879d00d8fb4d5de95f', 'message': 'Copy ironic/common files to magnum/common for RPC server\n\nThese are straight copies of ironic/ironic/common files needed\nfor the RPC server.\n\nChange-Id: If493eaee919e55b2e5f85f2411c0b0d813f21bde\n'}, {'number': 3, 'created': '2014-12-06 21:21:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/edc3e3aec9bd21d5d9867402260878848eaf0e64', 'message': 'Copy ironic/common files to magnum/common for RPC server\n\nThese are straight copies of ironic/ironic/common files needed\nfor the RPC server.\n\nChange-Id: If493eaee919e55b2e5f85f2411c0b0d813f21bde\n'}, {'number': 4, 'created': '2014-12-06 21:26:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/a1f17503b2fc43e684b57ba943b17183ee427cf5', 'message': 'Copy ironic/common files to magnum/common for RPC server\n\nThese are straight copies of ironic/ironic/common files needed\nfor the RPC server.\n\nChange-Id: If493eaee919e55b2e5f85f2411c0b0d813f21bde\n'}, {'number': 5, 'created': '2014-12-06 21:33:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/64c7560800f7ae92acc464f7a2b2ff402d5da521', 'message': 'Copy ironic/common files to magnum/common for RPC server\n\nThese are straight copies of ironic/ironic/common files needed\nfor the RPC server.\n\nChange-Id: If493eaee919e55b2e5f85f2411c0b0d813f21bde\n'}, {'number': 6, 'created': '2014-12-07 00:26:10.000000000', 'files': ['magnum/common/rpc.py', 'magnum/common/context.py', 'magnum/common/config.py', 'magnum/common/service.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/3f85ccce3a55f37c39e10d382f4dfc8cd5a5fd91', 'message': 'Copy ironic/common files to magnum/common for RPC server\n\nThese are straight copies of ironic/ironic/common files needed\nfor the RPC server.\n\nChange-Id: If493eaee919e55b2e5f85f2411c0b0d813f21bde\n'}]",0,139833,3f85ccce3a55f37c39e10d382f4dfc8cd5a5fd91,15,2,6,2834,,,0,"Copy ironic/common files to magnum/common for RPC server

These are straight copies of ironic/ironic/common files needed
for the RPC server.

Change-Id: If493eaee919e55b2e5f85f2411c0b0d813f21bde
",git fetch https://review.opendev.org/openstack/magnum refs/changes/33/139833/3 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/common/context.py', 'magnum/common/rpc.py', 'magnum/common/config.py', 'magnum/common/service.py']",4,873d8df9da31f9fc7b256b3ae594d05a4d5a1f83,,"# -*- encoding: utf-8 -*-# Copyright © 2012 eNovance <licensing@enovance.com># Author: Julien Danjou <julien@danjou.info> # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import socketfrom oslo import messaging from oslo.utils import importutils from magnum.common import config from magnum.common.i18n import _LE from magnum.common.i18n import _LI from magnum.common import rpc from magnum.objects import base as objects_base from magnum.openstack.common import context from magnum.openstack.common import log from magnum.openstack.common import service service_opts = [ cfg.IntOpt('periodic_interval', default=60, help='Seconds between running periodic tasks.'), cfg.StrOpt('host', default=socket.getfqdn(), help='Name of this node. This can be an opaque identifier. ' 'It is not necessarily a hostname, FQDN, or IP address. ' 'However, the node name must be valid within ' 'an AMQP key, and if using ZeroMQ, a valid ' 'hostname, FQDN, or IP address.'), ] cfg.CONF.register_opts(service_opts) LOG = log.getLogger(__name__) class RPCService(service.Service): def __init__(self, host, manager_module, manager_class): super(RPCService, self).__init__() self.host = host manager_module = importutils.try_import(manager_module) manager_class = getattr(manager_module, manager_class) self.manager = manager_class(host, manager_module.MANAGER_TOPIC) self.topic = self.manager.topic self.rpcserver = None def start(self): super(RPCService, self).start() admin_context = context.RequestContext('admin', 'admin', is_admin=True) self.tg.add_dynamic_timer( self.manager.periodic_tasks, periodic_interval_max=cfg.CONF.periodic_interval, context=admin_context) self.manager.init_host() target = messaging.Target(topic=self.topic, server=self.host) endpoints = [self.manager] serializer = objects_base.IronicObjectSerializer() self.rpcserver = rpc.get_server(target, endpoints, serializer) self.rpcserver.start() LOG.info(_LI('Created RPC server for service %(service)s on host ' '%(host)s.'), {'service': self.topic, 'host': self.host}) def stop(self): super(RPCService, self).stop() try: self.rpcserver.stop() self.rpcserver.wait() except Exception as e: LOG.exception(_LE('Service error occurred when stopping the ' 'RPC server. Error: %s'), e) try: self.manager.del_host() except Exception as e: LOG.exception(_LE('Service error occurred when cleaning up ' 'the RPC manager. Error: %s'), e) LOG.info(_LI('Stopped RPC server for service %(service)s on host ' '%(host)s.'), {'service': self.topic, 'host': self.host}) config.parse_args(argv) cfg.set_defaults(log.log_opts, default_log_levels=['amqp=WARN', 'amqplib=WARN', 'qpid.messaging=INFO', 'sqlalchemy=WARN', 'keystoneclient=INFO', 'stevedore=INFO', 'eventlet.wsgi.server=WARN', 'iso8601=WARN', 'paramiko=WARN', 'requests=WARN', 'neutronclient=WARN', 'glanceclient=WARN', 'magnum.openstack.common=WARN', ]) log.setup('magnum')","# Copyright 2013 - Red Hat, Inc.# Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at# http://www.apache.org/licenses/LICENSE-2.0# distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from magnum.openstack.common import log as logging cfg.CONF(argv[1:], project='magnum') logging.setup('magnum') # may make sense to load DB here",298,48
openstack%2Fmagnum~master~If818b318a75be783a29c8120f43ab43176aadcc9,openstack/magnum,master,If818b318a75be783a29c8120f43ab43176aadcc9,Remove common/rpc directory,MERGED,2014-12-06 20:15:53.000000000,2014-12-07 02:05:02.000000000,2014-12-07 02:05:01.000000000,"[{'_account_id': 3}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-12-06 20:15:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/fc321790552b079073b9f7f019690ea7f140cb8f', 'message': 'Remove common/rpc directory\n\nIronic uses common/rpc.py rather then a directory.  As a result of copying\nthis common code from ironic, need to remove the directory.\n\nChange-Id: If818b318a75be783a29c8120f43ab43176aadcc9\n'}, {'number': 2, 'created': '2014-12-06 21:20:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/095f8d97363ba95f7292c757ce42227a3c3bdde8', 'message': 'Remove common/rpc directory\n\nIronic uses common/rpc.py rather then a directory.  As a result of copying\nthis common code from ironic, need to remove the directory.\n\nChange-Id: If818b318a75be783a29c8120f43ab43176aadcc9\n'}, {'number': 3, 'created': '2014-12-06 21:21:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/9535baef78a693bec687cf5712de9eeb506fb5ef', 'message': 'Remove common/rpc directory\n\nIronic uses common/rpc.py rather then a directory.  As a result of copying\nthis common code from ironic, need to remove the directory.\n\nChange-Id: If818b318a75be783a29c8120f43ab43176aadcc9\n'}, {'number': 4, 'created': '2014-12-06 21:26:20.000000000', 'files': ['magnum/common/rpc/__init__.py', 'magnum/common/rpc/service.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/efaeeb0457825120604fc6b38fef8409c3c57ca7', 'message': 'Remove common/rpc directory\n\nIronic uses common/rpc.py rather then a directory.  As a result of copying\nthis common code from ironic, need to remove the directory.\n\nChange-Id: If818b318a75be783a29c8120f43ab43176aadcc9\n'}]",0,139832,efaeeb0457825120604fc6b38fef8409c3c57ca7,10,2,4,2834,,,0,"Remove common/rpc directory

Ironic uses common/rpc.py rather then a directory.  As a result of copying
this common code from ironic, need to remove the directory.

Change-Id: If818b318a75be783a29c8120f43ab43176aadcc9
",git fetch https://review.opendev.org/openstack/magnum refs/changes/32/139832/1 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/common/rpc/__init__.py', 'magnum/common/rpc/service.py']",2,fc321790552b079073b9f7f019690ea7f140cb8f,,,"# Copyright 2014 - Rackspace Hosting # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """"""Common RPC service and API tools for Magnum."""""" import eventlet from oslo.config import cfg from oslo import messaging from oslo.serialization import jsonutils import magnum.common.context # NOTE(paulczar): # Ubuntu 14.04 forces librabbitmq when kombu is used # Unfortunately it forces a version that has a crash # bug. Calling eventlet.monkey_patch() tells kombu # to use libamqp instead. eventlet.monkey_patch() # NOTE(asalkeld): # The magnum.openstack.common.rpc entries are for compatability # with devstack rpc_backend configuration values. TRANSPORT_ALIASES = { 'magnum.openstack.common.rpc.impl_kombu': 'rabbit', 'magnum.openstack.common.rpc.impl_qpid': 'qpid', 'magnum.openstack.common.rpc.impl_zmq': 'zmq', } class JsonPayloadSerializer(messaging.NoOpSerializer): @staticmethod def serialize_entity(context, entity): return jsonutils.to_primitive(entity, convert_instances=True) class RequestContextSerializer(messaging.Serializer): def __init__(self, base=None): self._base = base or messaging.NoOpSerializer() def serialize_entity(self, context, entity): if not self._base: return entity return self._base.serialize_entity(context, entity) def deserialize_entity(self, context, entity): if not self._base: return entity return self._base.deserialize_entity(context, entity) def serialize_context(self, context): return context.to_dict() def deserialize_context(self, context): return magnum.common.context.RequestContext.from_dict(context) class Service(object): _server = None def __init__(self, topic, server, handlers): serializer = RequestContextSerializer(JsonPayloadSerializer()) transport = messaging.get_transport(cfg.CONF, aliases=TRANSPORT_ALIASES) # TODO(asalkeld) add support for version='x.y' target = messaging.Target(topic=topic, server=server) self._server = messaging.get_rpc_server(transport, target, handlers, serializer=serializer) def serve(self): self._server.start() self._server.wait() class API(object): def __init__(self, transport=None, context=None, topic=None): serializer = RequestContextSerializer(JsonPayloadSerializer()) if transport is None: transport = messaging.get_transport(cfg.CONF, aliases=TRANSPORT_ALIASES) self._context = context if topic is None: topic = '' target = messaging.Target(topic=topic) self._client = messaging.RPCClient(transport, target, serializer=serializer) def _call(self, method, *args, **kwargs): return self._client.call(self._context, method, *args, **kwargs) def _cast(self, method, *args, **kwargs): self._client.cast(self._context, method, *args, **kwargs) def echo(self, message): self._cast('echo', message=message) ",0,107
openstack%2Fmagnum~master~I09c67b1ddf2e74f32aad7482b69f563b4695696e,openstack/magnum,master,I09c67b1ddf2e74f32aad7482b69f563b4695696e,Add dependencies from oslo-incubator for RPC services,MERGED,2014-12-06 20:15:53.000000000,2014-12-07 02:04:01.000000000,2014-12-07 02:04:01.000000000,"[{'_account_id': 3}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-12-06 20:15:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/5fca6de277ac904183f69e5285669857ae328feb', 'message': 'Add dependencies from oslo-incubator for RPC services\n\nThese dependencies are required to create an RPC service.  They are direct\ncopies of oslo-incubator.git master using python ./update.py.\n\nChange-Id: I09c67b1ddf2e74f32aad7482b69f563b4695696e\n'}, {'number': 2, 'created': '2014-12-06 21:20:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/6723e76a6ebf8e4dcfda961d64dd492833af0485', 'message': 'Add dependencies from oslo-incubator for RPC services\n\nThese dependencies are required to create an RPC service.  They are direct\ncopies of oslo-incubator.git master using python ./update.py.\n\nChange-Id: I09c67b1ddf2e74f32aad7482b69f563b4695696e\n'}, {'number': 3, 'created': '2014-12-06 21:21:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/2553d52aac922dd0a2e7db2c369dcc1193e6ba24', 'message': 'Add dependencies from oslo-incubator for RPC services\n\nThese dependencies are required to create an RPC service.  They are direct\ncopies of oslo-incubator.git master using python ./update.py.\n\nChange-Id: I09c67b1ddf2e74f32aad7482b69f563b4695696e\n'}, {'number': 4, 'created': '2014-12-06 21:26:20.000000000', 'files': ['magnum/openstack/common/threadgroup.py', 'magnum/openstack/common/periodic_task.py', 'magnum/openstack/common/loopingcall.py', 'magnum/openstack/common/eventlet_backdoor.py', 'magnum/openstack/common/service.py', 'openstack-common.conf', 'magnum/openstack/common/systemd.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/a957f3de241d5f5b229656c7d8a6cc55ca345043', 'message': 'Add dependencies from oslo-incubator for RPC services\n\nThese dependencies are required to create an RPC service.  They are direct\ncopies of oslo-incubator.git master using python ./update.py.\n\nChange-Id: I09c67b1ddf2e74f32aad7482b69f563b4695696e\n'}]",0,139831,a957f3de241d5f5b229656c7d8a6cc55ca345043,12,2,4,2834,,,0,"Add dependencies from oslo-incubator for RPC services

These dependencies are required to create an RPC service.  They are direct
copies of oslo-incubator.git master using python ./update.py.

Change-Id: I09c67b1ddf2e74f32aad7482b69f563b4695696e
",git fetch https://review.opendev.org/openstack/magnum refs/changes/31/139831/4 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/openstack/common/periodic_task.py', 'magnum/openstack/common/loopingcall.py', 'magnum/openstack/common/eventlet_backdoor.py', 'openstack-common.conf', 'magnum/openstack/common/systemd.py']",5,5fca6de277ac904183f69e5285669857ae328feb,,"# Copyright 2012-2014 Red Hat, Inc. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """""" Helper module for systemd service readiness notification. """""" import os import socket import sys from magnum.openstack.common import log as logging LOG = logging.getLogger(__name__) def _abstractify(socket_name): if socket_name.startswith('@'): # abstract namespace socket socket_name = '\0%s' % socket_name[1:] return socket_name def _sd_notify(unset_env, msg): notify_socket = os.getenv('NOTIFY_SOCKET') if notify_socket: sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM) try: sock.connect(_abstractify(notify_socket)) sock.sendall(msg) if unset_env: del os.environ['NOTIFY_SOCKET'] except EnvironmentError: LOG.debug(""Systemd notification failed"", exc_info=True) finally: sock.close() def notify(): """"""Send notification to Systemd that service is ready. For details see http://www.freedesktop.org/software/systemd/man/sd_notify.html """""" _sd_notify(False, 'READY=1') def notify_once(): """"""Send notification once to Systemd that service is ready. Systemd sets NOTIFY_SOCKET environment variable with the name of the socket listening for notifications from services. This method removes the NOTIFY_SOCKET environment variable to ensure notification is sent only once. """""" _sd_notify(True, 'READY=1') def onready(notify_socket, timeout): """"""Wait for systemd style notification on the socket. :param notify_socket: local socket address :type notify_socket: string :param timeout: socket timeout :type timeout: float :returns: 0 service ready 1 service not ready 2 timeout occurred """""" sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM) sock.settimeout(timeout) sock.bind(_abstractify(notify_socket)) try: msg = sock.recv(512) except socket.timeout: return 2 finally: sock.close() if 'READY=1' in msg: return 0 else: return 1 if __name__ == '__main__': # simple CLI for testing if len(sys.argv) == 1: notify() elif len(sys.argv) >= 2: timeout = float(sys.argv[1]) notify_socket = os.getenv('NOTIFY_SOCKET') if notify_socket: retval = onready(notify_socket, timeout) sys.exit(retval) ",,622,0
openstack%2Fmagnum~master~I5fb73100b8fd4365fada7b17d4f4780fc387a1ca,openstack/magnum,master,I5fb73100b8fd4365fada7b17d4f4780fc387a1ca,Update openstack.common from oslo-incubator,MERGED,2014-12-06 20:15:53.000000000,2014-12-07 02:01:05.000000000,2014-12-07 02:01:05.000000000,"[{'_account_id': 3}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-12-06 20:15:53.000000000', 'files': ['magnum/openstack/common/_i18n.py', 'magnum/openstack/common/versionutils.py', 'magnum/openstack/common/log.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/ab5b4265e5deaaeed9c7c6e762df44a9303401bf', 'message': 'Update openstack.common from oslo-incubator\n\nrun update.py from oslo-icubator.git on the repository.\n\nChange-Id: I5fb73100b8fd4365fada7b17d4f4780fc387a1ca\n'}]",0,139830,ab5b4265e5deaaeed9c7c6e762df44a9303401bf,6,2,1,2834,,,0,"Update openstack.common from oslo-incubator

run update.py from oslo-icubator.git on the repository.

Change-Id: I5fb73100b8fd4365fada7b17d4f4780fc387a1ca
",git fetch https://review.opendev.org/openstack/magnum refs/changes/30/139830/1 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/openstack/common/_i18n.py', 'magnum/openstack/common/versionutils.py', 'magnum/openstack/common/log.py']",3,ab5b4265e5deaaeed9c7c6e762df44a9303401bf,," syslog = RFCSysLogHandler(address='/dev/log', facility=facility) else: syslog = logging.handlers.SysLogHandler(address='/dev/log', facility=facility)", syslog = RFCSysLogHandler(facility=facility) else: syslog = logging.handlers.SysLogHandler(facility=facility),56,29
openstack%2Fmagnum~master~I077aaa80fad40a10148bcab93c5278e5871e899d,openstack/magnum,master,I077aaa80fad40a10148bcab93c5278e5871e899d,Remove stray print which caused magnum-db-manage to fail,MERGED,2014-12-05 01:10:05.000000000,2014-12-07 02:00:53.000000000,2014-12-07 02:00:52.000000000,"[{'_account_id': 3}, {'_account_id': 5638}, {'_account_id': 7770}]","[{'number': 1, 'created': '2014-12-05 01:10:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/6a7b9bb102bd4dcca0883beca2a2b82f9a80caa4', 'message': 'Remove stray print which caused magnum-db-manage to fail\n\nA stray debug print was causing magnum-db-manage to fail because the\nconfiguration file was read after the print.  The print printed out\nconfiguration information.\n\nChange-Id: I077aaa80fad40a10148bcab93c5278e5871e899d\n'}, {'number': 2, 'created': '2014-12-05 16:48:58.000000000', 'files': ['magnum/cmd/db_manage.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/3a7e9fb43515f8c5b2410d183af5d704447fbfed', 'message': 'Remove stray print which caused magnum-db-manage to fail\n\nA stray debug print was causing magnum-db-manage to fail because the\nconfiguration file was read after the print.  The print printed out\nconfiguration information.\n\nChange-Id: I077aaa80fad40a10148bcab93c5278e5871e899d\n'}]",0,139294,3a7e9fb43515f8c5b2410d183af5d704447fbfed,9,3,2,2834,,,0,"Remove stray print which caused magnum-db-manage to fail

A stray debug print was causing magnum-db-manage to fail because the
configuration file was read after the print.  The print printed out
configuration information.

Change-Id: I077aaa80fad40a10148bcab93c5278e5871e899d
",git fetch https://review.opendev.org/openstack/magnum refs/changes/94/139294/2 && git format-patch -1 --stdout FETCH_HEAD,['magnum/cmd/db_manage.py'],1,6a7b9bb102bd4dcca0883beca2a2b82f9a80caa4,,from magnum.common import config, print ('manager is %s' % get_manager()),1,1
openstack%2Fcinder~stable%2Ficehouse~I6107b996d5da808c3222696a9549ee06c22f80b9,openstack/cinder,stable/icehouse,I6107b996d5da808c3222696a9549ee06c22f80b9,Remove check_uptodate.sh check from tox.ini,MERGED,2014-12-06 09:09:23.000000000,2014-12-06 22:31:38.000000000,2014-12-06 22:31:37.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 979}, {'_account_id': 1420}, {'_account_id': 2243}, {'_account_id': 10621}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-06 09:09:23.000000000', 'files': ['run_tests.sh', 'etc/cinder/cinder.conf.sample', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/cinder/commit/e44633599cf65d03e07642d0627dd19e882fb7f0', 'message': 'Remove check_uptodate.sh check from tox.ini\n\nThis removes the check_uptodate.sh test from the py2* tox envs and\nrun_tests.sh. It also syncs the sample config one last time to document\nsome new options available to users running with the newer\noslo.messaging versions.\n\nConflicts:\n\tetc/cinder/cinder.conf.sample\n\ttox.ini\n\nChange-Id: I6107b996d5da808c3222696a9549ee06c22f80b9\nCloses-bug: #1399085\n(cherry picked from commit 31b58310ddf519c241191919be8ffcbdaf0e4983)\n'}]",0,139791,e44633599cf65d03e07642d0627dd19e882fb7f0,9,7,1,1955,,,0,"Remove check_uptodate.sh check from tox.ini

This removes the check_uptodate.sh test from the py2* tox envs and
run_tests.sh. It also syncs the sample config one last time to document
some new options available to users running with the newer
oslo.messaging versions.

Conflicts:
	etc/cinder/cinder.conf.sample
	tox.ini

Change-Id: I6107b996d5da808c3222696a9549ee06c22f80b9
Closes-bug: #1399085
(cherry picked from commit 31b58310ddf519c241191919be8ffcbdaf0e4983)
",git fetch https://review.opendev.org/openstack/cinder refs/changes/91/139791/1 && git format-patch -1 --stdout FETCH_HEAD,"['run_tests.sh', 'etc/cinder/cinder.conf.sample', 'tox.ini']",3,e44633599cf65d03e07642d0627dd19e882fb7f0,bug/1399085,, {toxinidir}/tools/config/check_uptodate.sh,69,9
openstack%2Fcinder~master~I9b0933979b0df44d8ccd46699ee58acdda2df14b,openstack/cinder,master,I9b0933979b0df44d8ccd46699ee58acdda2df14b,Updated from global requirements,MERGED,2014-12-04 22:35:57.000000000,2014-12-06 22:13:51.000000000,2014-12-05 18:30:12.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-04 22:35:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b9f6830ed12f4046b3f1710aa69c6f7099b23f46', 'message': 'Updated from global requirements\n\nChange-Id: I9b0933979b0df44d8ccd46699ee58acdda2df14b\n'}, {'number': 2, 'created': '2014-12-05 16:32:56.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/cinder/commit/433c836e559e5f65072b3ac6c8309bc631e939d5', 'message': 'Updated from global requirements\n\nChange-Id: I9b0933979b0df44d8ccd46699ee58acdda2df14b\n'}]",0,139226,433c836e559e5f65072b3ac6c8309bc631e939d5,18,9,2,11131,,,0,"Updated from global requirements

Change-Id: I9b0933979b0df44d8ccd46699ee58acdda2df14b
",git fetch https://review.opendev.org/openstack/cinder refs/changes/26/139226/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,b9f6830ed12f4046b3f1710aa69c6f7099b23f46,openstack/requirements,"oslo.messaging>=1.4.0,!=1.5.0",oslo.messaging>=1.4.0,1,1
openstack%2Foslo.concurrency~stable%2Fjuno~I2128bed5623f5caaaababfddac0f301e97864432,openstack/oslo.concurrency,stable/juno,I2128bed5623f5caaaababfddac0f301e97864432,Updated from global requirements,MERGED,2014-11-27 10:37:33.000000000,2014-12-06 21:56:02.000000000,2014-12-06 21:56:02.000000000,"[{'_account_id': 3}, {'_account_id': 979}, {'_account_id': 1297}, {'_account_id': 1669}, {'_account_id': 1955}, {'_account_id': 2472}, {'_account_id': 6786}, {'_account_id': 13290}]","[{'number': 1, 'created': '2014-11-27 10:37:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.concurrency/commit/2a1c447f1599c3a9974cd695cfaf72ccd1008fea', 'message': 'Updated from global requirements\n\nChange-Id: I2128bed5623f5caaaababfddac0f301e97864432\n'}, {'number': 2, 'created': '2014-12-03 21:02:29.000000000', 'files': ['test-requirements.txt'], 'web_link': 'https://opendev.org/openstack/oslo.concurrency/commit/23da92ba07bdbdea905a9a24554800ed4609012c', 'message': 'Updated from global requirements\n\nChange-Id: I2128bed5623f5caaaababfddac0f301e97864432\n'}]",0,137581,23da92ba07bdbdea905a9a24554800ed4609012c,18,8,2,11131,,,0,"Updated from global requirements

Change-Id: I2128bed5623f5caaaababfddac0f301e97864432
",git fetch https://review.opendev.org/openstack/oslo.concurrency refs/changes/81/137581/2 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'test-requirements.txt', 'requirements-py3.txt']",3,2a1c447f1599c3a9974cd695cfaf72ccd1008fea,openstack/requirements,"oslo.config>=1.4.0,<1.5 # Apache-2.0 oslo.i18n>=1.0.0,<1.5 # Apache-2.0 oslo.utils>=1.0.0,<1.1 # Apache-2.0",oslo.config>=1.4.0 # Apache-2.0 oslo.i18n>=1.0.0 # Apache-2.0 oslo.utils>=1.0.0 # Apache-2.0,9,9
openstack%2Ftaskflow~master~I28a9d66afa7f7b733b2963b8cee3edd45696561b,openstack/taskflow,master,I28a9d66afa7f7b733b2963b8cee3edd45696561b,Refactor parts of the job lock/job condition zookeeper usage,MERGED,2014-09-13 00:38:24.000000000,2014-12-06 21:51:04.000000000,2014-12-06 21:51:03.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 11024}]","[{'number': 1, 'created': '2014-09-13 00:38:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/eddd0b649e31082e30dfda5d3f68046ba080ccb7', 'message': 'Refactor parts of the job lock/job condition zookeeper usage\n\nThis reduces the necessary locks to operations which really only\nneed to be locked, removing some of the ones which are not needed\naround read only operations or operations which are thread safe\nwhen used with dictionaries (popping a single item for example)\nor checking if a *string* key is in a dictionary, or fetching a\ndictionaries length...\n\nChange-Id: I28a9d66afa7f7b733b2963b8cee3edd45696561b\n'}, {'number': 2, 'created': '2014-10-02 17:50:21.000000000', 'files': ['taskflow/jobs/backends/impl_zookeeper.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/95b30d60cd1be5cbdf953e30c379822aad08ab07', 'message': 'Refactor parts of the job lock/job condition zookeeper usage\n\nThis reduces the necessary locks to operations which really only\nneed to be locked, removing some of the ones which are not needed\naround read only operations or operations which are thread safe\nwhen used with dictionaries (popping a single item for example)\nor checking if a *string* key is in a dictionary, or fetching a\ndictionaries length...\n\nChange-Id: I28a9d66afa7f7b733b2963b8cee3edd45696561b\n'}]",0,121275,95b30d60cd1be5cbdf953e30c379822aad08ab07,10,3,2,1297,,,0,"Refactor parts of the job lock/job condition zookeeper usage

This reduces the necessary locks to operations which really only
need to be locked, removing some of the ones which are not needed
around read only operations or operations which are thread safe
when used with dictionaries (popping a single item for example)
or checking if a *string* key is in a dictionary, or fetching a
dictionaries length...

Change-Id: I28a9d66afa7f7b733b2963b8cee3edd45696561b
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/75/121275/2 && git format-patch -1 --stdout FETCH_HEAD,['taskflow/jobs/backends/impl_zookeeper.py'],1,eddd0b649e31082e30dfda5d3f68046ba080ccb7,," self._job_cond = threading.Condition() return len(self._known_jobs) with self._job_cond: job = self._known_jobs.pop(path, None) with self._job_cond: if path not in self._known_jobs: # Fire off the request to populate this job asynchronously. # # This method is *usually* called from a asynchronous # handler so it's better to exit from this quickly to # allow other asynchronous handlers to be executed. request = self._client.get_async(path) child_proc = functools.partial(self._process_child, path) if delayed: request.rawlink(child_proc) else: child_proc(request) if job_path not in self._known_jobs: fail_msg_tpl += "", unknown job"" raise excp.NotFound(fail_msg_tpl % (job_uuid)) with self._job_cond:"," self._job_lock = threading.RLock() self._job_cond = threading.Condition(self._job_lock) with self._job_lock: return len(self._known_jobs) with self._job_lock: with self._job_lock: job = self._known_jobs.pop(path, None) with self._job_lock: with self._job_lock: if path not in self._known_jobs: # Fire off the request to populate this job asynchronously. # # This method is *usually* called from a asynchronous # handler so it's better to exit from this quickly to # allow other asynchronous handlers to be executed. request = self._client.get_async(path) child_proc = functools.partial(self._process_child, path) if delayed: request.rawlink(child_proc) else: child_proc(request) with self._job_lock: if job_path not in self._known_jobs: fail_msg_tpl += "", unknown job"" raise excp.NotFound(fail_msg_tpl % (job_uuid)) with self._job_lock:",21,26
openstack%2Ftaskflow~master~I4bc4db1e9a08153feda8f6e991804b60e94eedbc,openstack/taskflow,master,I4bc4db1e9a08153feda8f6e991804b60e94eedbc,Add WBE protocol message version properties,ABANDONED,2014-06-22 20:34:27.000000000,2014-12-06 21:29:33.000000000,,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 8895}, {'_account_id': 9608}, {'_account_id': 9648}]","[{'number': 1, 'created': '2014-06-22 20:34:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/9cca2402bc8f8b22cc391539b51cb54bdd3e70c8', 'message': 'Add WBE protocol message version properties\n\nIn order for the WBE messages and message users to be\nversion aware we first need to include a version that\ncan be later used to detect what message version is\nbeing sent and adjust reception accordingly.\n\nChange-Id: I4bc4db1e9a08153feda8f6e991804b60e94eedbc\n'}, {'number': 2, 'created': '2014-06-25 17:57:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/40aa4d388b8a14d2f5c912f4262655d1f78bc27e', 'message': 'Add WBE protocol message version properties\n\nIn order for the WBE messages and message users to be\nversion aware we first need to include a version that\ncan be later used to detect what message version is\nbeing sent and adjust reception accordingly.\n\nIn this patch we are also removing a few not so useful\ntests that were affected by this new version addition.\nThose tests check the str() and repr() of a request\nand in general are not useful tests since such tests\ndo not add real value. So instead of  adjusting them\nI made the executive decision to just remove them.\n\nChange-Id: I4bc4db1e9a08153feda8f6e991804b60e94eedbc\n'}, {'number': 3, 'created': '2014-08-15 06:11:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/2edb01c4c6a6825c03c404c36659ebffcb02cfc5', 'message': 'Add WBE protocol message version properties\n\nIn order for the WBE messages and message users to be\nversion aware we first need to include a version that\ncan be later used to detect what message version is\nbeing sent and adjust reception accordingly.\n\nIn this patch we are also removing a few not so useful\ntests that were affected by this new version addition.\nThose tests check the str() and repr() of a request\nand in general are not useful tests since such tests\ndo not add real value. So instead of  adjusting them\nI made the executive decision to just remove them.\n\nChange-Id: I4bc4db1e9a08153feda8f6e991804b60e94eedbc\n'}, {'number': 4, 'created': '2014-08-15 15:27:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/b8f86518c25b4c31c09efa93a27db1c3a551bd54', 'message': 'Add WBE protocol message version properties\n\nIn order for the WBE messages and message users to be\nversion aware we first need to include a version that\ncan be later used to detect what message version is\nbeing sent and adjust reception accordingly.\n\nIn this patch we are also removing a few not so useful\ntests that were affected by this new version addition.\nThose tests check the str() and repr() of a request\nand in general are not useful tests since such tests\ndo not add real value. So instead of  adjusting them\nI made the executive decision to just remove them.\n\nChange-Id: I4bc4db1e9a08153feda8f6e991804b60e94eedbc\n'}, {'number': 5, 'created': '2014-11-17 15:57:35.000000000', 'files': ['taskflow/engines/worker_based/proxy.py', 'taskflow/tests/unit/worker_based/test_proxy.py', 'taskflow/engines/worker_based/protocol.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/97f2f64c8b8ed417daa955be1653e0957cc46714', 'message': 'Add WBE protocol message version properties\n\nIn order for the WBE messages and message users to be\nversion aware we first need to include a version that\ncan be later used to detect what message version is\nbeing sent and adjust reception accordingly. This is\ncurrently not being used, but having it available makes\nit possible to use it in the future (if/when needed).\n\nChange-Id: I4bc4db1e9a08153feda8f6e991804b60e94eedbc\n'}]",0,101778,97f2f64c8b8ed417daa955be1653e0957cc46714,28,5,5,1297,,,0,"Add WBE protocol message version properties

In order for the WBE messages and message users to be
version aware we first need to include a version that
can be later used to detect what message version is
being sent and adjust reception accordingly. This is
currently not being used, but having it available makes
it possible to use it in the future (if/when needed).

Change-Id: I4bc4db1e9a08153feda8f6e991804b60e94eedbc
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/78/101778/3 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/engines/worker_based/proxy.py', 'taskflow/tests/unit/worker_based/test_proxy.py', 'taskflow/tests/unit/worker_based/test_protocol.py', 'taskflow/engines/worker_based/protocol.py']",4,9cca2402bc8f8b22cc391539b51cb54bdd3e70c8,wbe_message_version," def __str__(self): return ""<%s, v%s> %s"" % (self.TYPE, misc.get_version_string(self), self.to_dict()) @property def version(self): """"""Message (major, minor) version number tuple."""""" return (1, 0)"," def __str__(self): return ""<%s> %s"" % (self.TYPE, self.to_dict())",12,11
openstack%2Ftaskflow~master~I45b8ed988c52496d2d2243186abf968bee323542,openstack/taskflow,master,I45b8ed988c52496d2d2243186abf968bee323542,Retain conductor backwards compat of arguments for one release,ABANDONED,2014-10-20 23:59:46.000000000,2014-12-06 21:25:49.000000000,,"[{'_account_id': 3}, {'_account_id': 11024}]","[{'number': 1, 'created': '2014-10-20 23:59:46.000000000', 'files': ['taskflow/conductors/single_threaded.py', 'taskflow/conductors/base.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/54d3ece37aa0ea775e0a1b9010fca4017ea2a7b1', 'message': ""Retain conductor backwards compat of arguments for one release\n\nIn order break people less, lets let the old argument be passed\nin for 'engine_conf' (this will emit depreciation if used) and\nmake it so that previous conductor work won't break until a later\nrelease (after people can address the depreciation messages).\n\nChange-Id: I45b8ed988c52496d2d2243186abf968bee323542\n""}]",0,129758,54d3ece37aa0ea775e0a1b9010fca4017ea2a7b1,3,2,1,1297,,,0,"Retain conductor backwards compat of arguments for one release

In order break people less, lets let the old argument be passed
in for 'engine_conf' (this will emit depreciation if used) and
make it so that previous conductor work won't break until a later
release (after people can address the depreciation messages).

Change-Id: I45b8ed988c52496d2d2243186abf968bee323542
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/58/129758/1 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/conductors/single_threaded.py', 'taskflow/conductors/base.py']",2,54d3ece37aa0ea775e0a1b9010fca4017ea2a7b1,," # This is the old way of creating engines, keep it active for # at least one release to allow folks to switch to the newer # way (using 'engine' and 'engine_options' instead). engine_conf=None, self._engine_conf = engine_conf engine_conf=self._engine_conf,",,8,1
openstack%2Fcinder~master~I67452a3d8b63d83f0c72411e3e1397623be3f530,openstack/cinder,master,I67452a3d8b63d83f0c72411e3e1397623be3f530,Imported Translations from Transifex,MERGED,2014-11-27 06:10:42.000000000,2014-12-06 20:23:05.000000000,2014-12-05 16:43:54.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-11-27 06:10:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1b906755588bfb508b1bf9ef6e714b5b58135708', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I67452a3d8b63d83f0c72411e3e1397623be3f530\n'}, {'number': 2, 'created': '2014-11-28 06:09:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/33d04c85572d087e83e7b5d41805efaafda8f651', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I67452a3d8b63d83f0c72411e3e1397623be3f530\n'}, {'number': 3, 'created': '2014-11-29 06:05:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/d77d02d6fecf4d0a0ed910d4d6b623cf1047141e', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I67452a3d8b63d83f0c72411e3e1397623be3f530\n'}, {'number': 4, 'created': '2014-11-30 06:05:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/42afe7de570dff3c9edadaa80f02219271626b73', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I67452a3d8b63d83f0c72411e3e1397623be3f530\n'}, {'number': 5, 'created': '2014-12-01 06:09:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/603dcb139b512a0f01b298d9e37ce52d6b737e08', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I67452a3d8b63d83f0c72411e3e1397623be3f530\n'}, {'number': 6, 'created': '2014-12-02 06:08:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/390edf1fc85361e8177a8c8d1d686025cd20a408', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I67452a3d8b63d83f0c72411e3e1397623be3f530\n'}, {'number': 7, 'created': '2014-12-03 06:08:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a6066a7c901bfe9ad0f25f3420896e5d97626dfa', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I67452a3d8b63d83f0c72411e3e1397623be3f530\n'}, {'number': 8, 'created': '2014-12-04 06:09:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/5d8ab662b4875bfe31539b226bfd49544d67fb10', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I67452a3d8b63d83f0c72411e3e1397623be3f530\n'}, {'number': 9, 'created': '2014-12-05 06:07:51.000000000', 'files': ['cinder/locale/cinder.pot', 'cinder/locale/pt_BR/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/cinder-log-error.pot', 'cinder/locale/cinder-log-info.pot', 'cinder/locale/de/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/cinder-log-warning.pot'], 'web_link': 'https://opendev.org/openstack/cinder/commit/6ce7dbd8f542812c101b6d375de05476706cbf8e', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I67452a3d8b63d83f0c72411e3e1397623be3f530\n'}]",0,137524,6ce7dbd8f542812c101b6d375de05476706cbf8e,62,11,9,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: I67452a3d8b63d83f0c72411e3e1397623be3f530
",git fetch https://review.opendev.org/openstack/cinder refs/changes/24/137524/4 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/locale/cinder.pot', 'cinder/locale/pt_BR/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/cinder-log-error.pot', 'cinder/locale/cinder-log-info.pot', 'cinder/locale/de/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/cinder-log-warning.pot']",6,1b906755588bfb508b1bf9ef6e714b5b58135708,transifex/translations,"""Project-Id-Version: cinder 2015.1.dev317.gb07748f\n""""POT-Creation-Date: 2014-11-27 06:10+0000\n""#: cinder/volume/drivers/pure.py:148 #, python-format msgid ""Volume deletion failed with message: %s"" msgstr """" #: cinder/volume/drivers/pure.py:199#: cinder/volume/drivers/xio.py:471 #, python-format msgid ""IOnetworks GET failed (%d)"" msgstr """" #: cinder/volume/drivers/xio.py:601 #, python-format msgid ""Could not get status for %(name)s (%(status)d)."" msgstr """" #: cinder/volume/drivers/xio.py:609 msgid ""No volume node in XML content."" msgstr """" #: cinder/volume/drivers/xio.py:617 #, python-format msgid ""No status payload for volume %s."" msgstr """" #: cinder/volume/drivers/xio.py:707 #, python-format msgid ""Volume %(name)s already presented (%(status)d)!"" msgstr """" #: cinder/volume/drivers/xio.py:984 #, python-format msgid ""Could not get pool information (%s)!"" msgstr """" #: cinder/volume/drivers/xio.py:1224 #, python-format msgid ""Delete volume: %s not found!"" msgstr """" ","""Project-Id-Version: cinder 2015.1.dev307.g200a505\n""""POT-Creation-Date: 2014-11-26 06:10+0000\n""#: cinder/volume/drivers/pure.py:200",375,62
openstack%2Fcinder~master~I1e0c546a72c75193e8c742c02dc189d178a0312c,openstack/cinder,master,I1e0c546a72c75193e8c742c02dc189d178a0312c,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:40:49.000000000,2014-12-06 20:06:30.000000000,2014-12-05 15:15:49.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 7665}, {'_account_id': 8959}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}]","[{'number': 1, 'created': '2014-12-05 03:40:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/d9141f38f11907d5069402a4d1e7b60b0900c264', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I1e0c546a72c75193e8c742c02dc189d178a0312c\n'}, {'number': 2, 'created': '2014-12-05 04:41:51.000000000', 'files': ['doc/source/devref/gerrit.rst', 'CONTRIBUTING.md', 'doc/source/devref/development.environment.rst'], 'web_link': 'https://opendev.org/openstack/cinder/commit/0ced872d5ffa6f5452d3bca2d80972e7ca350be8', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I1e0c546a72c75193e8c742c02dc189d178a0312c\n'}]",1,139314,0ced872d5ffa6f5452d3bca2d80972e7ca350be8,18,11,2,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I1e0c546a72c75193e8c742c02dc189d178a0312c
",git fetch https://review.opendev.org/openstack/cinder refs/changes/14/139314/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/devref/gerrit.rst', 'CONTRIBUTING.md', 'doc/source/devref/development.environment.rst']",3,d9141f38f11907d5069402a4d1e7b60b0900c264,infra-manual,Grab the code::.. _GerritWorkflow: http://docs.openstack.org/infra/manual/developers.html#development-workflow,Grab the code from GitHub::.. _GerritWorkflow: http://wiki.openstack.org/GerritWorkflow,8,8
openstack%2Fcinder~master~Iaef411e254186fbba80c21903d6f94538a808d93,openstack/cinder,master,Iaef411e254186fbba80c21903d6f94538a808d93,Remove the check_uptodate conf checks,MERGED,2014-12-04 20:42:53.000000000,2014-12-06 19:50:21.000000000,2014-12-05 06:14:38.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 7198}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}]","[{'number': 1, 'created': '2014-12-04 20:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6bd1d1ba5278b0072964ef3df60c5800e24af7c7', 'message': ""Remove the checkup2date conf checks\n\nWe recently removed the checkupdate tests for sample.conf\nhere Ie4e25a8cd36782007a8934cc4573632f5215c2b1\n\nTo stop gating on this as it's just a big fat\npain when external libs change.\n\nInstead, we'll start generating this in an external\njob and publishing somewhere.\n\nFunny thing though, we actually have a direct call in\nrun_tests.sh as well.\n\nThis patch removes the second call in run_tests.sh\n\nChange-Id: Iaef411e254186fbba80c21903d6f94538a808d93\n""}, {'number': 2, 'created': '2014-12-04 20:44:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/833dda8f501e39fdefac14bf20260d18f7c8c227', 'message': ""Remove the check_uptodate conf checks\n\nWe recently removed the checkupdate tests for sample.conf\nhere Ie4e25a8cd36782007a8934cc4573632f5215c2b1\n\nTo stop gating on this as it's just a big fat\npain when external libs change.\n\nInstead, we'll start generating this in an external\njob and publishing somewhere.\n\nFunny thing though, we actually have a direct call in\nrun_tests.sh as well.\n\nThis patch removes the second call in run_tests.sh\n\nChange-Id: Iaef411e254186fbba80c21903d6f94538a808d93\n""}, {'number': 3, 'created': '2014-12-05 04:06:45.000000000', 'files': ['run_tests.sh'], 'web_link': 'https://opendev.org/openstack/cinder/commit/18a7f13d38ca3c74fa1ebaf4d03afebb4216e578', 'message': ""Remove the check_uptodate conf checks\n\nWe recently removed the checkupdate tests for sample.conf\nhere Ie4e25a8cd36782007a8934cc4573632f5215c2b1\n\nTo stop gating on this as it's just a big fat\npain when external libs change.\n\nInstead, we'll start generating this in an external\njob and publishing somewhere.\n\nFunny thing though, we actually have a direct call in\nrun_tests.sh as well.\n\nThis patch removes the second call in run_tests.sh\n\nChange-Id: Iaef411e254186fbba80c21903d6f94538a808d93\n""}]",0,139187,18a7f13d38ca3c74fa1ebaf4d03afebb4216e578,18,9,3,2243,,,0,"Remove the check_uptodate conf checks

We recently removed the checkupdate tests for sample.conf
here Ie4e25a8cd36782007a8934cc4573632f5215c2b1

To stop gating on this as it's just a big fat
pain when external libs change.

Instead, we'll start generating this in an external
job and publishing somewhere.

Funny thing though, we actually have a direct call in
run_tests.sh as well.

This patch removes the second call in run_tests.sh

Change-Id: Iaef411e254186fbba80c21903d6f94538a808d93
",git fetch https://review.opendev.org/openstack/cinder refs/changes/87/139187/2 && git format-patch -1 --stdout FETCH_HEAD,['run_tests.sh'],1,6bd1d1ba5278b0072964ef3df60c5800e24af7c7,remove_sample_conf,, ${wrapper} bash ./tools/config/check_uptodate.sh,0,1
openstack%2Ftaskflow~master~Ifa62cd506ebdcdeaf64c2b210e9644f1f88d8eb4,openstack/taskflow,master,Ifa62cd506ebdcdeaf64c2b210e9644f1f88d8eb4,Show compilation actions when __debug__ enabled,ABANDONED,2014-12-04 23:57:01.000000000,2014-12-06 19:20:50.000000000,,"[{'_account_id': 3}, {'_account_id': 1297}]","[{'number': 1, 'created': '2014-12-04 23:57:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/889a75110308358f97b9d1d40227bf4e9c1b44e8', 'message': 'Show compilation actions when __debug__ enabled\n\nTo be able to easily understand what compilation is\ndoing and what is being merged, relinked, created during\ncompilation it is useful to trace the actions and output\nthem for when this situation is desireable.\n\nTo support this add a set of calls that will only get\nturned on when __debug__ (they will be stripped out by\nthe python optimization mechanism) that output this info\nat a debug log level so that it can be manually enabled.\n\nChange-Id: Ifa62cd506ebdcdeaf64c2b210e9644f1f88d8eb4\n'}, {'number': 2, 'created': '2014-12-05 00:37:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/3436fdf9a4d1e2a6ba063dcb7ea2ebe55d5c8887', 'message': 'Show compilation actions when __debug__ enabled\n\nTo be able to easily understand what compilation is\ndoing and what is being merged, relinked, created during\ncompilation it is useful to trace the actions and output\nthem for when this situation is desireable.\n\nTo support this add a set of calls that will only get\nturned on when __debug__ (they will be stripped out by\nthe python optimization mechanism) that output this info\nat a debug log level so that it can be manually enabled.\n\nChange-Id: Ifa62cd506ebdcdeaf64c2b210e9644f1f88d8eb4\n'}, {'number': 3, 'created': '2014-12-05 06:10:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/80c989987d33198ee78f6b891857052b58280a45', 'message': 'Show compilation actions when __debug__ enabled\n\nTo be able to easily understand what compilation is\ndoing and what is being merged, relinked, created during\ncompilation it is useful to trace the actions and output\nthem for when this situation is desireable.\n\nTo support this add a set of calls that will only get\nturned on when __debug__ (they will be stripped out by\nthe python optimization mechanism) that output this info\nat a debug log level so that it can be manually enabled.\n\nChange-Id: Ifa62cd506ebdcdeaf64c2b210e9644f1f88d8eb4\n'}, {'number': 4, 'created': '2014-12-05 06:15:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/e505bd29e6275dc19c4c22a3da48531e3ea6e85c', 'message': 'Show compilation actions when __debug__ enabled\n\nTo be able to easily understand what compilation is\ndoing and what is being merged, relinked, created during\ncompilation it is useful to trace the actions and output\nthem for when this situation is desireable.\n\nTo support this add a set of calls that will only get\nturned on when __debug__ (they will be stripped out by\nthe python optimization mechanism) that output this info\nat a debug log level so that it can be manually enabled.\n\nChange-Id: Ifa62cd506ebdcdeaf64c2b210e9644f1f88d8eb4\n'}, {'number': 5, 'created': '2014-12-05 06:55:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/8d010cd1d5dded5a9a5d38c83258cf36a41867e4', 'message': 'Show compilation actions when __debug__ enabled\n\nTo be able to easily understand what compilation is\ndoing and what is being merged, relinked, created during\ncompilation it is useful to trace the actions and output\nthem for when this situation is desireable.\n\nTo support this add a set of calls that will only get\nturned on when __debug__ (they will be stripped out by\nthe python optimization mechanism) that output this info\nat a debug log level so that it can be manually enabled.\n\nChange-Id: Ifa62cd506ebdcdeaf64c2b210e9644f1f88d8eb4\n'}, {'number': 6, 'created': '2014-12-06 00:46:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/2127127c93809c8ac0f55d804fc311b203b3669a', 'message': 'Show compilation actions when __debug__ enabled\n\nTo be able to easily understand what compilation is\ndoing and what is being merged, relinked, created during\ncompilation it is useful to trace the actions and output\nthem for when this situation is desireable.\n\nTo support this add a set of calls that will only get\nturned on when __debug__ (they will be stripped out by\nthe python optimization mechanism) that output this info\nat a debug log level so that it can be manually enabled.\n\nChange-Id: Ifa62cd506ebdcdeaf64c2b210e9644f1f88d8eb4\n'}, {'number': 7, 'created': '2014-12-06 00:55:15.000000000', 'files': ['taskflow/engines/action_engine/compiler.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/de8faa636385c5166f72179d83f64ef14d96a4c3', 'message': 'Show compilation actions when __debug__ enabled\n\nTo be able to easily understand what compilation is\ndoing and what is being merged, relinked, created during\ncompilation it is useful to trace the actions and output\nthem for when this situation is desireable.\n\nTo support this add a set of calls that will only get\nturned on when __debug__ (they will be stripped out by\nthe python optimization mechanism) that output this info\nat a debug log level so that it can be manually enabled.\n\nChange-Id: Ifa62cd506ebdcdeaf64c2b210e9644f1f88d8eb4\n'}]",0,139270,de8faa636385c5166f72179d83f64ef14d96a4c3,15,2,7,1297,,,0,"Show compilation actions when __debug__ enabled

To be able to easily understand what compilation is
doing and what is being merged, relinked, created during
compilation it is useful to trace the actions and output
them for when this situation is desireable.

To support this add a set of calls that will only get
turned on when __debug__ (they will be stripped out by
the python optimization mechanism) that output this info
at a debug log level so that it can be manually enabled.

Change-Id: Ifa62cd506ebdcdeaf64c2b210e9644f1f88d8eb4
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/70/139270/3 && git format-patch -1 --stdout FETCH_HEAD,['taskflow/engines/action_engine/compiler.py'],1,889a75110308358f97b9d1d40227bf4e9c1b44e8,compile-visualize,"def _log_indented(indent, text, spacer="" ""): """"""Logs with an indent/prefix of a given number of spaces."""""" LOG.debug(""%s%s"", spacer * indent, text) self._indent = 0 if __debug__: if parent is not None: _log_indented(self._indent, ""- Decomposing task '%s' with parent '%s'"" % (task.name, parent.item.name)) else: _log_indented(self._indent, ""- Decomposing task '%s' with no parent (must be"" "" the root)"" % task.name) if __debug__: if parent is not None: _log_indented(self._indent, ""- Decomposing flow '%s' with parent '%s'"" % (flow.name, parent.item.name)) else: _log_indented(self._indent, ""- Decomposing flow '%s' with no parent (must be"" "" the root)"" % flow.name) if __debug__: _log_indented(self._indent, ""- Relinking decomposed '%s' -> decomposed"" "" '%s'"" % (u.name, v.name)) if __debug__: _log_indented(self._indent, ""- Compiling '%s'"" % item.name) self._indent += 4 if __debug__: _log_indented(self._indent, ""- Decomposed '%s' into:"" % item.name) for line in graph.pformat().splitlines(): _log_indented(self._indent + 2, line) self._indent -= 4 self._indent = 0 if __debug__: _log_indented(self._indent, ""Compilation of '%s' started"" % self._root.name) LOG.debug(""Translated '%s' into the following components:"", self._root) LOG.debug(""Graph"") _log_indented(4, line) LOG.debug(""Hierarchy"") _log_indented(4, line)"," LOG.debug(""Translated '%s'"", self._root) LOG.debug(""Graph:"") LOG.debug("" %s"", line) LOG.debug(""Hierarchy:"") LOG.debug("" %s"", line)",46,5
openstack%2Fhorizon~master~I0ea59aa76d414eebeb9d3b407e2d7111fb75b9ae,openstack/horizon,master,I0ea59aa76d414eebeb9d3b407e2d7111fb75b9ae,Imported Translations from Transifex,MERGED,2014-12-06 06:09:16.000000000,2014-12-06 19:17:43.000000000,2014-12-06 19:17:42.000000000,"[{'_account_id': 3}, {'_account_id': 1941}, {'_account_id': 6914}]","[{'number': 1, 'created': '2014-12-06 06:09:16.000000000', 'files': ['openstack_dashboard/locale/fr/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en_GB/LC_MESSAGES/django.po', 'openstack_dashboard/locale/sr/LC_MESSAGES/django.po', 'openstack_dashboard/locale/pl_PL/LC_MESSAGES/django.po', 'openstack_dashboard/locale/de/LC_MESSAGES/django.po', 'openstack_dashboard/locale/cs/LC_MESSAGES/django.po', 'openstack_dashboard/locale/nl_NL/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en/LC_MESSAGES/django.po', 'openstack_dashboard/locale/hi/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ja/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ko_KR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ru/LC_MESSAGES/django.po', 'openstack_dashboard/locale/es/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_CN/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_TW/LC_MESSAGES/django.po', 'openstack_dashboard/locale/pt_BR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en_AU/LC_MESSAGES/django.po'], 'web_link': 'https://opendev.org/openstack/horizon/commit/60cfc4e2340a34455ee7a74164a8011e15409dd0', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I0ea59aa76d414eebeb9d3b407e2d7111fb75b9ae\n'}]",0,139786,60cfc4e2340a34455ee7a74164a8011e15409dd0,7,3,1,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: I0ea59aa76d414eebeb9d3b407e2d7111fb75b9ae
",git fetch https://review.opendev.org/openstack/horizon refs/changes/86/139786/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/locale/fr/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en_GB/LC_MESSAGES/django.po', 'openstack_dashboard/locale/sr/LC_MESSAGES/django.po', 'openstack_dashboard/locale/pl_PL/LC_MESSAGES/django.po', 'openstack_dashboard/locale/de/LC_MESSAGES/django.po', 'openstack_dashboard/locale/cs/LC_MESSAGES/django.po', 'openstack_dashboard/locale/nl_NL/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en/LC_MESSAGES/django.po', 'openstack_dashboard/locale/hi/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ja/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ko_KR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/ru/LC_MESSAGES/django.po', 'openstack_dashboard/locale/es/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_CN/LC_MESSAGES/django.po', 'openstack_dashboard/locale/zh_TW/LC_MESSAGES/django.po', 'openstack_dashboard/locale/pt_BR/LC_MESSAGES/django.po', 'openstack_dashboard/locale/en_AU/LC_MESSAGES/django.po']",17,60cfc4e2340a34455ee7a74164a8011e15409dd0,transifex/translations,"""POT-Creation-Date: 2014-12-05 20:04-0600\n"" ""PO-Revision-Date: 2014-12-05 23:08+0000\n""#: api/nova.py:102 api/nova.py:112 dashboards/project/databases/tables.py:230#: dashboards/identity/projects/workflows.py:142#: dashboards/project/instances/workflows/create_instance.py:79#: dashboards/project/volumes/volumes/tables.py:359#: dashboards/identity/projects/workflows.py:539#: dashboards/identity/projects/workflows.py:47 usage/quotas.py:62#: dashboards/identity/projects/workflows.py:48#: dashboards/identity/projects/workflows.py:49#: dashboards/identity/projects/workflows.py:51 usage/quotas.py:65#: dashboards/identity/projects/workflows.py:54#: dashboards/project/volumes/volumes/tables.py:368 usage/quotas.py:74#: dashboards/identity/projects/workflows.py:55#: dashboards/identity/projects/workflows.py:57 usage/quotas.py:76#: dashboards/identity/projects/workflows.py:58#: dashboards/identity/projects/workflows.py:59 #: dashboards/identity/projects/workflows.py:71#: dashboards/project/access_and_security/floating_ips/tables.py:221#: dashboards/identity/projects/workflows.py:62 #: dashboards/identity/projects/workflows.py:68#: dashboards/project/instances/workflows/create_instance.py:543#: dashboards/identity/projects/workflows.py:64 #: dashboards/identity/projects/workflows.py:70#: dashboards/identity/projects/workflows.py:60#: dashboards/project/volumes/volumes/forms.py:660#: dashboards/project/instances/workflows/create_instance.py:82#: dashboards/project/access_and_security/floating_ips/tables.py:201#: dashboards/project/instances/workflows/create_instance.py:106 #: dashboards/project/volumes/volumes/forms.py:552#: dashboards/identity/projects/workflows.py:148 #: dashboards/identity/projects/workflows.py:518#: dashboards/project/instances/workflows/create_instance.py:52#: dashboards/project/databases/views.py:149#: dashboards/project/instances/views.py:350#: dashboards/identity/projects/workflows.py:146#: dashboards/project/volumes/volumes/forms.py:488 #: dashboards/project/volumes/volumes/forms.py:529#: dashboards/project/instances/workflows/create_instance.py:741#: dashboards/project/instances/workflows/create_instance.py:751#: dashboards/identity/projects/workflows.py:72#: dashboards/project/instances/workflows/create_instance.py:694#: dashboards/identity/projects/workflows.py:73#: dashboards/identity/projects/workflows.py:75#: dashboards/identity/projects/workflows.py:74#: dashboards/project/volumes/volumes/forms.py:200#: dashboards/project/volumes/volumes/forms.py:525 #: dashboards/project/volumes/volumes/forms.py:549 #: dashboards/project/volumes/volumes/forms.py:609 #: dashboards/project/volumes/volumes/forms.py:657#: dashboards/project/instances/workflows/create_instance.py:100 #: dashboards/project/instances/workflows/create_instance.py:415#: dashboards/project/volumes/volumes/forms.py:217#: dashboards/admin/volumes/volumes/views.py:34 #: dashboards/project/volumes/volumes/views.py:53 #, python-format msgid ""Volume Details: %(volume_name)s"" msgstr """" #: dashboards/admin/volumes/volumes/views.py:69 #: dashboards/project/volumes/volumes/views.py:70#: dashboards/identity/projects/workflows.py:136#: dashboards/identity/projects/workflows.py:196 #: dashboards/identity/projects/workflows.py:294#: dashboards/identity/projects/workflows.py:266#: dashboards/identity/projects/workflows.py:364#: dashboards/identity/projects/workflows.py:255#: dashboards/identity/projects/workflows.py:257#: dashboards/identity/projects/workflows.py:258#: dashboards/identity/projects/workflows.py:280#: dashboards/identity/projects/workflows.py:353#: dashboards/identity/projects/workflows.py:355#: dashboards/identity/projects/workflows.py:356#: dashboards/identity/groups/views.py:119 #, python-format msgid ""Group Management: %(group_name)s"" msgstr """" #: dashboards/identity/groups/views.py:130#: dashboards/identity/groups/views.py:151#: dashboards/identity/projects/workflows.py:376#: dashboards/identity/projects/workflows.py:538#: dashboards/identity/projects/workflows.py:45#: dashboards/identity/projects/workflows.py:98#: dashboards/identity/projects/workflows.py:103#: dashboards/identity/projects/workflows.py:110 #: dashboards/identity/projects/workflows.py:117#: dashboards/identity/projects/workflows.py:112 #: dashboards/identity/projects/workflows.py:119#: dashboards/identity/projects/workflows.py:139#: dashboards/identity/projects/workflows.py:163 #: dashboards/identity/projects/workflows.py:521#: dashboards/identity/projects/workflows.py:164#: dashboards/identity/projects/workflows.py:182#: dashboards/identity/projects/workflows.py:249 #: dashboards/identity/projects/workflows.py:256#: dashboards/identity/projects/workflows.py:347 #: dashboards/identity/projects/workflows.py:354#: dashboards/identity/projects/workflows.py:377#: dashboards/identity/projects/workflows.py:378#: dashboards/identity/projects/workflows.py:503#: dashboards/identity/projects/workflows.py:523#: dashboards/identity/projects/workflows.py:540#: dashboards/identity/projects/workflows.py:541#: dashboards/identity/projects/workflows.py:630#: dashboards/identity/projects/workflows.py:690#: dashboards/identity/projects/workflows.py:694#: dashboards/identity/projects/workflows.py:769#: dashboards/identity/projects/workflows.py:803#: dashboards/project/instances/workflows/create_instance.py:592#: dashboards/project/instances/workflows/create_instance.py:550#: dashboards/project/access_and_security/security_groups/views.py:124#: dashboards/project/access_and_security/floating_ips/tables.py:199#: dashboards/project/volumes/volumes/tables.py:420#: dashboards/project/volumes/volumes/forms.py:453#: dashboards/project/volumes/volumes/forms.py:455#: dashboards/project/access_and_security/security_groups/forms.py:286#: dashboards/project/access_and_security/security_groups/forms.py:289#: dashboards/project/access_and_security/security_groups/forms.py:292#: dashboards/project/access_and_security/security_groups/forms.py:295#: dashboards/project/access_and_security/security_groups/forms.py:314#: dashboards/project/access_and_security/security_groups/forms.py:319#: dashboards/project/access_and_security/security_groups/forms.py:322#: dashboards/project/access_and_security/security_groups/forms.py:325#: dashboards/project/access_and_security/security_groups/forms.py:374#: dashboards/project/access_and_security/security_groups/forms.py:397#: dashboards/project/access_and_security/security_groups/forms.py:403#: dashboards/project/access_and_security/security_groups/views.py:83#: dashboards/project/access_and_security/security_groups/views.py:66 #, python-format msgid ""Manage Security Group Rules: %(security_group)s"" msgstr """" #: dashboards/project/access_and_security/security_groups/views.py:131#: dashboards/project/instances/workflows/create_instance.py:619#: dashboards/project/instances/workflows/create_instance.py:85#: dashboards/project/instances/workflows/create_instance.py:136#: dashboards/project/volumes/volumes/forms.py:205#: dashboards/project/instances/workflows/create_instance.py:437#: dashboards/project/instances/workflows/create_instance.py:837#: dashboards/project/instances/workflows/create_instance.py:737#: dashboards/project/instances/views.py:364#: dashboards/project/database_backups/views.py:106 #, python-format msgid ""Backup Details: %(backup_name)s"" msgstr """" #: dashboards/project/instances/workflows/create_instance.py:839#: dashboards/project/instances/workflows/create_instance.py:856#: dashboards/project/volumes/volumes/forms.py:614#: dashboards/project/volumes/volumes/forms.py:618#: dashboards/project/instances/workflows/create_instance.py:836#: dashboards/project/databases/views.py:104 #: dashboards/project/instances/views.py:261 #, python-format msgid ""Instance Details: %(instance_name)s"" msgstr """" #: dashboards/project/databases/views.py:117#: dashboards/project/instances/workflows/create_instance.py:86#: dashboards/project/instances/workflows/create_instance.py:698#: dashboards/project/instances/workflows/create_instance.py:700#: dashboards/project/instances/workflows/create_instance.py:722#: dashboards/project/instances/workflows/create_instance.py:724#: dashboards/project/instances/workflows/create_instance.py:838#: dashboards/project/firewalls/views.py:129#: dashboards/project/firewalls/views.py:168#: dashboards/project/firewalls/views.py:202#: dashboards/project/firewalls/views.py:146#: dashboards/project/firewalls/views.py:183 #: dashboards/project/firewalls/views.py:251 #: dashboards/project/firewalls/views.py:285#: dashboards/project/firewalls/views.py:218#: dashboards/project/firewalls/views.py:132 #, python-format msgid ""Edit Rule %(rule_name)s"" msgstr """" #: dashboards/project/firewalls/views.py:171 #, python-format msgid ""Edit Policy %s"" msgstr """" #: dashboards/project/firewalls/views.py:205 #, python-format msgid ""Edit Firewall %s"" msgstr """" #: dashboards/project/images/images/views.py:94 #, python-format msgid ""Image Details: %(image_name)s"" msgstr """" #: dashboards/project/images/images/views.py:109#: dashboards/project/volumes/volumes/forms.py:485#: dashboards/project/volumes/volumes/forms.py:553#: dashboards/project/instances/workflows/create_instance.py:783#: dashboards/project/instances/workflows/create_instance.py:439#: dashboards/project/instances/workflows/create_instance.py:801#: dashboards/project/instances/workflows/create_instance.py:802#: dashboards/project/instances/workflows/create_instance.py:166 #: dashboards/project/instances/workflows/create_instance.py:811#: dashboards/project/instances/views.py:361#: dashboards/project/instances/workflows/create_instance.py:528#: dashboards/project/instances/views.py:275#: dashboards/project/instances/views.py:296#: dashboards/project/instances/views.py:305#: dashboards/project/instances/views.py:314#: dashboards/project/instances/views.py:322#: dashboards/project/instances/views.py:375#: dashboards/project/volumes/volumes/tables.py:357#: dashboards/project/instances/workflows/create_instance.py:53#: dashboards/project/instances/workflows/create_instance.py:67#: dashboards/project/instances/workflows/create_instance.py:88#: dashboards/project/instances/workflows/create_instance.py:91#: dashboards/project/instances/workflows/create_instance.py:93#: dashboards/project/instances/workflows/create_instance.py:94#: dashboards/project/instances/workflows/create_instance.py:97#: dashboards/project/instances/workflows/create_instance.py:102#: dashboards/project/instances/workflows/create_instance.py:113#: dashboards/project/instances/workflows/create_instance.py:117#: dashboards/project/instances/workflows/create_instance.py:120 #: dashboards/project/volumes/volumes/forms.py:418#: dashboards/project/instances/workflows/create_instance.py:123#: dashboards/project/instances/workflows/create_instance.py:129#: dashboards/project/instances/workflows/create_instance.py:132#: dashboards/project/instances/workflows/create_instance.py:152#: dashboards/project/instances/workflows/create_instance.py:153#: dashboards/project/instances/workflows/create_instance.py:154#: dashboards/project/instances/workflows/create_instance.py:157#: dashboards/project/instances/workflows/create_instance.py:164#: dashboards/project/instances/workflows/create_instance.py:171#: dashboards/project/instances/workflows/create_instance.py:211#: dashboards/project/instances/workflows/create_instance.py:227#: dashboards/project/instances/workflows/create_instance.py:234#: dashboards/project/instances/workflows/create_instance.py:241#: dashboards/project/instances/workflows/create_instance.py:261#: dashboards/project/instances/workflows/create_instance.py:282#: dashboards/project/instances/workflows/create_instance.py:292 #: dashboards/project/instances/workflows/create_instance.py:306 msgid ""You must select an image."" msgstr ""You must select an image."" #: dashboards/project/instances/workflows/create_instance.py:300 msgid ""You must set volume size"" msgstr ""You must set volume size"" #: dashboards/project/instances/workflows/create_instance.py:303 msgid ""Volume size must be greater than 0"" msgstr ""Volume size must be greater than 0"" #: dashboards/project/instances/workflows/create_instance.py:317 #: dashboards/project/instances/workflows/create_instance.py:335#: dashboards/project/instances/workflows/create_instance.py:322#: dashboards/project/instances/workflows/create_instance.py:329#: dashboards/project/instances/workflows/create_instance.py:372 #: dashboards/project/volumes/volumes/forms.py:285#: dashboards/project/instances/workflows/create_instance.py:378 #: dashboards/project/volumes/volumes/forms.py:288#: dashboards/project/instances/workflows/create_instance.py:380 #: dashboards/project/volumes/volumes/forms.py:290#: dashboards/project/instances/workflows/create_instance.py:402#: dashboards/project/instances/workflows/create_instance.py:412 #: dashboards/project/volumes/volumes/forms.py:192#: dashboards/project/instances/workflows/create_instance.py:417#: dashboards/project/instances/workflows/create_instance.py:451#: dashboards/project/instances/workflows/create_instance.py:453#: dashboards/project/instances/workflows/create_instance.py:466 #: dashboards/project/volumes/volumes/forms.py:303#: dashboards/project/instances/workflows/create_instance.py:468#: dashboards/project/instances/workflows/create_instance.py:470#: dashboards/project/instances/workflows/create_instance.py:482#: dashboards/project/instances/workflows/create_instance.py:485#: dashboards/project/instances/workflows/create_instance.py:487#: dashboards/project/instances/workflows/create_instance.py:530#: dashboards/project/instances/workflows/create_instance.py:534#: dashboards/project/instances/workflows/create_instance.py:540#: dashboards/project/instances/workflows/create_instance.py:546#: dashboards/project/instances/workflows/create_instance.py:551#: dashboards/project/instances/workflows/create_instance.py:567#: dashboards/project/instances/workflows/create_instance.py:571#: dashboards/project/instances/workflows/create_instance.py:573#: dashboards/project/instances/workflows/create_instance.py:582#: dashboards/project/instances/workflows/create_instance.py:614#: dashboards/project/instances/workflows/create_instance.py:618#: dashboards/project/instances/workflows/create_instance.py:622#: dashboards/project/instances/workflows/create_instance.py:626#: dashboards/project/instances/workflows/create_instance.py:630 #: dashboards/project/instances/workflows/create_instance.py:635#: dashboards/project/instances/workflows/create_instance.py:639 #: dashboards/project/instances/workflows/create_instance.py:644#: dashboards/project/instances/workflows/create_instance.py:671#: dashboards/project/instances/workflows/create_instance.py:679#: dashboards/project/instances/workflows/create_instance.py:706#: dashboards/project/instances/workflows/create_instance.py:709#: dashboards/project/instances/workflows/create_instance.py:784#: dashboards/project/instances/workflows/create_instance.py:788#: dashboards/project/instances/workflows/create_instance.py:789#: dashboards/project/instances/workflows/create_instance.py:815#: dashboards/project/instances/workflows/create_instance.py:853#: dashboards/project/instances/workflows/create_instance.py:913#: dashboards/project/networks/views.py:145 #, python-format msgid ""Network Detail: %(network_name)s"" msgstr """" #: dashboards/project/stacks/views.py:221#: dashboards/project/stacks/views.py:207 #, python-format msgid ""Stack Detail: %(stack_name)s"" msgstr """" #: dashboards/project/stacks/views.py:232#: dashboards/project/stacks/views.py:255 #, python-format msgid ""Resource Detail: %s"" msgstr """" #: dashboards/project/stacks/views.py:267#: dashboards/project/stacks/views.py:280#: dashboards/project/volumes/backups/views.py:70#: dashboards/project/volumes/backups/views.py:56 #, python-format msgid ""Volume Backup Details: %(backup_name)s"" msgstr """" #: dashboards/project/volumes/snapshots/views.py:86#: dashboards/project/volumes/snapshots/views.py:72 #, python-format msgid ""Volume Snapshot Details: %(snapshot_name)s"" msgstr """" #: dashboards/project/volumes/volumes/tables.py:441#: dashboards/project/volumes/volumes/forms.py:114#: dashboards/project/volumes/volumes/forms.py:122#: dashboards/project/volumes/volumes/forms.py:134#: dashboards/project/volumes/volumes/forms.py:142#: dashboards/project/volumes/volumes/forms.py:153#: dashboards/project/volumes/volumes/forms.py:163#: dashboards/project/volumes/volumes/forms.py:170#: dashboards/project/volumes/volumes/forms.py:193#: dashboards/project/volumes/volumes/forms.py:206#: dashboards/project/volumes/volumes/forms.py:218#: dashboards/project/volumes/volumes/forms.py:227#: dashboards/project/volumes/volumes/forms.py:236 msgid ""No volume type"" msgstr ""No volume type"" #: dashboards/project/volumes/volumes/forms.py:254#: dashboards/project/volumes/volumes/forms.py:258#: dashboards/project/volumes/volumes/forms.py:262#: dashboards/project/volumes/volumes/forms.py:271#: dashboards/project/volumes/volumes/forms.py:325#: dashboards/project/volumes/volumes/forms.py:338#: dashboards/project/volumes/volumes/forms.py:346#: dashboards/project/volumes/volumes/forms.py:357#: dashboards/project/volumes/volumes/forms.py:366#: dashboards/project/volumes/volumes/forms.py:373#: dashboards/project/volumes/volumes/forms.py:389#: dashboards/project/volumes/volumes/forms.py:397#: dashboards/project/volumes/volumes/forms.py:414#: dashboards/project/volumes/volumes/forms.py:415#: dashboards/project/volumes/volumes/forms.py:422#: dashboards/project/volumes/volumes/forms.py:461#: dashboards/project/volumes/volumes/forms.py:471#: dashboards/project/volumes/volumes/forms.py:480#: dashboards/project/volumes/volumes/forms.py:504#: dashboards/project/volumes/volumes/forms.py:507#: dashboards/project/volumes/volumes/forms.py:520#: dashboards/project/volumes/volumes/forms.py:538#: dashboards/project/volumes/volumes/forms.py:544#: dashboards/project/volumes/volumes/forms.py:558#: dashboards/project/volumes/volumes/forms.py:593#: dashboards/project/volumes/volumes/forms.py:600#: dashboards/project/volumes/volumes/forms.py:626#: dashboards/project/volumes/volumes/forms.py:632#: dashboards/project/volumes/volumes/forms.py:646#: dashboards/project/volumes/volumes/forms.py:652#: dashboards/project/volumes/volumes/forms.py:661#: dashboards/project/volumes/volumes/forms.py:662#: dashboards/project/volumes/volumes/forms.py:663#: dashboards/project/volumes/volumes/forms.py:680#: dashboards/project/volumes/volumes/forms.py:689#: dashboards/project/volumes/volumes/forms.py:705#: dashboards/project/volumes/volumes/forms.py:714#: dashboards/project/volumes/volumes/views.py:252#: dashboards/project/volumes/volumes/tables.py:361#: dashboards/project/volumes/volumes/tables.py:364#: dashboards/project/volumes/volumes/tables.py:385#: dashboards/project/volumes/volumes/tables.py:394#: dashboards/project/volumes/volumes/tables.py:422#: dashboards/project/volumes/volumes/tables.py:431#: dashboards/project/volumes/volumes/views.py:109 #: dashboards/project/volumes/volumes/views.py:152 #: dashboards/project/volumes/volumes/views.py:235#: dashboards/project/volumes/volumes/views.py:143#: dashboards/project/volumes/volumes/views.py:171 #: dashboards/project/volumes/volumes/views.py:301#: dashboards/project/volumes/volumes/views.py:204#: usage/base.py:186 usage/quotas.py:382#: usage/quotas.py:368","""POT-Creation-Date: 2014-12-03 20:48-0600\n"" ""PO-Revision-Date: 2014-12-04 03:01+0000\n""#: api/nova.py:102 api/nova.py:112 #: dashboards/project/access_and_security/floating_ips/tables.py:198 #: dashboards/project/access_and_security/floating_ips/tables.py:201 #: dashboards/project/databases/tables.py:230#: dashboards/project/volumes/volumes/tables.py:356#: dashboards/identity/projects/workflows.py:141#: dashboards/project/instances/workflows/create_instance.py:78#: dashboards/project/volumes/volumes/tables.py:360#: dashboards/identity/projects/workflows.py:528#: dashboards/identity/projects/workflows.py:46 usage/quotas.py:62#: dashboards/identity/projects/workflows.py:47#: dashboards/identity/projects/workflows.py:48#: dashboards/identity/projects/workflows.py:50 usage/quotas.py:65#: dashboards/identity/projects/workflows.py:53#: dashboards/project/volumes/volumes/tables.py:369 usage/quotas.py:74#: dashboards/identity/projects/workflows.py:54#: dashboards/identity/projects/workflows.py:56 usage/quotas.py:76#: dashboards/identity/projects/workflows.py:57#: dashboards/identity/projects/workflows.py:58 #: dashboards/identity/projects/workflows.py:70#: dashboards/project/access_and_security/floating_ips/tables.py:223#: dashboards/identity/projects/workflows.py:61 #: dashboards/identity/projects/workflows.py:67#: dashboards/project/instances/workflows/create_instance.py:501#: dashboards/identity/projects/workflows.py:63 #: dashboards/identity/projects/workflows.py:69#: dashboards/identity/projects/workflows.py:59#: dashboards/project/volumes/volumes/forms.py:648#: dashboards/project/instances/workflows/create_instance.py:81#: dashboards/project/access_and_security/floating_ips/tables.py:203#: dashboards/project/instances/workflows/create_instance.py:105 #: dashboards/project/volumes/volumes/forms.py:540#: dashboards/identity/projects/workflows.py:147 #: dashboards/identity/projects/workflows.py:507#: dashboards/project/instances/workflows/create_instance.py:51#: dashboards/project/databases/views.py:146#: dashboards/project/instances/views.py:347#: dashboards/identity/projects/workflows.py:145#: dashboards/project/volumes/volumes/forms.py:476 #: dashboards/project/volumes/volumes/forms.py:517#: dashboards/project/instances/workflows/create_instance.py:699#: dashboards/project/instances/workflows/create_instance.py:709#: dashboards/identity/projects/workflows.py:71#: dashboards/project/instances/workflows/create_instance.py:652#: dashboards/identity/projects/workflows.py:72#: dashboards/identity/projects/workflows.py:74#: dashboards/identity/projects/workflows.py:73#: dashboards/project/volumes/volumes/forms.py:204#: dashboards/project/volumes/volumes/forms.py:513 #: dashboards/project/volumes/volumes/forms.py:537 #: dashboards/project/volumes/volumes/forms.py:597 #: dashboards/project/volumes/volumes/forms.py:645#: dashboards/project/instances/workflows/create_instance.py:99 #: dashboards/project/instances/workflows/create_instance.py:373#: dashboards/project/volumes/volumes/forms.py:221#: dashboards/admin/volumes/templates/volumes/volumes/detail.html:6 #: dashboards/project/volumes/templates/volumes/volumes/detail.html:6 msgid ""Volume Details: "" msgstr ""Volume Details: "" #: dashboards/admin/volumes/templates/volumes/volumes/detail.html:6 #: dashboards/project/volumes/templates/volumes/volumes/detail.html:6 msgid ""Volume Details:"" msgstr ""Volume Details:"" #: dashboards/admin/volumes/volumes/views.py:61 #: dashboards/project/volumes/volumes/views.py:66#: dashboards/identity/projects/workflows.py:135#: dashboards/identity/projects/workflows.py:195 #: dashboards/identity/projects/workflows.py:293#: dashboards/identity/projects/workflows.py:265#: dashboards/identity/projects/workflows.py:363#: dashboards/identity/projects/workflows.py:254#: dashboards/identity/projects/workflows.py:256#: dashboards/identity/projects/workflows.py:257#: dashboards/identity/projects/workflows.py:279#: dashboards/identity/projects/workflows.py:352#: dashboards/identity/projects/workflows.py:354#: dashboards/identity/projects/workflows.py:355#: dashboards/identity/groups/views.py:126#: dashboards/identity/groups/views.py:147#: dashboards/identity/groups/templates/groups/manage.html:6 msgid ""Group Management: "" msgstr ""Group Management: "" #: dashboards/identity/projects/workflows.py:374#: dashboards/identity/projects/workflows.py:527#: dashboards/identity/projects/workflows.py:44#: dashboards/identity/projects/workflows.py:97#: dashboards/identity/projects/workflows.py:102#: dashboards/identity/projects/workflows.py:109 #: dashboards/identity/projects/workflows.py:116#: dashboards/identity/projects/workflows.py:111 #: dashboards/identity/projects/workflows.py:118#: dashboards/identity/projects/workflows.py:138#: dashboards/identity/projects/workflows.py:162 #: dashboards/identity/projects/workflows.py:510#: dashboards/identity/projects/workflows.py:163#: dashboards/identity/projects/workflows.py:181#: dashboards/identity/projects/workflows.py:248 #: dashboards/identity/projects/workflows.py:255#: dashboards/identity/projects/workflows.py:346 #: dashboards/identity/projects/workflows.py:353#: dashboards/identity/projects/workflows.py:376#: dashboards/identity/projects/workflows.py:377#: dashboards/identity/projects/workflows.py:502#: dashboards/identity/projects/workflows.py:512#: dashboards/identity/projects/workflows.py:529#: dashboards/identity/projects/workflows.py:530#: dashboards/identity/projects/workflows.py:625#: dashboards/identity/projects/workflows.py:660#: dashboards/identity/projects/workflows.py:664#: dashboards/identity/projects/workflows.py:737#: dashboards/identity/projects/workflows.py:770#: dashboards/project/instances/workflows/create_instance.py:550#: dashboards/project/instances/workflows/create_instance.py:508#: dashboards/project/access_and_security/security_groups/views.py:120#: dashboards/project/access_and_security/floating_ips/tables.py:200#: dashboards/project/volumes/volumes/tables.py:421#: dashboards/project/volumes/volumes/forms.py:441#: dashboards/project/volumes/volumes/forms.py:443#: dashboards/project/access_and_security/security_groups/forms.py:297#: dashboards/project/access_and_security/security_groups/forms.py:300#: dashboards/project/access_and_security/security_groups/forms.py:303#: dashboards/project/access_and_security/security_groups/forms.py:306#: dashboards/project/access_and_security/security_groups/forms.py:319#: dashboards/project/access_and_security/security_groups/forms.py:324#: dashboards/project/access_and_security/security_groups/forms.py:327#: dashboards/project/access_and_security/security_groups/forms.py:330#: dashboards/project/access_and_security/security_groups/forms.py:363#: dashboards/project/access_and_security/security_groups/forms.py:386#: dashboards/project/access_and_security/security_groups/forms.py:392#: dashboards/project/access_and_security/security_groups/views.py:79#: dashboards/project/access_and_security/security_groups/views.py:127#: dashboards/project/access_and_security/templates/access_and_security/security_groups/detail.html:6 msgid ""Manage Security Group Rules: "" msgstr ""Manage Security Group Rules: "" #: dashboards/project/instances/workflows/create_instance.py:577#: dashboards/project/instances/workflows/create_instance.py:84#: dashboards/project/instances/workflows/create_instance.py:135#: dashboards/project/volumes/volumes/forms.py:209#: dashboards/project/instances/workflows/create_instance.py:395#: dashboards/project/instances/workflows/create_instance.py:795#: dashboards/project/instances/workflows/create_instance.py:695#: dashboards/project/instances/views.py:361#: dashboards/project/database_backups/templates/database_backups/details.html:6 msgid ""Backup Details: "" msgstr ""Backup Details: "" #: dashboards/project/instances/workflows/create_instance.py:797#: dashboards/project/instances/workflows/create_instance.py:814#: dashboards/project/volumes/volumes/forms.py:602#: dashboards/project/volumes/volumes/forms.py:606#: dashboards/project/instances/workflows/create_instance.py:794#: dashboards/project/databases/views.py:114#: dashboards/project/databases/templates/databases/detail.html:6 #: dashboards/project/instances/templates/instances/detail.html:6 msgid ""Instance Details: "" msgstr ""Instance Details: "" #: dashboards/project/instances/workflows/create_instance.py:85#: dashboards/project/instances/workflows/create_instance.py:656#: dashboards/project/instances/workflows/create_instance.py:658#: dashboards/project/instances/workflows/create_instance.py:680#: dashboards/project/instances/workflows/create_instance.py:682#: dashboards/project/instances/workflows/create_instance.py:796#: dashboards/project/firewalls/views.py:142#: dashboards/project/firewalls/views.py:177 #: dashboards/project/firewalls/views.py:243 #: dashboards/project/firewalls/views.py:277#: dashboards/project/firewalls/views.py:210#: dashboards/project/firewalls/templates/firewalls/updatefirewall.html:6 msgid ""Edit Firewall "" msgstr ""Edit Firewall "" #: dashboards/project/firewalls/templates/firewalls/updatepolicy.html:6 msgid ""Edit Policy "" msgstr ""Edit Policy "" #: dashboards/project/firewalls/templates/firewalls/updaterule.html:6 msgid ""Edit Rule "" msgstr ""Edit Rule "" #: dashboards/project/images/images/views.py:106#: dashboards/project/volumes/volumes/forms.py:473#: dashboards/project/volumes/volumes/forms.py:541#: dashboards/project/images/templates/images/images/detail.html:7 msgid ""Image Details: "" msgstr ""Image Details: "" #: dashboards/project/images/templates/images/images/detail.html:7 msgid ""Image Details:"" msgstr ""Image Details:"" #: dashboards/project/instances/workflows/create_instance.py:741#: dashboards/project/instances/workflows/create_instance.py:397#: dashboards/project/instances/workflows/create_instance.py:759#: dashboards/project/instances/workflows/create_instance.py:760#: dashboards/project/instances/workflows/create_instance.py:165 #: dashboards/project/instances/workflows/create_instance.py:769#: dashboards/project/instances/views.py:358#: dashboards/project/instances/workflows/create_instance.py:486#: dashboards/project/instances/views.py:272#: dashboards/project/instances/views.py:293#: dashboards/project/instances/views.py:302#: dashboards/project/instances/views.py:311#: dashboards/project/instances/views.py:319#: dashboards/project/instances/views.py:372#: dashboards/project/volumes/volumes/tables.py:358#: dashboards/project/instances/workflows/create_instance.py:52#: dashboards/project/instances/workflows/create_instance.py:66#: dashboards/project/instances/workflows/create_instance.py:87#: dashboards/project/instances/workflows/create_instance.py:90#: dashboards/project/instances/workflows/create_instance.py:92#: dashboards/project/instances/workflows/create_instance.py:93#: dashboards/project/instances/workflows/create_instance.py:96#: dashboards/project/instances/workflows/create_instance.py:101#: dashboards/project/instances/workflows/create_instance.py:112#: dashboards/project/instances/workflows/create_instance.py:116#: dashboards/project/instances/workflows/create_instance.py:119 #: dashboards/project/volumes/volumes/forms.py:406#: dashboards/project/instances/workflows/create_instance.py:122#: dashboards/project/instances/workflows/create_instance.py:128#: dashboards/project/instances/workflows/create_instance.py:131#: dashboards/project/instances/workflows/create_instance.py:151#: dashboards/project/instances/workflows/create_instance.py:152#: dashboards/project/instances/workflows/create_instance.py:153#: dashboards/project/instances/workflows/create_instance.py:156#: dashboards/project/instances/workflows/create_instance.py:163#: dashboards/project/instances/workflows/create_instance.py:170#: dashboards/project/instances/workflows/create_instance.py:181#: dashboards/project/instances/workflows/create_instance.py:208#: dashboards/project/instances/workflows/create_instance.py:215#: dashboards/project/instances/workflows/create_instance.py:222#: dashboards/project/instances/workflows/create_instance.py:237 msgid ""You must set volume size"" msgstr ""You must set volume size"" #: dashboards/project/instances/workflows/create_instance.py:240 msgid ""Volume size must be greater than 0"" msgstr ""Volume size must be greater than 0"" #: dashboards/project/instances/workflows/create_instance.py:243 msgid ""You must select an image."" msgstr ""You must select an image."" #: dashboards/project/instances/workflows/create_instance.py:266#: dashboards/project/instances/workflows/create_instance.py:283#: dashboards/project/instances/workflows/create_instance.py:296 #: dashboards/project/instances/workflows/create_instance.py:313#: dashboards/project/instances/workflows/create_instance.py:301#: dashboards/project/instances/workflows/create_instance.py:307#: dashboards/project/instances/workflows/create_instance.py:330 #: dashboards/project/volumes/volumes/forms.py:273#: dashboards/project/instances/workflows/create_instance.py:336 #: dashboards/project/volumes/volumes/forms.py:276#: dashboards/project/instances/workflows/create_instance.py:338 #: dashboards/project/volumes/volumes/forms.py:278#: dashboards/project/instances/workflows/create_instance.py:360#: dashboards/project/instances/workflows/create_instance.py:370 #: dashboards/project/volumes/volumes/forms.py:196#: dashboards/project/instances/workflows/create_instance.py:375#: dashboards/project/instances/workflows/create_instance.py:409#: dashboards/project/instances/workflows/create_instance.py:411#: dashboards/project/instances/workflows/create_instance.py:424 #: dashboards/project/volumes/volumes/forms.py:291#: dashboards/project/instances/workflows/create_instance.py:426#: dashboards/project/instances/workflows/create_instance.py:428#: dashboards/project/instances/workflows/create_instance.py:440#: dashboards/project/instances/workflows/create_instance.py:443#: dashboards/project/instances/workflows/create_instance.py:445#: dashboards/project/instances/workflows/create_instance.py:488#: dashboards/project/instances/workflows/create_instance.py:492#: dashboards/project/instances/workflows/create_instance.py:498#: dashboards/project/instances/workflows/create_instance.py:504#: dashboards/project/instances/workflows/create_instance.py:509#: dashboards/project/instances/workflows/create_instance.py:525#: dashboards/project/instances/workflows/create_instance.py:529#: dashboards/project/instances/workflows/create_instance.py:531#: dashboards/project/instances/workflows/create_instance.py:540#: dashboards/project/instances/workflows/create_instance.py:572#: dashboards/project/instances/workflows/create_instance.py:576#: dashboards/project/instances/workflows/create_instance.py:580#: dashboards/project/instances/workflows/create_instance.py:584#: dashboards/project/instances/workflows/create_instance.py:588 #: dashboards/project/instances/workflows/create_instance.py:593#: dashboards/project/instances/workflows/create_instance.py:597 #: dashboards/project/instances/workflows/create_instance.py:602#: dashboards/project/instances/workflows/create_instance.py:629#: dashboards/project/instances/workflows/create_instance.py:637#: dashboards/project/instances/workflows/create_instance.py:664#: dashboards/project/instances/workflows/create_instance.py:667#: dashboards/project/instances/workflows/create_instance.py:742#: dashboards/project/instances/workflows/create_instance.py:746#: dashboards/project/instances/workflows/create_instance.py:747#: dashboards/project/instances/workflows/create_instance.py:773#: dashboards/project/instances/workflows/create_instance.py:811#: dashboards/project/instances/workflows/create_instance.py:871#: dashboards/project/networks/templates/networks/detail.html:6 msgid ""Network Detail: "" msgstr ""Network Detail: "" #: dashboards/project/stacks/views.py:218#: dashboards/project/stacks/views.py:229#: dashboards/project/stacks/views.py:262#: dashboards/project/stacks/views.py:275#: dashboards/project/stacks/templates/stacks/detail.html:6 msgid ""Stack Detail: "" msgstr ""Stack Detail: "" #: dashboards/project/stacks/templates/stacks/resource.html:6 msgid ""Resource Detail: "" msgstr ""Resource Detail: "" #: dashboards/project/volumes/backups/views.py:67#: dashboards/project/volumes/snapshots/views.py:83#: dashboards/project/volumes/templates/volumes/backups/detail.html:6 msgid ""Volume Backup Details: "" msgstr ""Volume Backup Details: "" #: dashboards/project/volumes/templates/volumes/backups/detail.html:6 msgid ""Volume Backup Details:"" msgstr ""Volume Backup Details:"" #: dashboards/project/volumes/templates/volumes/snapshots/detail.html:6 msgid ""Volume Snapshot Details: "" msgstr ""Volume Snapshot Details: "" #: dashboards/project/volumes/templates/volumes/snapshots/detail.html:6 msgid ""Volume Snapshot Details:"" msgstr ""Volume Snapshot Details:"" #: dashboards/project/volumes/volumes/tables.py:442#: dashboards/project/volumes/volumes/forms.py:101 msgid ""No volume type"" msgstr ""No volume type"" #: dashboards/project/volumes/volumes/forms.py:121#: dashboards/project/volumes/volumes/forms.py:129#: dashboards/project/volumes/volumes/forms.py:140#: dashboards/project/volumes/volumes/forms.py:148#: dashboards/project/volumes/volumes/forms.py:159#: dashboards/project/volumes/volumes/forms.py:168#: dashboards/project/volumes/volumes/forms.py:175#: dashboards/project/volumes/volumes/forms.py:197#: dashboards/project/volumes/volumes/forms.py:210#: dashboards/project/volumes/volumes/forms.py:222#: dashboards/project/volumes/volumes/forms.py:231#: dashboards/project/volumes/volumes/forms.py:242#: dashboards/project/volumes/volumes/forms.py:246#: dashboards/project/volumes/volumes/forms.py:250#: dashboards/project/volumes/volumes/forms.py:259#: dashboards/project/volumes/volumes/forms.py:313#: dashboards/project/volumes/volumes/forms.py:326#: dashboards/project/volumes/volumes/forms.py:334#: dashboards/project/volumes/volumes/forms.py:345#: dashboards/project/volumes/volumes/forms.py:354#: dashboards/project/volumes/volumes/forms.py:361#: dashboards/project/volumes/volumes/forms.py:377#: dashboards/project/volumes/volumes/forms.py:385#: dashboards/project/volumes/volumes/forms.py:402#: dashboards/project/volumes/volumes/forms.py:403#: dashboards/project/volumes/volumes/forms.py:410#: dashboards/project/volumes/volumes/forms.py:449#: dashboards/project/volumes/volumes/forms.py:459#: dashboards/project/volumes/volumes/forms.py:468#: dashboards/project/volumes/volumes/forms.py:492#: dashboards/project/volumes/volumes/forms.py:495#: dashboards/project/volumes/volumes/forms.py:508#: dashboards/project/volumes/volumes/forms.py:526#: dashboards/project/volumes/volumes/forms.py:532#: dashboards/project/volumes/volumes/forms.py:546#: dashboards/project/volumes/volumes/forms.py:581#: dashboards/project/volumes/volumes/forms.py:588#: dashboards/project/volumes/volumes/forms.py:614#: dashboards/project/volumes/volumes/forms.py:620#: dashboards/project/volumes/volumes/forms.py:634#: dashboards/project/volumes/volumes/forms.py:640#: dashboards/project/volumes/volumes/forms.py:649#: dashboards/project/volumes/volumes/forms.py:650#: dashboards/project/volumes/volumes/forms.py:651#: dashboards/project/volumes/volumes/forms.py:668#: dashboards/project/volumes/volumes/forms.py:677#: dashboards/project/volumes/volumes/forms.py:693#: dashboards/project/volumes/volumes/forms.py:702#: dashboards/project/volumes/volumes/views.py:248#: dashboards/project/volumes/volumes/tables.py:362#: dashboards/project/volumes/volumes/tables.py:365#: dashboards/project/volumes/volumes/tables.py:386#: dashboards/project/volumes/volumes/tables.py:395#: dashboards/project/volumes/volumes/tables.py:423#: dashboards/project/volumes/volumes/tables.py:432#: dashboards/project/volumes/volumes/views.py:105 #: dashboards/project/volumes/volumes/views.py:148 #: dashboards/project/volumes/volumes/views.py:231#: dashboards/project/volumes/volumes/views.py:139#: dashboards/project/volumes/volumes/views.py:167 #: dashboards/project/volumes/volumes/views.py:297#: dashboards/project/volumes/volumes/views.py:200#: usage/base.py:186 usage/quotas.py:372#: usage/quotas.py:358",6816,6867
openstack%2Fdevstack~master~I2eba3738342cb4835a992aa1152939a8dc2f74da,openstack/devstack,master,I2eba3738342cb4835a992aa1152939a8dc2f74da,Adding tempest install to devstack,MERGED,2014-11-24 14:29:28.000000000,2014-12-06 18:26:53.000000000,2014-12-06 18:26:52.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 2750}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-24 14:29:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/c5cb2c0b1ad0483562c30c785cf61c84f5d45a8d', 'message': ""Adding tempest install to devstack\n\nStack.sh doesn't install tempest, but it would be useful if it did.\n\nChange-Id: I2eba3738342cb4835a992aa1152939a8dc2f74da\n""}, {'number': 2, 'created': '2014-12-05 21:08:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/15ed4a832383c0849e81ccff9eb98f9dd0fe0740', 'message': ""Adding tempest install to devstack\n\nStack.sh doesn't install tempest, but it would be useful if it did.\n\nChange-Id: I2eba3738342cb4835a992aa1152939a8dc2f74da\n""}, {'number': 3, 'created': '2014-12-05 21:09:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/a4292314e23f67adeb48e983e768fe8a71050001', 'message': ""Adding tempest install to devstack\n\nStack.sh doesn't install tempest, but it would be useful if it did.\n\nChange-Id: I2eba3738342cb4835a992aa1152939a8dc2f74da\n""}, {'number': 4, 'created': '2014-12-05 21:10:28.000000000', 'files': ['samples/local.conf'], 'web_link': 'https://opendev.org/openstack/devstack/commit/fb3ce0bd6be04a5f23d33f3444a48a92fd8d0af1', 'message': ""Adding tempest install to devstack\n\nStack.sh doesn't install tempest, but it would be useful if it did.\n\nChange-Id: I2eba3738342cb4835a992aa1152939a8dc2f74da\n""}]",0,136767,fb3ce0bd6be04a5f23d33f3444a48a92fd8d0af1,14,4,4,9194,,,0,"Adding tempest install to devstack

Stack.sh doesn't install tempest, but it would be useful if it did.

Change-Id: I2eba3738342cb4835a992aa1152939a8dc2f74da
",git fetch https://review.opendev.org/openstack/devstack refs/changes/67/136767/4 && git format-patch -1 --stdout FETCH_HEAD,['stack.sh'],1,c5cb2c0b1ad0483562c30c785cf61c84f5d45a8d,tempestinstall,# Install tempest tests install_tempest ,,3,0
openstack%2Fpython-saharaclient~master~I734ba72e755f1417611392c2324d6589d14ec62e,openstack/python-saharaclient,master,I734ba72e755f1417611392c2324d6589d14ec62e,Update oslo-incubator importutils,MERGED,2014-12-04 18:28:39.000000000,2014-12-06 17:59:16.000000000,2014-12-06 17:59:16.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 8090}, {'_account_id': 8411}]","[{'number': 1, 'created': '2014-12-04 18:28:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/04878c8aece2bc86944008a39c4c9af35e14b30f', 'message': 'Update oslo-incubator importutils\n\nimportutils has graduated to oslo.utils\n\nThe openstack/common/importutils.py remains while apiclient still uses\nit, and we still use apiclient\n\nChange-Id: I734ba72e755f1417611392c2324d6589d14ec62e\n'}, {'number': 2, 'created': '2014-12-04 19:17:58.000000000', 'files': ['saharaclient/client.py'], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/665f3872657122d345e5521524a3652098adf4fa', 'message': 'Update oslo-incubator importutils\n\nimportutils has graduated to oslo.utils\n\nThe openstack/common/importutils.py remains while apiclient still uses\nit, and we still use apiclient\n\nChange-Id: I734ba72e755f1417611392c2324d6589d14ec62e\n'}]",0,139144,665f3872657122d345e5521524a3652098adf4fa,12,5,2,7555,,,0,"Update oslo-incubator importutils

importutils has graduated to oslo.utils

The openstack/common/importutils.py remains while apiclient still uses
it, and we still use apiclient

Change-Id: I734ba72e755f1417611392c2324d6589d14ec62e
",git fetch https://review.opendev.org/openstack/python-saharaclient refs/changes/44/139144/2 && git format-patch -1 --stdout FETCH_HEAD,['saharaclient/client.py'],1,04878c8aece2bc86944008a39c4c9af35e14b30f,,from oslo.utils import importutils,from saharaclient.openstack.common import importutils,1,1
openstack%2Fpython-saharaclient~master~Iba23b2153b9f7be6ea6efe7ff189080a96c45168,openstack/python-saharaclient,master,Iba23b2153b9f7be6ea6efe7ff189080a96c45168,Update oslo-incubator apiclient.exceptions,MERGED,2014-12-04 18:28:39.000000000,2014-12-06 17:59:10.000000000,2014-12-06 17:59:09.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 7555}, {'_account_id': 8411}]","[{'number': 1, 'created': '2014-12-04 18:28:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/c86832b4485fc6586276b18cfcda132881845d1f', 'message': 'Update oslo-incubator apiclient.exceptions\n\nChanges -\n * deprecate apiclient package\n * Handle different format of api exception\n * Split cliutils\n * Remove code that moved to oslo.i18n\n\nChange-Id: Iba23b2153b9f7be6ea6efe7ff189080a96c45168\n'}, {'number': 2, 'created': '2014-12-04 19:17:58.000000000', 'files': ['saharaclient/openstack/common/apiclient/exceptions.py'], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/f0fa95de32922658e9277a6e8ac15ad55efb1a40', 'message': 'Update oslo-incubator apiclient.exceptions\n\nChanges -\n * deprecate apiclient package\n * Handle different format of api exception\n * Split cliutils\n * Remove code that moved to oslo.i18n\n\nChange-Id: Iba23b2153b9f7be6ea6efe7ff189080a96c45168\n'}]",0,139143,f0fa95de32922658e9277a6e8ac15ad55efb1a40,14,5,2,7555,,,0,"Update oslo-incubator apiclient.exceptions

Changes -
 * deprecate apiclient package
 * Handle different format of api exception
 * Split cliutils
 * Remove code that moved to oslo.i18n

Change-Id: Iba23b2153b9f7be6ea6efe7ff189080a96c45168
",git fetch https://review.opendev.org/openstack/python-saharaclient refs/changes/43/139143/1 && git format-patch -1 --stdout FETCH_HEAD,['saharaclient/openstack/common/apiclient/exceptions.py'],1,c86832b4485fc6586276b18cfcda132881845d1f,,"######################################################################## # # THIS MODULE IS DEPRECATED # # Please refer to # https://etherpad.openstack.org/p/kilo-saharaclient-library-proposals for # the discussion leading to this deprecation. # # We recommend checking out the python-openstacksdk project # (https://launchpad.net/python-openstacksdk) instead. # ######################################################################## from saharaclient.openstack.common._i18n import _ if isinstance(body, dict): error = body.get(list(body)[0]) if isinstance(error, dict): kwargs[""message""] = (error.get(""message"") or error.get(""faultstring"")) kwargs[""details""] = (error.get(""details"") or six.text_type(body))","from saharaclient.openstack.common.gettextutils import _class MissingArgs(ClientException): """"""Supplied arguments are not sufficient for calling a function."""""" def __init__(self, missing): self.missing = missing msg = _(""Missing arguments: %s"") % "", "".join(missing) super(MissingArgs, self).__init__(msg) if isinstance(body, dict) and isinstance(body.get(""error""), dict): error = body[""error""] kwargs[""message""] = error.get(""message"") kwargs[""details""] = error.get(""details"")",21,13
openstack%2Fpython-saharaclient~master~I5ed76b7e45f6db65802dfcad10fdcbc58c0f790f,openstack/python-saharaclient,master,I5ed76b7e45f6db65802dfcad10fdcbc58c0f790f,Update oslo-incubator cliutils,MERGED,2014-12-04 18:28:39.000000000,2014-12-06 17:59:04.000000000,2014-12-06 17:59:03.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 7555}, {'_account_id': 8411}]","[{'number': 1, 'created': '2014-12-04 18:28:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/7ccfc8e3a3ef3b36203e354612829fe7365f05f1', 'message': 'Update oslo-incubator cliutils\n\nChanges -\n * cliutils restore python3 string decode\n * Split cliutils\n * remove caching param from prettytable call\n * Remove code that moved to oslo.i18n\n * Switch oslo-incubator to use oslo.utils and remove old modules\n\nChange-Id: I5ed76b7e45f6db65802dfcad10fdcbc58c0f790f\n'}, {'number': 2, 'created': '2014-12-04 19:17:58.000000000', 'files': ['saharaclient/openstack/common/cliutils.py'], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/d37a48ad4802ec6d1670bc94417ba0fc81e48acc', 'message': 'Update oslo-incubator cliutils\n\nChanges -\n * cliutils restore python3 string decode\n * Split cliutils\n * remove caching param from prettytable call\n * Remove code that moved to oslo.i18n\n * Switch oslo-incubator to use oslo.utils and remove old modules\n\nChange-Id: I5ed76b7e45f6db65802dfcad10fdcbc58c0f790f\n'}]",0,139142,d37a48ad4802ec6d1670bc94417ba0fc81e48acc,15,5,2,7555,,,0,"Update oslo-incubator cliutils

Changes -
 * cliutils restore python3 string decode
 * Split cliutils
 * remove caching param from prettytable call
 * Remove code that moved to oslo.i18n
 * Switch oslo-incubator to use oslo.utils and remove old modules

Change-Id: I5ed76b7e45f6db65802dfcad10fdcbc58c0f790f
",git fetch https://review.opendev.org/openstack/python-saharaclient refs/changes/42/139142/2 && git format-patch -1 --stdout FETCH_HEAD,['saharaclient/openstack/common/cliutils.py'],1,7ccfc8e3a3ef3b36203e354612829fe7365f05f1,,"from oslo.utils import encodeutils from oslo.utils import strutilsfrom saharaclient.openstack.common._i18n import _ class MissingArgs(Exception): """"""Supplied arguments are not sufficient for calling a function."""""" def __init__(self, missing): self.missing = missing msg = _(""Missing arguments: %s"") % "", "".join(missing) super(MissingArgs, self).__init__(msg) raise MissingArgs(missing) pt = prettytable.PrettyTable(field_labels) if six.PY3: print(encodeutils.safe_encode(pt.get_string(**kwargs)).decode()) else: print(encodeutils.safe_encode(pt.get_string(**kwargs))) pt = prettytable.PrettyTable([dict_property, 'Value']) if six.PY3: print(encodeutils.safe_encode(pt.get_string()).decode()) else: print(encodeutils.safe_encode(pt.get_string()))","from saharaclient.openstack.common.apiclient import exceptions from saharaclient.openstack.common.gettextutils import _ from saharaclient.openstack.common import strutils from saharaclient.openstack.common import uuidutils raise exceptions.MissingArgs(missing) pt = prettytable.PrettyTable(field_labels, caching=False) print(strutils.safe_encode(pt.get_string(**kwargs))) pt = prettytable.PrettyTable([dict_property, 'Value'], caching=False) print(strutils.safe_encode(pt.get_string()))def find_resource(manager, name_or_id, **find_args): """"""Look for resource in a given manager. Used as a helper for the _find_* methods. Example: .. code-block:: python def _find_hypervisor(cs, hypervisor): #Get a hypervisor by name or ID. return cliutils.find_resource(cs.hypervisors, hypervisor) """""" # first try to get entity as integer id try: return manager.get(int(name_or_id)) except (TypeError, ValueError, exceptions.NotFound): pass # now try to get entity as uuid try: if six.PY2: tmp_id = strutils.safe_encode(name_or_id) else: tmp_id = strutils.safe_decode(name_or_id) if uuidutils.is_uuid_like(tmp_id): return manager.get(tmp_id) except (TypeError, ValueError, exceptions.NotFound): pass # for str id which is not uuid if getattr(manager, 'is_alphanum_id_allowed', False): try: return manager.get(name_or_id) except exceptions.NotFound: pass try: try: return manager.find(human_id=name_or_id, **find_args) except exceptions.NotFound: pass # finally try to find entity by name try: resource = getattr(manager, 'resource_class', None) name_attr = resource.NAME_ATTR if resource else 'name' kwargs = {name_attr: name_or_id} kwargs.update(find_args) return manager.find(**kwargs) except exceptions.NotFound: msg = _(""No %(name)s with a name or "" ""ID of '%(name_or_id)s' exists."") % \ { ""name"": manager.resource_class.__name__.lower(), ""name_or_id"": name_or_id } raise exceptions.CommandError(msg) except exceptions.NoUniqueMatch: msg = _(""Multiple %(name)s matches found for "" ""'%(name_or_id)s', use an ID to be more specific."") % \ { ""name"": manager.resource_class.__name__.lower(), ""name_or_id"": name_or_id } raise exceptions.CommandError(msg) ",23,77
openstack%2Fpython-saharaclient~master~I5168b3cd60ab4c987f12152d3c3f09b8cc562d09,openstack/python-saharaclient,master,I5168b3cd60ab4c987f12152d3c3f09b8cc562d09,Updating oslo-incubator,MERGED,2014-12-04 18:28:39.000000000,2014-12-06 17:58:58.000000000,2014-12-06 17:58:57.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 7555}, {'_account_id': 8411}]","[{'number': 1, 'created': '2014-12-04 18:28:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/63011303601020e21e07296d304a7f321c3f5c8e', 'message': 'Updating oslo-incubator\n\nChanges -\n * Allow tempest to use new log w/o oslo.i18n\n * Fix i18n import\n * Remove code that moved to oslo.i18n\n * Let oslotest manage the six.move setting for mox\n\nChange-Id: I5168b3cd60ab4c987f12152d3c3f09b8cc562d09\n'}, {'number': 2, 'created': '2014-12-04 19:17:58.000000000', 'files': ['saharaclient/openstack/common/_i18n.py', 'saharaclient/openstack/common/__init__.py'], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/1178326e385235f0c72e5202e356255b1a05cd1e', 'message': 'Updating oslo-incubator\n\nChanges -\n * Allow tempest to use new log w/o oslo.i18n\n * Fix i18n import\n * Remove code that moved to oslo.i18n\n * Let oslotest manage the six.move setting for mox\n\nChange-Id: I5168b3cd60ab4c987f12152d3c3f09b8cc562d09\n'}]",0,139141,1178326e385235f0c72e5202e356255b1a05cd1e,14,5,2,7555,,,0,"Updating oslo-incubator

Changes -
 * Allow tempest to use new log w/o oslo.i18n
 * Fix i18n import
 * Remove code that moved to oslo.i18n
 * Let oslotest manage the six.move setting for mox

Change-Id: I5168b3cd60ab4c987f12152d3c3f09b8cc562d09
",git fetch https://review.opendev.org/openstack/python-saharaclient refs/changes/41/139141/2 && git format-patch -1 --stdout FETCH_HEAD,"['saharaclient/openstack/common/_i18n.py', 'saharaclient/openstack/common/__init__.py']",2,63011303601020e21e07296d304a7f321c3f5c8e,,,"# # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import six six.add_move(six.MovedModule('mox', 'mox', 'mox3.mox')) ",45,17
openstack%2Fpython-saharaclient~master~I213cda7dcdaef78bbb8fae8fbafed95a68f8f452,openstack/python-saharaclient,master,I213cda7dcdaef78bbb8fae8fbafed95a68f8f452,Update oslo-incubator strutils,MERGED,2014-12-04 18:28:39.000000000,2014-12-06 17:58:48.000000000,2014-12-06 17:58:47.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 7555}, {'_account_id': 8411}]","[{'number': 1, 'created': '2014-12-04 18:28:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/d0c5c9ac97f88bdf60feba97a07ea60cb251ea2c', 'message': ""Update oslo-incubator strutils\n\nstrutils has moved to oslo.utils\n\nThe saharaclient/openstack/common/strutils.py remains as it's needed\nby apiclient.\n\nChange-Id: I213cda7dcdaef78bbb8fae8fbafed95a68f8f452\n""}, {'number': 2, 'created': '2014-12-04 19:17:58.000000000', 'files': ['requirements.txt', 'openstack-common.conf', 'saharaclient/shell.py'], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/b59de4ea6b5fd099741c25ee5893f77e5a453b81', 'message': ""Update oslo-incubator strutils\n\nstrutils has moved to oslo.utils\n\nThe saharaclient/openstack/common/strutils.py remains as it's needed\nby apiclient.\n\nChange-Id: I213cda7dcdaef78bbb8fae8fbafed95a68f8f452\n""}]",0,139140,b59de4ea6b5fd099741c25ee5893f77e5a453b81,14,5,2,7555,,,0,"Update oslo-incubator strutils

strutils has moved to oslo.utils

The saharaclient/openstack/common/strutils.py remains as it's needed
by apiclient.

Change-Id: I213cda7dcdaef78bbb8fae8fbafed95a68f8f452
",git fetch https://review.opendev.org/openstack/python-saharaclient refs/changes/40/139140/2 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'openstack-common.conf', 'saharaclient/shell.py']",3,d0c5c9ac97f88bdf60feba97a07ea60cb251ea2c,,from oslo.utils import strutils,from saharaclient.openstack.common import strutils,2,2
openstack%2Fcinder~master~I3b7fe4765c1c568a6a7713f86cadece52b130911,openstack/cinder,master,I3b7fe4765c1c568a6a7713f86cadece52b130911,Remove lio_initiator_iqns,MERGED,2014-10-09 20:41:57.000000000,2014-12-06 17:42:37.000000000,2014-12-05 16:20:28.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 2243}, {'_account_id': 2861}, {'_account_id': 4523}, {'_account_id': 5538}, {'_account_id': 5997}, {'_account_id': 7160}, {'_account_id': 9008}, {'_account_id': 10503}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12369}, {'_account_id': 12779}]","[{'number': 1, 'created': '2014-10-09 20:41:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2b5cca737a0d65c7c99538aabedfdc92addc5dfb', 'message': 'Remove lio_initiator_iqns\n\nThis is a holdover from the initial LIO implementation\nwhich did not manage ACLs sufficiently.  This is\nunneeded now and handled by:\n67dd248 LIO iSCSI initiator ACL auto-config\n\nDocImpact: remove lio_initiator_iqns cfg opt\n\nChange-Id: I3b7fe4765c1c568a6a7713f86cadece52b130911\n'}, {'number': 2, 'created': '2014-11-07 09:38:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/36e57ea1c90ba02567d85dd8c5a50b4f4d13de38', 'message': 'Remove lio_initiator_iqns\n\nThis is a holdover from the initial LIO implementation\nwhich did not manage ACLs sufficiently.  This is\nunneeded now and handled by:\n67dd248 LIO iSCSI initiator ACL auto-config\n\nDocImpact: remove lio_initiator_iqns cfg opt\n\nCloses-Bug: #1390383\n\nChange-Id: I3b7fe4765c1c568a6a7713f86cadece52b130911\n'}, {'number': 3, 'created': '2014-12-03 21:44:21.000000000', 'files': ['etc/cinder/cinder.conf.sample', 'cinder/volume/driver.py', 'cinder/tests/conf_fixture.py', 'cinder/brick/iscsi/iscsi.py', 'cinder/volume/targets/lio.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/6a8803f27b11da7b54d62f3e0b08c58b6bd685ce', 'message': 'Remove lio_initiator_iqns\n\nThis is a holdover from the initial LIO implementation\nwhich did not manage ACLs sufficiently.  This is\nunneeded now and handled by:\n67dd248 LIO iSCSI initiator ACL auto-config\n\nDocImpact: remove lio_initiator_iqns cfg opt\n\nCloses-Bug: #1390383\n\nChange-Id: I3b7fe4765c1c568a6a7713f86cadece52b130911\n'}]",3,127342,6a8803f27b11da7b54d62f3e0b08c58b6bd685ce,40,17,3,4523,,,0,"Remove lio_initiator_iqns

This is a holdover from the initial LIO implementation
which did not manage ACLs sufficiently.  This is
unneeded now and handled by:
67dd248 LIO iSCSI initiator ACL auto-config

DocImpact: remove lio_initiator_iqns cfg opt

Closes-Bug: #1390383

Change-Id: I3b7fe4765c1c568a6a7713f86cadece52b130911
",git fetch https://review.opendev.org/openstack/cinder refs/changes/42/127342/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/driver.py', 'etc/cinder/cinder.conf.sample', 'cinder/brick/iscsi/iscsi.py']",3,2b5cca737a0d65c7c99538aabedfdc92addc5dfb,bug/1390383," def __init__(self, root_helper,"," def __init__(self, root_helper, lio_initiator_iqns='', self.lio_initiator_iqns = lio_initiator_iqns extra_args = [] if self.lio_initiator_iqns: extra_args.append(self.lio_initiator_iqns) if extra_args: command_args.extend(extra_args)",2,20
openstack%2Fsahara-specs~master~I28414f7ad8cffb44af3c613406580d0c0988e25e,openstack/sahara-specs,master,I28414f7ad8cffb44af3c613406580d0c0988e25e,Adding security guidelines documentation spec,MERGED,2014-12-04 19:59:56.000000000,2014-12-06 17:40:19.000000000,2014-12-06 17:40:18.000000000,"[{'_account_id': 3}, {'_account_id': 2807}, {'_account_id': 6786}, {'_account_id': 8411}, {'_account_id': 10670}]","[{'number': 1, 'created': '2014-12-04 19:59:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/a92510a22f0c3601b09b6c5373fb8ef3386db39f', 'message': 'Adding security guidelines documentation spec\n\nChange-Id: I28414f7ad8cffb44af3c613406580d0c0988e25e\nPartial-Implements: bp security-guidelines-doc\n'}, {'number': 2, 'created': '2014-12-04 22:22:19.000000000', 'files': ['specs/kilo/security-guidelines-doc.rst'], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/b1ccb527dd18a590c86d428f083415c6eef19c8b', 'message': 'Adding security guidelines documentation spec\n\nChange-Id: I28414f7ad8cffb44af3c613406580d0c0988e25e\nPartial-Implements: bp security-guidelines-doc\n'}]",0,139170,b1ccb527dd18a590c86d428f083415c6eef19c8b,14,5,2,10670,,,0,"Adding security guidelines documentation spec

Change-Id: I28414f7ad8cffb44af3c613406580d0c0988e25e
Partial-Implements: bp security-guidelines-doc
",git fetch https://review.opendev.org/openstack/sahara-specs refs/changes/70/139170/2 && git format-patch -1 --stdout FETCH_HEAD,['specs/kilo/security-guidelines-doc.rst'],1,a92510a22f0c3601b09b6c5373fb8ef3386db39f,bp/security-guidelines-doc,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ============================================= Creation of Security Guidelines Documentation ============================================= https://blueprints.launchpad.net/sahara/+spec/security-guidelines-doc As Sahara increases in functionality and complexity, the need for clear, concise documentation grows. This is especially true for security related topics as they are currently under-represented. This specification proposes the creation of a document that will provide a source for security related topics, guides, and instructions as they pertain to Sahara. Problem description =================== There is currently a distinct lack of centralized, security related, documentation for Sahara. Several features have been implemented to address security shortfalls and they could use broadened discussions of their application and proper usage. Additionally there is no current documentation which discusses the specific procedures for securing the individual plugin technologies at use within Sahara. Proposed change =============== This specification proposes the creation of Sahara specific documentation addressing the security concerns of users and operators. This documentation will cover current features, best practices, and guidelines for installing and operating Sahara. The documentation generated by this effort will be included in the OpenStack Security Guide[1]. Bug patches will be generated against the current OpenStack manuals, and the OpenStack Security Group will be engaged with respect to finalizing and including the documentation. The process will be broken down into sub-chapter sections that will make up a Sahara chapter for the OpenStack Security Guide. Initially these sub-chapters will include; Sahara controller installs, current feature discussions, and plugin specific topics. The information provided is intended to be updated as new methodologies, plugins, and features are implemented. It will also be open to patching through the standrd OpenStack workflows by the community at large. Alternatives ------------ Creation of a separate document managed by the Sahara team outside the purview of the OpenStack manuals, and distributed through the Sahara documentation. This solution would be non-ideal as it creates an alternate path for OpenStack manuals that is outside the expected locations for end users. Creation of a separate document as above with the exception that it will be maintained with the other OpenStack manuals. This option might be more plausible than the previous, but it still suffers from the problem of creating an alternate location for security related guidelines that is separate from the official manual. It also bears the burden of creating a new project within the manuals infrastructure. Data model impact ----------------- None REST API impact --------------- None Other end user impact --------------------- None Deployer impact --------------- None Developer impact ---------------- None Sahara-image-elements impact ---------------------------- None Sahara-dashboard / Horizon impact --------------------------------- None Implementation ============== Assignee(s) ----------- Primary assignee: mimccune (Michael McCune) Other contributors: None Work Items ---------- * Create a bug against the OpenStack manuals for Sahara chapter inclusion * Create the documentation and change requests for the following sub-chapters: * Sahara controller install guides, with security related focus * Feature discussions and examples * plugin specific topics * Hadoop * Spark * Storm Dependencies ============ None Testing ======= Format testing as provided by the security guide project. Documentation Impact ==================== This specification proposes the creation of hypertext and PDF documentation. References ========== [1]: http://docs.openstack.org/security-guide/content/ch_preface.html ",,151,0
openstack%2Fsahara-specs~master~I8d954e347ceaad3e9ab32ce9162d7ac36e82ebd3,openstack/sahara-specs,master,I8d954e347ceaad3e9ab32ce9162d7ac36e82ebd3,Specification Repository Backlog Refactor,MERGED,2014-11-25 21:00:14.000000000,2014-12-06 17:38:02.000000000,2014-12-06 17:38:01.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7132}, {'_account_id': 7555}, {'_account_id': 7710}, {'_account_id': 8411}]","[{'number': 1, 'created': '2014-11-25 21:00:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/a5e42c7975d7ab0e1ce0c567bf2749b7b76fd8bb', 'message': 'Adding backlog to specs repo\n\nChanges\n* adding a backlog directory to specs directory\n* adding a README to the backlog directory\n* removing approved directory in specs/juno\n* moving implemented specs from specs/juno/implemented to specs/juno\n* removing specs/juno/redirects\n* updating README to highlight new directory structure, and general\n  cleanup\n* updating doc index to use new locations for juno specs\n* adding backlog to doc index\n\nChange-Id: I8d954e347ceaad3e9ab32ce9162d7ac36e82ebd3\nImplements: bp/add-backlog-to-specs\n'}, {'number': 2, 'created': '2014-11-25 21:03:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/2032484c613748074625ce1853dcf6bdfc6aa79e', 'message': 'Specification Repository Backlog Refactor\n\nThis change implements the backlog refactor.\n\nChanges\n* adding a backlog directory to specs directory\n* adding a README to the backlog directory\n* removing approved directory in specs/juno\n* moving implemented specs from specs/juno/implemented to specs/juno\n* removing specs/juno/redirects\n* updating README to highlight new directory structure, and general\n  cleanup\n* updating doc index to use new locations for juno specs\n* adding backlog to doc index\n\nChange-Id: I8d954e347ceaad3e9ab32ce9162d7ac36e82ebd3\nImplements: blueprint add-backlog-to-specs\n'}, {'number': 3, 'created': '2014-12-02 14:27:40.000000000', 'files': ['specs/juno/move-rest-samples-to-docs.rst', 'specs/juno/edp-spark-job-type.rst', 'specs/juno/edp-move-examples.rst', 'README.rst', 'specs/juno/append-to-remote-file.rst', 'specs/juno/cdh-plugin.rst', 'specs/juno/edp-refactor-job-manager.rst', 'specs/juno/redirects', 'specs/juno/ceilometer-integration.rst', 'specs/juno/cluster-persist-sahara-configuration.rst', 'specs/juno/approved/.gitignore', 'doc/source/index.rst', 'specs/juno/edp-spark-standalone.rst', 'specs/backlog/README.rst', 'specs/juno/edp-swift-trust-authentication.rst', 'specs/juno/error-handling-in-provisioning.rst', 'specs/juno/cluster-secgroups.rst', 'specs/juno/anti-affinity-via-server-groups.rst'], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/89d4deb2f58117e4ee5228dfd95792089a6ace55', 'message': 'Specification Repository Backlog Refactor\n\nThis change implements the backlog refactor.\n\nChanges\n* adding a backlog directory to specs directory\n* adding a README to the backlog directory\n* removing approved directory in specs/juno\n* moving implemented specs from specs/juno/implemented to specs/juno\n* removing specs/juno/redirects\n* updating README to highlight new directory structure, and general\n  cleanup\n* updating doc index to use new locations for juno specs\n* adding backlog to doc index\n* inverting redirects for older implemented juno specs\n\nChange-Id: I8d954e347ceaad3e9ab32ce9162d7ac36e82ebd3\nImplements: blueprint add-backlog-to-specs\n'}]",1,137211,89d4deb2f58117e4ee5228dfd95792089a6ace55,18,6,3,10670,,,0,"Specification Repository Backlog Refactor

This change implements the backlog refactor.

Changes
* adding a backlog directory to specs directory
* adding a README to the backlog directory
* removing approved directory in specs/juno
* moving implemented specs from specs/juno/implemented to specs/juno
* removing specs/juno/redirects
* updating README to highlight new directory structure, and general
  cleanup
* updating doc index to use new locations for juno specs
* adding backlog to doc index
* inverting redirects for older implemented juno specs

Change-Id: I8d954e347ceaad3e9ab32ce9162d7ac36e82ebd3
Implements: blueprint add-backlog-to-specs
",git fetch https://review.opendev.org/openstack/sahara-specs refs/changes/11/137211/2 && git format-patch -1 --stdout FETCH_HEAD,"['specs/juno/move-rest-samples-to-docs.rst', 'specs/juno/edp-spark-job-type.rst', 'specs/juno/edp-move-examples.rst', 'README.rst', 'specs/juno/append-to-remote-file.rst', 'specs/juno/cdh-plugin.rst', 'specs/juno/edp-refactor-job-manager.rst', 'specs/juno/redirects', 'specs/juno/ceilometer-integration.rst', 'specs/juno/cluster-persist-sahara-configuration.rst', 'specs/juno/approved/.gitignore', 'doc/source/index.rst', 'specs/juno/edp-spark-standalone.rst', 'specs/backlog/README.rst', 'specs/juno/edp-swift-trust-authentication.rst', 'specs/juno/error-handling-in-provisioning.rst', 'specs/juno/cluster-secgroups.rst', 'specs/juno/anti-affinity-via-server-groups.rst']",18,a5e42c7975d7ab0e1ce0c567bf2749b7b76fd8bb,bp/add-backlog-to-specs,,,58,33
openstack%2Fsahara-specs~master~I36001e593b60a8ad1a5e2941f3883001cec8189e,openstack/sahara-specs,master,I36001e593b60a8ad1a5e2941f3883001cec8189e,Added spec for indirect VMs access,MERGED,2014-10-14 23:08:46.000000000,2014-12-06 17:37:49.000000000,2014-12-06 17:37:48.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7132}, {'_account_id': 7555}, {'_account_id': 7710}, {'_account_id': 8090}, {'_account_id': 8411}, {'_account_id': 10670}, {'_account_id': 12038}, {'_account_id': 12039}, {'_account_id': 12561}]","[{'number': 1, 'created': '2014-10-14 23:08:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/c967b8fb181065fdb0a6dcc6af7ddf3cdbfa44cf', 'message': 'Added spec for indirect VMs access\n\nChange-Id: I36001e593b60a8ad1a5e2941f3883001cec8189e\nBlueprint: indirect-vm-access\n'}, {'number': 2, 'created': '2014-10-16 17:11:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/40ef9e62704c28bd697fe984dca392ea08df634e', 'message': 'Added spec for indirect VMs access\n\nChange-Id: I36001e593b60a8ad1a5e2941f3883001cec8189e\nBlueprint: indirect-vm-access\n'}, {'number': 3, 'created': '2014-10-17 06:05:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/7753a57afd370d157bebc2b11e9483e5178de4d8', 'message': 'Added spec for indirect VMs access\n\nChange-Id: I36001e593b60a8ad1a5e2941f3883001cec8189e\nBlueprint: indirect-vm-access\n'}, {'number': 4, 'created': '2014-10-20 21:49:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/f275fdfb5d53f715de7f34a29264a2f8b4cff18d', 'message': 'Added spec for indirect VMs access\n\nChange-Id: I36001e593b60a8ad1a5e2941f3883001cec8189e\nBlueprint: indirect-vm-access\n'}, {'number': 5, 'created': '2014-10-31 23:46:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/7b3359d952fecf89efe4418d6184676495cf071d', 'message': 'Added spec for indirect VMs access\n\nChange-Id: I36001e593b60a8ad1a5e2941f3883001cec8189e\nBlueprint: indirect-vm-access\n'}, {'number': 6, 'created': '2014-11-03 20:48:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/b00cb8353588ace2b5f92b81f2f2ee8d11e15c41', 'message': 'Added spec for indirect VMs access\n\nChange-Id: I36001e593b60a8ad1a5e2941f3883001cec8189e\nBlueprint: indirect-vm-access\n'}, {'number': 7, 'created': '2014-11-07 00:07:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/611c834f1beff3c27f4af1414af081d64d54ca4d', 'message': 'Added spec for indirect VMs access\n\nChange-Id: I36001e593b60a8ad1a5e2941f3883001cec8189e\nBlueprint: indirect-vm-access\n'}, {'number': 8, 'created': '2014-11-07 22:11:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/75ad410a6047b6c155a172e50aae501111f7a86f', 'message': 'Added spec for indirect VMs access\n\nChange-Id: I36001e593b60a8ad1a5e2941f3883001cec8189e\nBlueprint: indirect-vm-access\n'}, {'number': 9, 'created': '2014-11-07 23:54:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/c6ee9b1f277010a141168d9e9b18aef4d48de510', 'message': 'Added spec for indirect VMs access\n\nChange-Id: I36001e593b60a8ad1a5e2941f3883001cec8189e\nBlueprint: indirect-vm-access\n'}, {'number': 10, 'created': '2014-11-28 18:30:06.000000000', 'files': ['specs/kilo/indirect-vm-access.rst'], 'web_link': 'https://opendev.org/openstack/sahara-specs/commit/4e952f495d494b82d9185c7e55d9ef19d3b03702', 'message': 'Added spec for indirect VMs access\n\nChange-Id: I36001e593b60a8ad1a5e2941f3883001cec8189e\nBlueprint: indirect-vm-access\n'}]",54,128475,4e952f495d494b82d9185c7e55d9ef19d3b03702,73,11,10,8411,,,0,"Added spec for indirect VMs access

Change-Id: I36001e593b60a8ad1a5e2941f3883001cec8189e
Blueprint: indirect-vm-access
",git fetch https://review.opendev.org/openstack/sahara-specs refs/changes/75/128475/10 && git format-patch -1 --stdout FETCH_HEAD,['specs/kilo/indirect-vm-access.rst'],1,c967b8fb181065fdb0a6dcc6af7ddf3cdbfa44cf,bp/indirect-vm-access,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Indirect access to VMs ========================================== https://blueprints.launchpad.net/sahara/+spec/indirect-vm-access This blueprint proposes one more way for Sahara to manage VMs. Management could be done via additional VM that works as proxy node. In this case access from controller should be given for one VM only. Problem description =================== Currently there are several ways to give Sahara access to VMs: 1. flat private network * not secure * doesn't work with neutron 2. floating IPs * all nodes need to have floating IPs - floating IPs are limited resource - floating IPs are usually for external world, not for access from controller * all nodes need to be accessible from controller nodes - it is more complicated in HA mode * access to data nodes should be secure 3. net_ns * hard co configure * can be inappropriate * doesn't work in HA mode 4. agents * not implemented yet * require external message queue accessible from VMs and controllers * require maintenance of agents So, there can be cases when none of listed approaches work. Proposed change =============== This blueprint proposes one more way to access VMs by Sahara. Sahara will spawn one more VM (proxy node) and gain access to it using any of the methods mentioned above. After that access to all other VMs will be through this proxy VM. This proxy could have really small flavor (cerros or similar). So, proposed workflow: 1. Sahara spawns all infrastructure plus additional node (proxy node) 2. Sahara creates special security group for proxy node 3. Sahara communicates with all VMs via this proxy Pros: we need access to only one VM. Cons: additional VM; indirect access could be slower Alternatives ------------ This blueprint offers one more way to access VMs. All existing way will remain unchanged. Data model impact ----------------- We will need to store: 1. flag if new access mode need to be used (boolean) 2. flavor for proxy node (it could be much smaller) 3. image for proxy node (simple linux, the only requirement is sshd installed) 4. user for proxy node image (may be hardcoded) REST API impact --------------- Additional field for cluster and cluster templates (see previous section) Other end user impact --------------------- None Deployer impact --------------- One more deployment option to consider. Developer impact ---------------- None Sahara-image-elements impact ---------------------------- None Sahara-dashboard / Horizon impact --------------------------------- Additional fields in UI Implementation ============== Assignee(s) ----------- Primary Assignee: Andrew Lazarev (alazarev) Work Items ---------- * Shara changes * Python client changes * UI changes * Doc changes Dependencies ============ None Testing ======= Manually Documentation Impact ==================== The feature need to be documented. References ========== None",,154,0
openstack%2Fsahara-dashboard~master~I8eb6e939ab528e5446a11d53f49765a790eb4fdb,openstack/sahara-dashboard,master,I8eb6e939ab528e5446a11d53f49765a790eb4fdb,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:52:03.000000000,2014-12-06 17:34:21.000000000,2014-12-06 17:34:20.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 7710}, {'_account_id': 8411}]","[{'number': 1, 'created': '2014-12-05 03:52:03.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/sahara-dashboard/commit/11e991c5f1a932ca350b03fc6a9ba1ca911b5bf8', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I8eb6e939ab528e5446a11d53f49765a790eb4fdb\n'}]",0,139388,11e991c5f1a932ca350b03fc6a9ba1ca911b5bf8,10,5,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I8eb6e939ab528e5446a11d53f49765a790eb4fdb
",git fetch https://review.opendev.org/openstack/sahara-dashboard refs/changes/88/139388/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,11e991c5f1a932ca350b03fc6a9ba1ca911b5bf8,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Fcinder~master~Ie4e25a8cd36782007a8934cc4573632f5215c2b1,openstack/cinder,master,Ie4e25a8cd36782007a8934cc4573632f5215c2b1,Bring cinder up-to-date with new oslo libraries,MERGED,2014-12-03 18:25:02.000000000,2014-12-06 16:54:20.000000000,2014-12-04 05:38:51.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 1207}, {'_account_id': 2243}, {'_account_id': 2750}, {'_account_id': 4523}, {'_account_id': 5997}, {'_account_id': 7198}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-03 18:25:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/f54e87a49e73b07d4fe8506df6827b0469ce1910', 'message': 'Bring cinder up-to-date with new oslo libraries\n\nNew versions of the oslo libraries have been released and\nthis has broken Cinder in the gate.  The first issue is\ncausing all unit tests to fail with the message:\n\n""NoSuchOptError: no such option: fake_rabbit""\n\nThis was due to commit bcb3b23b8f6e7d01e38fdc031982558711bb7586\nremoving that option from oslo/messaging/_drivers/impl_rabbit.py .\nWe were trying to set that option in cinder/tests/conf_fixture.py .\nI have removed that code.\n\nSecond, the update has made etc/cinder.conf.sample go out of date\nagain.  I am merging the changes to bring it back up to date.\n\nChange-Id: Ie4e25a8cd36782007a8934cc4573632f5215c2b1\n'}, {'number': 2, 'created': '2014-12-03 19:43:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6b86a8bd37a87b700bed7039c76248aa4be0c8cd', 'message': 'Bring cinder up-to-date with new oslo libraries\n\nNew versions of the oslo libraries have been released and\nthis has broken Cinder in the gate.  The first issue is\ncausing all unit tests to fail with the message:\n\n""NoSuchOptError: no such option: fake_rabbit""\n\nThis was due to commit bcb3b23b8f6e7d01e38fdc031982558711bb7586\nremoving that option from oslo/messaging/_drivers/impl_rabbit.py .\nWe were trying to set that option in cinder/tests/conf_fixture.py .\nI have removed that code.\n\nSecond, the update has made etc/cinder.conf.sample go out of date\nagain.  I am merging the changes to bring it back up to date.\n\n(Pulled from gate, this is already failing on new oslo updates in the 2 hrs that it was put out there).\n\nChange-Id: Ie4e25a8cd36782007a8934cc4573632f5215c2b1'}, {'number': 3, 'created': '2014-12-03 20:39:09.000000000', 'files': ['etc/cinder/cinder.conf.sample', 'cinder/tests/conf_fixture.py', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/cinder/commit/313c33049f8d0cb0e41cef52f35f5ce96ac3373a', 'message': 'Bring cinder up-to-date with new oslo libraries\n\nNew versions of the oslo libraries have been released and\nthis has broken Cinder in the gate.  The first issue is\ncausing all unit tests to fail with the message:\n\n""NoSuchOptError: no such option: fake_rabbit""\n\nThis was due to commit bcb3b23b8f6e7d01e38fdc031982558711bb7586\nremoving that option from oslo/messaging/_drivers/impl_rabbit.py .\nWe were trying to set that option in cinder/tests/conf_fixture.py .\nI have removed that code.\n\nSecond, the update has made etc/cinder.conf.sample go out of date\nagain.  I am merging the changes to bring it back up to date.\n\nThis change also disables execution of check_uptodate.sh execution\nfrom the gate.\n\nChange-Id: Ie4e25a8cd36782007a8934cc4573632f5215c2b1\n'}]",0,138816,313c33049f8d0cb0e41cef52f35f5ce96ac3373a,49,16,3,7198,,,0,"Bring cinder up-to-date with new oslo libraries

New versions of the oslo libraries have been released and
this has broken Cinder in the gate.  The first issue is
causing all unit tests to fail with the message:

""NoSuchOptError: no such option: fake_rabbit""

This was due to commit bcb3b23b8f6e7d01e38fdc031982558711bb7586
removing that option from oslo/messaging/_drivers/impl_rabbit.py .
We were trying to set that option in cinder/tests/conf_fixture.py .
I have removed that code.

Second, the update has made etc/cinder.conf.sample go out of date
again.  I am merging the changes to bring it back up to date.

This change also disables execution of check_uptodate.sh execution
from the gate.

Change-Id: Ie4e25a8cd36782007a8934cc4573632f5215c2b1
",git fetch https://review.opendev.org/openstack/cinder refs/changes/16/138816/2 && git format-patch -1 --stdout FETCH_HEAD,"['etc/cinder/cinder.conf.sample', 'cinder/tests/conf_fixture.py']",2,f54e87a49e73b07d4fe8506df6827b0469ce1910,,," conf.set_default('fake_rabbit', True)",53,9
openstack%2Fpython-openstackclient~master~I6ba3249b67408571624709e17f8aa2ac6d80237d,openstack/python-openstackclient,master,I6ba3249b67408571624709e17f8aa2ac6d80237d,Followup for ec2 credentials command fix,MERGED,2014-12-05 23:48:54.000000000,2014-12-06 16:50:20.000000000,2014-12-06 16:50:19.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 6482}]","[{'number': 1, 'created': '2014-12-05 23:48:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/aa4dfd028da81f561b3bd8a3b213203d6fb736ba', 'message': ""Followup for ec2 credentials command fix\n\nAdd functional tests for 'ec2 credentials' commands.\n\nAlso fix tenant_id in output for create and show.\n\nChange-Id: I6ba3249b67408571624709e17f8aa2ac6d80237d\n""}, {'number': 2, 'created': '2014-12-06 04:42:38.000000000', 'files': ['functional/tests/test_identity.py', 'openstackclient/identity/v2_0/ec2creds.py'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/1a25cbaf8f2c1643181ef6233f72a57aaac5404d', 'message': ""Followup for ec2 credentials command fix\n\nAdd functional tests for 'ec2 credentials' commands.\n\nAlso fix tenant_id in output for create and show.\n\nChange-Id: I6ba3249b67408571624709e17f8aa2ac6d80237d\n""}]",1,139757,1a25cbaf8f2c1643181ef6233f72a57aaac5404d,10,3,2,970,,,0,"Followup for ec2 credentials command fix

Add functional tests for 'ec2 credentials' commands.

Also fix tenant_id in output for create and show.

Change-Id: I6ba3249b67408571624709e17f8aa2ac6d80237d
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/57/139757/2 && git format-patch -1 --stdout FETCH_HEAD,"['functional/tests/test_identity.py', 'openstackclient/identity/v2_0/ec2creds.py']",2,aa4dfd028da81f561b3bd8a3b213203d6fb736ba,fix-ec2-creds, if 'tenant_id' in info: info.update( {'project_id': info.pop('tenant_id')} ) if 'tenant_id' in info: info.update( {'project_id': info.pop('tenant_id')} ) ,,41,0
openstack%2Fproject-config~master~I0fa99587f3d6e45c2546faded3cd55d5b2b83690,openstack/project-config,master,I0fa99587f3d6e45c2546faded3cd55d5b2b83690,Rename gerrit-powered-agenda to yaml2ical,MERGED,2014-12-02 16:31:43.000000000,2014-12-06 16:39:19.000000000,2014-12-06 16:39:18.000000000,"[{'_account_id': 3}, {'_account_id': 4146}, {'_account_id': 5263}, {'_account_id': 6133}, {'_account_id': 6316}, {'_account_id': 6547}, {'_account_id': 6786}]","[{'number': 1, 'created': '2014-12-02 16:31:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/dc2c630815af9f0c2154237753667155a2375a10', 'message': 'Rename gerrit-powered-agenda to yaml2ical\n\nLast part of the split between code and meeting descriptions in what was\npreviously known as gerrit-powered-agenda. Rename the code repository to\nmatch the module name, before we can publish it to PyPI and make use of\nit in openstack-infra/irc-meetings gate checks.\n\nChange-Id: I0fa99587f3d6e45c2546faded3cd55d5b2b83690\n'}, {'number': 2, 'created': '2014-12-02 16:43:21.000000000', 'files': ['jenkins/jobs/projects.yaml', 'gerrit/projects.yaml', 'zuul/layout.yaml', 'gerrit/acls/openstack-infra/gerrit-powered-agenda.config', 'gerrit/acls/openstack-infra/yaml2ical.config'], 'web_link': 'https://opendev.org/openstack/project-config/commit/44c89472e9aab1f30b0edfb339cfaef6d702c4f0', 'message': 'Rename gerrit-powered-agenda to yaml2ical\n\nLast part of the split between code and meeting descriptions in what was\npreviously known as gerrit-powered-agenda. Rename the code repository to\nmatch new module name (following https://review.openstack.org/136427),\nbefore we can publish it to PyPI in a subsequent change and and finally\nmake use of it in openstack-infra/irc-meetings gate checks.\n\nChange-Id: I0fa99587f3d6e45c2546faded3cd55d5b2b83690\n'}]",1,138432,44c89472e9aab1f30b0edfb339cfaef6d702c4f0,20,7,2,308,,,0,"Rename gerrit-powered-agenda to yaml2ical

Last part of the split between code and meeting descriptions in what was
previously known as gerrit-powered-agenda. Rename the code repository to
match new module name (following https://review.openstack.org/136427),
before we can publish it to PyPI in a subsequent change and and finally
make use of it in openstack-infra/irc-meetings gate checks.

Change-Id: I0fa99587f3d6e45c2546faded3cd55d5b2b83690
",git fetch https://review.opendev.org/openstack/project-config refs/changes/32/138432/2 && git format-patch -1 --stdout FETCH_HEAD,"['jenkins/jobs/projects.yaml', 'gerrit/projects.yaml', 'zuul/layout.yaml', 'gerrit/acls/openstack-infra/gerrit-powered-agenda.config', 'gerrit/acls/openstack-infra/yaml2ical.config']",5,dc2c630815af9f0c2154237753667155a2375a10,gerrit-powered-agenda-rename,"[access ""refs/heads/*""] abandon = group yaml2ical-core label-Code-Review = -2..+2 group yaml2ical-core label-Workflow = -1..+1 group yaml2ical-core [access ""refs/tags/*""] pushSignedTag = group yaml2ical-release [receive] requireChangeId = true requireContributorAgreement = true [submit] mergeContent = true ",,36,37
openstack%2Fproject-config~master~Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3,openstack/project-config,master,Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3,Move tooz from Stackforge to OpenStack,MERGED,2014-11-18 10:32:00.000000000,2014-12-06 16:39:12.000000000,2014-12-06 16:39:11.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 1561}, {'_account_id': 1669}, {'_account_id': 1994}, {'_account_id': 2271}, {'_account_id': 2472}, {'_account_id': 2813}, {'_account_id': 4146}, {'_account_id': 5196}, {'_account_id': 5263}, {'_account_id': 5638}, {'_account_id': 6133}, {'_account_id': 6159}, {'_account_id': 6316}, {'_account_id': 6547}, {'_account_id': 6786}, {'_account_id': 6928}, {'_account_id': 7450}, {'_account_id': 8122}, {'_account_id': 9107}]","[{'number': 1, 'created': '2014-11-18 10:32:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/9c5fc80b47fe1216772bd4d183c1db175a2bceba', 'message': 'Move tooz from Stackforge to OpenStack\n\nThe Oslo program is adopting tooz according to\n  https://review.openstack.org/#/c/122439/\n\nChange-Id: Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3\n'}, {'number': 2, 'created': '2014-11-18 12:33:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/bed7b87192bd0f068df95d81bd0fcb0ada45a622', 'message': ""Move tooz from Stackforge to OpenStack\n\nThe Oslo program is adopting tooz according to\n  https://review.openstack.org/#/c/122439/\n  https://review.openstack.org/#/c/135265/\n\nThe project is gated against Python 2.6 too because it's already being\nusde by at least the Juno release of Ceilometer.\n\nChange-Id: Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3\n""}, {'number': 3, 'created': '2014-11-18 13:39:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/c9ad3a3017e279f8142b3e3160adcfb562cd5757', 'message': ""Move tooz from Stackforge to OpenStack\n\nThe Oslo program is adopting tooz according to\n  https://review.openstack.org/#/c/122439/\n  https://review.openstack.org/#/c/135265/\n\nThe project is gated against Python 2.6 too because it's already being\nusde by at least the Juno release of Ceilometer.\n\nChange-Id: Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3\n""}, {'number': 4, 'created': '2014-11-19 16:46:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/804b089b50a535cce7c4ab252d6661603afa5e12', 'message': ""Move tooz from Stackforge to OpenStack\n\nThe Oslo program is adopting tooz according to\n  https://review.openstack.org/#/c/122439/\n  https://review.openstack.org/#/c/135265/\n\nThe project is gated against Python 2.6 too because it's already being\nusde by at least the Juno release of Ceilometer.\n\nThis also fixes the link between Gerrit and Launchpad.\n\nChange-Id: Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3\n""}, {'number': 5, 'created': '2014-11-20 09:29:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/9243a04f702dc128ffef5d4810a6dbb6d3ce10ca', 'message': ""Move tooz from Stackforge to OpenStack\n\nThe Oslo program is adopting tooz according to\n  https://review.openstack.org/#/c/122439/\n  https://review.openstack.org/#/c/135265/\n\nThe project is gated against Python 2.6 too because it's already being\nusde by at least the Juno release of Ceilometer.\n\nThis also fixes the link between Gerrit and Launchpad.\n\nChange-Id: Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3\n""}, {'number': 6, 'created': '2014-11-26 09:41:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/78c7ccc0c4530a332615c255cbc62b49688df259', 'message': ""Move tooz from Stackforge to OpenStack\n\nThe Oslo program is adopting tooz according to\n  https://review.openstack.org/#/c/122439/\n  https://review.openstack.org/#/c/135265/\n\nThe project is gated against Python 2.6 too because it's already being\nusde by at least the Juno release of Ceilometer.\n\nThis also fixes the link between Gerrit and Launchpad.\n\nChange-Id: Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3\n""}, {'number': 7, 'created': '2014-11-26 15:27:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/01843b56ca94b9c0d0d8141ee1d3f1005349ff19', 'message': ""Move tooz from Stackforge to OpenStack\n\nThe Oslo program is adopting tooz according to\n  https://review.openstack.org/#/c/122439/\n  https://review.openstack.org/#/c/135265/\n\nThe project is gated against Python 2.6 too because it's already being\nusde by at least the Juno release of Ceilometer.\n\nThis also fixes the link between Gerrit and Launchpad.\n\nBlueprint: tooz-adoption\n\nChange-Id: Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3\n""}, {'number': 8, 'created': '2014-11-26 15:36:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/4ce4eb026f404e6b6a6158c48370c6c5889ef884', 'message': ""Move tooz from Stackforge to OpenStack\n\nThe Oslo program is adopting tooz according to\n  https://review.openstack.org/#/c/122439/\n  https://review.openstack.org/#/c/135265/\n\nThe project is gated against Python 2.6 too because it's already being\nusde by at least the Juno release of Ceilometer.\n\nThis also fixes the link between Gerrit and Launchpad.\n\nBlueprint: tooz-adoption\n\nChange-Id: Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3\n""}, {'number': 9, 'created': '2014-12-04 07:43:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/d7f1e82bdfea9bb8a4e38b3dbcc474637196ab8d', 'message': ""Move tooz from Stackforge to OpenStack\n\nThe Oslo program is adopting tooz according to\n  https://review.openstack.org/#/c/122439/\n  https://review.openstack.org/#/c/135265/\n\nThe project is gated against Python 2.6 too because it's already being\nusde by at least the Juno release of Ceilometer.\n\nThis also fixes the link between Gerrit and Launchpad.\n\nBlueprint: tooz-adoption\n\nChange-Id: Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3\n""}, {'number': 10, 'created': '2014-12-04 08:13:38.000000000', 'files': ['gerritbot/channels.yaml', 'jenkins/jobs/projects.yaml', 'gerrit/acls/openstack/tooz.config', 'gerrit/projects.yaml', 'zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/4831bfc81a9f16c55db62d70a3aefb00c4aedc14', 'message': ""Move tooz from Stackforge to OpenStack\n\nThe Oslo program is adopting tooz according to\n  https://review.openstack.org/#/c/122439/\n  https://review.openstack.org/#/c/135265/\n\nThe project is gated against Python 2.6 too because it's already being\nusde by at least the Juno release of Ceilometer.\n\nThis also fixes the link between Gerrit and Launchpad.\n\nBlueprint: tooz-adoption\n\nChange-Id: Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3\n""}]",10,135215,4831bfc81a9f16c55db62d70a3aefb00c4aedc14,51,21,10,1669,,,0,"Move tooz from Stackforge to OpenStack

The Oslo program is adopting tooz according to
  https://review.openstack.org/#/c/122439/
  https://review.openstack.org/#/c/135265/

The project is gated against Python 2.6 too because it's already being
usde by at least the Juno release of Ceilometer.

This also fixes the link between Gerrit and Launchpad.

Blueprint: tooz-adoption

Change-Id: Iab7eff71e06cfc7ffe3fa8d99385c34a0a18f3d3
",git fetch https://review.opendev.org/openstack/project-config refs/changes/15/135215/10 && git format-patch -1 --stdout FETCH_HEAD,"['gerritbot/channels.yaml', 'jenkins/jobs/projects.yaml', 'gerrit/acls/openstack/tooz.config', 'gerrit/projects.yaml', 'zuul/layout.yaml']",5,9c5fc80b47fe1216772bd4d183c1db175a2bceba,taskflow-move, - name: openstack/tooz template: - name: merge-check - name: python-jobs - name: python3-jobs - name: openstack-server-publish-jobs - name: publish-to-pypi - name: lib-forward-testing check: - gate-tooz-tox-py27-zookeeper - gate-tooz-tox-py34-zookeeper - gate-tooz-tox-py27-redis - gate-tooz-tox-py34-redis - gate-tooz-tox-py27-memcached - gate-tooz-tox-py34-memcached - gate-tooz-tox-py27-postgresql - gate-tooz-tox-py34-postgresql - gate-tooz-tox-py27-mysql - gate-tooz-tox-py34-mysql - gate-tooz-python26 gate: - gate-tooz-tox-py27-zookeeper - gate-tooz-tox-py34-zookeeper - gate-tooz-tox-py27-redis - gate-tooz-tox-py34-redis - gate-tooz-tox-py27-memcached - gate-tooz-tox-py34-memcached - gate-tooz-tox-py27-postgresql - gate-tooz-tox-py34-postgresql - gate-tooz-tox-py27-mysql - gate-tooz-tox-py34-mysql - gate-tooz-python26 post: - tooz-branch-tarball - tooz-coverage , - name: stackforge/tooz template: - name: merge-check - name: python-jobs - name: python3-jobs - name: publish-to-pypi - name: docs-on-rtfd check: - gate-tooz-tox-py27-zookeeper - gate-tooz-tox-py34-zookeeper - gate-tooz-tox-py27-redis - gate-tooz-tox-py34-redis - gate-tooz-tox-py27-memcached - gate-tooz-tox-py34-memcached - gate-tooz-tox-py27-postgresql - gate-tooz-tox-py34-postgresql - gate-tooz-tox-py27-mysql - gate-tooz-tox-py34-mysql - gate-tooz-python26 gate: - gate-tooz-tox-py27-zookeeper - gate-tooz-tox-py34-zookeeper - gate-tooz-tox-py27-redis - gate-tooz-tox-py34-redis - gate-tooz-tox-py27-memcached - gate-tooz-tox-py34-memcached - gate-tooz-tox-py27-postgresql - gate-tooz-tox-py34-postgresql - gate-tooz-tox-py27-mysql - gate-tooz-tox-py34-mysql - gate-tooz-python26 post: - tooz-branch-tarball - tooz-coverage ,45,40
openstack-attic%2Fidentity-api~master~I2907a0c4b2c9b4c3dceffb0a0cd9e2405f866c65,openstack-attic/identity-api,master,I2907a0c4b2c9b4c3dceffb0a0cd9e2405f866c65,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:41:42.000000000,2014-12-06 16:38:17.000000000,2014-12-06 16:38:16.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 6772}]","[{'number': 1, 'created': '2014-12-05 03:41:42.000000000', 'files': ['tools/rfc.sh'], 'web_link': 'https://opendev.org/openstack-attic/identity-api/commit/322034bb23c7d8d1160077de8018f40e918d3147', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I2907a0c4b2c9b4c3dceffb0a0cd9e2405f866c65\n'}]",0,139328,322034bb23c7d8d1160077de8018f40e918d3147,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I2907a0c4b2c9b4c3dceffb0a0cd9e2405f866c65
",git fetch https://review.opendev.org/openstack-attic/identity-api refs/changes/28/139328/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/rfc.sh'],1,322034bb23c7d8d1160077de8018f40e918d3147,infra-manual," echo ""\thttp://docs.openstack.org/infra/manual/developers.html#development-workflow"""," echo ""\thttp://wiki.openstack.org/GerritWorkflow""",1,1
openstack-attic%2Fcompute-api~master~Ide5b55aa1e76ae088142401f06def207e8c82215,openstack-attic/compute-api,master,Ide5b55aa1e76ae088142401f06def207e8c82215,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:40:59.000000000,2014-12-06 16:34:03.000000000,2014-12-06 16:34:03.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 6772}]","[{'number': 1, 'created': '2014-12-05 03:40:59.000000000', 'files': ['tools/rfc.sh'], 'web_link': 'https://opendev.org/openstack-attic/compute-api/commit/91604e03d01d792d29fd7ac111236ddf1a2ff1b1', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Ide5b55aa1e76ae088142401f06def207e8c82215\n'}]",0,139317,91604e03d01d792d29fd7ac111236ddf1a2ff1b1,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Ide5b55aa1e76ae088142401f06def207e8c82215
",git fetch https://review.opendev.org/openstack-attic/compute-api refs/changes/17/139317/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/rfc.sh'],1,91604e03d01d792d29fd7ac111236ddf1a2ff1b1,infra-manual," echo ""\thttp://docs.openstack.org/infra/manual/developers.html#development-workflow"""," echo ""\thttp://wiki.openstack.org/GerritWorkflow""",1,1
openstack-attic%2Fobject-api~master~I5806071063b0577b38a7b9b7cf1c0a6031327ee0,openstack-attic/object-api,master,I5806071063b0577b38a7b9b7cf1c0a6031327ee0,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:42:45.000000000,2014-12-06 16:33:13.000000000,2014-12-06 16:33:13.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 6772}]","[{'number': 1, 'created': '2014-12-05 03:42:45.000000000', 'files': ['tools/rfc.sh'], 'web_link': 'https://opendev.org/openstack-attic/object-api/commit/88b56408556f32c3bf0f3fe0f00222f1ec11d559', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I5806071063b0577b38a7b9b7cf1c0a6031327ee0\n'}]",0,139342,88b56408556f32c3bf0f3fe0f00222f1ec11d559,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I5806071063b0577b38a7b9b7cf1c0a6031327ee0
",git fetch https://review.opendev.org/openstack-attic/object-api refs/changes/42/139342/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/rfc.sh'],1,88b56408556f32c3bf0f3fe0f00222f1ec11d559,infra-manual," echo ""\thttp://docs.openstack.org/infra/manual/developers.html#development-workflow"""," echo ""\thttp://wiki.openstack.org/GerritWorkflow""",1,1
openstack%2Fnova~master~I0a11e89d3a0bbdfccac569c09009e576ea2e1173,openstack/nova,master,I0a11e89d3a0bbdfccac569c09009e576ea2e1173,Add VIF_VHOSTUSER,ABANDONED,2014-05-28 11:24:40.000000000,2014-12-06 16:29:49.000000000,,"[{'_account_id': 3}, {'_account_id': 782}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 2711}, {'_account_id': 2750}, {'_account_id': 3217}, {'_account_id': 5170}, {'_account_id': 6524}, {'_account_id': 7823}, {'_account_id': 7916}, {'_account_id': 8412}, {'_account_id': 8531}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10257}, {'_account_id': 10385}, {'_account_id': 11547}, {'_account_id': 11647}, {'_account_id': 12274}]","[{'number': 1, 'created': '2014-05-28 11:24:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/40117949b266b2a549881fc00d33cc0a43c015a6', 'message': 'Add VIF_SNABB based on Libvirt with vhost-user + huge pages\n\nChange-Id: I0a11e89d3a0bbdfccac569c09009e576ea2e1173\n'}, {'number': 2, 'created': '2014-06-17 08:57:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/57e0d6e57bf53e1de3f1622faa6987e07f2e8465', 'message': 'Add VIF_VHOSTUSER\n\nThis is based on a new QEMU feature called vhost-user for connecting\nto a user-space vswitch via a unix socket.\n\nChange-Id: I0a11e89d3a0bbdfccac569c09009e576ea2e1173\n'}, {'number': 3, 'created': '2014-08-01 16:42:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/90aaf849d6d2cd763806f59ac1f88139b5cb0de7', 'message': 'Add VIF_VHOSTUSER\n\nThis is based on a new QEMU feature called vhost-user for connecting\nto a user-space vswitch via a unix socket.\n\nChange-Id: I0a11e89d3a0bbdfccac569c09009e576ea2e1173\nSigned-off-by: Michele Paolino <m.paolino@virtualopensystems.com>\n'}, {'number': 4, 'created': '2014-08-13 08:25:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8f45b06274952417b3c7290f9cd942adaa5e2c26', 'message': 'Add VIF_VHOSTUSER\n\nThis is based on a new QEMU feature called vhost-user for connecting\nto a user-space vswitch via a unix socket.\n\nChange-Id: I0a11e89d3a0bbdfccac569c09009e576ea2e1173\nSigned-off-by: Michele Paolino <m.paolino@virtualopensystems.com>\n'}, {'number': 5, 'created': '2014-08-13 09:12:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/db1fa24e7e66a48c352567bf0624656583883355', 'message': 'Add VIF_VHOSTUSER\n\nThis is based on a new QEMU feature called vhost-user for connecting\nto a user-space vswitch via a unix socket.\n\nChange-Id: I0a11e89d3a0bbdfccac569c09009e576ea2e1173\nSigned-off-by: Michele Paolino <m.paolino@virtualopensystems.com>\n'}, {'number': 6, 'created': '2014-08-18 09:00:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/73de05f879f3cb343a85bd08b9f893c53e3255cb', 'message': 'Add VIF_VHOSTUSER\n\nThis is based on a new QEMU feature called vhost-user for connecting\nto a user-space vswitch via a unix socket.\n\nChange-Id: I0a11e89d3a0bbdfccac569c09009e576ea2e1173\nSigned-off-by: Michele Paolino <m.paolino@virtualopensystems.com>\n'}, {'number': 7, 'created': '2014-08-22 14:44:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/97263072185bdf99e8a796398035f362443ce653', 'message': 'Add VIF_VHOSTUSER\n\nThis is based on a new QEMU feature called vhost-user for connecting\nto a user-space vswitch via a unix socket.\n\nblueprint vif-vhostuser\n\nChange-Id: I0a11e89d3a0bbdfccac569c09009e576ea2e1173\nSigned-off-by: Michele Paolino <m.paolino@virtualopensystems.com>'}, {'number': 8, 'created': '2014-08-27 09:03:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e1a32b0b7551facde7188b24830358bd98c383e1', 'message': 'Add VIF_VHOSTUSER\n\nThis is based on a new QEMU feature called vhost-user for connecting\nto a user-space vswitch via a unix socket.\n\nChange-Id: I0a11e89d3a0bbdfccac569c09009e576ea2e1173\nSigned-off-by: Michele Paolino <m.paolino@virtualopensystems.com>\n'}, {'number': 9, 'created': '2014-09-15 12:57:54.000000000', 'files': ['nova/virt/libvirt/vif.py', 'nova/tests/virt/libvirt/test_vif.py', 'nova/virt/libvirt/config.py', 'nova/network/model.py', 'nova/virt/libvirt/designer.py', 'nova/tests/virt/libvirt/test_config.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/0c7f96f21de544975e7d2c98650baec958d8b32c', 'message': 'Add VIF_VHOSTUSER\n\nThis is based on a new QEMU feature called vhost-user for connecting\nto a user-space vswitch via a unix socket.\n\nChange-Id: I0a11e89d3a0bbdfccac569c09009e576ea2e1173\nSigned-off-by: Michele Paolino <m.paolino@virtualopensystems.com>\n'}]",17,96140,0c7f96f21de544975e7d2c98650baec958d8b32c,109,21,9,7916,,,0,"Add VIF_VHOSTUSER

This is based on a new QEMU feature called vhost-user for connecting
to a user-space vswitch via a unix socket.

Change-Id: I0a11e89d3a0bbdfccac569c09009e576ea2e1173
Signed-off-by: Michele Paolino <m.paolino@virtualopensystems.com>
",git fetch https://review.opendev.org/openstack/nova refs/changes/40/96140/8 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/libvirt/driver.py', 'nova/virt/libvirt/vif.py', 'nova/network/model.py', 'nova/virt/libvirt/config.py', 'nova/virt/libvirt/designer.py']",5,40117949b266b2a549881fc00d33cc0a43c015a6,bp/vif-vhostuser,"def set_vif_host_backend_vhostuser_config(conf, socket_path, socket_mode): """"""Populate a LibvirtConfigGuestInterface instance with vhostuser socket details"""""" conf.net_type = ""vhostuser"" conf.vhostsock = socket_path conf.vhostsock_mode = socket_mode ",,65,1
openstack%2Fcinder~master~Ied2196b548c91324fdfce39ec4f8801e59da4f6c,openstack/cinder,master,Ied2196b548c91324fdfce39ec4f8801e59da4f6c,Use object.property instead of object.dump()['property'],MERGED,2014-10-09 23:41:10.000000000,2014-12-06 15:01:29.000000000,2014-12-03 18:26:04.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 1736}, {'_account_id': 2759}, {'_account_id': 4523}, {'_account_id': 8247}, {'_account_id': 9003}, {'_account_id': 10068}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-10-09 23:41:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2f983a5cb54995d7a9865f38dfc9c37618115e20', 'message': ""Use object.property instead of object.dump()['property']\n\nLess verbose and requires less work for the library.\n\nChange-Id: Ied2196b548c91324fdfce39ec4f8801e59da4f6c\n""}, {'number': 2, 'created': '2014-12-03 05:00:03.000000000', 'files': ['bin/cinder-rtstool'], 'web_link': 'https://opendev.org/openstack/cinder/commit/13d524389ef727a2ce1d5a9e640062a89bdb6e54', 'message': ""Use object.property instead of object.dump()['property']\n\nLess verbose and requires less work for the library.\n\nChange-Id: Ied2196b548c91324fdfce39ec4f8801e59da4f6c\n""}]",0,127385,13d524389ef727a2ce1d5a9e640062a89bdb6e54,27,14,2,6064,,,0,"Use object.property instead of object.dump()['property']

Less verbose and requires less work for the library.

Change-Id: Ied2196b548c91324fdfce39ec4f8801e59da4f6c
",git fetch https://review.opendev.org/openstack/cinder refs/changes/85/127385/1 && git format-patch -1 --stdout FETCH_HEAD,['bin/cinder-rtstool'],1,2f983a5cb54995d7a9865f38dfc9c37618115e20,dev-rtstool, if x.name == name: if t.wwn == target_iqn: for acl in tpg.node_acls: print(x.wwn) if x.wwn == iqn: if x.name == iqn:, if x.dump()['name'] == name: if t.dump()['wwn'] == target_iqn: for acl in tpg.dump()['node_acls']: print(x.dump()['wwn']) if x.dump()['wwn'] == iqn: if x.dump()['name'] == iqn:,6,6
openstack%2Fkeystone~master~I6fa468d020e5032fcde37e5e06e1a49410a71f80,openstack/keystone,master,I6fa468d020e5032fcde37e5e06e1a49410a71f80,Updated from global requirements,MERGED,2014-12-06 00:02:14.000000000,2014-12-06 14:32:32.000000000,2014-12-06 14:32:32.000000000,"[{'_account_id': 3}, {'_account_id': 6486}]","[{'number': 1, 'created': '2014-12-06 00:02:14.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/keystone/commit/2355f3a85ea87159d8d4ec943239e21c72b61ab0', 'message': 'Updated from global requirements\n\nChange-Id: I6fa468d020e5032fcde37e5e06e1a49410a71f80\n'}]",0,139763,2355f3a85ea87159d8d4ec943239e21c72b61ab0,6,2,1,11131,,,0,"Updated from global requirements

Change-Id: I6fa468d020e5032fcde37e5e06e1a49410a71f80
",git fetch https://review.opendev.org/openstack/keystone refs/changes/63/139763/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,2355f3a85ea87159d8d4ec943239e21c72b61ab0,openstack/requirements,oslo.concurrency>=0.3.0 # Apache-2.0,oslo.concurrency>=0.1.0 # Apache-2.0,1,1
openstack%2Fcinder~master~I4db42d2521d7e6018f4f7ad0c4ab13441871675e,openstack/cinder,master,I4db42d2521d7e6018f4f7ad0c4ab13441871675e,NetApp 7mode NFS driver doesn't honor netapp_vfiler option,MERGED,2014-12-02 20:04:56.000000000,2014-12-06 13:44:19.000000000,2014-12-03 03:29:52.000000000,"[{'_account_id': 3}, {'_account_id': 5997}, {'_account_id': 7198}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9366}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11079}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-02 20:04:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/685fc0f0a07ec3be7051293d7a879f7672bd85d9', 'message': ""NetApp 7mode NFS driver doesn't honor netapp_vfiler option\n\nThis patch fixes the NetApp 7mode NFS driver to register and use\nthe netapp_vfiler option if it is configured in cinder.conf.\n\nDocImpact\nChange-Id: I4db42d2521d7e6018f4f7ad0c4ab13441871675e\nCloses-Bug: 1381716\n""}, {'number': 2, 'created': '2014-12-02 20:58:49.000000000', 'files': ['cinder/volume/drivers/netapp/dataontap/nfs_7mode.py', 'cinder/volume/drivers/netapp/dataontap/nfs_cmode.py', 'etc/cinder/cinder.conf.sample', 'cinder/tests/test_netapp_nfs.py', 'cinder/tests/volume/drivers/netapp/dataontap/client/test_client_7mode.py', 'cinder/volume/drivers/netapp/dataontap/nfs_base.py', 'cinder/volume/drivers/netapp/dataontap/client/client_7mode.py', 'cinder/volume/drivers/netapp/options.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/504cb9f3d9954138fcb62d2fbb20b8505ca0f0ac', 'message': ""NetApp 7mode NFS driver doesn't honor netapp_vfiler option\n\nThis patch fixes the NetApp 7mode NFS driver to register and use\nthe netapp_vfiler option if it is configured in cinder.conf.\n\nDocImpact\nChange-Id: I4db42d2521d7e6018f4f7ad0c4ab13441871675e\nCloses-Bug: 1381716\n""}]",0,138513,504cb9f3d9954138fcb62d2fbb20b8505ca0f0ac,28,14,2,9366,,,0,"NetApp 7mode NFS driver doesn't honor netapp_vfiler option

This patch fixes the NetApp 7mode NFS driver to register and use
the netapp_vfiler option if it is configured in cinder.conf.

DocImpact
Change-Id: I4db42d2521d7e6018f4f7ad0c4ab13441871675e
Closes-Bug: 1381716
",git fetch https://review.opendev.org/openstack/cinder refs/changes/13/138513/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/netapp/dataontap/nfs_7mode.py', 'cinder/volume/drivers/netapp/dataontap/nfs_cmode.py', 'etc/cinder/cinder.conf.sample', 'cinder/tests/test_netapp_nfs.py', 'cinder/tests/volume/drivers/netapp/dataontap/client/test_client_7mode.py', 'cinder/volume/drivers/netapp/dataontap/nfs_base.py', 'cinder/volume/drivers/netapp/dataontap/client/client_7mode.py', 'cinder/volume/drivers/netapp/options.py']",8,685fc0f0a07ec3be7051293d7a879f7672bd85d9,bug/1381716, 'family of Data ONTAP operating in 7-Mode. Only use this ', 'family of Data ONTAP operating in 7-Mode and the ' 'storage protocol selected is iSCSI. Only use this ',38,16
openstack%2Fcinder~master~I7814c3da9c0e6fcf3143969e74304a48cafcb3d1,openstack/cinder,master,I7814c3da9c0e6fcf3143969e74304a48cafcb3d1,"Revert ""Fix Brocade FC SAN lookup MITM vulnerability""",MERGED,2014-12-02 20:39:11.000000000,2014-12-06 13:25:09.000000000,2014-12-04 19:25:19.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 5997}, {'_account_id': 6491}, {'_account_id': 7198}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-02 20:39:11.000000000', 'files': ['cinder/tests/zonemanager/test_brcd_fc_san_lookup_service.py', 'cinder/zonemanager/drivers/brocade/brcd_fc_san_lookup_service.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/57103807c5e7fad7276f97ac82f8704f17f4b846', 'message': 'Revert ""Fix Brocade FC SAN lookup MITM vulnerability""\n\nThis reverts commit ab4f57212683baec45d5b682bdd3952ff58249ed.\n\nThe change is being reverted as it broke the Brocade FC SAN lookup\nfunctionality.  The change uses configuration options from\nssh_utils that are not initialized when the Brocade driver is\nrun causing an exception to be thrown complaining that\nCONF.ssh_hosts_key_file is used before it is initialized.\n\nThe right solution is to change the Brocade driver to use ssh_utils to\nmake SSH connections.\n\nConflicts:\n\n\tcinder/zonemanager/drivers/brocade/brcd_fc_san_lookup_service.py\n\nChange-Id: I7814c3da9c0e6fcf3143969e74304a48cafcb3d1\nCloses-bug: 1398488\n'}]",0,138526,57103807c5e7fad7276f97ac82f8704f17f4b846,34,14,1,7198,,,0,"Revert ""Fix Brocade FC SAN lookup MITM vulnerability""

This reverts commit ab4f57212683baec45d5b682bdd3952ff58249ed.

The change is being reverted as it broke the Brocade FC SAN lookup
functionality.  The change uses configuration options from
ssh_utils that are not initialized when the Brocade driver is
run causing an exception to be thrown complaining that
CONF.ssh_hosts_key_file is used before it is initialized.

The right solution is to change the Brocade driver to use ssh_utils to
make SSH connections.

Conflicts:

	cinder/zonemanager/drivers/brocade/brcd_fc_san_lookup_service.py

Change-Id: I7814c3da9c0e6fcf3143969e74304a48cafcb3d1
Closes-bug: 1398488
",git fetch https://review.opendev.org/openstack/cinder refs/changes/26/138526/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/zonemanager/test_brcd_fc_san_lookup_service.py', 'cinder/zonemanager/drivers/brocade/brcd_fc_san_lookup_service.py']",2,57103807c5e7fad7276f97ac82f8704f17f4b846,bug/1398488," self.client = self.create_ssh_client(**kwargs) def create_ssh_client(self, **kwargs): known_hosts_file = kwargs.get('known_hosts_file', None) if known_hosts_file is None: ssh_client.load_system_host_keys() else: ssh_client.load_host_keys(known_hosts_file) missing_key_policy = kwargs.get('missing_key_policy', None) if missing_key_policy is None: missing_key_policy = paramiko.WarningPolicy()",from oslo.config import cfgCONF = cfg.CONF self.client = self.create_ssh_client() def create_ssh_client(self): known_hosts_file = CONF.ssh_hosts_key_file if not known_hosts_file: raise exception.ParameterNotFound(param='ssh_hosts_key_file') ssh_client.load_host_keys(known_hosts_file) if CONF.strict_ssh_host_key_policy: missing_key_policy = paramiko.RejectPolicy() else: missing_key_policy = paramiko.AutoAddPolicy(),17,20
openstack%2Fswift-specs~master~I9adcd8979af8f1d0ebd912c503ae36a8041ed32d,openstack/swift-specs,master,I9adcd8979af8f1d0ebd912c503ae36a8041ed32d,Spec for at-rest encryption.,MERGED,2014-09-22 18:58:15.000000000,2014-12-06 12:51:06.000000000,2014-12-06 12:51:05.000000000,"[{'_account_id': 3}, {'_account_id': 330}, {'_account_id': 1179}, {'_account_id': 2622}, {'_account_id': 2696}, {'_account_id': 6783}, {'_account_id': 6802}, {'_account_id': 6968}, {'_account_id': 7012}, {'_account_id': 7063}, {'_account_id': 7233}, {'_account_id': 7764}, {'_account_id': 7847}, {'_account_id': 8623}, {'_account_id': 9205}, {'_account_id': 9528}, {'_account_id': 11330}, {'_account_id': 11716}, {'_account_id': 11861}, {'_account_id': 12430}, {'_account_id': 13556}]","[{'number': 1, 'created': '2014-09-22 18:58:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/f1010eeba3e24496ec27e6cc39f9adda3a9eef45', 'message': 'Proposed spec for on-disk encryption\n\nChange-Id: I9adcd8979af8f1d0ebd912c503ae36a8041ed32d\n'}, {'number': 2, 'created': '2014-09-23 01:57:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/6f697620f4c2545976f688ad478295463db27a0b', 'message': 'First draft of spec for on-disk encryption.\n\nThere\'s still one open question; once that\'s hashed out, we can remove\nthe ""draft"" label and review it for real.\n\nChange-Id: I9adcd8979af8f1d0ebd912c503ae36a8041ed32d\n'}, {'number': 3, 'created': '2014-09-23 19:31:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/f1b7e3b92bfd43725adc206d7e56fa85922733ef', 'message': 'First draft of spec for on-disk encryption.\n\nThere\'s still one open question; once that\'s hashed out, we can remove\nthe ""draft"" label and review it for real.\n\nChange-Id: I9adcd8979af8f1d0ebd912c503ae36a8041ed32d\n'}, {'number': 4, 'created': '2014-10-10 13:09:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/43af26fe405ff2d4bca5fecdd6d3485db342a789', 'message': 'Draft specification for storage encryption in Swift\n\nThis draft serves for agreeing on the security goals of encryption\nsupport in Swift, its design and important implementation choices.\n\nThis spec revises the ""spec for on-disk encryption"" here\n  https://review.openstack.org/#/c/123220/\nand extends the security model and the design. It also takes\ninto account some of the discussion there.\n\nFor relevant background, see also the earlier blueprint\n  https://blueprints.launchpad.net/swift/+spec/swift-enc-proxy\n\nThe change-id points to relevant prior post of design for on-disk\nencryption.\n\nSecurityImpact\n\nChange-Id: I9adcd8979af8f1d0ebd912c503ae36a8041ed32d\n'}, {'number': 5, 'created': '2014-10-18 00:16:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/a40a417375db6c127ab7ba87c97abe93ffb0c479', 'message': 'First draft of spec for at-rest encryption.\n\nSecurityImpact\n\nChange-Id: I9adcd8979af8f1d0ebd912c503ae36a8041ed32d\n'}, {'number': 6, 'created': '2014-10-18 16:52:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/052b591d53953600d40c5d9fd4e93dc2b0ab6b71', 'message': 'First draft of spec for at-rest encryption.\n\nSecurityImpact\n\nChange-Id: I9adcd8979af8f1d0ebd912c503ae36a8041ed32d\n'}, {'number': 7, 'created': '2014-11-11 23:27:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/ddc1587158b7756d70a8f80008dcf59bf361cacf', 'message': 'Spec for at-rest encryption.\n\nSecurityImpact\n\nChange-Id: I9adcd8979af8f1d0ebd912c503ae36a8041ed32d\n'}, {'number': 8, 'created': '2014-11-13 00:11:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/ffc19a16073df25f0bf5dec72158b361b574b6e6', 'message': 'Spec for at-rest encryption.\n\nSecurityImpact\n\nChange-Id: I9adcd8979af8f1d0ebd912c503ae36a8041ed32d\n'}, {'number': 9, 'created': '2014-11-14 19:31:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/2588c710dc48d184cb168fb7f810dc0db960733a', 'message': 'Spec for at-rest encryption.\n\nSecurityImpact\n\nChange-Id: I9adcd8979af8f1d0ebd912c503ae36a8041ed32d\n'}, {'number': 10, 'created': '2014-11-14 19:53:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/aab4ae0ee9d5bc02d88c3970568483bb28c03e55', 'message': 'Spec for at-rest encryption.\n\nSecurityImpact\n\nChange-Id: I9adcd8979af8f1d0ebd912c503ae36a8041ed32d\n'}, {'number': 11, 'created': '2014-11-19 16:50:38.000000000', 'files': ['specs/in_progress/at_rest_encryption.rst'], 'web_link': 'https://opendev.org/openstack/swift-specs/commit/97ecc00b898e0b578c352f747320d2f1863a5149', 'message': 'Spec for at-rest encryption.\n\nSecurityImpact\n\nChange-Id: I9adcd8979af8f1d0ebd912c503ae36a8041ed32d\n'}]",186,123220,97ecc00b898e0b578c352f747320d2f1863a5149,112,21,11,2622,,,0,"Spec for at-rest encryption.

SecurityImpact

Change-Id: I9adcd8979af8f1d0ebd912c503ae36a8041ed32d
",git fetch https://review.opendev.org/openstack/swift-specs refs/changes/20/123220/9 && git format-patch -1 --stdout FETCH_HEAD,['specs/swift/on_disk_encryption.rst'],1,f1010eeba3e24496ec27e6cc39f9adda3a9eef45,crypto,":: This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ****************** On-Disk Encryption ****************** 1. Summary ========== To better protect the data in their clusters, Swift operators may wish to have objects stored in an encrypted form. This spec describes a plan to add an operator-managed encryption capability to Swift. The Threat Model ---------------- Swift objects are typically stored on disk as files in a standard POSIX filesystem; in the typical 3-replica case, an object is represented as 3 files on 3 distinct filesystems on within the cluster. An attacker may gain access to disks in a number of ways. When a disk fails, it may be returned to the manufacturer under warranty; since it has failed, erasing the data may not be possible, but the data may still be present on the platters. When disks reach end-of-life, they are discarded, and if not properly wiped, may still contain data. An attacker may simply break into the data center and steal disks. This spec describes a way to minimize the information leaked when an attacker can read the data on the filesystems holding Swift objects. Not The Threat Model -------------------- There are other ways to attack a Swift cluster, but this spec does not address them. In particular, this spec does not address these threats: * an attacker gains access to Swift's internal network * an attacker gains read/write access to Swift's object disks * an attacker modifies Swift's code If these threats are mitigated at all, it is a fortunate byproduct, but it is not the intent of this spec to address them. 2. Encryption and Key Management ================================ There are two logical parts to on-disk encryption. The first part is the crypto engine; this performs the actual encryption and decryption of the data and metadata, and it also takes care of any necessary data integrity functions. The second part is the keymaster. This is the part responsible for retrieving the secret key for the requested object. The crypto engine and the keymaster shall be implemented as three separate pieces of middleware. The crypto engine shall have both ""decrypter"" and ""encrypter"" filter-factory functions, and the keymaster filter shall sit between them. Example:: [pipeline:main] pipeline = catch_errors gatekeeper ... decrypter keymaster encrypter proxy-logging proxy-server The encrypter middleware is responsible for encrypting the object's data and metadata on a PUT or POST request. The decrypter middleware is responsible for decrypting the object's data and metadata on a GET or HEAD response. DELETE and OPTIONS requests are unaffected by encryption, so neither the encrypter nor decrypter need to do anything. The decrypter, encrypter, and keymaster will all use the hook in the WSGI environment at ""swift.copy_hook"" to ensure that COPY requests result in the new object being encrypted with its own key, not with the source object's key. Inter-Middleware Communication ------------------------------ The keymaster communicates the encryption key to the encrypter and decrypter middlewares by placing it in the WSGI environment dictionary at the name ""swift.crypto.key"". On a PUT or POST request, the keymaster must place the key in the WSGI environment during request processing; that is, before passing the request to the remainder of the middleware pipeline. This is so that the encrypter can encrypt the object's data in a streaming fashion without buffering the whole object. On a GET or HEAD request, the keymaster must place the key in the WSGI environment before returning control to the decrypter. It need not be done at request-handling time. This lets attributes of the key be stored in sysmeta, for example the key ID in an external database. 3. Cryptographic Algorithm Choice ================================= 3.1. Configurability of Cryptographic Algorithm ----------------------------------------------- The crypto engine uses AES-256 in Galois Counter Mode (GCM) for both secrecy and integrity. That's AES-256 in Counter (CTR) mode, plus Galois authentication. The encryption mode is not operator-selectable. Choosing an encryption mode requires a good, up-to-date understanding of cryptography; choosing the wrong encryption mode has dire consequences. Requiring operators to pick encryption and authentication algorithms is an open invitation for them to get it wrong. Operators only have one choice here: use on-disk encryption or don't. 3.2. Justification of Cryptographic Algorithm --------------------------------------------- CTR mode basically turns a block cipher into a stream cipher, so dealing with range GET requests becomes much easier. No modification of the byte ranges is required. When decrypting, some padding will be required to align the requested data to AES's 16-byte block size, but that can all be done at the proxy level. PyCrypto supports GHASH (the hash function for Galois Counter Mode), though it may not have made it into a release yet. GCM is fast, especially on Intel chips with some fancy new instructions. See https://crypto.stanford.edu/RealWorldCrypto/slides/gueron.pdf for more details and benchmark results. Notably, it's significantly faster than AES-CTR-256 + HMAC-SHA1. While it may seem reasonable to use HMAC-MD5 given that the encrypter will already have to compute the MD5 hash of the uploaded object (to check the client-supplied etag, if given), the way HMAC is constructed does not allow the encrypter to reuse that work. \\u1f63f 4. Robustness ============= If things go wrong due to bugs, it's possible for garbage to be returned instead of the client's object. This section is a place to collect various things we'll need for the sake of robustness. 4.1 Wrong Key ------------- If the keymaster hands back a different key for decryption than was used for encryption, the client will receive garbage. To prevent this, the encrypter will add a sysmeta header to the encrypted object containing the MD5 hash of the key used to encrypt the object. The decrypter will compare the MD5 hash of the decryption key to the MD5 hash stored in the object sysmeta and raise an error if they do not match. 4.2 No Key ---------- If the keymaster fails to add a key to the WSGI environment, then the client will receive the ciphertext of the object instead of the plaintext, which looks to the client like garbage. However, we can tell if an object is encrypted or not by the presence of system metadata headers (e.g. the MD5 hash of the encryption key), so the decrypter can prevent this by raising an error if no key was provided for the decryption of an encrypted object. 5. Metadata Encryption ====================== 5.1 User Metadata ----------------- Not just the contents of an object are sensitive; metadata is sensitive too. User metadata will be encrypted with the same key and IV as the rest of the metadata. Since metadata values must be valid UTF-8 strings, the encrypted values will be Base64-encoded for storage. Also, the object's plaintext etag is obviously sensitive information and will be stored encrypted as well. Note that the actual metadata field ""Etag"" will contain the MD5 hash of the ciphertext so that the object server will not error out on an object PUT, and also so that the object auditor will not quarantine the object due to hash mismatch. This refers to the sysmeta field where the MD5 of the plaintext is ultimately stored. Careful selection of counter values for etag encryption will be required. Re-use of a counter for two different metadata items will make metadata trivially decryptable by any attacker who understands how XOR works. 5.2 System Metadata ------------------- See ""Open Questions"" section at the end of this document. 5.3 Container Listings ====================== The etag in the container listings will be the MD5 of the ciphertext, while the etag on a GET or HEAD request will be the MD5 of the plaintext. It is not possible to make the etag match up everywhere. The etag is stored in three locations: * object metadata, ""Etag"" field: MD5 of ciphertext * object metadata, some sysmeta field: MD5 of plaintext * container listing: ??? Since the MD5 of the plaintext is sensitive enough to be stored encrypted, it seems foolish to then store it in the clear in the container database. Note that this *will not* break SLO, since it relies on object HEAD requests for manifest validation, and then it checks etags against the headers on GET responses. Also, this *will not* break DLO, since DLOs don't check etags at all. 6. Open Questions ================= The questions here should be resolved prior to accepting this spec. 6.1 Sysmeta Encryption ====================== Should sysmeta be encrypted as well? Some sysmeta values can be sensitive, but if middleware is moved from pre-crypto to post-crypto or vice versa, all its existing sysmeta may become unusable garbage from its viewpoint. ",,238,0
openstack%2Fcinder~master~I2c2f0be6e7a9d0866f063d98e1f8213f62fb9f92,openstack/cinder,master,I2c2f0be6e7a9d0866f063d98e1f8213f62fb9f92,Fix rpc initialization of cinder-manager volume,MERGED,2014-12-02 09:09:16.000000000,2014-12-06 12:47:31.000000000,2014-12-02 16:54:37.000000000,"[{'_account_id': 3}, {'_account_id': 1207}, {'_account_id': 2813}, {'_account_id': 7198}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-02 09:09:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a46598d1a324fd2344e062d6590a550fe52a3b08', 'message': ""Fix rpc initialization of cinder-manager volume\n\nVolumeCommands.rpc_client is a property, so when the oslo.config argparser\nintrospect the class VolumeCommands at module loading time, it launch the method.\nBut the method depends on a initialized oslo.config.cfg.CONF object, but\nthis one is not yet initialized.\n\nSo don't use python property, to initialize the rpc_client correctly.\n\nChange-Id: I2c2f0be6e7a9d0866f063d98e1f8213f62fb9f92\nCloses-bug: #1398319\n""}, {'number': 2, 'created': '2014-12-02 09:36:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/44e980359b2d9357f3a4c950f4a366564be0249b', 'message': ""Fix rpc initialization of cinder-manager volume\n\nVolumeCommands.rpc_client is a property, so when the oslo.config argparser\nintrospect the class VolumeCommands at module loading time, it launch the\nmethod. But the method depends on a initialized oslo.config.cfg.CONF object,\nbut this one is not yet initialized.\n\nSo don't use python property, to initialize the rpc_client correctly.\n\nChange-Id: I2c2f0be6e7a9d0866f063d98e1f8213f62fb9f92\nCloses-bug: #1398319\n""}, {'number': 3, 'created': '2014-12-02 14:06:11.000000000', 'files': ['bin/cinder-manage'], 'web_link': 'https://opendev.org/openstack/cinder/commit/d496b7879517c1891a8ea260cb205c4a4694e247', 'message': ""Fix rpc initialization of cinder-manager volume\n\nVolumeCommands.rpc_client is a property, so when the oslo.config argparser\nintrospect the class VolumeCommands at module loading time, it launch the\nmethod. But the method depends on a initialized oslo.config.cfg.CONF object,\nbut this one is not yet initialized.\n\nSo don't use python property, to initialize the rpc_client correctly.\n\nNo test because of bug: #1398401\n\nChange-Id: I2c2f0be6e7a9d0866f063d98e1f8213f62fb9f92\nCloses-bug: #1398319\n""}]",0,138287,d496b7879517c1891a8ea260cb205c4a4694e247,26,10,3,2813,,,0,"Fix rpc initialization of cinder-manager volume

VolumeCommands.rpc_client is a property, so when the oslo.config argparser
introspect the class VolumeCommands at module loading time, it launch the
method. But the method depends on a initialized oslo.config.cfg.CONF object,
but this one is not yet initialized.

So don't use python property, to initialize the rpc_client correctly.

No test because of bug: #1398401

Change-Id: I2c2f0be6e7a9d0866f063d98e1f8213f62fb9f92
Closes-bug: #1398319
",git fetch https://review.opendev.org/openstack/cinder refs/changes/87/138287/3 && git format-patch -1 --stdout FETCH_HEAD,['bin/cinder-manage'],1,a46598d1a324fd2344e062d6590a550fe52a3b08,bug/1398401, def rpc_client(self): if self._client is None: if not rpc.initialized(): rpc.init(CONF) target = messaging.Target(topic=CONF.volume_topic) self._client = rpc.get_client(target) cctxt = self.rpc_client().prepare(server=host), @property def rpc_client(self): if not rpc.initialized(): rpc.init(CONF) target = messaging.Target(topic=CONF.volume_topic) self._client = rpc.get_client(target) cctxt = self.rpc_client.prepare(server=host),6,7
openstack%2Fcinder~master~I5e0b1e5382c65176a74441bd4fe40d066317de48,openstack/cinder,master,I5e0b1e5382c65176a74441bd4fe40d066317de48,Fix 3PAR drivers attempt to locate existing host,MERGED,2014-12-02 00:25:25.000000000,2014-12-06 11:50:24.000000000,2014-12-03 00:26:42.000000000,"[{'_account_id': 3}, {'_account_id': 2243}, {'_account_id': 2759}, {'_account_id': 4355}, {'_account_id': 5997}, {'_account_id': 6043}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9624}, {'_account_id': 10248}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 11903}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-02 00:25:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/770042831a13e4e4068311870dcde48aebe35e9e', 'message': 'Fix 3PAR drivers attempt to locate existing host\n\nThis patch fixes the current 3PAR drivers around attempting to locate 3PAR host\nthat might already have volumes attached. The FC driver is using the correct\nREST based queryHost but is not specifying the parameters wwns=wwns when\ncalling it. This was accidentally merged during the removal on the local file\nlocks.\n\nThe iSCSI driver was actually calling a SSH based findHost command instead of\nthe REST based queryHost based on iqns. The SSH based command was failing with\na session key error after a long idle time between attaches.\nCloses-Bug: 1398206\n\nChange-Id: I5e0b1e5382c65176a74441bd4fe40d066317de48\n'}, {'number': 2, 'created': '2014-12-02 00:29:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/f2b0a122e4043941b914b98e805403c78b1d5d9c', 'message': 'Fix 3PAR drivers attempt to locate existing host\n\nThis patch fixes the current 3PAR drivers around attempting to locate\n3PAR host that might already have volumes attached. The FC driver is\nusing the correct REST based queryHost but is not specifying the\nparameters wwns=wwns when calling it. This was accidentally merged\nduring the removal on the local file locks.\n\nThe iSCSI driver was actually calling a SSH based findHost command\ninstead of the REST based queryHost based on iqns. The SSH based\ncommand was failing with a session key error after a long idle time\nbetween attaches.\nCloses-Bug: 1398206\n\nChange-Id: I5e0b1e5382c65176a74441bd4fe40d066317de48\n'}, {'number': 3, 'created': '2014-12-02 01:42:02.000000000', 'files': ['cinder/volume/drivers/san/hp/hp_3par_fc.py', 'cinder/volume/drivers/san/hp/hp_3par_iscsi.py', 'cinder/tests/test_hp3par.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/1e83a22507238a2a2ff5ac4bb5497c984666105d', 'message': 'Fix 3PAR drivers attempt to locate existing host\n\nThis patch fixes the current 3PAR drivers around attempting to locate\n3PAR host that might already have volumes attached. The FC driver is\nusing the correct REST based queryHost but is not specifying the\nparameters wwns=wwns when calling it. This was accidentally merged\nduring the removal on the local file locks.\n\nThe iSCSI driver was actually calling a SSH based findHost command\ninstead of the REST based queryHost based on iqns. The SSH based\ncommand was failing with a session key error after a long idle time\nbetween attaches.\nCloses-Bug: 1398206\n\nChange-Id: I5e0b1e5382c65176a74441bd4fe40d066317de48\n'}]",3,138212,1e83a22507238a2a2ff5ac4bb5497c984666105d,36,17,3,6043,,,0,"Fix 3PAR drivers attempt to locate existing host

This patch fixes the current 3PAR drivers around attempting to locate
3PAR host that might already have volumes attached. The FC driver is
using the correct REST based queryHost but is not specifying the
parameters wwns=wwns when calling it. This was accidentally merged
during the removal on the local file locks.

The iSCSI driver was actually calling a SSH based findHost command
instead of the REST based queryHost based on iqns. The SSH based
command was failing with a session key error after a long idle time
between attaches.
Closes-Bug: 1398206

Change-Id: I5e0b1e5382c65176a74441bd4fe40d066317de48
",git fetch https://review.opendev.org/openstack/cinder refs/changes/12/138212/2 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/san/hp/hp_3par_fc.py', 'etc/cinder/cinder.conf.sample', 'cinder/volume/drivers/san/hp/hp_3par_iscsi.py', 'cinder/tests/test_hp3par.py']",4,770042831a13e4e4068311870dcde48aebe35e9e,bug/1398206," mock.call.queryHost(wwns=['123456789012345', '123456789054321']), mock.call.queryHost(wwns=['123456789012345', '123456789054321']), mock_client.queryHost.return_value = { 'members': [{ 'name': self.FAKE_HOST }] } mock.ANY, mock_client.queryHost.return_value = None mock.call.queryHost(iqns=['iqn.1993-08.org.debian:01:222']), mock_client.queryHost.return_value = None mock.call.queryHost(iqns=['iqn.1993-08.org.debian:01:222']), mock_client.queryHost.return_value = { 'members': [{ 'name': 'fakehost.foo' }] } mock.call.queryHost(iqns=['iqn.1993-08.org.debian:01:222']), mock_client.queryHost.return_value = { 'members': [{ 'name': 'fakehost.foo' }] } mock.call.queryHost(iqns=['iqn.1993-08.org.debian:01:222']),"," mock.call.queryHost(['123456789012345', '123456789054321']), mock.call.queryHost(['123456789012345', '123456789054321']), mock_client.findHost.return_value = self.FAKE_HOST mock.call.findHost(iqn='iqn.1993-08.org.debian:01:222'), mock_client.findHost.return_value = None mock.call.findHost(iqn='iqn.1993-08.org.debian:01:222'), mock_client.findHost.return_value = None mock.call.findHost(iqn='iqn.1993-08.org.debian:01:222'), mock_client.findHost.return_value = 'fakehost.foo' mock.call.findHost(iqn='iqn.1993-08.org.debian:01:222'), mock_client.findHost.return_value = 'fakehost.foo' mock.call.findHost(iqn='iqn.1993-08.org.debian:01:222'),",39,18
openstack%2Fcinder~master~I1e2a985c1b41f4837e316af0655ef7203273cd7a,openstack/cinder,master,I1e2a985c1b41f4837e316af0655ef7203273cd7a,Remove driver compatibility in volume manager,MERGED,2014-12-01 21:43:21.000000000,2014-12-06 11:31:33.000000000,2014-12-02 08:35:55.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 2243}, {'_account_id': 2759}, {'_account_id': 4355}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9171}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-01 21:43:21.000000000', 'files': ['cinder/tests/test_drivers_compatibility.py', 'cinder/volume/manager.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/12c0a186cb3be5a4f2c30a2496067925f96bfe0d', 'message': 'Remove driver compatibility in volume manager\n\nThe old driver locations were deprecated in Juno. This removes the code\nfor deprecating and tests.\n\nChange-Id: I1e2a985c1b41f4837e316af0655ef7203273cd7a\n'}]",0,138188,12c0a186cb3be5a4f2c30a2496067925f96bfe0d,19,13,1,170,,,0,"Remove driver compatibility in volume manager

The old driver locations were deprecated in Juno. This removes the code
for deprecating and tests.

Change-Id: I1e2a985c1b41f4837e316af0655ef7203273cd7a
",git fetch https://review.opendev.org/openstack/cinder refs/changes/88/138188/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/test_drivers_compatibility.py', 'cinder/volume/manager.py']",2,12c0a186cb3be5a4f2c30a2496067925f96bfe0d,,,"MAPPING = { 'cinder.volume.drivers.storwize_svc.StorwizeSVCDriver': 'cinder.volume.drivers.ibm.storwize_svc.StorwizeSVCDriver', 'cinder.volume.drivers.xiv_ds8k.XIVDS8KDriver': 'cinder.volume.drivers.ibm.xiv_ds8k.XIVDS8KDriver', 'cinder.volume.drivers.san.hp_lefthand.HpSanISCSIDriver': 'cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver', 'cinder.volume.drivers.gpfs.GPFSDriver': 'cinder.volume.drivers.ibm.gpfs.GPFSDriver', } if volume_driver in MAPPING: LOG.warn(_LW(""Driver path %s is deprecated, update your "" ""configuration to the new path.""), volume_driver) volume_driver = MAPPING[volume_driver]",0,92
openstack%2Fopenstack-doc-tools~master~Ib947ebfc78ed80447d0b3b8895d9e96919fb1b00,openstack/openstack-doc-tools,master,Ib947ebfc78ed80447d0b3b8895d9e96919fb1b00,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:42:48.000000000,2014-12-06 11:28:16.000000000,2014-12-06 11:28:16.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-12-05 03:42:48.000000000', 'files': ['README.rst'], 'web_link': 'https://opendev.org/openstack/openstack-doc-tools/commit/49d4c6d2ed264f415227b847c507aa221b2469f9', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Ib947ebfc78ed80447d0b3b8895d9e96919fb1b00\n'}]",0,139343,49d4c6d2ed264f415227b847c507aa221b2469f9,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Ib947ebfc78ed80447d0b3b8895d9e96919fb1b00
",git fetch https://review.opendev.org/openstack/openstack-doc-tools refs/changes/43/139343/1 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,49d4c6d2ed264f415227b847c507aa221b2469f9,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Fcinder~master~I8faf1d8097bf8412d4e169ec3503821351795561,openstack/cinder,master,I8faf1d8097bf8412d4e169ec3503821351795561,Volume type access extension,MERGED,2014-08-14 23:42:46.000000000,2014-12-06 11:12:33.000000000,2014-12-03 05:14:31.000000000,"[{'_account_id': 3}, {'_account_id': 136}, {'_account_id': 1207}, {'_account_id': 1736}, {'_account_id': 2243}, {'_account_id': 2759}, {'_account_id': 5538}, {'_account_id': 5997}, {'_account_id': 6491}, {'_account_id': 7156}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9533}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 11880}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12033}, {'_account_id': 12202}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-08-14 23:42:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1031111ad5d9fa3ba09f8a7e8820b16ae1488432', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 2, 'created': '2014-08-15 02:27:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1f17e5889bcf68cd51a5b2bc55e908af224894b7', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 3, 'created': '2014-08-15 21:01:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/96e78b798872db43aad83fd1585b61628346d1bc', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 4, 'created': '2014-08-15 22:32:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/273ad54871a1c74e59a63e387c6d938a28bf65e7', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 5, 'created': '2014-08-18 23:13:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1e91e7abfdbf6062f1b1e63026fa0f9dec780039', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 6, 'created': '2014-08-19 02:53:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9ddf211a09c066b49c348953d122fd9b4ce2eaeb', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nDocImpact: New volume type access extension\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 7, 'created': '2014-08-25 17:42:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/4fb65d1049191ba3c1adbd1fe16a5d2bd86ed757', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nDocImpact: New volume type access extension\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 8, 'created': '2014-09-02 17:49:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/8697dbbedc439564117b3420ef5ce27fa2f5228f', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nDocImpact: New volume type access extension\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 9, 'created': '2014-09-02 17:57:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/557f0bb64f89c973d6ac202c7c429a8f66787f82', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nDocImpact: New volume type access extension\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 10, 'created': '2014-11-17 19:00:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/0232f70926ba8663bec23fb467cb6341dede6258', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nDocImpact: New volume type access extension\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 11, 'created': '2014-11-17 20:34:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/ad220254659af47646fe5c7f14e213d2b2708807', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nDocImpact: New volume type access extension\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 12, 'created': '2014-11-18 19:28:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c997414fc4357ad20060a01f606b2561afd47cc4', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nDocImpact: New volume type access extension\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 13, 'created': '2014-11-19 15:26:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/80f7bf5f1d5cff05f1c91adca25b92e2d416b73f', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nDocImpact: New volume type access extension\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 14, 'created': '2014-12-01 23:10:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/f6bd9ca5222d457d9d30ae813629881efe121325', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nDocImpact: New volume type access extension\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}, {'number': 15, 'created': '2014-12-01 23:52:50.000000000', 'files': ['cinder/api/extensions.py', 'cinder/api/v1/types.py', 'cinder/api/v2/router.py', 'etc/cinder/policy.json', 'cinder/tests/policy.json', 'cinder/db/api.py', 'cinder/volume/volume_types.py', 'cinder/api/v2/types.py', 'cinder/db/sqlalchemy/api.py', 'cinder/tests/api/contrib/test_volume_type_access.py', 'cinder/tests/api/contrib/test_types_extra_specs.py', 'cinder/db/sqlalchemy/migrate_repo/versions/032_sqlite_downgrade.sql', 'cinder/tests/test_volume_types.py', 'cinder/db/sqlalchemy/migrate_repo/versions/032_add_volume_type_projects.py', 'cinder/api/contrib/types_manage.py', 'cinder/tests/api/v2/test_types.py', 'cinder/exception.py', 'cinder/tests/api/contrib/test_types_manage.py', 'cinder/api/contrib/volume_type_access.py', 'cinder/tests/api/v1/test_types.py', 'cinder/tests/test_migrations.py', 'cinder/utils.py', 'cinder/db/sqlalchemy/models.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/6c0f50b1ec933a61b84d806e748afd9cb74e5cd7', 'message': 'Volume type access extension\n\nThis extension adds the ability to manage volume type access:\n* Volume types are public by default\n* Private volume types can be created by setting\n  the is_public boolean field to False at creation time.\n* Access to a private volume type can be controlled\n  by adding or removing a project from it.\n* Private volume types without projects are only visible\n  by users with the admin role/context.\n\nImplementation details and unit tests were mostly adapted\nfrom Nova flavor access extension.\n\nDocImpact: New volume type access extension\nImplements: blueprint private-volume-types\nChange-Id: I8faf1d8097bf8412d4e169ec3503821351795561\n'}]",52,114395,6c0f50b1ec933a61b84d806e748afd9cb74e5cd7,119,22,15,7156,,,0,"Volume type access extension

This extension adds the ability to manage volume type access:
* Volume types are public by default
* Private volume types can be created by setting
  the is_public boolean field to False at creation time.
* Access to a private volume type can be controlled
  by adding or removing a project from it.
* Private volume types without projects are only visible
  by users with the admin role/context.

Implementation details and unit tests were mostly adapted
from Nova flavor access extension.

DocImpact: New volume type access extension
Implements: blueprint private-volume-types
Change-Id: I8faf1d8097bf8412d4e169ec3503821351795561
",git fetch https://review.opendev.org/openstack/cinder refs/changes/95/114395/15 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/api/v1/types.py', 'cinder/api/v2/router.py', 'cinder/api/v1/router.py', 'cinder/db/api.py', 'cinder/volume/volume_types.py', 'cinder/api/v2/types.py', 'cinder/db/sqlalchemy/api.py', 'cinder/db/sqlalchemy/migrate_repo/versions/024_add_volume_type_projects.py', 'cinder/tests/api/contrib/test_types_extra_specs.py', 'cinder/db/sqlalchemy/migrate_repo/versions/024_sqlite_downgrade.sql', 'cinder/api/contrib/types_manage.py', 'cinder/tests/api/v2/test_types.py', 'cinder/exception.py', 'cinder/tests/api/contrib/test_types_manage.py', 'cinder/api/contrib/volume_type_access.py', 'cinder/tests/api/v1/test_types.py', 'cinder/tests/test_migrations.py', 'cinder/db/sqlalchemy/models.py']",18,1031111ad5d9fa3ba09f8a7e8820b16ae1488432,bp/private-volume-types," is_public = Column(Boolean, default=True)class VolumeTypeProjects(BASE, CinderBase): """"""Represent projects associated volume_types."""""" __tablename__ = ""volume_type_projects"" __table_args__ = (schema.UniqueConstraint( ""volume_type_id"", ""project_id"", ""deleted"", name=""uniq_volume_type_projects0volume_type_id0project_id0deleted""), ) id = Column(Integer, primary_key=True) volume_type_id = Column(Integer, ForeignKey('volume_types.id'), nullable=False) project_id = Column(String(255)) volume_type = relationship( VolumeTypes, backref=""projects"", foreign_keys=volume_type_id, primaryjoin='and_(' 'VolumeTypeProjects.volume_type_id == VolumeTypes.id,' 'VolumeTypeProjects.deleted == False)') ",,646,45
openstack%2Fcinder~master~Idfc183f2ed1ad73b64fc893efcc07972c95926c6,openstack/cinder,master,Idfc183f2ed1ad73b64fc893efcc07972c95926c6,Don't use _execute directly in brick/iscsi,MERGED,2014-12-01 21:03:27.000000000,2014-12-06 10:53:01.000000000,2014-12-02 00:33:18.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 2243}, {'_account_id': 5997}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-01 21:03:27.000000000', 'files': ['cinder/volume/iscsi.py', 'cinder/brick/iscsi/iscsi.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/a53198feb550b1bc0976b310a9e24fcf2334dc1f', 'message': ""Don't use _execute directly in brick/iscsi\n\nThe brick/iscsi module has a run helper that\nshould be used for executing commands.  There\nare a number of inconsistencies where _execute\nis called directly.\n\nThis patch moves everythign to use the run method\nto keep things consistent and also to fix up some\npotential issues with variables becoming corrupt\nunder heavy load.\n\nChange-Id: Idfc183f2ed1ad73b64fc893efcc07972c95926c6\n""}]",3,138180,a53198feb550b1bc0976b310a9e24fcf2334dc1f,17,11,1,2243,,,0,"Don't use _execute directly in brick/iscsi

The brick/iscsi module has a run helper that
should be used for executing commands.  There
are a number of inconsistencies where _execute
is called directly.

This patch moves everythign to use the run method
to keep things consistent and also to fix up some
potential issues with variables becoming corrupt
under heavy load.

Change-Id: Idfc183f2ed1ad73b64fc893efcc07972c95926c6
",git fetch https://review.opendev.org/openstack/cinder refs/changes/80/138180/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/iscsi.py', 'cinder/brick/iscsi/iscsi.py']",2,a53198feb550b1bc0976b310a9e24fcf2334dc1f,use_run_method_in_brick_iscsi," # NOTE(jdg): cmd is a prefix to the target helper utility we # use. This can be tgtadm, cinder-rtstool etc def _run(self, cmd, *args, **kwargs): return self._execute(cmd, *args, **kwargs) (out, _err) = self._run('tgt-admin', '--show', run_as_root=True) (out, _err) = self._run('tgt-admin', '--show', run_as_root=True) (out, err) = self._run('tgtadm', '--lld', 'iscsi', '--op', 'new', '--mode', 'logicalunit', '--tid', tid, '--lun', '1', '-b', path, run_as_root=True) # NOTE(jdg): Debug is ok here because the caller # will just generate the CHAP creds and create the # file based on the None return (out, err) = self._run('tgt-admin', '--update', name, run_as_root=True) (out, err) = self._run('tgtadm', '--lld', 'iscsi', '--op', 'show', '--mode', 'target', run_as_root=True) self._run('tgt-admin', '--force', '--delete', iqn, run_as_root=True, attempts=CONF.num_shell_tries) self._run('tgt-admin', '--delete', iqn, run_as_root=True) self._run(self._cmd, '--op', 'new', self._run(self._cmd, '--op', 'delete', self._run(self._cmd, '--op', 'show', self._run(self._cmd, '--op', 'new', self._run(self._cmd, '--op', 'delete', self._run(self._cmd, '--op', 'new', self._run('cinder-rtstool', 'verify') (out, _err) = self._run('cinder-rtstool', 'get-targets', run_as_root=True) command_args = ['create', self._run('cinder-rtstool', *command_args, run_as_root=True) self._run('cinder-rtstool', 'delete', iqn, run_as_root=True) self._run('cinder-rtstool', 'add-initiator', volume_iqn, auth_user, auth_pass, connector['initiator'], run_as_root=True) self._run('cinder-rtstool', 'delete-initiator', volume_iqn, connector['initiator'], run_as_root=True)"," def _run(self, *args, **kwargs): self._execute(self._cmd, *args, run_as_root=True, **kwargs) (out, _err) = self._execute('tgt-admin', '--show', run_as_root=True) (out, _err) = self._execute('tgt-admin', '--show', run_as_root=True) (out, err) = self._execute('tgtadm', '--lld', 'iscsi', '--op', 'new', '--mode', 'logicalunit', '--tid', tid, '--lun', '1', '-b', path, run_as_root=True) (out, err) = self._execute('tgt-admin', '--update', name, run_as_root=True) (out, err) = self._execute('tgtadm', '--lld', 'iscsi', '--op', 'show', '--mode', 'target', run_as_root=True) self._execute('tgt-admin', '--force', '--delete', iqn, run_as_root=True, attempts=CONF.num_shell_tries) self._execute('tgt-admin', '--delete', iqn, run_as_root=True) self._run('--op', 'new', self._run('--op', 'delete', self._run('--op', 'show', self._run('--op', 'new', self._run('--op', 'delete', self._run('--op', 'new', self._execute('cinder-rtstool', 'verify') (out, _err) = self._execute('cinder-rtstool', 'get-targets', run_as_root=True) command_args = ['cinder-rtstool', 'create', self._execute(*command_args, run_as_root=True) self._execute('cinder-rtstool', 'delete', iqn, run_as_root=True) self._execute('cinder-rtstool', 'add-initiator', volume_iqn, auth_user, auth_pass, connector['initiator'], run_as_root=True) self._execute('cinder-rtstool', 'delete-initiator', volume_iqn, connector['initiator'], run_as_root=True)",65,56
openstack%2Fcinder~master~If96f4fab7d9709dbb0726b1dca6237d7bc66ecbe,openstack/cinder,master,If96f4fab7d9709dbb0726b1dca6237d7bc66ecbe,Deal with tgt already exists errors,MERGED,2014-12-01 20:32:39.000000000,2014-12-06 10:33:18.000000000,2014-12-02 07:12:46.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 6491}, {'_account_id': 6873}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-01 20:32:39.000000000', 'files': ['cinder/brick/iscsi/iscsi.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/d7bd65e8e9ff800d370effc1a8d24ea900a5d435', 'message': ""Deal with tgt already exists errors\n\nThis patch intends to provide a clear marker for\nER in the case that create export fails due to the\ntarget entry already existing.\n\nAlso this patch will enable us to go ahead and just use\nthe existing target rather than bomb out and fail everything.\n\nRoot cause of why we're getting a second create is still\nunknown and needs addressed, but this might help in getting\nmore info as well as keeping things stable until we address\nthe root issue.\n\nChange-Id: If96f4fab7d9709dbb0726b1dca6237d7bc66ecbe\nPartial-Bug: #1398078\n""}]",1,138173,d7bd65e8e9ff800d370effc1a8d24ea900a5d435,15,11,1,2243,,,0,"Deal with tgt already exists errors

This patch intends to provide a clear marker for
ER in the case that create export fails due to the
target entry already existing.

Also this patch will enable us to go ahead and just use
the existing target rather than bomb out and fail everything.

Root cause of why we're getting a second create is still
unknown and needs addressed, but this might help in getting
more info as well as keeping things stable until we address
the root issue.

Change-Id: If96f4fab7d9709dbb0726b1dca6237d7bc66ecbe
Partial-Bug: #1398078
",git fetch https://review.opendev.org/openstack/cinder refs/changes/73/138173/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/brick/iscsi/iscsi.py'],1,d7bd65e8e9ff800d370effc1a8d24ea900a5d435,bug/1398078," if ""target already exists"" in err: LOG.warning(_LW('Create iscsi target failed for ' 'target already exists')) # NOTE(jdg): We've run into some cases where the cmd being # sent was not correct. May be related to using the # executor direclty? # Adding the additional Warning message above to provide # a very cleary marker for ER, and if the tgt exists let's # just try and use it and move along. # Ref bug: #1398078 pass else: # Don't forget to remove the persistent file we created os.unlink(volume_path) raise exception.ISCSITargetCreateFailed(volume_id=vol_id)", #Don't forget to remove the persistent file we created os.unlink(volume_path) raise exception.ISCSITargetCreateFailed(volume_id=vol_id),15,4
openstack%2Fcinder~master~I9423b253d9842295850f469757e928bf5856967a,openstack/cinder,master,I9423b253d9842295850f469757e928bf5856967a,Fix find_autodoc_modules.sh to support OSX,MERGED,2014-12-01 20:20:04.000000000,2014-12-06 10:13:47.000000000,2014-12-02 07:12:55.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 6491}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-01 20:20:04.000000000', 'files': ['doc/find_autodoc_modules.sh'], 'web_link': 'https://opendev.org/openstack/cinder/commit/edd69bf358567b58ed186ec4bb7d53558aad9233', 'message': 'Fix find_autodoc_modules.sh to support OSX\n\nWhen running \'find cinder/\' on OSX platforms, all returned paths\nwill have 2 slashes (//). Because the script only strips\nthe first slash as per the CINDER_DIR value, we end up\nwith module names such as ""cinder..db.api"" in the documentation.\n\nThis change trims the leading dot if found to avoid this situation.\n\nChange-Id: I9423b253d9842295850f469757e928bf5856967a\n'}]",0,138171,edd69bf358567b58ed186ec4bb7d53558aad9233,13,9,1,7156,,,0,"Fix find_autodoc_modules.sh to support OSX

When running 'find cinder/' on OSX platforms, all returned paths
will have 2 slashes (//). Because the script only strips
the first slash as per the CINDER_DIR value, we end up
with module names such as ""cinder..db.api"" in the documentation.

This change trims the leading dot if found to avoid this situation.

Change-Id: I9423b253d9842295850f469757e928bf5856967a
",git fetch https://review.opendev.org/openstack/cinder refs/changes/71/138171/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/find_autodoc_modules.sh'],1,edd69bf358567b58ed186ec4bb7d53558aad9233,doc_osx_support, relative=cinder.`echo ${x} | sed -e 's$^'${CINDER_DIR}'$$' -e 's/.py$//' -e 's$/$.$g' -e 's$^.$$'`, relative=cinder.`echo ${x} | sed -e 's$^'${CINDER_DIR}'$$' -e 's/.py$//' -e 's$/$.$g'`,1,1
openstack%2Fcinder~master~I180dc74e4535bfde19f1741cff975f5ec675dd21,openstack/cinder,master,I180dc74e4535bfde19f1741cff975f5ec675dd21,Raise exception if invalid IP is specified,MERGED,2014-12-01 15:58:45.000000000,2014-12-06 09:54:22.000000000,2014-12-01 21:17:17.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 7198}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 11459}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-01 15:58:45.000000000', 'files': ['cinder/tests/test_netapp_eseries_iscsi.py', 'cinder/volume/drivers/netapp/eseries/iscsi.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/6fb9f5c02f557eb419615abb07cda9d044407c90', 'message': 'Raise exception if invalid IP is specified\n\nThis patch ensures that all values specified for the configuration\noption netapp_controller_ips are valid, rather than logging and\ncontinuing with only the valid addresses.\n\nCloses-Bug: 1396718\n\nChange-Id: I180dc74e4535bfde19f1741cff975f5ec675dd21\n'}]",0,138106,6fb9f5c02f557eb419615abb07cda9d044407c90,13,9,1,9186,,,0,"Raise exception if invalid IP is specified

This patch ensures that all values specified for the configuration
option netapp_controller_ips are valid, rather than logging and
continuing with only the valid addresses.

Closes-Bug: 1396718

Change-Id: I180dc74e4535bfde19f1741cff975f5ec675dd21
",git fetch https://review.opendev.org/openstack/cinder refs/changes/06/138106/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/test_netapp_eseries_iscsi.py', 'cinder/volume/drivers/netapp/eseries/iscsi.py']",2,6fb9f5c02f557eb419615abb07cda9d044407c90,bug/1396718," raise exception.NoValidHost( _(""Controller IP '%(host)s' could not be resolved: %(e)s."") % {'host': host, 'e': e})", return None if not ips: msg = _('Controller ips not valid after resolution.') raise exception.NoValidHost(reason=msg),50,4
openstack%2Fsahara~master~Ia53a90f9b632b54a79906f07610c7a1ae1c70218,openstack/sahara,master,Ia53a90f9b632b54a79906f07610c7a1ae1c70218,Don't provide CONF to the AuthProtocol middleware,MERGED,2014-12-02 00:25:21.000000000,2014-12-06 09:29:43.000000000,2014-12-06 04:55:21.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7125}, {'_account_id': 7132}, {'_account_id': 7191}, {'_account_id': 7213}, {'_account_id': 7710}, {'_account_id': 7745}, {'_account_id': 8411}, {'_account_id': 8871}, {'_account_id': 10670}]","[{'number': 1, 'created': '2014-12-02 00:25:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/28d7a213b06c50a1c583dac8a0c302933412635f', 'message': ""Don't provide CONF to the AuthProtocol middleware\n\nThe conf value that is passed to the AuthProtocol middleware is a\ndictionary that is expected to be passed from paste. They are overrides\nof oslo.conf not the conf object itself. If we don't pass anything it\nwill use the standard CONF object.\n\nChange-Id: Ia53a90f9b632b54a79906f07610c7a1ae1c70218\n""}, {'number': 2, 'created': '2014-12-02 20:03:35.000000000', 'files': ['sahara/cli/sahara_engine.py', 'sahara/api/acl.py', 'sahara/main.py'], 'web_link': 'https://opendev.org/openstack/sahara/commit/a4d0926628d4af90e77deaa73eddf47993eab2f0', 'message': ""Don't provide CONF to the AuthProtocol middleware\n\nThe conf value that is passed to the AuthProtocol middleware is a\ndictionary that is expected to be passed from paste. They are overrides\nof oslo.conf not the conf object itself. If we don't pass anything it\nwill use the standard CONF object.\n\nCloses-Bug: #1398524\nChange-Id: Ia53a90f9b632b54a79906f07610c7a1ae1c70218\n""}]",0,138211,a4d0926628d4af90e77deaa73eddf47993eab2f0,51,11,2,7191,,,0,"Don't provide CONF to the AuthProtocol middleware

The conf value that is passed to the AuthProtocol middleware is a
dictionary that is expected to be passed from paste. They are overrides
of oslo.conf not the conf object itself. If we don't pass anything it
will use the standard CONF object.

Closes-Bug: #1398524
Change-Id: Ia53a90f9b632b54a79906f07610c7a1ae1c70218
",git fetch https://review.opendev.org/openstack/sahara refs/changes/11/138211/2 && git format-patch -1 --stdout FETCH_HEAD,"['sahara/cli/sahara_engine.py', 'sahara/api/acl.py', 'sahara/main.py']",3,28d7a213b06c50a1c583dac8a0c302933412635f,middleware, app.wsgi_app = acl.wrap(app.wsgi_app)," app.wsgi_app = acl.wrap(app.wsgi_app, CONF)",4,12
openstack%2Fopenstack-manuals~master~I28a79dd39b2ff7f1800b19167c1074d47cc9a913,openstack/openstack-manuals,master,I28a79dd39b2ff7f1800b19167c1074d47cc9a913,Generated and added configuration reference tables for Ironic,MERGED,2014-12-05 11:43:04.000000000,2014-12-06 09:24:02.000000000,2014-12-06 09:24:00.000000000,"[{'_account_id': 3}, {'_account_id': 964}, {'_account_id': 6547}, {'_account_id': 6772}]","[{'number': 1, 'created': '2014-12-05 11:43:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/17ea1d5c5ef6f23e1462fd3bf9f57f00a1b94ede', 'message': 'Generated configuration reference tables for Ironic\n\nChange-Id: I28a79dd39b2ff7f1800b19167c1074d47cc9a913\n'}, {'number': 2, 'created': '2014-12-05 11:50:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/52a7000b00803f725a754d33c77b20643d849433', 'message': 'Generated and added configuration reference tables for Ironic\n\nChange-Id: I28a79dd39b2ff7f1800b19167c1074d47cc9a913\n'}, {'number': 3, 'created': '2014-12-05 21:00:12.000000000', 'files': ['doc/common/tables/ironic-swift.xml', 'doc/common/tables/ironic-ca.xml', 'doc/common/tables/ironic-debug.xml', 'doc/common/tables/ironic-policy.xml', 'doc/common/tables/ironic-api.xml', 'doc/common/tables/ironic-dhcp.xml', 'doc/common/tables/ironic-disk_partitioner.xml', 'doc/common/tables/ironic-ipmi.xml', 'doc/config-reference/bk-config-ref.xml', 'doc/common/tables/ironic-auth_token.xml', 'doc/common/tables/ironic-auth.xml', 'doc/common/tables/ironic-qpid.xml', 'doc/common/tables/ironic-console.xml', 'doc/common/tables/ironic-ssh.xml', 'doc/common/tables/ironic-ilo.xml', 'doc/common/tables/ironic-rabbitmq.xml', 'doc/common/tables/ironic-snmp.xml', 'doc/common/tables/ironic-logging.xml', 'doc/common/tables/ironic-pxe.xml', 'doc/common/tables/ironic-rpc.xml', 'doc/common/tables/ironic-common.xml', 'doc/common/tables/ironic-database.xml', 'doc/common/tables/ironic-glance.xml', 'doc/common/tables/ironic-seamicro.xml', 'doc/common/tables/ironic-zeromq.xml', 'doc/config-reference/ch_baremetalconfigure.xml', 'doc/common/tables/ironic-amqp.xml', 'doc/common/tables/ironic-conductor.xml', 'doc/common/tables/ironic-neutron.xml', 'doc/common/tables/ironic-redis.xml', 'doc/common/tables/ironic-agent.xml', 'doc/common/tables/ironic-rootwrap.xml'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/a992c0e1caffa7bd1d552eb75d9be84965e5a66b', 'message': 'Generated and added configuration reference tables for Ironic\n\nChange-Id: I28a79dd39b2ff7f1800b19167c1074d47cc9a913\n'}]",2,139604,a992c0e1caffa7bd1d552eb75d9be84965e5a66b,12,4,3,167,,,0,"Generated and added configuration reference tables for Ironic

Change-Id: I28a79dd39b2ff7f1800b19167c1074d47cc9a913
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/04/139604/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/common/tables/ironic-swift.xml', 'doc/common/tables/ironic-ca.xml', 'doc/common/tables/ironic-debug.xml', 'doc/common/tables/ironic-policy.xml', 'doc/common/tables/ironic-api.xml', 'doc/common/tables/ironic-dhcp.xml', 'doc/common/tables/ironic-disk_partitioner.xml', 'doc/common/tables/ironic-ipmi.xml', 'doc/common/tables/ironic-auth_token.xml', 'doc/common/tables/ironic-auth.xml', 'doc/common/tables/ironic-qpid.xml', 'doc/common/tables/ironic-console.xml', 'doc/common/tables/ironic-ssh.xml', 'doc/common/tables/ironic-ilo.xml', 'doc/common/tables/ironic-rabbitmq.xml', 'doc/common/tables/ironic-snmp.xml', 'doc/common/tables/ironic-logging.xml', 'doc/common/tables/ironic-pxe.xml', 'doc/common/tables/ironic-rpc.xml', 'doc/common/tables/ironic-common.xml', 'doc/common/tables/ironic-database.xml', 'doc/common/tables/ironic-glance.xml', 'doc/common/tables/ironic-seamicro.xml', 'doc/common/tables/ironic-zeromq.xml', 'doc/common/tables/ironic-amqp.xml', 'doc/common/tables/ironic-conductor.xml', 'doc/common/tables/ironic-neutron.xml', 'doc/common/tables/ironic-redis.xml', 'doc/common/tables/ironic-agent.xml', 'doc/common/tables/ironic-rootwrap.xml']",30,17ea1d5c5ef6f23e1462fd3bf9f57f00a1b94ede,tables_ironic,"<?xml version='1.0' encoding='UTF-8'?> <para xmlns=""http://docbook.org/ns/docbook"" version=""5.0""> <!-- Warning: Do not edit this file. It is automatically generated and your changes will be overwritten. The tool to do so lives in openstack-doc-tools repository. --> <table rules=""all"" xml:id=""config_table_ironic_rootwrap""> <caption>Description of rootwrap configuration options</caption> <col width=""50%""/> <col width=""50%""/> <thead> <tr> <th>Configuration option = Default value</th> <th>Description</th> </tr> </thead> <tbody> <tr> <th colspan=""2"">[DEFAULT]</th> </tr> <tr> <td>filters_path = /etc/ironic/rootwrap.d,/usr/share/ironic/rootwrap</td> <td>List of directories to load filter definitions from (separated by ','). These directories MUST all be only writeable by root !</td> </tr> <tr> <td>exec_dirs = /sbin,/usr/sbin,/bin,/usr/bin</td> <td>List of directories to search executables in, in case filters do not explicitely specify a full path (separated by ',') If not specified, defaults to system PATH environment variable. These directories MUST all be only writeable by root !</td> </tr> <tr> <td>use_syslog = False</td> <td>Enable logging to syslog Default value is False</td> </tr> <tr> <td>syslog_log_facility = syslog</td> <td>Which syslog facility to use. Valid values include auth, authpriv, syslog, user0, user1... Default value is 'syslog'</td> </tr> <tr> <td>syslog_log_level = ERROR</td> <td>Which messages to log. INFO means log all usage ERROR means only log unsuccessful attempts</td> </tr> </tbody> </table> </para> ",,1650,0
openstack%2Fcinder~master~I3a61e995ea41fc0324b5cb60e3c96e3d9dc56637,openstack/cinder,master,I3a61e995ea41fc0324b5cb60e3c96e3d9dc56637,Fix check_ssh_injection in cinder/utils,MERGED,2014-12-01 13:56:08.000000000,2014-12-06 09:16:06.000000000,2014-12-01 21:11:27.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 1736}, {'_account_id': 7198}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11459}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-12-01 13:56:08.000000000', 'files': ['cinder/tests/test_utils.py', 'cinder/utils.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/78d9c0366b08c64f39930c2375d6622041fc8abe', 'message': 'Fix check_ssh_injection in cinder/utils\n\ncheck_ssh_injection is used to prevent commands being modified using\nspecially constructed strings containing special characters.\n\nThe function includes a loop over the special characters to compare\nthem against each arg. If the special character is the same as the arg\nit gets ignored.\n\nThis commit modifies this part of the function so that args that are\nexactly equal to one of the special characters will cause an exception\nto be raised.\n\nChange-Id: I3a61e995ea41fc0324b5cb60e3c96e3d9dc56637\nCloses-Bug: #1398002\n'}]",0,138068,78d9c0366b08c64f39930c2375d6622041fc8abe,20,12,1,7219,,,0,"Fix check_ssh_injection in cinder/utils

check_ssh_injection is used to prevent commands being modified using
specially constructed strings containing special characters.

The function includes a loop over the special characters to compare
them against each arg. If the special character is the same as the arg
it gets ignored.

This commit modifies this part of the function so that args that are
exactly equal to one of the special characters will cause an exception
to be raised.

Change-Id: I3a61e995ea41fc0324b5cb60e3c96e3d9dc56637
Closes-Bug: #1398002
",git fetch https://review.opendev.org/openstack/cinder refs/changes/68/138068/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/test_utils.py', 'cinder/utils.py']",2,78d9c0366b08c64f39930c2375d6622041fc8abe,ssh_injection, if c not in arg:, if arg == c:,6,2
openstack%2Ftaskflow~master~I2e77766e6491be91e6744d4acf84eac16e39c627,openstack/taskflow,master,I2e77766e6491be91e6744d4acf84eac16e39c627,Just use 4 spaces for classifier indents,MERGED,2014-12-06 05:11:46.000000000,2014-12-06 08:18:06.000000000,2014-12-06 08:18:05.000000000,"[{'_account_id': 3}, {'_account_id': 1297}]","[{'number': 1, 'created': '2014-12-06 05:11:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/bc5c7df0810a73954487a0655e23c2b34fc36896', 'message': 'Just use 4 spaces for classifier indents\n\nChange-Id: I2e77766e6491be91e6744d4acf84eac16e39c627\n'}, {'number': 2, 'created': '2014-12-06 05:12:33.000000000', 'files': ['setup.cfg'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/dc393513b49320bb5cbaaa4c582db292ec3be796', 'message': 'Just use 4 spaces for classifier indents\n\nKeep the setup.cfg file consistently using only\nfour spaces and not having mixed 4 and 8 spaces.\n\nChange-Id: I2e77766e6491be91e6744d4acf84eac16e39c627\n'}]",0,139783,dc393513b49320bb5cbaaa4c582db292ec3be796,13,2,2,1297,,,0,"Just use 4 spaces for classifier indents

Keep the setup.cfg file consistently using only
four spaces and not having mixed 4 and 8 spaces.

Change-Id: I2e77766e6491be91e6744d4acf84eac16e39c627
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/83/139783/1 && git format-patch -1 --stdout FETCH_HEAD,['setup.cfg'],1,bc5c7df0810a73954487a0655e23c2b34fc36896,, Development Status :: 4 - Beta Environment :: OpenStack Intended Audience :: Developers Intended Audience :: Information Technology License :: OSI Approved :: Apache Software License Operating System :: POSIX :: Linux Programming Language :: Python Programming Language :: Python :: 2 Programming Language :: Python :: 2.6 Programming Language :: Python :: 2.7 Programming Language :: Python :: 3 Programming Language :: Python :: 3.3 Programming Language :: Python :: 3.4 Topic :: Software Development :: Libraries Topic :: System :: Distributed Computing, Development Status :: 4 - Beta Environment :: OpenStack Intended Audience :: Developers Intended Audience :: Information Technology License :: OSI Approved :: Apache Software License Operating System :: POSIX :: Linux Programming Language :: Python Programming Language :: Python :: 2 Programming Language :: Python :: 2.6 Programming Language :: Python :: 2.7 Programming Language :: Python :: 3 Programming Language :: Python :: 3.3 Programming Language :: Python :: 3.4 Topic :: Software Development :: Libraries Topic :: System :: Distributed Computing,15,15
openstack%2Ftaskflow~master~Ic267eaeeda70046e37e6edeb296a956f32c82edf,openstack/taskflow,master,Ic267eaeeda70046e37e6edeb296a956f32c82edf,Ensure frozen attribute is set in fsm clones/copies,MERGED,2014-12-05 01:40:32.000000000,2014-12-06 07:44:13.000000000,2014-12-06 07:44:12.000000000,"[{'_account_id': 3}, {'_account_id': 1297}]","[{'number': 1, 'created': '2014-12-05 01:40:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/e5cac47efe223ac748eeaeb7b57931fe7bce9be1', 'message': 'Ensure frozen attribute is set in clones/copies\n\nChange-Id: Ic267eaeeda70046e37e6edeb296a956f32c82edf\n'}, {'number': 2, 'created': '2014-12-05 01:53:47.000000000', 'files': ['taskflow/types/fsm.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/2f037360d6d2d4eaf803f8d8f9374da2d980d68f', 'message': 'Ensure frozen attribute is set in fsm clones/copies\n\nChange-Id: Ic267eaeeda70046e37e6edeb296a956f32c82edf\n'}]",0,139297,2f037360d6d2d4eaf803f8d8f9374da2d980d68f,15,2,2,1297,,,0,"Ensure frozen attribute is set in fsm clones/copies

Change-Id: Ic267eaeeda70046e37e6edeb296a956f32c82edf
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/97/139297/2 && git format-patch -1 --stdout FETCH_HEAD,['taskflow/types/fsm.py'],1,e5cac47efe223ac748eeaeb7b57931fe7bce9be1,, c.frozen = self.frozen,,1,0
openstack%2Fsahara~master~I5ef5385a31f92ab0831e780c96b06da503106f9e,openstack/sahara,master,I5ef5385a31f92ab0831e780c96b06da503106f9e,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:52:00.000000000,2014-12-06 07:37:26.000000000,2014-12-06 02:00:13.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 8411}]","[{'number': 1, 'created': '2014-12-05 03:52:00.000000000', 'files': ['doc/source/devref/gerrit.rst', 'doc/source/devref/how_to_participate.rst', 'CONTRIBUTING.rst', 'doc/source/devref/development.environment.rst', 'doc/source/devref/devstack.rst'], 'web_link': 'https://opendev.org/openstack/sahara/commit/a074c02eea8197a48778217e4a60baea7af8893e', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I5ef5385a31f92ab0831e780c96b06da503106f9e\n'}]",0,139387,a074c02eea8197a48778217e4a60baea7af8893e,14,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I5ef5385a31f92ab0831e780c96b06da503106f9e
",git fetch https://review.opendev.org/openstack/sahara refs/changes/87/139387/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'doc/source/devref/gerrit.rst', 'doc/source/devref/how_to_participate.rst', 'doc/source/devref/development.environment.rst', 'doc/source/devref/devstack.rst']",5,a074c02eea8197a48778217e4a60baea7af8893e,infra-manual, $ git clone https://git.openstack.org/cgit/openstack-dev/devstack.git, $ git clone https://github.com/openstack-dev/devstack.git,10,11
openstack%2Fsahara~master~I0ac05536c5b2beb3eb80b3671ab686edecb3e3e8,openstack/sahara,master,I0ac05536c5b2beb3eb80b3671ab686edecb3e3e8,Updated from global requirements,MERGED,2014-12-04 21:34:31.000000000,2014-12-06 06:57:51.000000000,2014-12-06 02:08:31.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 7710}, {'_account_id': 8090}, {'_account_id': 8411}, {'_account_id': 10670}]","[{'number': 1, 'created': '2014-12-04 21:34:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/80301fb54c6b62d86b43f973f28201e7cf82831e', 'message': 'Updated from global requirements\n\nChange-Id: I0ac05536c5b2beb3eb80b3671ab686edecb3e3e8\n'}, {'number': 2, 'created': '2014-12-04 22:43:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/61a5d000d855bd80d2a7c68735e086b94063297f', 'message': 'Updated from global requirements\n\nChange-Id: I0ac05536c5b2beb3eb80b3671ab686edecb3e3e8\n'}, {'number': 3, 'created': '2014-12-05 16:40:22.000000000', 'files': ['requirements.txt', 'test-requirements.txt'], 'web_link': 'https://opendev.org/openstack/sahara/commit/af5d7862b8087a502ec470b5758b777e7fc28903', 'message': 'Updated from global requirements\n\nChange-Id: I0ac05536c5b2beb3eb80b3671ab686edecb3e3e8\n'}]",0,139209,af5d7862b8087a502ec470b5758b777e7fc28903,23,7,3,11131,,,0,"Updated from global requirements

Change-Id: I0ac05536c5b2beb3eb80b3671ab686edecb3e3e8
",git fetch https://review.opendev.org/openstack/sahara refs/changes/09/139209/1 && git format-patch -1 --stdout FETCH_HEAD,['test-requirements.txt'],1,80301fb54c6b62d86b43f973f28201e7cf82831e,openstack/requirements,python-saharaclient>=0.7.6,python-saharaclient>=0.7.5,1,1
openstack%2Ftaskflow~master~I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872,openstack/taskflow,master,I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872,Add a testcase that can test + verify runtime task ordering,ABANDONED,2014-11-19 01:45:46.000000000,2014-12-06 06:43:08.000000000,,"[{'_account_id': 3}, {'_account_id': 1297}]","[{'number': 1, 'created': '2014-11-19 01:45:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/b4b16065b50e4eebc7810cf7623bc14496eda75d', 'message': 'Add a testcase that can test + verify runtime task ordering\n\nIn order to make sure that all engines run all tests following\nthe pattern prescribed ordering add a new testcase that can be\neasily extended with new flow definitions that will be\nautomatically verified when execution completes, this ensures that\nthe expected ordering that is defined in a engines compilation\nexecution graph is maintained during runtime execution.\n\nChange-Id: I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872\n'}, {'number': 2, 'created': '2014-11-19 01:48:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/3d541006c18d61d94909cdc1f6d31e8f0575c637', 'message': 'Add a testcase that can test + verify runtime task ordering\n\nIn order to make sure that all engines run all tests following\nthe pattern prescribed ordering add a new testcase that can be\neasily extended with new flow definitions that will be\nautomatically verified when execution completes, this ensures that\nthe expected ordering that is defined in a engines compilation\nexecution graph is maintained during runtime execution.\n\nChange-Id: I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872\n'}, {'number': 3, 'created': '2014-11-19 22:41:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/9a9239c70f76f6fd482dfd011149eb73d4411d2f', 'message': 'Add a testcase that can test + verify runtime task ordering\n\nIn order to make sure that all engines run all tests following\nthe pattern prescribed ordering add a new testcase that can be\neasily extended with new flow definitions that will be\nautomatically verified when execution completes, this ensures that\nthe expected ordering that is defined in a engines compilation\nexecution graph is maintained during runtime execution.\n\nChange-Id: I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872\n'}, {'number': 4, 'created': '2014-11-19 22:49:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/5601de6f2b1ab454ab3953ba2ef509fe48b252d8', 'message': 'Add a testcase that can test + verify runtime task ordering\n\nIn order to make sure that all engines run all tests following\nthe pattern prescribed ordering add a new testcase that can be\neasily extended with new flow definitions that will be\nautomatically verified when execution completes, this ensures that\nthe expected ordering that is defined in a engines compilation\nexecution graph is maintained during runtime execution.\n\nChange-Id: I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872\n'}, {'number': 5, 'created': '2014-11-19 22:57:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/2cfed4b116f1eef15f82f45561e26413b103ebb6', 'message': 'Add a testcase that can test + verify runtime task ordering\n\nIn order to make sure that all engines run all tests following\nthe pattern prescribed ordering add a new testcase that can be\neasily extended with new flow definitions that will be\nautomatically verified when execution completes, this ensures that\nthe expected ordering that is defined in a engines compilation\nexecution graph is maintained during runtime execution.\n\nChange-Id: I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872\n'}, {'number': 6, 'created': '2014-11-19 23:15:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/34f7f0cff82f3005b82ef6862a784e25f0c19b9b', 'message': 'Add a testcase that can test + verify runtime task ordering\n\nIn order to make sure that all engines run all tests following\nthe pattern prescribed ordering add a new testcase that can be\neasily extended with new flow definitions that will be\nautomatically verified when execution completes, this ensures that\nthe expected ordering that is defined in a engines compilation\nexecution graph is maintained during runtime execution.\n\nChange-Id: I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872\n'}, {'number': 7, 'created': '2014-11-19 23:29:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/b2a162f45740b6bbaa102eb59134647b2f01bd43', 'message': 'Add a testcase that can test + verify runtime task ordering\n\nIn order to make sure that all engines run all tests following\nthe pattern prescribed ordering add a new testcase that can be\neasily extended with new flow definitions that will be\nautomatically verified when execution completes, this ensures that\nthe expected ordering that is defined in a engines compilation\nexecution graph is maintained during runtime execution.\n\nChange-Id: I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872\n'}, {'number': 8, 'created': '2014-12-02 19:47:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/090492fe41b20ee70934d967a5b0b7a7534f84d2', 'message': 'Add a testcase that can test + verify runtime task ordering\n\nIn order to make sure that all engines run all tests following\nthe pattern prescribed ordering add a new testcase that can be\neasily extended with new flow definitions that will be\nautomatically verified when execution completes, this ensures that\nthe expected ordering that is defined in a engines compilation\nexecution graph is maintained during runtime execution.\n\nChange-Id: I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872\n'}, {'number': 9, 'created': '2014-12-05 06:17:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/7d0b44c284ab446099f68a60cace6dfd171a78ff', 'message': 'Add a testcase that can test + verify runtime task ordering\n\nIn order to make sure that all engines run all tests following\nthe pattern prescribed ordering add a new testcase that can be\neasily extended with new flow definitions that will be\nautomatically verified when execution completes, this ensures that\nthe expected ordering that is defined in a engines compilation\nexecution graph is maintained during runtime execution.\n\nChange-Id: I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872\n'}, {'number': 10, 'created': '2014-12-05 06:55:55.000000000', 'files': ['taskflow/test.py', 'taskflow/listeners/base.py', 'taskflow/tests/unit/test_engine_ordering.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/38143b65bd66677efb00b33bc752f55a931af0c3', 'message': 'Add a testcase that can test + verify runtime task ordering\n\nIn order to make sure that all engines run all tests following\nthe pattern prescribed ordering add a new testcase that can be\neasily extended with new flow definitions that will be\nautomatically verified when execution completes, this ensures that\nthe expected ordering that is defined in a engines compilation\nexecution graph is maintained during runtime execution.\n\nChange-Id: I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872\n'}]",0,135478,38143b65bd66677efb00b33bc752f55a931af0c3,20,2,10,1297,,,0,"Add a testcase that can test + verify runtime task ordering

In order to make sure that all engines run all tests following
the pattern prescribed ordering add a new testcase that can be
easily extended with new flow definitions that will be
automatically verified when execution completes, this ensures that
the expected ordering that is defined in a engines compilation
execution graph is maintained during runtime execution.

Change-Id: I82f031b86cb4c7df3ef93c8c33e0ebecf5e41872
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/78/135478/6 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/test.py', 'taskflow/listeners/base.py', 'taskflow/tests/unit/test_engine_ordering.py']",3,b4b16065b50e4eebc7810cf7623bc14496eda75d,,"# -*- coding: utf-8 -*- # Copyright (C) 2014 Yahoo! Inc. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import six from taskflow import engines from taskflow import exceptions from taskflow.listeners import base from taskflow.patterns import linear_flow as lf from taskflow.patterns import unordered_flow as uf from taskflow import states from taskflow import test from taskflow.tests import utils as test_utils from taskflow.utils import misc class OrderingListener(base.ListenerBase): def __init__(self, engine, task_listen_for=(misc.Notifier.ANY,), flow_listen_for=(misc.Notifier.ANY,)): super(OrderingListener, self).__init__( engine, task_listen_for=task_listen_for, flow_listen_for=flow_listen_for) self.ordering = [] def _task_receiver(self, state, details): self.ordering.append((details['task_name'], state)) class SampleAdderMeta(type): """"""Translates samples into test cases/methods."""""" def __new__(cls, name, parents, dct): def generate_test(name, sample): def test(self): pat = self._patternize(name, sample) e = self._make_engine(pat) e.compile() with OrderingListener(e) as capturer: e.run() self._validate_ordering(sample, capturer.ordering, e.compilation) return test samples = {} for p in parents: if hasattr(p, 'SAMPLES'): samples.update(getattr(p, 'SAMPLES')) samples.update(dct.get('SAMPLES', {})) for base_name, sample in six.iteritems(samples): test_name = ""test_%s"" % base_name dct[test_name] = generate_test(base_name, sample) return type.__new__(cls, name, parents, dct) @six.add_metaclass(SampleAdderMeta) class OrderingTest(object): # This dictionary contains a simple DSL for specifying patterns, that will # be turned into test cases that will run during the ordering tests. In the # future it should be possible these samples from a directory so that we # can easily add on new ordering validations... SAMPLES = { 'linear_basic': [('b', None), ('c', None)], 'linear_basic_nested': [('b', None), ('c', [('e', None)])], 'linear_basic_empty': [('b', None), ('c', []), ('d', None)], 'unordered_basic': set([('b', None), ('d', None)]), 'unordered_basic_empty': [('b', None), ('c', ()), ('d', None)], 'unordered_basic_nested': set([('b', None), ('c', (('c-1', None), ('c-2', None))), ('d', None)]), } def _make_engine(self, flow): raise exceptions.NotImplementedError(""_make_engine() must be"" "" overridden if an engine is"" "" desired"") def _validate_ordering(self, sample, execution_ordering, compilation): for n in compilation.execution_graph.nodes_iter(): n_index = execution_ordering.index((n.name, states.RUNNING)) for n_pred in compilation.execution_graph.bfs_predecessors_iter(n): n_pred_index = execution_ordering.index((n_pred.name, states.RUNNING)) self.assertGreater(n_pred_index, n_index, message=""Atom '%s' was discovered running"" "" before atom '%s'; %s < %s"" % (n, n_pred, n_pred_index, n_index)) def _patternize(self, name, root): if root is None: return test_utils.NoopTask(name) elif isinstance(root, (list, tuple)): f = lf.Flow(name) for n, child in root: f.add(self._patternize(n, child)) return f elif isinstance(root, set): f = uf.Flow(name) for n, child in root: f.add(self._patternize(n, child)) return f else: raise TypeError(""Unknown how to patternize"" "" %s; type '%s'"" % (name, type(root))) class SerialEngineOrderingTest(OrderingTest, test.TestCase): def _make_engine(self, flow): return engines.load(flow, engine='serial') class ParallelEngineOrderingTest(OrderingTest, test.TestCase): def _make_engine(self, flow): return engines.load(flow, engine='parallel') ",,133,2
openstack%2Fmanila~master~Ica171410bfdfa001da5188450a0c3740b4b8eb65,openstack/manila,master,Ica171410bfdfa001da5188450a0c3740b4b8eb65,Add support for volume types with Generic driver,MERGED,2014-11-10 09:02:20.000000000,2014-12-06 06:37:10.000000000,2014-12-06 06:37:09.000000000,"[{'_account_id': 3}, {'_account_id': 6491}, {'_account_id': 7102}, {'_account_id': 8851}, {'_account_id': 9067}, {'_account_id': 11878}]","[{'number': 1, 'created': '2014-11-10 09:02:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/35d85326c24152335496d9f91fb04b14d43487a4', 'message': 'Add support for volume types with Generic driver\n\nAdd configuration option cinder_volume_type which can be used\nto set volume type for cinder. If that option is not None than\ncinder volumes will be create with type from that option.\n\nImplements blueprint cinder-volume-types-with-generic-driver\n\nChange-Id: Ica171410bfdfa001da5188450a0c3740b4b8eb65\n'}, {'number': 2, 'created': '2014-11-13 16:44:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/360ef7145b8f3aff574b5d7af418b415c3d47730', 'message': 'Add support for volume types with Generic driver\n\nAdd configuration option cinder_volume_type which can be used\nto set volume type for cinder. If that option is not None than\ncinder volumes will be create with type from that option.\n\nImplements blueprint cinder-volume-types-with-generic-driver\n\nChange-Id: Ica171410bfdfa001da5188450a0c3740b4b8eb65\n'}, {'number': 3, 'created': '2014-11-18 22:08:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/611de59671cf3194c1913276f099de68da03a5d5', 'message': 'Add support for volume types with Generic driver\n\nAdd configuration option cinder_volume_type which can be used\nto set volume type for cinder. If that option is not None than\ncinder volumes will be create with type from that option.\n\nImplements blueprint cinder-volume-types-with-generic-driver\n\nChange-Id: Ica171410bfdfa001da5188450a0c3740b4b8eb65\n'}, {'number': 4, 'created': '2014-11-18 22:20:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/f80fe59e60527876cedbd3c9cf3688df3aab1764', 'message': 'Add support for volume types with Generic driver\n\nAdd configuration option cinder_volume_type which can be used\nto set volume type for cinder. If that option is not None then\ncinder volumes will be created with type from that option.\n\nImplements blueprint cinder-volume-types-with-generic-driver\n\nChange-Id: Ica171410bfdfa001da5188450a0c3740b4b8eb65\n'}, {'number': 5, 'created': '2014-12-03 13:20:08.000000000', 'files': ['manila/volume/cinder.py', 'manila/tests/share/drivers/test_generic.py', 'manila/tests/volume/test_cinder.py', 'manila/share/drivers/generic.py'], 'web_link': 'https://opendev.org/openstack/manila/commit/a7a517dec9fa6143b01f9e26280b4bff3ebd1ee1', 'message': 'Add support for volume types with Generic driver\n\nAdd configuration option cinder_volume_type which can be used\nto set volume type for cinder. If that option is not None then\ncinder volumes will be created with type from that option.\n\nImplements blueprint cinder-volume-types-with-generic-driver\n\nChange-Id: Ica171410bfdfa001da5188450a0c3740b4b8eb65\n'}]",18,133408,a7a517dec9fa6143b01f9e26280b4bff3ebd1ee1,31,6,5,6529,,,0,"Add support for volume types with Generic driver

Add configuration option cinder_volume_type which can be used
to set volume type for cinder. If that option is not None then
cinder volumes will be created with type from that option.

Implements blueprint cinder-volume-types-with-generic-driver

Change-Id: Ica171410bfdfa001da5188450a0c3740b4b8eb65
",git fetch https://review.opendev.org/openstack/manila refs/changes/08/133408/2 && git format-patch -1 --stdout FETCH_HEAD,"['manila/volume/cinder.py', 'manila/tests/share/drivers/test_generic.py', 'manila/tests/volume/test_cinder.py', 'manila/share/drivers/generic.py']",4,35d85326c24152335496d9f91fb04b14d43487a4,bp/cinder-volume-types-with-generic-driver," cfg.StrOpt('cinder_volume_type', default=None, help='Cinder volume type for all volumes created by driver'), snapshot=volume_snapshot, volume_type=self.configuration.cinder_volume_type)", snapshot=volume_snapshot),27,4
openstack%2Fcinder~master~I2eeb44e768871e02e407f93243aabef993f96d53,openstack/cinder,master,I2eeb44e768871e02e407f93243aabef993f96d53,Adding volume driver for X-IO ISE,MERGED,2014-08-22 06:15:06.000000000,2014-12-06 06:24:28.000000000,2014-11-27 03:42:01.000000000,"[{'_account_id': 3}, {'_account_id': 25}, {'_account_id': 170}, {'_account_id': 1207}, {'_account_id': 1736}, {'_account_id': 2243}, {'_account_id': 4355}, {'_account_id': 4523}, {'_account_id': 5538}, {'_account_id': 5997}, {'_account_id': 6043}, {'_account_id': 6491}, {'_account_id': 7198}, {'_account_id': 9008}, {'_account_id': 9416}, {'_account_id': 10271}, {'_account_id': 10503}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 12778}, {'_account_id': 12779}, {'_account_id': 12780}, {'_account_id': 13049}]","[{'number': 1, 'created': '2014-08-22 06:15:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/0151dcd10174917a7c3d2be6bf42392dce7aef9c', 'message': 'Adding volume driver for X-IO ISE\n\nThis checkin adds volume driver for X-IO ISE storage device.\nDriver has support for both ISCSI and FC.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 2, 'created': '2014-08-22 21:36:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/e61b6d7bb1becf45f7ca1adf02d5df8daf1989ce', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 3, 'created': '2014-08-22 23:30:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/afafede7bf0b337be8635b4714b5a3d40d918fc7', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 4, 'created': '2014-08-23 02:54:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/063757eab20cda6df9d1b92522df8329053c19ba', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 5, 'created': '2014-08-25 16:19:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/dc1dd32b31b474a8f2078aae6ee69e29bdf25cdf', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 6, 'created': '2014-08-25 17:46:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2b4dd259e879bd9e074181278eff38762ca64fcb', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 7, 'created': '2014-08-25 23:35:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/728b609891891d1ba0a05bf711848b5a3c1e92f7', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 8, 'created': '2014-08-29 13:00:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6d817dbe08619e14d06a14632a2a3bc803f28334', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 9, 'created': '2014-08-29 15:27:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/eb2845ca143a98c803e52a88d073a49de6552838', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 10, 'created': '2014-08-29 17:33:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b02fa0bb50928d3edd53f0b8c299807720da3098', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nThis was originally started under change 71598.\nhttps://review.openstack.org/#/c/71598/\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 11, 'created': '2014-09-03 15:10:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/78f60f4156d4e7a9cd8088cc45e3680f4ce4bf0d', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 12, 'created': '2014-09-04 20:16:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b26d657f5ea5151c29469ef6cce720a2b1cb53b9', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 13, 'created': '2014-09-22 22:23:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/026c8668cf86646072f46b25bdcebe0591741e35', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 14, 'created': '2014-09-22 22:29:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/ddbab4720658bac1840882b89fee4bba08a040c5', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 15, 'created': '2014-10-16 20:36:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/48b5e1d7ea01cb111485b00a71254332864fa5d9', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 16, 'created': '2014-10-17 15:25:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/99db6c1d9496b094beb972ea407e4f3ec7104165', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 17, 'created': '2014-10-17 17:38:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b4af3b6b973eb31bf5724541d515394176e3b3ee', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 18, 'created': '2014-10-21 16:47:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/7e30a55fc5777c690d55539c75d4a768bec880c6', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 19, 'created': '2014-10-23 23:53:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/d04cb05378de4b4a29179f19f189c4a11f356fa5', 'message': 'Adding volume driver for X-IO ISE\n\nUpdated checkin with fixes for issues found by Jenkins.\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results for ISCSI driver:\nhttps://launchpadlibrarian.net/182934501/cert-test-pass-iscsi\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 20, 'created': '2014-10-27 15:20:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/619d1097a1cdad44e5288aa9c9a8e1580f266dc0', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 21, 'created': '2014-10-27 15:23:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/f14a72523ae306b8b0b782fabdec46a5a62bb5b3', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 22, 'created': '2014-10-31 18:20:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2af4f0cc2e4d34c01b4a4722cc1eb78f46509c50', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 23, 'created': '2014-11-03 17:38:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/52942741a6133bd38382cfbd35a90be031f14308', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 24, 'created': '2014-11-20 04:59:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a0100c7c14c3be1bfe4eb6932c0cee4c1d526229', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 25, 'created': '2014-11-20 06:17:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/181dc5b9eead165c551691cef4f091e62219399c', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 26, 'created': '2014-11-20 17:35:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c80a97e3685d5612d080836d192b5629081c924c', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 27, 'created': '2014-11-20 18:45:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/bfa158115b6b1b6add4a51f9a925b2a7365f7c64', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 28, 'created': '2014-11-21 21:55:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/70f523d4817f50f9515ade1486b81619b490e5fd', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\n'}, {'number': 29, 'created': '2014-11-23 06:45:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/e5ee8349092bb279dc1ea29c7520bf7129878014', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nBlueprints:\nhttps://blueprints.launchpad.net/cinder/+spec/xio-iscsi-fc-volume-driver\nhttps://blueprints.launchpad.net/cinder/+spec/xio-volume-driver-1-1\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\nImplements: blueprint xio-volume-driver-1-1\n'}, {'number': 30, 'created': '2014-11-24 23:05:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/41d6bb0578cf14b7921e6b7cd89d32773305f73f', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nBlueprints:\nhttps://blueprints.launchpad.net/cinder/+spec/xio-iscsi-fc-volume-driver\nhttps://blueprints.launchpad.net/cinder/+spec/xio-volume-driver-1-1\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\nImplements: blueprint xio-volume-driver-1-1\n'}, {'number': 31, 'created': '2014-11-24 23:28:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/af17aab6fde2652c5040b8a762ac37e51ed22829', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nBlueprints:\nhttps://blueprints.launchpad.net/cinder/+spec/xio-iscsi-fc-volume-driver\nhttps://blueprints.launchpad.net/cinder/+spec/xio-volume-driver-1-1\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\nImplements: blueprint xio-volume-driver-1-1\n'}, {'number': 32, 'created': '2014-11-25 00:20:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1d00e4b16675b68ca5e22624d5ec79ab077ab40a', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nBlueprints:\nhttps://blueprints.launchpad.net/cinder/+spec/xio-iscsi-fc-volume-driver\nhttps://blueprints.launchpad.net/cinder/+spec/xio-volume-driver-1-1\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\nImplements: blueprint xio-volume-driver-1-1\n'}, {'number': 33, 'created': '2014-11-26 17:05:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/95da34465aeb5bbb56c134282271d6e91a73176a', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nThis change also addresses version 1.1.0 of the driver that\nadds support for:\n- QoS\n- Volume affinity\n- Thin\n- Retype\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\nImplements: blueprint xio-volume-driver-1-1\n'}, {'number': 34, 'created': '2014-11-26 17:31:26.000000000', 'files': ['etc/cinder/cinder.conf.sample', 'cinder/volume/driver.py', 'cinder/volume/drivers/xio.py', 'cinder/exception.py', 'cinder/tests/test_xio.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/6e289f33869a7d57f0cc2795dcc99b78ea025655', 'message': 'Adding volume driver for X-IO ISE\n\nDriver is implemented using one base class XIOISEDriver and two\nclasses, one for each storage protocol.\n\nThis change also addresses version 1.1.0 of the driver that\nadds support for:\n- QoS\n- Volume affinity\n- Thin\n- Retype\n\nDriver Cert results:\nhttps://bugs.launchpad.net/cinder/+bug/1360078\n\nUnit test tests all required interfaces for both driver types.\n\nChange-Id: I2eeb44e768871e02e407f93243aabef993f96d53\nImplements: blueprint xio-iscsi-fc-volume-driver\nImplements: blueprint xio-volume-driver-1-1\n'}]",69,116186,6e289f33869a7d57f0cc2795dcc99b78ea025655,213,29,34,10271,,,0,"Adding volume driver for X-IO ISE

Driver is implemented using one base class XIOISEDriver and two
classes, one for each storage protocol.

This change also addresses version 1.1.0 of the driver that
adds support for:
- QoS
- Volume affinity
- Thin
- Retype

Driver Cert results:
https://bugs.launchpad.net/cinder/+bug/1360078

Unit test tests all required interfaces for both driver types.

Change-Id: I2eeb44e768871e02e407f93243aabef993f96d53
Implements: blueprint xio-iscsi-fc-volume-driver
Implements: blueprint xio-volume-driver-1-1
",git fetch https://review.opendev.org/openstack/cinder refs/changes/86/116186/32 && git format-patch -1 --stdout FETCH_HEAD,"['etc/cinder/cinder.conf.sample', 'cinder/volume/drivers/xio.py', 'cinder/exception.py', 'cinder/tests/test_xio.py']",4,0151dcd10174917a7c3d2be6bf42392dce7aef9c,bp/xio-iscsi-fc-volume-driver,"# Copyright (c) 2014 X-IO Technologies. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import mock import sys import string from oslo.config import cfg from cinder import exception from cinder import test from cinder.openstack.common import log as logging from cinder.volume.drivers import xio from cinder.volume.drivers.xio import xio_opts from cinder.volume.drivers.san.san import san_opts LOG = logging.getLogger(""cinder.volume.driver"") ISE_IP1 = '10.12.12.1' ISE_IP2 = '10.11.12.2' ISE_ISCSI_IP = '1.2.3.4' ISE_GID = 'isegid' ISE_IQN = ISE_GID ISE_WWN = ISE_GID VOLUME_SIZE = 10 NEW_VOLUME_SIZE = 20 VOLUME1 = { 'id' : '1', 'name' : 'volume1', 'size' : VOLUME_SIZE } VOLUME2 = { 'id' : '2', 'name' : 'volume2', 'size' : VOLUME_SIZE } SNAPSHOT1 = { 'name' : 'snapshot1', 'volume_name' : VOLUME1['name'] } CLONE1 = { 'id' : '3', 'name' : 'clone1', 'size' : VOLUME_SIZE } HOST1 = 'host1' ISCSI_CONN = {'initiator': 'init_iqn', 'host' : HOST1 } FC_CONN = {'wwpns': ['init_wwn1', 'init_wwn2'], 'host' : HOST1 } ISE_HTTP_IP = 'http://' + ISE_IP1 ISE_VOLUME1_LOCATION = '/storage/volumes/volume1' ISE_VOLUME1_LOCATION_URL = ISE_HTTP_IP + ISE_VOLUME1_LOCATION ISE_VOLUME2_LOCATION = '/storage/volumes/volume2' ISE_VOLUME2_LOCATION_URL = ISE_HTTP_IP + ISE_VOLUME2_LOCATION ISE_SNAPSHOT_LOCATION = '/storage/volumes/snapshot1' ISE_SNAPSHOT_LOCATION_URL = ISE_HTTP_IP + ISE_SNAPSHOT_LOCATION ISE_CLONE_LOCATION = '/storage/volumes/clone1' ISE_CLONE_LOCATION_URL = ISE_HTTP_IP + ISE_CLONE_LOCATION ISE_ALLOCATION_LOCATION = '/storage/allocations/a1' ISE_ALLOCATION_LOCATION_URL = ISE_HTTP_IP + ISE_ALLOCATION_LOCATION ISE_GET_QUERY_XML =\ """"""<array> <globalid>ABC12345</globalid> <controllers> <controller> <ipaddress>%s</ipaddress> <rank value=""1""/> </controller> <controller> <ipaddress>%s</ipaddress> <rank value=""0""/> </controller> </controllers> </array>"""""" % (ISE_IP1, ISE_IP2) ISE_GET_QUERY_RESP =\ { 'status' : 200, 'location' : '', 'content' : "" "".join(ISE_GET_QUERY_XML.split()) } ISE_GET_STORAGE_POOLS_XML =\ """""" <pools> <pool> <id>1</id> <available total=""25915""></available> <used total=""0""></used> </pool> </pools> """""" ISE_GET_STORAGE_POOLS_RESP =\ { 'status' : 200, 'location' : '', 'content' : "" "".join(ISE_GET_STORAGE_POOLS_XML.split()) } ISE_GET_VOL1_STATUS_XML =\ """"""<volumes> <volume self=""%s""> <status value=""0"" string=""Operational""> <details> <detail>Prepared</detail> </details> </status> </volume> </volumes>"""""" % (ISE_VOLUME1_LOCATION_URL) ISE_GET_VOL1_STATUS_RESP =\ { 'status' : 200, 'location' : 'u%s' % ISE_VOLUME1_LOCATION_URL, 'content' : "" "".join(ISE_GET_VOL1_STATUS_XML.split()) } ISE_GET_VOL2_STATUS_XML =\ """"""<volumes> <volume self=""%s""> <status value=""0"" string=""Operational""> <details> <detail>Prepared</detail> </details> </status> </volume> </volumes>"""""" % (ISE_VOLUME2_LOCATION_URL) ISE_GET_VOL2_STATUS_RESP =\ { 'status' : 200, 'location' : 'u%s' % ISE_VOLUME2_LOCATION_URL, 'content' : "" "".join(ISE_GET_VOL2_STATUS_XML.split()) } ISE_GET_SNAP1_STATUS_XML =\ """"""<volumes> <volume self=""%s""> <status value=""0"" string=""Operational""> <details> <detail>Prepared</detail> </details> </status> </volume> </volumes>"""""" % (ISE_SNAPSHOT_LOCATION_URL) ISE_GET_SNAP1_STATUS_RESP =\ { 'status' : 200, 'location' : 'u%s' % ISE_SNAPSHOT_LOCATION_URL, 'content' : "" "".join(ISE_GET_SNAP1_STATUS_XML.split()) } ISE_GET_CLONE1_STATUS_XML =\ """"""<volumes> <volume self=""%s""> <status value=""0"" string=""Operational""> <details> <detail>Prepared</detail> </details> </status> </volume> </volumes>"""""" % (ISE_CLONE_LOCATION_URL) ISE_GET_CLONE1_STATUS_RESP =\ { 'status' : 200, 'location' : 'u%s' % ISE_CLONE_LOCATION_URL, 'content' : "" "".join(ISE_GET_CLONE1_STATUS_XML.split()) } ISE_CREATE_VOLUME_XML =\ """"""<volume/>"""""" ISE_CREATE_VOLUME_RESP =\ { 'status' : 201, 'location' : ISE_VOLUME1_LOCATION_URL, 'content' : "" "".join(ISE_CREATE_VOLUME_XML.split()) } ISE_GET_IONETWORKS_XML =\ """""" <chap> <chapin value=""0"" string=""disabled""> <username/> <password/> </chapin> <chapout value=""0"" string=""disabled""> <username/> <password/> </chapout> </chap> """""" ISE_GET_IONETWORKS_RESP =\ { 'status' : 200, 'location' : '', 'content' : "" "".join(ISE_GET_IONETWORKS_XML.split()) } ISE_DELETE_VOLUME_XML =\ """""" <volumes/> """""" ISE_DELETE_VOLUME_RESP =\ { 'status' : 204, 'location' : '', 'content' : "" "".join(ISE_DELETE_VOLUME_XML.split()) } ISE_GET_ALLOC_WITH_EP_XML =\ """""" <allocations> <allocation self=""%s""> <volume> <volumename>%s</volumename> </volume> <endpoints> <hostname>%s</hostname> </endpoints> <lun>1</lun> </allocation> </allocations> """""" % (ISE_ALLOCATION_LOCATION_URL,\ VOLUME1['name'], HOST1) ISE_GET_ALLOC_WITH_EP_RESP =\ { 'status' : 200, 'location': ISE_ALLOCATION_LOCATION_URL, 'content' : "" "".join(ISE_GET_ALLOC_WITH_EP_XML.split()) } ISE_DELETE_ALLOC_XML =\ """""" <allocations/> """""" ISE_DELETE_ALLOC_RESP =\ { 'status' : 204, 'location' : '', 'content' : "" "".join(ISE_DELETE_ALLOC_XML.split()) } ISE_GET_HOSTS_NOHOST_XML =\ """"""<hosts self=""http://ip/storage/hosts""/>"""""" ISE_GET_HOSTS_NOHOST_RESP =\ { 'status' : 200, 'location' : '', 'content' : "" "".join(ISE_GET_HOSTS_NOHOST_XML.split()) } ISE_GET_HOSTS_HOST1_XML =\ """""" <hosts self=""http://ip/storage/hosts""> <host self=""http://ip/storage/hosts/1""> <name>host1</name> <id>1</id> <endpoints self=""http://ip/storage/endpoints""> <endpoint self=""http://ip/storage/endpoints/ep1""> <globalid>hostgid</globalid> </endpoint> </endpoints> </host> </hosts> """""" ISE_GET_HOSTS_HOST1_RESP =\ { 'status' : 200, 'location' : '', 'content' : "" "".join(ISE_GET_HOSTS_HOST1_XML.split()) } ISE_CREATE_HOST_XML =\ """"""<hosts self=""http://ip/storage/hosts""/>"""""" ISE_CREATE_HOST_RESP =\ { 'status' : 201, 'location' : 'http://ip/storage/hosts/host1', 'content' : "" "".join(ISE_CREATE_HOST_XML.split()) } ISE_CREATE_ALLOC_XML =\ """"""<allocations self=""http://ip/storage/allocations""/>"""""" ISE_CREATE_ALLOC_RESP =\ { 'status' : 201, 'location' : ISE_ALLOCATION_LOCATION_URL, 'content' : "" "".join(ISE_CREATE_ALLOC_XML.split()) } ISE_GET_ENDPOINTS_XML =\ """"""<endpoints self=""http://ip/storage/endpoints""> <endpoint type=""array"" self=""http://ip/storage/endpoints/isegid""> <globalid>isegid</globalid> <protocol>iSCSI</protocol> <array self=""http://ip/storage/arrays/ise1""> <globalid>ise1</globalid> </array> <host/> <allocations self=""http://ip/storage/allocations""> <allocation self=""%s""> <globalid> a1 </globalid> </allocation> </allocations> </endpoint> <endpoint type=""array"" self=""http://ip/storage/endpoints/isegid""> <globalid>isegid</globalid> <protocol>Fibre Channel</protocol> <array self=""http://ip/storage/arrays/ise1""> <globalid>ise1</globalid> </array> <host/> <allocations self=""http://ip/storage/allocations""> <allocation self=""%s""> <globalid> a1 </globalid> </allocation> </allocations> </endpoint> </endpoints>"""""" % (ISE_ALLOCATION_LOCATION_URL, ISE_ALLOCATION_LOCATION_URL) ISE_GET_ENDPOINTS_RESP =\ { 'status' : 200, 'location' : '', 'content' : "" "".join(ISE_GET_ENDPOINTS_XML.split()) } ISE_GET_CONTROLLERS_XML =\ """""" <controllers self=""http://ip/storage/arrays/controllers""> <controller> <status/> <ioports> <ioport> <ipaddresses> <ipaddress>%s</ipaddress> </ipaddresses> <endpoint> <globalid>isegid</globalid> </endpoint> </ioport> </ioports> <fcports> <fcport> <wwn>isegid</wwn> </fcport> </fcports> </controller> </controllers> """""" % (ISE_ISCSI_IP) ISE_GET_CONTROLLERS_RESP =\ { 'status' : 200, 'location' : '', 'content' : "" "".join(ISE_GET_CONTROLLERS_XML.split()) } ISE_CREATE_SNAPSHOT_XML =\ """"""<snapshot/>"""""" ISE_CREATE_SNAPSHOT_RESP =\ { 'status' : 201, 'location' : ISE_SNAPSHOT_LOCATION_URL, 'content' : "" "".join(ISE_CREATE_SNAPSHOT_XML.split()) } ISE_PREP_SNAPSHOT_XML =\ """"""<snapshot/>"""""" ISE_PREP_SNAPSHOT_RESP =\ { 'status' : 202, 'location' : ISE_SNAPSHOT_LOCATION_URL, 'content' : "" "".join(ISE_PREP_SNAPSHOT_XML.split()) } ISE_EXTEND_VOLUME_XML =\ """"""<volume/>"""""" ISE_EXTEND_VOLUME_RESP =\ { 'status' : 201, 'location' : ISE_VOLUME1_LOCATION_URL, 'content' : "" "".join(ISE_EXTEND_VOLUME_XML.split()) } REST_RESPONSES =\ { 'POST': [('*/storage/arrays/*/volumes/*', ISE_CREATE_VOLUME_RESP), ('*/storage/arrays/*/hosts', ISE_CREATE_HOST_RESP), ('*/storage/arrays/*/allocations', ISE_CREATE_ALLOC_RESP), ('*%s'%(ISE_VOLUME1_LOCATION), ISE_PREP_SNAPSHOT_RESP), ('*%s'%(ISE_SNAPSHOT_LOCATION), ISE_PREP_SNAPSHOT_RESP),], 'PUT': [('*%s'%(ISE_SNAPSHOT_LOCATION), ISE_CREATE_SNAPSHOT_RESP), ('*%s'%(ISE_CLONE_LOCATION), ISE_CREATE_SNAPSHOT_RESP), ('*%s'%(ISE_VOLUME1_LOCATION), ISE_EXTEND_VOLUME_RESP),], 'DELETE': [('*%s'%(ISE_VOLUME1_LOCATION), ISE_DELETE_VOLUME_RESP), ('*%s'%(ISE_SNAPSHOT_LOCATION), ISE_DELETE_VOLUME_RESP), ('*%s'%(ISE_ALLOCATION_LOCATION), ISE_DELETE_ALLOC_RESP)], 'GET': [('*/query', ISE_GET_QUERY_RESP), ('*/storage/pools', ISE_GET_STORAGE_POOLS_RESP), ('*/storage/arrays/*/volumes/%s'%VOLUME1['name'],\ ISE_GET_VOL1_STATUS_RESP), ('*/storage/arrays/*/volumes/%s'%VOLUME2['name'],\ ISE_GET_VOL2_STATUS_RESP), ('*/storage/arrays/*/volumes/%s'%SNAPSHOT1['name'],\ ISE_GET_SNAP1_STATUS_RESP), ('*/storage/arrays/*/volumes/%s'%CLONE1['name'],\ ISE_GET_CLONE1_STATUS_RESP), ('*/storage/arrays/*/ionetworks', ISE_GET_IONETWORKS_RESP), ('*/storage/arrays/*/allocations', ISE_GET_ALLOC_WITH_EP_RESP), ('*/storage/arrays/*/hosts', ISE_GET_HOSTS_NOHOST_RESP), ('*%s'%(ISE_ALLOCATION_LOCATION), ISE_GET_ALLOC_WITH_EP_RESP), ('*/storage/arrays/*/endpoints', ISE_GET_ENDPOINTS_RESP), ('*/storage/arrays/*/controllers', ISE_GET_CONTROLLERS_RESP),] } DRIVER = ""cinder.volume.drivers.xio.XIOISEDriver"" @mock.patch(DRIVER + ""._opener"", autospec=True) class XIOISEDriverTestCase(object): # Test cases for X-IO volume driver def setUp(self): super(XIOISEDriverTestCase, self).setUp() self.configuration = mock.Mock() self.configuration.san_ip = ISE_IP1 self.configuration.san_user = 'fakeuser' self.configuration.san_password = 'fakepass' self.configuration.iscsi_ip_address = ISE_ISCSI_IP self.configuration.ise_use_ssl = False def setUpDriver(self, protocol): self.connector = None if protocol == 'iscsi': self.configuration.ise_protocol = 'iscsi' self.connector = ISCSI_CONN self.hostgid = self.connector['initiator'] self.driver = xio.XIOISEISCSIDriver(configuration=self.configuration) elif protocol == 'fc': self.configuration.ise_protocol = 'fibre_channel' self.connector = FC_CONN self.hostgid = self.connector['wwpns'][0] self.driver = xio.XIOISEFCDriver(configuration=self.configuration) def pick_return(self, *args, **kwargs): method = args[1] url = args[2] modurl = url modurl = modurl.split('://')[1] if '?' in modurl: (modurl, param_str) = modurl.split('?') params = dict((k.strip(), v.strip()) for k,v in (item.split('=') for item in param_str.split('&'))) if 'name' in params: modurl += '/%s' % params['name'] ops_list = REST_RESPONSES[method] for (templ_url, response) in ops_list: if self._compare_url(modurl, templ_url): if 'hostgid' in response['content']: string.replace(\ response['content'], 'hostgid', self.hostgid) return response err_msg = ""Did not find a response for %s %s"" % (method, url) self.driver.XIODriverException(err_msg) def _compare_url(self, url, template_url): items = url.split('/') titems = template_url.split('/') for (i, titem) in enumerate(titems): if titem != '*' and titem != items[i]: return False return True def test_do_setup(self, mock_req): mock_req.side_effect = self.pick_return self.driver.do_setup(None) def test_get_volume_stats(self, mock_req): mock_req.side_effect = self.pick_return self.driver.get_volume_stats(True) def test_create_volume(self, mock_req): mock_req.side_effect = self.pick_return self.driver.create_volume(VOLUME1) def test_delete_volume(self, mock_req): mock_req.side_effect = self.pick_return self.driver.delete_volume(VOLUME1) def test_initialize_connection(self, mock_req): mock_req.side_effect = self.pick_return exp_result = {} if self.configuration.ise_protocol == 'iscsi': exp_result = {""driver_volume_type"": ""iscsi"", ""data"": {\ ""target_lun"": '1', ""volume_id"": '1', ""access_mode"": 'rw', ""target_discovered"": False, ""target_iqn"": ISE_IQN, ""target_portal"": ISE_ISCSI_IP + "":3260"", }, } elif self.configuration.ise_protocol == 'fibre_channel': exp_result = {""driver_volume_type"": ""fibre_channel"", ""data"": {\ ""target_lun"": '1', ""volume_id"": '1', ""access_mode"": 'rw', ""target_discovered"": True, ""target_wwn"": ISE_WWN, }, } act_result =\ self.driver.initialize_connection(VOLUME1, self.connector) self.assertDictMatch(exp_result, act_result) def test_terminate_connection(self, mock_req): mock_req.side_effect = self.pick_return self.driver.terminate_connection(VOLUME1, self.connector) def test_create_snapshot(self, mock_req): mock_req.side_effect = self.pick_return self.driver.create_snapshot(SNAPSHOT1) def test_delete_snapshot(self, mock_req): mock_req.side_effect = self.pick_return self.driver.delete_snapshot(SNAPSHOT1) def test_clone_volume(self, mock_req): mock_req.side_effect = self.pick_return self.driver.create_cloned_volume(CLONE1, VOLUME1) def test_create_volume_from_snapshot(self, mock_req): mock_req.side_effect = self.pick_return self.driver.create_volume_from_snapshot(VOLUME1, SNAPSHOT1) def test_extend_volume(self, mock_req): mock_req.side_effect = self.pick_return self.driver.extend_volume(VOLUME1, NEW_VOLUME_SIZE) class XIOISEISCSIDriverTestCase(XIOISEDriverTestCase, test.TestCase): def setUp(self): super(XIOISEISCSIDriverTestCase, self).setUp() self.setUpDriver('iscsi') class XIOISEFCDriverTestCase(XIOISEDriverTestCase, test.TestCase): def setUp(self): super(XIOISEFCDriverTestCase, self).setUp() self.setUpDriver('fc') ",,1792,0
openstack%2Fglance~master~I795e004eac3f032217ff9cb0047f1976306fbb71,openstack/glance,master,I795e004eac3f032217ff9cb0047f1976306fbb71,Fix getaddrinfo if dnspython is installed,MERGED,2014-06-19 02:34:42.000000000,2014-12-06 06:07:13.000000000,2014-12-06 06:07:12.000000000,"[{'_account_id': 3}, {'_account_id': 24}, {'_account_id': 2537}, {'_account_id': 2711}, {'_account_id': 5202}, {'_account_id': 6484}, {'_account_id': 6493}, {'_account_id': 6549}, {'_account_id': 6737}, {'_account_id': 7823}, {'_account_id': 8127}, {'_account_id': 8158}, {'_account_id': 8759}, {'_account_id': 8871}, {'_account_id': 8959}, {'_account_id': 8992}, {'_account_id': 9751}, {'_account_id': 11356}, {'_account_id': 11391}, {'_account_id': 13717}]","[{'number': 1, 'created': '2014-06-19 02:34:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/ff36cddfc0f00f21c1db0c2bcb5c6035c2b8eb14', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 2, 'created': '2014-06-19 02:38:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/aa7a9fa07acb91bdcbd93a07acd08dda0367465a', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nResolves bug 1331885.\n\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 3, 'created': '2014-06-20 15:58:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/99e903da276925e58be444ae177b920d6236081e', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nResolves bug 1331885.\n\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 4, 'created': '2014-06-20 16:02:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/4d09e60ea5a1728877a17503cb5e529dc0ce1289', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nResolves bug 1331885.\n\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 5, 'created': '2014-06-21 05:55:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/92a4a1126364f733ff4ca4babf1eaf1f438dba74', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nResolves bug 1331885.\n\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 6, 'created': '2014-06-26 14:13:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/1d32c437709722e30be1e6626e8e4116dc6a380c', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nResolves bug 1331885.\n\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 7, 'created': '2014-07-15 13:44:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/fcfbc7d41bade96cac035e68e48322d2263beadc', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 8, 'created': '2014-07-15 17:53:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/a6702e912a80827212f173e4e4d3cdb0e8151d86', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 9, 'created': '2014-10-22 13:26:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/05501cbd4021ebbae33de9573131ccaff9bf7b8e', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 10, 'created': '2014-11-11 19:14:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/d12ce666080fbbf2165342c5451d709af36e2d70', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 11, 'created': '2014-11-12 15:15:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/3cea9b13b6818704ed25864d9c82f488b967a799', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 12, 'created': '2014-11-12 15:35:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/88d18facdc3be51dcc0ef0cff737df1e6410a9c9', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 13, 'created': '2014-11-13 03:37:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/3afaffd266a90ee5bb8eda1fa12872b46f0305d3', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 14, 'created': '2014-11-27 16:49:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/4616bc13e970fa83d43bff6bebf75802c989d017', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 15, 'created': '2014-11-28 09:16:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/2ec59fe03b82c845d08bea4dad1568594b7c4e4e', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 16, 'created': '2014-11-28 15:53:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/7e30e7d2ac6d02dd176b3935d43bd1a16a1e80a7', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 17, 'created': '2014-11-28 17:03:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/cb1081300158010d72d987e7881e4abec9da0332', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 18, 'created': '2014-12-01 10:11:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/387a19e9f1ca433f94aa130f633fcdaf59bdaeb2', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 19, 'created': '2014-12-01 11:20:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/bcf8e2a0a80d5f443eb241c74c9ab3d0ac4b9ada', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nThis patch has also been applied to other Openstack\nprojects such as nova (f2e49ad3) and keystone (3afd9791)\nin order to work around the same problem and is\nintended to be used in the same way.\n\nCo-authored-by: Hui Xiang <hui.xiang@canonical.com>\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 20, 'created': '2014-12-03 23:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/13b68632eba4277779a1df1942ee8ef142c985b7', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nThis patch has also been applied to other Openstack\nprojects such as nova (f2e49ad3) and keystone (3afd9791)\nin order to work around the same problem and is\nintended to be used in the same way.\n\nCo-authored-by: Hui Xiang <hui.xiang@canonical.com>\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 21, 'created': '2014-12-04 12:27:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/47980193cf50835cf1f2a94652fee97611840911', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nThis patch has also been applied to other Openstack\nprojects such as nova (f2e49ad3) and keystone (3afd9791)\nin order to work around the same problem and is\nintended to be used in the same way.\n\nCo-authored-by: Hui Xiang <hui.xiang@canonical.com>\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 22, 'created': '2014-12-04 15:07:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/a0310f7d9fd03a636cab2c41564b23f5c1eb46e8', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nThis patch has also been applied to other Openstack\nprojects such as nova (f2e49ad3) and keystone (3afd9791)\nin order to work around the same problem and is\nintended to be used in the same way.\n\nCo-authored-by: Hui Xiang <hui.xiang@canonical.com>\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}, {'number': 23, 'created': '2014-12-05 09:29:55.000000000', 'files': ['glance/cmd/__init__.py', 'glance/tests/unit/common/test_wsgi_ipv6.py', 'glance/tests/__init__.py'], 'web_link': 'https://opendev.org/openstack/glance/commit/bd2a8422d3e8c975d023982c038a59ca0257c6dd', 'message': ""Fix getaddrinfo if dnspython is installed\n\nIf dnspython is present in the environment then eventlet\nmonkeypatches socket.getaddrinfo() with an implementation\nwhich doesn't work for IPv6.\n\nThis patch has also been applied to other Openstack\nprojects such as nova (f2e49ad3) and keystone (3afd9791)\nin order to work around the same problem and is\nintended to be used in the same way.\n\nCo-authored-by: Hui Xiang <hui.xiang@canonical.com>\nCloses-Bug: 1331885\nChange-Id: I795e004eac3f032217ff9cb0047f1976306fbb71\n""}]",43,101079,bd2a8422d3e8c975d023982c038a59ca0257c6dd,140,20,23,7823,,,0,"Fix getaddrinfo if dnspython is installed

If dnspython is present in the environment then eventlet
monkeypatches socket.getaddrinfo() with an implementation
which doesn't work for IPv6.

This patch has also been applied to other Openstack
projects such as nova (f2e49ad3) and keystone (3afd9791)
in order to work around the same problem and is
intended to be used in the same way.

Co-authored-by: Hui Xiang <hui.xiang@canonical.com>
Closes-Bug: 1331885
Change-Id: I795e004eac3f032217ff9cb0047f1976306fbb71
",git fetch https://review.opendev.org/openstack/glance refs/changes/79/101079/14 && git format-patch -1 --stdout FETCH_HEAD,"['glance/cmd/__init__.py', 'glance/tests/__init__.py']",2,ff36cddfc0f00f21c1db0c2bcb5c6035c2b8eb14,bug/1331885,"import os import sys # NOTE All of this is because if dnspython is present in your # environment then eventlet monkeypatches socket.getaddrinfo() with an # implementation which doesn't work for IPv6. What we're checking here is # that the magic environment variable was set when the import happened. if ('eventlet' in sys.modules and os.environ.get('EVENTLET_NO_GREENDNS', '').lower() != 'yes'): raise ImportError('eventlet imported before glance/cmd/__init__ ' '(env var set to %s)' % os.environ.get('EVENTLET_NO_GREENDNS')) os.environ['EVENTLET_NO_GREENDNS'] = 'yes' ",,30,0
openstack%2Fdesignate~master~I745b666588002fb7fa4925d3ea3a441265c636bb,openstack/designate,master,I745b666588002fb7fa4925d3ea3a441265c636bb,Imported Translations from Transifex,MERGED,2014-12-05 06:08:33.000000000,2014-12-06 05:49:04.000000000,2014-12-06 05:49:04.000000000,"[{'_account_id': 3}, {'_account_id': 741}, {'_account_id': 8094}]","[{'number': 1, 'created': '2014-12-05 06:08:33.000000000', 'files': ['designate/locale/designate-log-warning.pot', 'designate/locale/fr/LC_MESSAGES/designate-log-warning.po'], 'web_link': 'https://opendev.org/openstack/designate/commit/05d7023098f293086827698951e44c941a5bb8be', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I745b666588002fb7fa4925d3ea3a441265c636bb\n'}]",0,139525,05d7023098f293086827698951e44c941a5bb8be,7,3,1,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: I745b666588002fb7fa4925d3ea3a441265c636bb
",git fetch https://review.opendev.org/openstack/designate refs/changes/25/139525/1 && git format-patch -1 --stdout FETCH_HEAD,"['designate/locale/designate-log-warning.pot', 'designate/locale/fr/LC_MESSAGES/designate-log-warning.po']",2,05d7023098f293086827698951e44c941a5bb8be,transifex/translations,"""POT-Creation-Date: 2014-12-05 06:08+0000\n"" ""PO-Revision-Date: 2014-12-04 15:28+0000\n""#: designate/mdns/notify.py:122#: designate/mdns/notify.py:193#: designate/mdns/notify.py:205#: designate/mdns/notify.py:218#: designate/mdns/service.py:116#: designate/mdns/service.py:136msgid ""TCP Timeout from: %(host)s:%(port)d"" #: designate/mdns/service.py:148","""POT-Creation-Date: 2014-10-31 06:09+0000\n"" ""PO-Revision-Date: 2014-10-30 15:32+0000\n""#: designate/mdns/notify.py:118#: designate/mdns/notify.py:188#: designate/mdns/notify.py:200#: designate/mdns/notify.py:213#: designate/mdns/service.py:114#: designate/mdns/service.py:124msgid """" ""got a packet with unexpected length from %(host)s:%(port)d. Expected length="" ""%(elen)d. Actual length=%(alen)d.""""a reçu un paquet d'une longueur inattendue de la part de %(host)s:%(port)d. "" ""Longueur attendue=%(elen)d. Longueur réelle=%(alen)d."" #: designate/mdns/service.py:139",20,26
openstack%2Fdesignate~master~I250e85a46f779cc24d67754c3a9c43179611de4c,openstack/designate,master,I250e85a46f779cc24d67754c3a9c43179611de4c,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:41:03.000000000,2014-12-06 05:48:58.000000000,2014-12-06 05:48:57.000000000,"[{'_account_id': 3}, {'_account_id': 741}, {'_account_id': 8094}]","[{'number': 1, 'created': '2014-12-05 03:41:03.000000000', 'files': ['doc/source/getting-involved.rst', 'README.md', 'doc/source/devstack.rst'], 'web_link': 'https://opendev.org/openstack/designate/commit/324063f684202a42d53e4b9f2b5a01ae8cb21925', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I250e85a46f779cc24d67754c3a9c43179611de4c\n'}]",1,139318,324063f684202a42d53e4b9f2b5a01ae8cb21925,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I250e85a46f779cc24d67754c3a9c43179611de4c
",git fetch https://review.opendev.org/openstack/designate refs/changes/18/139318/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/getting-involved.rst', 'README.md', 'doc/source/devstack.rst']",3,324063f684202a42d53e4b9f2b5a01ae8cb21925,infra-manual, $ git clone https://git.openstack.org/cgit/openstack-dev/devstack.git, $ git clone https://github.com/openstack-dev/devstack.git,3,3
openstack%2Fpython-designateclient~master~Ia1631b026e1d59e9e4b448bb2c96c8cf4c6e3421,openstack/python-designateclient,master,Ia1631b026e1d59e9e4b448bb2c96c8cf4c6e3421,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:44:34.000000000,2014-12-06 05:48:52.000000000,2014-12-06 05:48:51.000000000,"[{'_account_id': 3}, {'_account_id': 741}, {'_account_id': 8094}]","[{'number': 1, 'created': '2014-12-05 03:44:34.000000000', 'files': ['doc/source/contributing.rst'], 'web_link': 'https://opendev.org/openstack/python-designateclient/commit/e20b1d3f84bd330f1c730af07fe44c9eca719190', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Ia1631b026e1d59e9e4b448bb2c96c8cf4c6e3421\n'}]",0,139371,e20b1d3f84bd330f1c730af07fe44c9eca719190,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Ia1631b026e1d59e9e4b448bb2c96c8cf4c6e3421
",git fetch https://review.opendev.org/openstack/python-designateclient refs/changes/71/139371/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/contributing.rst'],1,e20b1d3f84bd330f1c730af07fe44c9eca719190,infra-manual,.. _Gerrit: http://docs.openstack.org/infra/manual/developers.html#development-workflow,.. _Gerrit: http://wiki.openstack.org/GerritWorkflow,1,1
openstack%2Fcinder~master~I2aca4a6a84bc8ddfa70bd47a331b6fac6f82220f,openstack/cinder,master,I2aca4a6a84bc8ddfa70bd47a331b6fac6f82220f,Remove Python 2.6 backwards compatibility code,MERGED,2014-11-26 08:17:21.000000000,2014-12-06 05:26:35.000000000,2014-11-26 16:00:53.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 1207}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}]","[{'number': 1, 'created': '2014-11-26 08:17:21.000000000', 'files': ['cinder/api/contrib/services.py', 'cinder/test.py', 'cinder/tests/test_storwize_svc.py', 'cinder/api/contrib/hosts.py', 'cinder/utils.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/10b8ccfdd7ed23726676b1a0a1d868b83e13401f', 'message': 'Remove Python 2.6 backwards compatibility code\n\nPython 2.6 is no longer supported in cinder as of kilo. The code in the\nproject that is specifically for compatibility with 2.6 is therefore no\nlonger required.\n\nThis commit removes code referenced as being required specifically for\ncompatibility with Python 2.6.\n\nThis commit removes:\n    - total_seconds from cinder/utils.py\n    - TestCase.assertGreater from cinder/test.py\n    - TestCase.assertGreaterEqual from cinder/test.py\n    - StorwizeSVCDriverTestCase.assertLessEqual from\n      cinder/tests/test_storwize_svc.py\n\nChange-Id: I2aca4a6a84bc8ddfa70bd47a331b6fac6f82220f\n'}]",0,137302,10b8ccfdd7ed23726676b1a0a1d868b83e13401f,15,9,1,7219,,,0,"Remove Python 2.6 backwards compatibility code

Python 2.6 is no longer supported in cinder as of kilo. The code in the
project that is specifically for compatibility with 2.6 is therefore no
longer required.

This commit removes code referenced as being required specifically for
compatibility with Python 2.6.

This commit removes:
    - total_seconds from cinder/utils.py
    - TestCase.assertGreater from cinder/test.py
    - TestCase.assertGreaterEqual from cinder/test.py
    - StorwizeSVCDriverTestCase.assertLessEqual from
      cinder/tests/test_storwize_svc.py

Change-Id: I2aca4a6a84bc8ddfa70bd47a331b6fac6f82220f
",git fetch https://review.opendev.org/openstack/cinder refs/changes/02/137302/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/api/contrib/services.py', 'cinder/test.py', 'cinder/api/contrib/hosts.py', 'cinder/tests/test_storwize_svc.py', 'cinder/utils.py']",5,10b8ccfdd7ed23726676b1a0a1d868b83e13401f,2.6_compat, elapsed = (timeutils.utcnow() - last_heartbeat).total_seconds(),"def total_seconds(td): """"""Local total_seconds implementation for compatibility with python 2.6."""""" if hasattr(td, 'total_seconds'): return td.total_seconds() else: return ((td.days * 86400 + td.seconds) * 10 ** 6 + td.microseconds) / 10.0 ** 6 elapsed = total_seconds(timeutils.utcnow() - last_heartbeat)",3,40
openstack%2Fmanila~master~Iaf7add8cd65f36a23a7e8955da7cabba800f11a3,openstack/manila,master,Iaf7add8cd65f36a23a7e8955da7cabba800f11a3,Imported Translations from Transifex,MERGED,2014-12-04 06:13:41.000000000,2014-12-06 05:10:26.000000000,2014-12-06 05:10:25.000000000,"[{'_account_id': 3}, {'_account_id': 6491}, {'_account_id': 8851}]","[{'number': 1, 'created': '2014-12-04 06:13:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/068c3703bafe62215cb4be8bb989c18dbf3db443', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: Iaf7add8cd65f36a23a7e8955da7cabba800f11a3\n'}, {'number': 2, 'created': '2014-12-05 06:11:59.000000000', 'files': ['manila/locale/manila.pot', 'manila/locale/manila-log-info.pot', 'manila/locale/de/LC_MESSAGES/manila-log-error.po', 'manila/locale/manila-log-error.pot'], 'web_link': 'https://opendev.org/openstack/manila/commit/ebe45f4a51cbf67beb987a2b41d889fa7c8e20b4', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: Iaf7add8cd65f36a23a7e8955da7cabba800f11a3\n'}]",0,138961,ebe45f4a51cbf67beb987a2b41d889fa7c8e20b4,10,3,2,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: Iaf7add8cd65f36a23a7e8955da7cabba800f11a3
",git fetch https://review.opendev.org/openstack/manila refs/changes/61/138961/1 && git format-patch -1 --stdout FETCH_HEAD,"['manila/locale/manila.pot', 'manila/locale/manila-log-info.pot', 'manila/locale/de/LC_MESSAGES/manila-log-error.po', 'manila/locale/manila-log-error.pot']",4,068c3703bafe62215cb4be8bb989c18dbf3db443,transifex/translations,"""Project-Id-Version: manila 2015.1.dev114.g2023930\n""""POT-Creation-Date: 2014-12-04 06:13+0000\n""#: manila/share/driver.py:90#: manila/share/drivers/ibm/gpfs.py:712 #, python-format msgid ""Invalid metadata %(attr)s for share %(share)s."" msgstr """" ","""Project-Id-Version: manila 2015.1.dev88.ga2b1757\n""""POT-Creation-Date: 2014-11-14 06:14+0000\n""#: manila/share/driver.py:76",264,61
openstack%2Fcinder~master~I90527d594081adbd8e32866b39594fda78d5c0e7,openstack/cinder,master,I90527d594081adbd8e32866b39594fda78d5c0e7,Imported Translations from Transifex,MERGED,2014-11-26 06:10:34.000000000,2014-12-06 05:07:41.000000000,2014-11-26 16:00:44.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}]","[{'number': 1, 'created': '2014-11-26 06:10:34.000000000', 'files': ['cinder/locale/cinder.pot', 'cinder/locale/pt_BR/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/cinder-log-error.pot', 'cinder/locale/cinder-log-info.pot', 'cinder/locale/de/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/cinder-log-warning.pot', 'cinder/locale/de/LC_MESSAGES/cinder-log-error.po'], 'web_link': 'https://opendev.org/openstack/cinder/commit/8964c55a203932fbe62c10ecd6fcd4f39c79fcf0', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: I90527d594081adbd8e32866b39594fda78d5c0e7\n'}]",0,137282,8964c55a203932fbe62c10ecd6fcd4f39c79fcf0,11,7,1,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: I90527d594081adbd8e32866b39594fda78d5c0e7
",git fetch https://review.opendev.org/openstack/cinder refs/changes/82/137282/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/locale/cinder.pot', 'cinder/locale/pt_BR/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/cinder-log-error.pot', 'cinder/locale/cinder-log-info.pot', 'cinder/locale/de/LC_MESSAGES/cinder-log-info.po', 'cinder/locale/cinder-log-warning.pot', 'cinder/locale/de/LC_MESSAGES/cinder-log-error.po']",7,8964c55a203932fbe62c10ecd6fcd4f39c79fcf0,transifex/translations,,"# Translations template for cinder. # Copyright (C) 2014 ORGANIZATION # This file is distributed under the same license as the cinder project. # # Translators: # Ettore Atalan <atalanttore@googlemail.com>, 2014 msgid """" msgstr """" ""Project-Id-Version: Cinder\n"" ""Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"" ""POT-Creation-Date: 2014-11-24 06:08+0000\n"" ""PO-Revision-Date: 2014-11-24 05:53+0000\n"" ""Last-Translator: openstackjenkins <jenkins@openstack.org>\n"" ""Language-Team: German (http://www.transifex.com/projects/p/cinder/language/"" ""de/)\n"" ""Language: de\n"" ""MIME-Version: 1.0\n"" ""Content-Type: text/plain; charset=UTF-8\n"" ""Content-Transfer-Encoding: 8bit\n"" ""Generated-By: Babel 1.3\n"" ""Plural-Forms: nplurals=2; plural=(n != 1);\n"" #: cinder/exception.py:93 msgid ""Exception in string format operation"" msgstr ""Ausnahme bei Zeichenfolgeformatoperation"" #: cinder/quota.py:783 #, python-format msgid ""Failed to commit reservations %s"" msgstr """" #: cinder/quota.py:804 #, python-format msgid ""Failed to roll back reservations %s"" msgstr """" #: cinder/wsgi.py:234 #, python-format msgid ""Failed to start %(name)s on %(_host)s:%(_port)s with SSL support."" msgstr """" #: cinder/api/extensions.py:244 #, python-format msgid ""Exception loading extension: %s"" msgstr ""Ausnahme beim Laden von Erweiterung: %s"" #: cinder/api/middleware/fault.py:46 #, python-format msgid ""Caught error: %s"" msgstr ""Fehler abgefangen: %s"" #: cinder/backup/manager.py:445 msgid ""Failed to update usages deleting backup"" msgstr """" #: cinder/backup/manager.py:629 msgid ""Backup driver has not been initialized"" msgstr ""Datensicherungstreiber wurde nicht initialisiert"" #: cinder/backup/drivers/ceph.py:460 #, python-format msgid """" ""Max retries reached deleting backup %(basename)s image of volume %(volume)s."" msgstr """" #: cinder/backup/drivers/ceph.py:496 #, python-format msgid ""Pipe1 failed - %s "" msgstr """" #: cinder/backup/drivers/ceph.py:510 #, python-format msgid ""Pipe2 failed - %s "" msgstr """" #: cinder/backup/drivers/ceph.py:974 msgid ""Differential restore failed, trying full restore"" msgstr """" #: cinder/backup/drivers/ceph.py:1176 #, python-format msgid ""Restore to volume %(volume)s finished with error - %(error)s."" msgstr """" #: cinder/backup/drivers/swift.py:159 #, python-format msgid ""single_user auth mode enabled, but %(param)s not set"" msgstr """" #: cinder/backup/drivers/swift.py:394 #, python-format msgid ""Backup volume metadata to swift failed: %s"" msgstr """" #: cinder/backup/drivers/tsm.py:252 #, python-format msgid """" ""backup: %(vol_id)s failed to remove backup hardlink from %(vpath)s to "" ""%(bpath)s.\n"" ""stdout: %(out)s\n"" "" stderr: %(err)s."" msgstr """" #: cinder/backup/drivers/tsm.py:531 #, python-format msgid """" ""delete: %(vol_id)s failed with stdout: %(out)s\n"" "" stderr: %(err)s"" msgstr """" #: cinder/brick/initiator/connector.py:141 #, python-format msgid ""Failed to access the device on the path %(path)s: %(error)s %(info)s."" msgstr """" #: cinder/brick/iscsi/iscsi.py:166 #, python-format msgid """" ""Failed to recover attempt to create iscsi backing lun for volume id:"" ""%(vol_id)s: %(e)s"" msgstr """" #: cinder/brick/iscsi/iscsi.py:257 #, python-format msgid """" ""Failed to create iscsi target for volume id:%(vol_id)s. Please ensure your "" ""tgtd config file contains 'include %(volumes_dir)s/*'"" msgstr """" ""Fehler beim Erstellen von iSCSI-Ziel für Datenträger-ID: %(vol_id)s. Stellen "" ""Sie sicher, dass Ihre tgtd-Konfigurationsdatei 'include %(volumes_dir)s/*' "" ""enthält"" #: cinder/brick/iscsi/iscsi.py:309 cinder/brick/iscsi/iscsi.py:333 #, python-format msgid ""Failed to remove iscsi target for volume id:%(vol_id)s: %(e)s"" msgstr """" #: cinder/brick/iscsi/iscsi.py:404 #, python-format msgid ""Failed to create iscsi target for volume id:%(vol_id)s: %(e)s"" msgstr """" #: cinder/brick/iscsi/iscsi.py:505 msgid ""cinder-rtstool is not installed correctly"" msgstr """" #: cinder/brick/iscsi/iscsi.py:545 cinder/brick/iscsi/iscsi.py:554 #, python-format msgid ""Failed to create iscsi target for volume id:%s."" msgstr """" #: cinder/brick/iscsi/iscsi.py:571 #, python-format msgid ""Failed to remove iscsi target for volume id:%s."" msgstr """" #: cinder/brick/iscsi/iscsi.py:600 #, python-format msgid ""Failed to add initiator iqn %s to target."" msgstr """" #: cinder/brick/iscsi/iscsi.py:614 #, python-format msgid ""Failed to delete initiator iqn %s to target."" msgstr """" #: cinder/brick/local_dev/lvm.py:78 msgid ""Error creating Volume Group"" msgstr """" #: cinder/brick/local_dev/lvm.py:79 cinder/brick/local_dev/lvm.py:161 #: cinder/brick/local_dev/lvm.py:509 cinder/brick/local_dev/lvm.py:539 #: cinder/brick/local_dev/lvm.py:582 cinder/brick/local_dev/lvm.py:663 #: cinder/brick/local_dev/lvm.py:701 #, python-format msgid ""Cmd :%s"" msgstr ""Cmd :%s"" #: cinder/brick/local_dev/lvm.py:80 cinder/brick/local_dev/lvm.py:162 #: cinder/brick/local_dev/lvm.py:510 cinder/brick/local_dev/lvm.py:540 #: cinder/brick/local_dev/lvm.py:583 cinder/brick/local_dev/lvm.py:664 #: cinder/brick/local_dev/lvm.py:702 #, python-format msgid ""StdOut :%s"" msgstr ""StdOut :%s"" #: cinder/brick/local_dev/lvm.py:81 cinder/brick/local_dev/lvm.py:163 #: cinder/brick/local_dev/lvm.py:511 cinder/brick/local_dev/lvm.py:541 #: cinder/brick/local_dev/lvm.py:584 cinder/brick/local_dev/lvm.py:665 #: cinder/brick/local_dev/lvm.py:703 #, python-format msgid ""StdErr :%s"" msgstr ""StdErr :%s"" #: cinder/brick/local_dev/lvm.py:160 msgid ""Error querying thin pool about data_percent"" msgstr """" #: cinder/brick/local_dev/lvm.py:508 msgid ""Error creating Volume"" msgstr """" #: cinder/brick/local_dev/lvm.py:538 msgid ""Error creating snapshot"" msgstr """" #: cinder/brick/local_dev/lvm.py:581 msgid ""Error activating LV"" msgstr """" #: cinder/brick/local_dev/lvm.py:662 msgid ""Error extending Volume"" msgstr """" #: cinder/brick/local_dev/lvm.py:700 msgid ""Error renaming logical volume"" msgstr """" #: cinder/db/sqlalchemy/migrate_repo/versions/002_quota_class.py:50 #: cinder/db/sqlalchemy/migrate_repo/versions/002_quota_class.py:75 #: cinder/db/sqlalchemy/migrate_repo/versions/002_quota_class.py:106 #, python-format msgid ""Table |%s| not created!"" msgstr ""Tabelle |%s| nicht erstellt!"" #: cinder/db/sqlalchemy/migrate_repo/versions/002_quota_class.py:128 msgid ""Dropping foreign key reservations_ibfk_1 failed."" msgstr """" #: cinder/db/sqlalchemy/migrate_repo/versions/002_quota_class.py:134 msgid ""quota_classes table not dropped"" msgstr ""'quota_classes'-Tabelle nicht gelöscht"" #: cinder/db/sqlalchemy/migrate_repo/versions/002_quota_class.py:141 msgid ""quota_usages table not dropped"" msgstr ""quota_usages-Tabelle nicht gelöscht"" #: cinder/db/sqlalchemy/migrate_repo/versions/002_quota_class.py:148 msgid ""reservations table not dropped"" msgstr ""reservations-Tabelle nicht gelöscht"" #: cinder/db/sqlalchemy/migrate_repo/versions/015_drop_migrations_table.py:32 msgid ""migrations table not dropped"" msgstr """" #: cinder/db/sqlalchemy/migrate_repo/versions/015_drop_migrations_table.py:62 #, python-format msgid ""Table |%s| not created"" msgstr ""Tabelle |%s| nicht erstellt"" #: cinder/db/sqlalchemy/migrate_repo/versions/021_add_default_quota_class.py:76 msgid ""Default quota class data not inserted into the DB."" msgstr """" #: cinder/openstack/common/loopingcall.py:95 msgid ""in fixed duration looping call"" msgstr ""in Schleifenaufruf mit festgelegter Dauer"" #: cinder/openstack/common/loopingcall.py:138 msgid ""in dynamic looping call"" msgstr ""in dynamischen Schleifenaufruf"" #: cinder/openstack/common/periodic_task.py:202 #, python-format msgid ""Error during %(full_task_name)s: %(e)s"" msgstr ""Fehler bei %(full_task_name)s: %(e)s"" #: cinder/openstack/common/policy.py:546 cinder/openstack/common/policy.py:826 #, python-format msgid ""Failed to understand rule %s"" msgstr ""Regel %s konnte nicht verstanden werden"" #: cinder/openstack/common/policy.py:556 #, python-format msgid ""No handler for matches of kind %s"" msgstr ""Kein Handler für Übereinstimmungen des Typs %s"" #: cinder/openstack/common/service.py:188 msgid ""Exception during rpc cleanup."" msgstr ""Ausnahme bei RPC Cleanup."" #: cinder/openstack/common/service.py:277 msgid ""Unhandled exception"" msgstr ""Nicht behandelte Ausnahme"" #: cinder/volume/api.py:240 msgid ""Failed to create api volume flow"" msgstr """" #: cinder/volume/api.py:274 msgid ""Failed to update quota for deleting volume"" msgstr ""Fehler beim Aktualisieren von Quote zum Löschen von Datenträger"" #: cinder/volume/driver.py:231 cinder/volume/drivers/hds/nfs.py:352 #: cinder/volume/drivers/netapp/nfs.py:188 #, python-format msgid ""Recovering from a failed execute. Try number %s"" msgstr ""Wiederherstellen nach fehlgeschlagener Ausführung. Anzahl Versuche: %s"" #: cinder/volume/driver.py:265 cinder/volume/manager.py:778 #, python-format msgid ""Error detaching volume %(volume)s, due to remove export failure."" msgstr """" #: cinder/volume/driver.py:474 cinder/volume/manager.py:911 #, python-format msgid """" ""Failed updating model of volume %(volume_id)s with driver provided model "" ""%(model)s"" msgstr """" #: cinder/volume/manager.py:216 #, python-format msgid ""Failed to fetch pool name for volume: %s"" msgstr """" #: cinder/volume/manager.py:276 #, python-format msgid ""Error encountered during initialization of driver: %(name)s"" msgstr """" #: cinder/volume/manager.py:301 #, python-format msgid ""Failed to re-export volume %s: setting to error state"" msgstr """" #: cinder/volume/manager.py:317 #, python-format msgid """" ""Error encountered during re-exporting phase of driver initialization: "" ""%(name)s"" msgstr """" #: cinder/volume/manager.py:374 msgid ""Failed to create manager volume flow"" msgstr """" #: cinder/volume/manager.py:468 #, python-format msgid ""Cannot delete volume %s: volume is busy"" msgstr """" #: cinder/volume/manager.py:495 msgid ""Failed to update usages deleting volume"" msgstr """" ""Nutzungen konnten nicht durch Löschen von Datenträger aktualisiert werden."" #: cinder/volume/manager.py:566 cinder/volume/manager.py:1874 #, python-format msgid """" ""Failed updating %(snapshot_id)s metadata using the provided volumes "" ""%(volume_id)s metadata"" msgstr """" #: cinder/volume/manager.py:611 #, python-format msgid ""Cannot delete snapshot %s: snapshot is busy"" msgstr """" #: cinder/volume/manager.py:641 cinder/volume/manager.py:1974 msgid ""Failed to update usages deleting snapshot"" msgstr """" ""Nutzungen konnten nicht durch Löschen von Momentaufnahme aktualisiert werden."" #: cinder/volume/manager.py:774 #, python-format msgid ""Error detaching volume %(volume)s, due to uninitialized driver."" msgstr """" #: cinder/volume/manager.py:810 #, python-format msgid """" ""Error occurred while uploading volume %(volume_id)s to image %(image_id)s."" msgstr """" #: cinder/volume/manager.py:999 #, python-format msgid """" ""Failed updating model of volume %(volume_id)s with drivers update %(model)s "" ""during xfr."" msgstr """" #: cinder/volume/manager.py:1346 #, python-format msgid ""volume %s: Error trying to extend volume"" msgstr """" #: cinder/volume/manager.py:1425 msgid ""Failed to update usages while retyping volume."" msgstr """" #: cinder/volume/manager.py:1461 #, python-format msgid """" ""Volume %s: driver error when trying to retype, falling back to generic "" ""mechanism."" msgstr """" #: cinder/volume/manager.py:1528 msgid ""Failed to create manage_existing flow."" msgstr """" #: cinder/volume/manager.py:1560 #, python-format msgid ""Failed to promote replica for volume %(id)s."" msgstr """" #: cinder/volume/manager.py:1592 #, python-format msgid ""Failed to sync replica for volume %(id)s."" msgstr """" #: cinder/volume/manager.py:1645 #, python-format msgid ""Error checking replication status for volume %s"" msgstr """" #: cinder/volume/manager.py:1677 #, python-format msgid ""Consistency group %s: create failed"" msgstr """" #: cinder/volume/manager.py:1769 msgid ""Failed to update usages deleting consistency groups."" msgstr """" #: cinder/volume/manager.py:1786 msgid ""Failed to update usages deleting volume."" msgstr """" #: cinder/volume/qos_specs.py:85 cinder/volume/qos_specs.py:106 #: cinder/volume/qos_specs.py:156 cinder/volume/qos_specs.py:198 #: cinder/volume/qos_specs.py:212 cinder/volume/qos_specs.py:226 #: cinder/volume/volume_types.py:45 #, python-format msgid ""DB error: %s"" msgstr ""Datenbank Fehler: %s"" #: cinder/volume/drivers/datera.py:181 msgid ""Failed to get updated stats from Datera Cluster."" msgstr """" #: cinder/volume/drivers/eqlx.py:225 #, python-format msgid ""Error running SSH command: \""%s\""."" msgstr """" #: cinder/volume/drivers/eqlx.py:343 msgid ""Failed to setup the Dell EqualLogic driver."" msgstr """" #: cinder/volume/drivers/eqlx.py:360 #, python-format msgid ""Failed to create volume \""%s\""."" msgstr """" #: cinder/volume/drivers/eqlx.py:370 #, python-format msgid ""Failed to add multihost-access for volume \""%s\""."" msgstr """" #: cinder/volume/drivers/eqlx.py:385 #, python-format msgid ""Failed to delete volume \""%s\""."" msgstr """" #: cinder/volume/drivers/eqlx.py:401 #, python-format msgid ""Failed to create snapshot of volume \""%s\""."" msgstr """" #: cinder/volume/drivers/eqlx.py:415 #, python-format msgid ""Failed to create volume from snapshot \""%s\""."" msgstr """" #: cinder/volume/drivers/eqlx.py:428 #, python-format msgid ""Failed to create clone of volume \""%s\""."" msgstr """" #: cinder/volume/drivers/eqlx.py:438 #, python-format msgid ""Failed to delete snapshot %(snap)s of volume %(vol)s."" msgstr """" #: cinder/volume/drivers/eqlx.py:459 #, python-format msgid ""Failed to initialize connection to volume \""%s\""."" msgstr """" #: cinder/volume/drivers/eqlx.py:474 #, python-format msgid ""Failed to terminate connection to volume \""%s\""."" msgstr """" #: cinder/volume/drivers/eqlx.py:500 #, python-format msgid ""Failed to ensure export of volume \""%s\""."" msgstr """" #: cinder/volume/drivers/eqlx.py:519 #, python-format msgid """" ""Failed to extend_volume %(name)s from %(current_size)sGB to %(new_size)sGB."" msgstr """" #: cinder/volume/drivers/fujitsu_eternus_dx_common.py:247 #, python-format msgid """" ""Error Create Volume: %(volumename)s. Return code: %(rc)lu. Error: %(error)s"" msgstr """" ""Fehler bei Erstellen von Datenträger: %(volumename)s. Rückgabecode: "" ""%(rc)lu. Fehler: %(error)s"" #: cinder/volume/drivers/fujitsu_eternus_dx_common.py:519 #: cinder/volume/drivers/emc/emc_vmax_common.py:1992 #, python-format msgid ""Volume %(name)s not found on the array. No volume to delete."" msgstr """" ""Datenträger %(name)s nicht auf Platteneinheit gefunden. Kein Datenträger zu "" ""löschen."" #: cinder/volume/drivers/fujitsu_eternus_dx_common.py:596 #, python-format msgid ""Cannot find Replication Service to create snapshot for volume %s."" msgstr """" ""Replizierungsservice zum Erstellen von Momentaufnahme für Datenträger %s "" ""kann nicht gefunden werden."" #: cinder/volume/drivers/fujitsu_eternus_dx_common.py:719 #, python-format msgid """" ""Snapshot: %(snapshot)s: volume: %(volume)s not found on the array. No "" ""snapshot to delete."" msgstr """" ""Momentaufnahme: %(snapshot)s: Datenträger: %(volume)s nicht auf "" ""Platteneinheit gefunden. Keine zu löschende Momentaufnahme."" #: cinder/volume/drivers/glusterfs.py:128 #, python-format msgid ""Exception during unmounting %s"" msgstr """" #: cinder/volume/drivers/glusterfs.py:139 #, python-format msgid ""Failed to umount %(share)s, reason=%(stderr)s"" msgstr """" #: cinder/volume/drivers/glusterfs.py:317 msgid ""Call to Nova delete snapshot failed"" msgstr """" #: cinder/volume/drivers/glusterfs.py:481 #: cinder/volume/drivers/remotefs.py:198 #, python-format msgid ""Exception during mounting %s"" msgstr ""Ausnahme beim Anhängen von %s"" #: cinder/volume/drivers/glusterfs.py:605 msgid ""Call to Nova to create snapshot failed"" msgstr """" #: cinder/volume/drivers/lvm.py:230 #, python-format msgid ""Unabled to delete due to existing snapshot for volume: %s"" msgstr """" #: cinder/volume/drivers/nimble.py:140 msgid """" ""Failed to create SOAP client.Check san_ip, username, password and make sure "" ""the array version is compatible"" msgstr """" #: cinder/volume/drivers/nimble.py:396 #, python-format msgid ""Re-throwing Exception %s"" msgstr """" #: cinder/volume/drivers/pure.py:149 #, python-format msgid ""Volume deletion failed with message: %s"" msgstr """" #: cinder/volume/drivers/pure.py:171 #, python-format msgid ""Snapshot deletion failed with message: %s"" msgstr """" #: cinder/volume/drivers/pure.py:229 msgid ""No reachable iSCSI-enabled ports on target array."" msgstr """" #: cinder/volume/drivers/pure.py:265 #, python-format msgid ""Unable to find host object in Purity with IQN: %(iqn)s."" msgstr """" #: cinder/volume/drivers/pure.py:278 #, python-format msgid ""Disconnection failed with message: %(msg)s."" msgstr """" #: cinder/volume/drivers/pure.py:358 msgid """" ""Unable to find usable REST API version. Response from Pure Storage REST API: "" msgstr """" #: cinder/volume/drivers/pure.py:372 #, python-format msgid ""Unable to connect to %r. Check san_ip."" msgstr """" #: cinder/volume/drivers/pure.py:379 msgid ""Response not in JSON: "" msgstr """" #: cinder/volume/drivers/pure.py:391 msgid """" ""All REST API versions supported by this version of the Pure Storage iSCSI "" ""driver are unavailable on array."" msgstr """" #: cinder/volume/drivers/rbd.py:217 #, python-format msgid ""error opening rbd image %s"" msgstr """" #: cinder/volume/drivers/rbd.py:311 msgid ""error connecting to ceph cluster."" msgstr """" #: cinder/volume/drivers/rbd.py:371 cinder/volume/drivers/sheepdog.py:179 msgid ""error refreshing volume stats"" msgstr ""Fehler beim Aktualisieren von Datenträgerstatistiken"" #: cinder/volume/drivers/remotefs.py:528 #, python-format msgid ""Failed to created Cinder secure environment indicator file: %s"" msgstr """" #: cinder/volume/drivers/solidfire.py:298 #, python-format msgid ""Failed to retrieve volume SolidFire-ID: %s in get_by_account!"" msgstr """" ""Fehler beim Abrufen von SolidFire-ID von Datenträger: %s in get_by_account!"" #: cinder/volume/drivers/solidfire.py:470 #, python-format msgid ""Volume %s, not found on SF Cluster."" msgstr ""Datenträger %s nicht auf SF-Cluster gefunden."" #: cinder/volume/drivers/solidfire.py:473 #, python-format msgid ""Found %(count)s volumes mapped to id: %(uuid)s."" msgstr ""%(count)s der ID %(uuid)s zugeordnete Datenträger gefunden."" #: cinder/volume/drivers/solidfire.py:555 #, python-format msgid """" ""Account for Volume ID %s was not found on the SolidFire Cluster while "" ""attempting delete_volume operation!"" msgstr """" #: cinder/volume/drivers/solidfire.py:558 msgid ""This usually means the volume was never successfully created."" msgstr """" #: cinder/volume/drivers/solidfire.py:574 #, python-format msgid """" ""Volume ID %s was not found on the SolidFire Cluster while attempting "" ""delete_volume operation!"" msgstr """" #: cinder/volume/drivers/solidfire.py:649 #, python-format msgid """" ""Volume ID %s was not found on the SolidFire Cluster while attempting "" ""extend_volume operation!"" msgstr """" #: cinder/volume/drivers/solidfire.py:677 msgid ""Failed to get updated stats"" msgstr ""Fehler beim Abrufen von aktualisierten Statistiken"" #: cinder/volume/drivers/solidfire.py:714 #, python-format msgid """" ""Volume ID %s was not found on the SolidFire Cluster while attempting "" ""attach_volume operation!"" msgstr """" #: cinder/volume/drivers/solidfire.py:740 #, python-format msgid """" ""Volume ID %s was not found on the SolidFire Cluster while attempting "" ""detach_volume operation!"" msgstr """" #: cinder/volume/drivers/solidfire.py:765 #, python-format msgid """" ""Volume ID %s was not found on the SolidFire Cluster while attempting "" ""accept_transfer operation!"" msgstr """" #: cinder/volume/drivers/solidfire.py:911 #, python-format msgid """" ""Account for Volume ID %s was not found on the SolidFire Cluster while "" ""attempting unmanage operation!"" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_common.py:376 #, python-format msgid ""Error Attaching volume %(vol)s "" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_common.py:567 #, python-format msgid ""PoolName %(poolName)s must be in the file %(emcConfigFileName)s "" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_common.py:574 #, python-format msgid """" ""Array Serial Number %(arrayName)s must be in the file %(emcConfigFileName)s "" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_common.py:677 #, python-format msgid """" ""Volume %(name)s not found on the array. No volume to migrate using retype."" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_common.py:689 #, python-format msgid """" ""Volume %(name)s is not suitable for storage assisted migration using retype"" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_common.py:976 #: cinder/volume/drivers/emc/emc_vmax_common.py:1034 #: cinder/volume/drivers/emc/emc_vmax_common.py:1423 #: cinder/volume/drivers/emc/emc_vmax_common.py:1730 #: cinder/volume/drivers/emc/emc_vmax_common.py:2063 #: cinder/volume/drivers/emc/emc_vmax_fast.py:513 #: cinder/volume/drivers/emc/emc_vmax_fast.py:655 #: cinder/volume/drivers/emc/emc_vmax_masking.py:208 #: cinder/volume/drivers/emc/emc_vmax_masking.py:848 #: cinder/volume/drivers/emc/emc_vmax_provision.py:493 #: cinder/volume/drivers/emc/emc_vmax_provision.py:504 #: cinder/volume/drivers/emc/emc_vmax_provision.py:512 #: cinder/volume/drivers/emc/emc_vmax_utils.py:304 #: cinder/volume/drivers/emc/emc_vmax_utils.py:362 #, python-format msgid ""Exception: %s"" msgstr ""Ausnahmesituation: %s"" #: cinder/volume/drivers/emc/emc_vmax_common.py:1100 msgid ""Error getting target pool name and array"" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_common.py:1112 msgid ""Error parsing target pool name, array, and fast policy"" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_common.py:1442 #, python-format msgid """" ""Target end points do not exist for hardware Id : %(hardwareIdInstance)s "" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_fast.py:580 #, python-format msgid ""Error disassociating storage group from policy: %s"" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_masking.py:321 #, python-format msgid ""Cannot get storage Group from job : %(storageGroupName)s. "" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_masking.py:338 #, python-format msgid """" ""Cannot add and verify tier policy association for storage group : "" ""%(storageGroupName)s to FAST policy : %(fastPolicyName)s. "" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_masking.py:368 #, python-format msgid """" ""Could not find port group : %(portGroupName)s. Check that the EMC "" ""configuration file has the correct port group name. "" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_masking.py:412 #: cinder/volume/drivers/emc/emc_vmax_masking.py:944 #, python-format msgid """" ""Initiator Name(s) %(initiatorNames)s are not on array %(storageSystemName)s "" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_masking.py:977 #, python-format msgid """" ""One of the components of the original masking view %(maskingViewName)s "" ""cannot be retrieved so please contact your system administrator to check "" ""that the correct initiator(s) are part of masking "" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_utils.py:293 #, python-format msgid ""_wait_for_job_complete failed after %(retries)d tries"" msgstr """" #: cinder/volume/drivers/emc/emc_vmax_utils.py:353 #, python-format msgid ""_wait_for_sync failed after %(retries)d tries"" msgstr """" #: cinder/volume/drivers/emc/emc_vnx_cli.py:213 msgid ""san_secondary_ip is configured as the same value as san_ip."" msgstr """" #: cinder/volume/drivers/emc/emc_vnx_cli.py:313 #, python-format msgid ""Error on enable compression on lun %s."" msgstr """" #: cinder/volume/drivers/emc/emc_vnx_cli.py:324 #, python-format msgid ""Error on adding lun to consistency group. %s"" msgstr """" #: cinder/volume/drivers/emc/emc_vnx_cli.py:1005 #, python-format msgid ""Invalid value for %(key)s, value is %(value)s."" msgstr """" #: cinder/volume/drivers/emc/emc_vnx_cli.py:1048 #, python-format msgid ""Error happened during storage pool querying, %s."" msgstr """" #: cinder/volume/drivers/emc/emc_vnx_cli.py:1539 msgid ""The given extra_spec or valid_values is None."" msgstr """" #: cinder/volume/drivers/emc/emc_vnx_cli.py:2588 msgid ""Error parsing output for FastCache Command."" msgstr """" #: cinder/volume/drivers/emc/xtremio.py:93 msgid ""can't create 2 volumes with the same name"" msgstr """" #: cinder/volume/drivers/emc/xtremio.py:96 #, python-format msgid ""Bad response from XMS, %s"" msgstr """" #: cinder/volume/drivers/emc/xtremio.py:100 #, python-format msgid ""bad API response, %s"" msgstr """" #: cinder/volume/drivers/emc/xtremio.py:285 #, python-format msgid ""Can't find volume to map %s"" msgstr """" #: cinder/volume/drivers/hds/hds.py:106 cinder/volume/drivers/hds/iscsi.py:84 #: cinder/volume/drivers/hds/nfs.py:73 #, python-format msgid ""XML exception reading parameter: %s"" msgstr """" #: cinder/volume/drivers/hds/hds.py:200 cinder/volume/drivers/hds/iscsi.py:272 #: cinder/volume/drivers/hds/nfs.py:192 #, python-format msgid ""No configuration found for service: %s"" msgstr ""Keine Konfiguration für den Dienst gefunden: %s"" #: cinder/volume/drivers/hds/hds.py:253 cinder/volume/drivers/hds/iscsi.py:350 #, python-format msgid ""HDP not found: %s"" msgstr ""HDP nicht gefunden: %s"" #: cinder/volume/drivers/hds/hds.py:292 cinder/volume/drivers/hds/iscsi.py:397 #, python-format msgid ""iSCSI portal not found for service: %s"" msgstr """" #: cinder/volume/drivers/hds/iscsi.py:683 #, python-format msgid ""Array mismatch %(myid)s vs %(arid)s"" msgstr """" #: cinder/volume/drivers/hitachi/hbsd_common.py:721 #, python-format msgid ""Failed to update volume status: %s"" msgstr """" #: cinder/volume/drivers/hitachi/hbsd_horcm.py:217 msgid ""Failed to shutdown horcm."" msgstr """" #: cinder/volume/drivers/hitachi/hbsd_horcm.py:278 msgid ""horcm command timeout."" msgstr """" #: cinder/volume/drivers/hitachi/hbsd_horcm.py:285 msgid ""Failed to authenticate user."" msgstr """" #: cinder/volume/drivers/hitachi/hbsd_horcm.py:294 msgid ""Failed to start horcm."" msgstr """" #: cinder/volume/drivers/hitachi/hbsd_horcm.py:298 msgid ""Unexpected error occurs in horcm."" msgstr """" #: cinder/volume/drivers/hitachi/hbsd_iscsi.py:189 #, python-format msgid ""Failed to add target(port: %s)"" msgstr """" #: cinder/volume/drivers/hitachi/hbsd_snm2.py:75 msgid ""snm2 command timeout."" msgstr ""snm2-Befehlszeitüberschreitung."" #: cinder/volume/drivers/hitachi/hbsd_snm2.py:89 msgid ""Unexpected error occurs in snm2."" msgstr """" #: cinder/volume/drivers/huawei/huawei_utils.py:41 #, python-format msgid ""parse_xml_file: %s"" msgstr ""parse_xml_file: %s"" #: cinder/volume/drivers/huawei/rest_common.py:83 msgid ""JSON transfer error"" msgstr ""JSON-Übertragungsfehler"" #: cinder/volume/drivers/huawei/ssh_common.py:502 #, python-format msgid ""_execute_cli: %s"" msgstr """" #: cinder/volume/drivers/huawei/ssh_common.py:934 #, python-format msgid ""map_volume: Volume %s was not found."" msgstr """" #: cinder/volume/drivers/huawei/ssh_common.py:1103 #, python-format msgid ""remove_map: Host %s does not exist."" msgstr """" #: cinder/volume/drivers/huawei/ssh_common.py:1108 #, python-format msgid ""remove_map: Volume %s does not exist."" msgstr """" #: cinder/volume/drivers/huawei/ssh_common.py:1313 msgid """" ""_get_device_type: The driver only supports Dorado5100 and Dorado 2100 G2 now."" msgstr """" #: cinder/volume/drivers/ibm/gpfs.py:126 #, python-format msgid ""Failed to issue mmgetstate command, error: %s."" msgstr """" #: cinder/volume/drivers/ibm/gpfs.py:137 #, python-format msgid ""GPFS is not active. Detailed output: %s."" msgstr ""GPFS ist nicht aktiv. Detaillierte Ausgabe: %s."" #: cinder/volume/drivers/ibm/gpfs.py:150 #, python-format msgid ""Failed to issue df command for path %(path)s, error: %(error)s."" msgstr """" #: cinder/volume/drivers/ibm/gpfs.py:166 cinder/volume/drivers/ibm/gpfs.py:255 #, python-format msgid ""Failed to issue mmlsconfig command, error: %s."" msgstr """" #: cinder/volume/drivers/ibm/gpfs.py:177 #, python-format msgid ""Failed to issue mmlsattr command on path %(path)s, error: %(error)s"" msgstr """" #: cinder/volume/drivers/ibm/gpfs.py:235 #, python-format msgid ""Failed to issue mmlsfs command for path %(path)s, error: %(error)s."" msgstr """" #: cinder/volume/drivers/ibm/gpfs.py:272 #, python-format msgid ""Failed to issue mmlsattr command for path %(path)s, error: %(error)s."" msgstr """" #: cinder/volume/drivers/ibm/storwize_svc/__init__.py:292 #, python-format msgid ""ensure_export: Volume %s not found on storage"" msgstr ""ensure_export: Datenträger %s nicht in Speicher gefunden"" #: cinder/volume/drivers/ibm/storwize_svc/__init__.py:386 #, python-format msgid ""Did not find expected column name in lsvdisk: %s"" msgstr ""Erwarteter Spaltenname nicht in lsvdisk gefunden: %s"" #: cinder/volume/drivers/ibm/storwize_svc/__init__.py:485 #, python-format msgid """" ""initialize_connection: Failed to collect return properties for volume "" ""%(vol)s and connector %(conn)s.\n"" msgstr """" ""initialize_connection: Fehler beim Erfassen von Rückgabeeigenschaften für "" ""Datenträger %(vol)s und Connector %(conn)s.\n"" #: cinder/volume/drivers/ibm/storwize_svc/helpers.py:474 msgid ""Protocol must be specified as '<in> iSCSI' or '<in> FC'."" msgstr """" #: cinder/volume/drivers/ibm/storwize_svc/helpers.py:489 msgid ""Replication must be specified as '<is> True' or '<is> False'."" msgstr """" #: cinder/volume/drivers/ibm/storwize_svc/ssh.py:167 msgid """" ""storwize_svc_multihostmap_enabled is set to False, not allowing multi host "" ""mapping."" msgstr """" #: cinder/volume/drivers/netapp/iscsi.py:340 #: cinder/volume/drivers/netapp/iscsi.py:505 #: cinder/volume/drivers/netapp/nfs.py:113 #, python-format msgid ""Resizing %s failed. Cleaning volume."" msgstr """" #: cinder/volume/drivers/netapp/iscsi.py:484 #, python-format msgid ""Message: %s"" msgstr ""Nachricht: %s"" #: cinder/volume/drivers/netapp/iscsi.py:486 #, python-format msgid ""Error getting lun attribute. Exception: %s"" msgstr """" #: cinder/volume/drivers/netapp/iscsi.py:615 #, python-format msgid ""Failure deleting staged tmp lun %s."" msgstr """" #: cinder/volume/drivers/netapp/iscsi.py:618 #, python-format msgid ""Unknown exception in post clone resize lun %s."" msgstr """" #: cinder/volume/drivers/netapp/iscsi.py:620 #, python-format msgid ""Exception details: %s"" msgstr """" #: cinder/volume/drivers/netapp/nfs.py:219 #, python-format msgid ""Resizing %s failed. Cleaning volume. "" msgstr """" #: cinder/volume/drivers/netapp/nfs.py:816 #: cinder/volume/drivers/netapp/nfs.py:1295 #, python-format msgid ""Exception creating vol %(name)s on share %(share)s. Details: %(ex)s"" msgstr """" #: cinder/volume/drivers/netapp/nfs.py:1092 #, python-format msgid ""Copy offload workflow unsuccessful. %s"" msgstr """" #: cinder/volume/drivers/netapp/nfs.py:1153 #, python-format msgid ""Error in workflow copy from cache. %s."" msgstr """" #: cinder/volume/drivers/netapp/client/base.py:59 #, python-format msgid """" ""Error provisioning volume %(lun_name)s on %(volume_name)s. Details: %(ex)s"" msgstr """" #: cinder/volume/drivers/netapp/client/base.py:162 #, python-format msgid ""Lun %(path)s geometry failed. Message - %(msg)s"" msgstr """" #: cinder/volume/drivers/netapp/eseries/iscsi.py:146 #, python-format msgid ""Error resolving host %(host)s. Error - %(e)s."" msgstr """" #: cinder/volume/drivers/netapp/eseries/iscsi.py:392 #: cinder/volume/drivers/netapp/eseries/iscsi.py:407 #, python-format msgid ""Error creating volume. Msg - %s."" msgstr """" #: cinder/volume/drivers/netapp/eseries/iscsi.py:430 #, python-format msgid ""Failure deleting snap vol. Error: %s."" msgstr """" #: cinder/volume/drivers/netapp/eseries/iscsi.py:461 #, python-format msgid ""Vol copy job status %s."" msgstr """" #: cinder/volume/drivers/nexenta/iscsi.py:202 #: cinder/volume/drivers/nexenta/nfs.py:203 #, python-format msgid """" ""Volume creation failed, deleting created snapshot %(volume_name)s@%(name)s"" msgstr """" #: cinder/volume/drivers/nexenta/jsonrpc.py:90 msgid ""No headers in server response"" msgstr ""Keine Header in Serverantwort"" #: cinder/volume/drivers/prophetstor/dpl_fc.py:59 #, python-format msgid ""Failed to get fiber channel info from storage due to %(stat)s"" msgstr """" #: cinder/volume/drivers/prophetstor/dpl_fc.py:83 #, python-format msgid ""Failed to get fiber channel target from storage server due to %(stat)s"" msgstr """" #: cinder/volume/drivers/prophetstor/dpl_fc.py:102 #, python-format msgid ""Failed to get target wwpns from storage due to %(stat)s"" msgstr """" #: cinder/volume/drivers/prophetstor/dpl_fc.py:121 msgid ""Failed to get sns table"" msgstr """" #: cinder/volume/drivers/prophetstor/dpl_fc.py:150 #, python-format msgid """" ""Volume %(volumeid)s failed to send assign command, ret: %(status)s output: "" ""%(output)s"" msgstr """" #: cinder/volume/drivers/prophetstor/dpl_fc.py:293 #, python-format msgid ""Failed to export fiber channel target due to %s"" msgstr """" #: cinder/volume/drivers/prophetstor/dplcommon.py:93 #, python-format msgid ""JSON encode params error: %s."" msgstr ""JSON-Kodierungsparameterfehler: %s."" #: cinder/volume/drivers/prophetstor/dplcommon.py:105 #, python-format msgid ""Connect to Flexvisor error: %s."" msgstr """" #: cinder/volume/drivers/prophetstor/dplcommon.py:109 #, python-format msgid ""Connect to Flexvisor failed: %s."" msgstr """" #: cinder/volume/drivers/prophetstor/dplcommon.py:133 #, python-format msgid ""Failed to send request: %s."" msgstr """" #: cinder/volume/drivers/prophetstor/dplcommon.py:142 msgid ""The Flexvisor service is unavailable."" msgstr ""Der Flexvisor-Dienst ist nicht verfügbar."" #: cinder/volume/drivers/prophetstor/dplcommon.py:156 #, python-format msgid ""Failed to get response: %s."" msgstr """" #: cinder/volume/drivers/prophetstor/dplcommon.py:165 #, python-format msgid """" ""%(method)s %(url)s unexpected response status: %(response)s (expects: "" ""%(expects)s)."" msgstr """" #: cinder/volume/drivers/prophetstor/dplcommon.py:184 #: cinder/volume/drivers/prophetstor/dplcommon.py:198 #, python-format msgid ""Call to json.loads() raised an exception: %s."" msgstr """" #: cinder/volume/drivers/prophetstor/dplcommon.py:188 #: cinder/volume/drivers/prophetstor/dplcommon.py:202 #, python-format msgid ""Read response raised an exception: %s."" msgstr """" #: cinder/volume/drivers/prophetstor/dplcommon.py:1337 #, python-format msgid ""Flexvisor failed to get pool list.(Error: %d)"" msgstr """" #: cinder/volume/drivers/prophetstor/dplcommon.py:1341 #, python-format msgid ""Flexvisor failed to get pool list due to %s."" msgstr """" #: cinder/volume/drivers/prophetstor/dplcommon.py:1395 #, python-format msgid ""Failed to get server info due to %(state)s."" msgstr """" #: cinder/volume/drivers/san/hp/hp_3par_common.py:501 #, python-format msgid ""Error extending volume: %(vol)s. Exception: %(ex)s"" msgstr """" #: cinder/volume/drivers/san/hp/hp_3par_common.py:877 #, python-format msgid ""Error creating QOS rule %s"" msgstr ""Fehler bei der Erstellung der QOS-Regel %s"" #: cinder/volume/drivers/san/hp/hp_3par_common.py:1325 #, python-format msgid ""Error extending volume %(id)s. Ex: %(ex)s"" msgstr """" #: cinder/volume/drivers/san/hp/hp_3par_common.py:1437 #, python-format msgid ""Error attaching volume %s"" msgstr """" #: cinder/volume/drivers/san/hp/hp_3par_common.py:1445 #, python-format msgid ""Error detaching volume %s"" msgstr """" #: cinder/volume/drivers/san/hp/hp_3par_common.py:2002 #, python-format msgid ""Exception during snapCPG revert: %s"" msgstr """" #: cinder/volume/drivers/san/hp/hp_3par_common.py:2070 #, python-format msgid ""Unexpected error when retype() tried to deleteVolumeSet(%s)"" msgstr """" #: cinder/volume/drivers/san/hp/hp_3par_common.py:2091 #: cinder/volume/drivers/san/hp/hp_3par_common.py:2094 #, python-format msgid ""Unexpected error when retype() revert tried to deleteVolumeSet(%s)"" msgstr """" #: cinder/volume/drivers/san/hp/hp_3par_common.py:2102 #, python-format msgid """" ""%(exception)s: Exception during revert of retype for volume %(volume_name)s. "" ""Original volume set/QOS settings may not have been fully restored."" msgstr """" #: cinder/volume/drivers/san/hp/hp_3par_common.py:2113 #, python-format msgid """" ""%(exception)s: Exception during revert of retype for volume %(volume_name)s. "" ""Failed to remove from new volume set %(new_vvs)s."" msgstr """" #: cinder/volume/drivers/san/hp/hp_3par_iscsi.py:514 #, python-format msgid ""Volume %s doesn't exist on array."" msgstr """" #: cinder/volume/drivers/san/hp/hp_lefthand_cliq_proxy.py:320 #: cinder/volume/drivers/san/hp/hp_lefthand_rest_proxy.py:188 msgid ""Volume did not exist. It will not be deleted"" msgstr ""Volumen nicht vorhanden. Es wird nicht gelöscht"" #: cinder/volume/drivers/san/hp/hp_lefthand_cliq_proxy.py:332 #: cinder/volume/drivers/san/hp/hp_lefthand_rest_proxy.py:221 msgid ""Snapshot did not exist. It will not be deleted"" msgstr """" #: cinder/volume/drivers/san/hp/hp_lefthand_rest_proxy.py:360 #, python-format msgid ""'%(value)s' is an invalid value for extra spec '%(key)s'"" msgstr """" #: cinder/volume/drivers/san/hp/hp_msa_common.py:74 #, python-format msgid ""Failed to connect to MSA Array (%(host)s): %(err)s"" msgstr """" #: cinder/volume/drivers/san/hp/hp_msa_common.py:79 msgid ""Failed to log on MSA Array (invalid login?)"" msgstr """" #: cinder/volume/drivers/san/hp/hp_msa_common.py:133 #, python-format msgid ""%s configuration option is not set"" msgstr ""%s-Konfigurationsoption ist nicht festgelegt"" #: cinder/volume/drivers/san/hp/hp_msa_common.py:174 msgid ""Volume must be detached to perform a clone operation."" msgstr """" #: cinder/volume/drivers/san/hp/hp_msa_common.py:251 #, python-format msgid ""Unable to get stats for VDisk (%s)"" msgstr """" #: cinder/volume/drivers/san/hp/hp_msa_common.py:260 msgid ""Connector doesn't provide wwpns"" msgstr """" #: cinder/volume/drivers/vmware/datastore.py:62 #, python-format msgid ""Storage profile: %s cannot be found in vCenter."" msgstr """" #: cinder/volume/drivers/vmware/datastore.py:213 msgid ""Error occurred while selecting datastore."" msgstr """" #: cinder/volume/drivers/vmware/vmdk.py:1890 #, python-format msgid ""Not able to configure PBM for VC server: %s"" msgstr """" #: cinder/volume/drivers/vmware/volumeops.py:1200 #, python-format msgid ""Virtual disk device of backing: %s not found."" msgstr """" #: cinder/volume/drivers/windows/remotefs.py:137 #, python-format msgid ""Could not get share %s capacity info."" msgstr """" #: cinder/volume/drivers/zfssa/restclient.py:282 #, python-format msgid ""REST Not Available: %s"" msgstr ""REST Nicht verfügbar: %s"" #: cinder/volume/drivers/zfssa/restclient.py:288 #, python-format msgid ""Server Busy retry request: %s"" msgstr """" #: cinder/volume/drivers/zfssa/restclient.py:294 #, python-format msgid ""Authorizing request: %(zfssaurl)sretry: %(retry)d ."" msgstr """" #: cinder/volume/drivers/zfssa/restclient.py:311 #, python-format msgid ""URLError: %s"" msgstr ""URLFehler: %s"" #: cinder/volume/drivers/zfssa/zfssaiscsi.py:249 #, python-format msgid ""Snapshot %s: has clones"" msgstr """" #: cinder/volume/drivers/zfssa/zfssaiscsi.py:363 #, python-format msgid ""Clone Volume:%(volume)s failed from source volume:%(src_vref)s"" msgstr """" #: cinder/volume/flows/common.py:60 #, python-format msgid """" ""Failed setting source volume %(source_volid)s back to its initial "" ""%(source_status)s status"" msgstr """" #: cinder/volume/flows/common.py:93 #, python-format msgid ""Failed updating volume %(volume_id)s with %(update)s"" msgstr """" #: cinder/volume/flows/api/create_volume.py:536 #, python-format msgid ""Failed destroying volume entry %s"" msgstr """" #: cinder/volume/flows/api/create_volume.py:620 #, python-format msgid ""Failed rolling back quota for %s reservations"" msgstr """" #: cinder/volume/flows/api/create_volume.py:666 #, python-format msgid ""Failed to update quota for deleting volume: %s"" msgstr """" #: cinder/volume/flows/api/create_volume.py:766 #: cinder/volume/flows/manager/create_volume.py:190 #, python-format msgid ""Volume %s: create failed"" msgstr """" #: cinder/volume/flows/api/create_volume.py:770 msgid ""Unexpected build error:"" msgstr """" #: cinder/volume/flows/manager/create_volume.py:488 #: cinder/volume/flows/manager/create_volume.py:499 #, python-format msgid """" ""Failed to copy image %(image_id)s to volume: %(volume_id)s, error: %(error)s"" msgstr """" #: cinder/volume/flows/manager/create_volume.py:494 #, python-format msgid ""Failed to copy image to volume: %(volume_id)s, error: %(error)s"" msgstr """" #: cinder/volume/flows/manager/create_volume.py:609 #, python-format msgid ""Unable to create volume. Volume driver %s not initialized"" msgstr """" ""Volumen kann nicht erstellt werden. Volumentreiber %s nicht initialisiert"" #: cinder/volume/flows/manager/manage_existing.py:46 #, python-format msgid ""Unable to manage existing volume. Volume driver %s not initialized."" msgstr """" ",1185,2452
openstack%2Fcinder~master~Ia35662190047450d054b1831aa1c530ca4656101,openstack/cinder,master,Ia35662190047450d054b1831aa1c530ca4656101,CiscoFCSanLookupService passing command as string,MERGED,2014-11-17 13:00:01.000000000,2014-12-06 04:48:25.000000000,2014-11-28 03:37:27.000000000,"[{'_account_id': 3}, {'_account_id': 2861}, {'_account_id': 5997}, {'_account_id': 6043}, {'_account_id': 6491}, {'_account_id': 7198}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10068}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-11-17 13:00:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/47d3460bff15e924f3c081d256c40544f0f1a605', 'message': 'Closes-Bug: 1385980\nChange-Id: Ia35662190047450d054b1831aa1c530ca4656101\n'}, {'number': 2, 'created': '2014-11-18 05:23:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/3f6c054042217d8a5e43205852072bf163e2a81b', 'message': ""The command to fetch the fcns database was passed as a string in the class CiscoFCSanLookupService(FCSanLookupService).  I has to be passed as list.\n\xa0\nBefore the changes\n-------------------\ncisco_fc_san_lookup_service.py:\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 cmd = ZoneConstant.FCNS_SHOW + fabric_vsan + ' | no-more’\n\xa0\nAfter the changes\n-----------------\ncisco_fc_san_lookup_service.py:\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 cmd = ([ZoneConstant.FCNS_SHOW , fabric_vsan , ' | no-more’])\n\nCloses-Bug: 1385980\nChange-Id: Ia35662190047450d054b1831aa1c530ca4656101\n""}, {'number': 3, 'created': '2014-11-18 06:05:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/0c55d5ad3e11ca08b9c64d911b4d128daa28e06e', 'message': ""CiscoFCSanLookupService passing command as string\n\nThe command to fetch the fcns database was passed\nas a string in the class CiscoFCSanLookupService\n(FCSanLookupService).  I has to be passed as list.\n\xa0\nBefore the changes\n-------------------\ncisco_fc_san_lookup_service.py:\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 cmd = ZoneConstant.FCNS_SHOW + fabric_vsan + ' | no-more’\n\xa0\nAfter the changes\n-----------------\ncisco_fc_san_lookup_service.py:\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 cmd = ([ZoneConstant.FCNS_SHOW , fabric_vsan , ' | no-more’])\n\nCloses-Bug: 1385980\nChange-Id: Ia35662190047450d054b1831aa1c530ca4656101\n""}, {'number': 4, 'created': '2014-11-19 08:23:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/5054108edbe5a13f079f08eb378828cd39dd44c3', 'message': ""CiscoFCSanLookupService passing command as string\n\nThe command to fetch the fcns database was passed\nas a string in the class CiscoFCSanLookupService\n(FCSanLookupService).  I has to be passed as list.\n\xa0\nBefore the changes\n-------------------\ncisco_fc_san_lookup_service.py:\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 cmd = ZoneConstant.FCNS_SHOW + fabric_vsan + ' | no-more’\n\xa0\nAfter the changes\n-----------------\ncisco_fc_san_lookup_service.py:\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 cmd = ([ZoneConstant.FCNS_SHOW , fabric_vsan , ' | no-more’])\n\ncinder.conf.sample is not part of this bug fix changes, removed it.\n\nCloses-Bug: 1385980\nChange-Id: Ia35662190047450d054b1831aa1c530ca4656101\n""}, {'number': 5, 'created': '2014-11-19 08:31:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/d0054c03d338afaa493ca2d81bcb22ca97ab5c1b', 'message': ""CiscoFCSanLookupService passing command as string\n\nThe command to fetch the fcns database was passed\nas a string in the class CiscoFCSanLookupService\n(FCSanLookupService).  I has to be passed as list.\n\xa0\nBefore the changes\n-------------------\ncisco_fc_san_lookup_service.py:\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 cmd = ZoneConstant.FCNS_SHOW + fabric_vsan + ' | no-more’\n\xa0\nAfter the changes\n-----------------\ncisco_fc_san_lookup_service.py:\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 cmd = ([ZoneConstant.FCNS_SHOW , fabric_vsan , ' | no-more’])\n\ncinder.conf.sample is not part of this bug fix changes.\n\nCloses-Bug: 1385980\nChange-Id: Ia35662190047450d054b1831aa1c530ca4656101\n""}, {'number': 6, 'created': '2014-11-26 05:48:07.000000000', 'files': ['cinder/zonemanager/drivers/cisco/cisco_fc_san_lookup_service.py', 'cinder/tests/zonemanager/test_cisco_fc_san_lookup_service.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/ae173f41b3d81e69fb22369c5a7bf046ce8e029f', 'message': 'CiscoFCSanLookupService passing command as string\n\nThe command to fetch the fcns database was passed\nas a string in the class CiscoFCSanLookupService\n(FCSanLookupService).  I has to be passed as list.\n\nUpdated the test_cisco_fc_san_lookup_service.py file\nwith the unit test for this change.\n\nCloses-Bug: 1385980\nChange-Id: Ia35662190047450d054b1831aa1c530ca4656101\n'}]",7,134925,ae173f41b3d81e69fb22369c5a7bf046ce8e029f,41,16,6,13639,,,0,"CiscoFCSanLookupService passing command as string

The command to fetch the fcns database was passed
as a string in the class CiscoFCSanLookupService
(FCSanLookupService).  I has to be passed as list.

Updated the test_cisco_fc_san_lookup_service.py file
with the unit test for this change.

Closes-Bug: 1385980
Change-Id: Ia35662190047450d054b1831aa1c530ca4656101
",git fetch https://review.opendev.org/openstack/cinder refs/changes/25/134925/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/zonemanager/drivers/cisco/cisco_fc_san_lookup_service.py', 'etc/cinder/cinder.conf.sample']",2,47d3460bff15e924f3c081d256c40544f0f1a605,bug/1385980,#qpid_hostname=cinder#rabbit_host=cinder,#qpid_hostname=localhost#rabbit_host=localhost,3,3
openstack%2Fcinder~master~Idc1c5961f134652e5cce51c84d7280f9d3864578,openstack/cinder,master,Idc1c5961f134652e5cce51c84d7280f9d3864578,Add cinder support for IBM FlashSystem.,MERGED,2014-10-22 01:42:57.000000000,2014-12-06 04:29:37.000000000,2014-12-04 20:58:43.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 4355}, {'_account_id': 6094}, {'_account_id': 6491}, {'_account_id': 7198}, {'_account_id': 9008}, {'_account_id': 10503}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12105}, {'_account_id': 12369}, {'_account_id': 12516}, {'_account_id': 12779}, {'_account_id': 12780}, {'_account_id': 13582}, {'_account_id': 13846}]","[{'number': 1, 'created': '2014-10-22 01:42:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/3f93048d8d95e3100e326bfdf01909cb7f8b4ac9', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint add-flashsystem-svc\nCloses-Bug: 1382945\n'}, {'number': 2, 'created': '2014-10-22 01:46:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1a5dc23acbc20b9d91a326c8c991663c1af62ca6', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint ibm-flashsystem-driver\nCloses-Bug: 1382945\n'}, {'number': 3, 'created': '2014-10-22 02:20:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a9c75978c9de2f9c61084a85423c58dca13d285e', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint ibm-flashsystem-driver\n'}, {'number': 4, 'created': '2014-10-22 16:27:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1bd56132ee22c7624eaad290f48c803f4fb00a4c', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint add-flashsystem-svc\nCloses-Bug: 1382945\n'}, {'number': 5, 'created': '2014-10-22 16:35:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/cde1386793ffbaee4ec779d0a32eb0da4133bd5c', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint ibm-flashsystem-driver\n'}, {'number': 6, 'created': '2014-10-29 03:13:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/5602a90717b0a21cd0b7012bb7fb245af83dadac', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint add-flashsystem-svc\nCloses-Bug: 1382945\n'}, {'number': 7, 'created': '2014-10-29 03:15:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b5a0663d454310540104b2ac21c28580db18512d', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint ibm-flashsystem-driver\n'}, {'number': 8, 'created': '2014-10-29 04:45:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/71250151cce24f4b33c4b4b772711f0561b30d5d', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint ibm-flashsystem-driver\n'}, {'number': 9, 'created': '2014-11-04 15:05:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c651a60efea33675ac43fd5579cf34c9f7953fa3', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint ibm-flashsystem-driver\n'}, {'number': 10, 'created': '2014-11-10 15:50:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/58506bbd194bb4b5bf79fe40e7515caeba180835', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint ibm-flashsystem-driver\n'}, {'number': 11, 'created': '2014-11-16 03:54:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/d742a59a5ae255932a403af802cede9ed717624e', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all required driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint ibm-flashsystem-driver\n'}, {'number': 12, 'created': '2014-11-19 02:07:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a637fb8d88fe2d1ae999deb0a598bba5eaeaa52b', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all required driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint ibm-flashsystem-driver\n'}, {'number': 13, 'created': '2014-11-19 04:03:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/4703b4d090faea5cf47a8a558d53bf986d937606', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all required driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint ibm-flashsystem-driver\n'}, {'number': 14, 'created': '2014-11-26 05:15:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/34c73d1b7b69b850cd9f88dde262ca085c3d851f', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all required driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint ibm-flashsystem-driver\n'}, {'number': 15, 'created': '2014-11-26 05:22:01.000000000', 'files': ['etc/cinder/cinder.conf.sample', 'cinder/volume/drivers/ibm/flashsystem.py', 'cinder/tests/test_ibm_flashsystem.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/efc310b5b68ff0a72566c9b35af39b2b6fb3a95b', 'message': 'Add cinder support for IBM FlashSystem.\n\nThis driver implements cinder FC volume driver.\nIt communicates with IBM FlashSystem using SSH management interface.\nIt supports all required driver features.\n\nChange-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578\nImplements: blueprint ibm-flashsystem-driver\n'}]",103,130091,efc310b5b68ff0a72566c9b35af39b2b6fb3a95b,93,20,15,12516,,,0,"Add cinder support for IBM FlashSystem.

This driver implements cinder FC volume driver.
It communicates with IBM FlashSystem using SSH management interface.
It supports all required driver features.

Change-Id: Idc1c5961f134652e5cce51c84d7280f9d3864578
Implements: blueprint ibm-flashsystem-driver
",git fetch https://review.opendev.org/openstack/cinder refs/changes/91/130091/12 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/drivers/ibm/flashsystem.py'],1,3f93048d8d95e3100e326bfdf01909cb7f8b4ac9,bp/ibm-flashsystem-driver,"# Copyright 2014 IBM Corp. # Copyright 2014 OpenStack Foundation # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # """""" Volume driver for IBM FlashSystem storage systems. Limitations: 1. Cinder driver only works when open_access_enabled=off. """""" import random import re import string import time import thread from oslo.config import cfg from cinder import context from cinder import exception from cinder.openstack.common import excutils from cinder.openstack.common import log as logging from cinder.openstack.common import loopingcall from cinder.openstack.common import processutils from cinder.openstack.common import strutils from cinder.openstack.common import units from cinder import utils from cinder.volume.drivers.san import san from cinder.volume import volume_types from cinder.volume import utils as volume_utils LOG = logging.getLogger(__name__) FLASHSYSTEM_VOLPOOL_NAME = 'mdiskgrp0' FLASHSYSTEM_VOL_IOGRP = 0 flashsystem_opts = [ cfg.StrOpt('flashsystem_connection_protocol', default='FC', help='Connection protocol (iSCSI/FC)'), cfg.BoolOpt('flashsystem_multipath_enabled', default=False, help='Connect with multipath (FC only; iSCSI multipath is ' 'controlled by Nova)'), cfg.BoolOpt('flashsystem_multihostmap_enabled', default=True, help='Allows vdisk to multi host mapping'), ] CONF = cfg.CONF CONF.register_opts(flashsystem_opts) class FlashSystemDriver(san.SanDriver): """"""IBM FlashSystem 840 iSCSI/FC volume driver. Version history: 1.0.0 - Initial driver """""" VERSION = ""1.0.0"" def __init__(self, *args, **kwargs): super(FlashSystemDriver, self).__init__(*args, **kwargs) self.configuration.append_config_values(flashsystem_opts) self._storage_nodes = {} self._protocol = None self._context = None self._system_name = None self._system_id = None def _ssh(self, ssh_cmd, check_exit_code=True): try: return self._run_ssh(ssh_cmd, check_exit_code) except processutils.ProcessExecutionError as e: msg = (('CLI Exception output:\n command: %(cmd)s\n ' 'stdout: %(out)s\n stderr: %(err)s') % {'cmd': ssh_cmd, 'out': e.stdout, 'err': e.stderr}) LOG.error(msg) raise exception.VolumeBackendAPIException(data=msg) def _append_dict(self, dict_, key, value): key, value = key.strip(), value.strip() obj = dict_.get(key, None) if obj is None: dict_[key] = value elif isinstance(obj, list): obj.append(value) dict_[key] = obj else: dict_[key] = [obj, value] return dict_ def _assert_ssh_return(self, test, fun, ssh_cmd, out, err): self._driver_assert(test, ('%(fun)s: Failed with unexpected CLI output.\n ' 'Command: %(cmd)s\n stdout: %(out)s\n stderr: %(err)s') % {'fun': fun, 'cmd': ssh_cmd, 'out': str(out), 'err': str(err)}) def _build_default_params(self): protocol = self.configuration.flashsystem_connection_protocol multipath = self.configuration.flashsystem_multipath_enabled if protocol.lower() == 'fc': protocol = 'FC' elif protocol.lower() == 'iscsi': protocol = 'iSCSI' return {'protocol': protocol, 'multipath': multipath} def _check_vdisk_params(self, params): # Check that the requested protocol is enabled if params['protocol'] != self._protocol: raise exception.InvalidInput( reason=('Illegal value %(prot)s specified for ' 'flashsystem_connection_protocol: ' 'valid value is %(enabled)s') % {'prot': params['protocol'], 'enabled': self._protocol}) def _connector_to_hostname_prefix(self, connector): """"""Translate connector info to storage system host name. Translate a host's name and IP to the prefix of its hostname on the storage subsystem. We create a host name from the host and IP address, replacing any invalid characters (at most 55 characters), and adding a random 8-character suffix to avoid collisions. The total length should be at most 63 characters. """""" # Build cleanup translation tables for host names invalid_ch_in_host = '' for num in range(0, 128): ch = str(chr(num)) if not ch.isalnum() and ch not in [' ', '.', '-', '_']: invalid_ch_in_host = invalid_ch_in_host + ch host_name = connector['host'] if isinstance(host_name, unicode): unicode_host_name_filter = dict((ord(unicode(char)), u'-') for char in invalid_ch_in_host) host_name = host_name.translate(unicode_host_name_filter) elif isinstance(host_name, str): string_host_name_filter = string.maketrans( invalid_ch_in_host, '-' * len(invalid_ch_in_host)) host_name = host_name.translate(string_host_name_filter) else: msg = ('_create_host: Cannot clean host name. Host name ' 'is not unicode or string') LOG.error(msg) raise exception.NoValidHost(reason=msg) host_name = str(host_name) # FlashSystem family doesn't like hostname that starts with number. if not re.match('^[A-Za-z]', host_name): host_name = '_' + host_name return host_name[:55] def _copy_vdisk_data(self, src_vdisk_name, src_vdisk_id, dest_vdisk_name, dest_vdisk_id): """"""Copy data from src vdisk to dest vdisk. To be able to copy data between vdisks, we must ensure that both vdisks have been mapped to host. If vdisk has not been mapped, it must be mapped firstly. When data copy completed, vdisk should be restored to previous mapped or non-mapped status. """""" LOG.debug('enter: _copy_vdisk_data: %(src)s -> %(dest)s.' % {'src': src_vdisk_name, 'dest': dest_vdisk_name}) connector = utils.brick_get_connector_properties() (src_map, src_lun_id) = self._is_vdisk_map( src_vdisk_name, connector) (dest_map, dest_lun_id) = self._is_vdisk_map( dest_vdisk_name, connector) src_map_device = None src_properties = None dest_map_device = None dest_properties = None if not dest_map: try: dest_lun_id = self._map_vdisk_to_host(dest_vdisk_name, connector) except Exception: with excutils.save_and_reraise_exception(): msg = ('_copy_vdisk_data: Failed to map vdisk %(vdisk)s' % {'vdisk': dest_vdisk_name}) LOG.error(msg) try: dest_properties = self._get_vdisk_map_properties( connector, dest_lun_id, dest_vdisk_name, dest_vdisk_id, self._get_vdisk_params(None)) dest_map_device = self._scan_device(dest_properties) except Exception: with excutils.save_and_reraise_exception(): if not dest_map: self._unmap_vdisk_from_host(dest_vdisk_name, self._get_vdisk_params(None), connector) self._remove_device(dest_properties, dest_map_device) msg = ('_copy_vdisk_data: Failed to get map properties ' 'for vdisk %(vdisk)s' % {'vdisk': dest_vdisk_name}) LOG.error(msg) if not src_map: try: src_lun_id = self._map_vdisk_to_host(src_vdisk_name, connector) except Exception: with excutils.save_and_reraise_exception(): msg = ('_copy_vdisk_data: Failed to map vdisk %(vdisk)s' % {'vdisk': src_vdisk_name}) LOG.error(msg) try: src_properties = self._get_vdisk_map_properties( connector, src_lun_id, src_vdisk_name, src_vdisk_id, self._get_vdisk_params(None)) src_map_device = self._scan_device(src_properties) except Exception: with excutils.save_and_reraise_exception(): if not src_map: self._unmap_vdisk_from_host(src_vdisk_name, self._get_vdisk_params(None), connector) self._remove_device(src_properties, src_map_device) if not dest_map: self._unmap_vdisk_from_host(dest_vdisk_name, self._get_vdisk_params(None), connector) self._remove_device(dest_properties, dest_map_device) msg = ('_copy_vdisk_data: Failed to get map properties ' 'for vdisk %(vdisk)s' % {'vdisk': src_vdisk_name}) LOG.error(msg) src_vdisk_attr = self._get_vdisk_attributes(src_vdisk_name) try: # vdisk capacity is bytes, translate into MB size_in_mb = int(src_vdisk_attr['capacity']) / units.Mi volume_utils.copy_volume( src_map_device['path'], dest_map_device['path'], size_in_mb, self.configuration.volume_dd_blocksize) except Exception: with excutils.save_and_reraise_exception(): msg = ('_copy_vdisk_data: Failed to copy %(src)s to %(dest)s.' % {'src': src_vdisk_name, 'dest': dest_vdisk_name}) LOG.error(msg) finally: if not dest_map: self._unmap_vdisk_from_host(dest_vdisk_name, self._get_vdisk_params(None), connector) self._remove_device(dest_properties, dest_map_device) if not src_map: self._unmap_vdisk_from_host(src_vdisk_name, self._get_vdisk_params(None), connector) self._remove_device(src_properties, src_map_device) LOG.debug('leave: _copy_vdisk_data: %(src)s -> %(dest)s.' % {'src': src_vdisk_name, 'dest': dest_vdisk_name}) def _create_host(self, connector): """"""Create a new host on the storage system. We create a host and associate it with the given connection information. """""" LOG.debug('enter: _create_host: host %s' % connector['host']) rand_id = str(random.randint(0, 99999999)).zfill(8) host_name = '%s-%s' % (self._connector_to_hostname_prefix(connector), rand_id) ports = [] if 'iSCSI' == self._protocol and 'initiator' in connector: ports.append('-iscsiname %s' % connector['initiator']) elif 'FC' == self._protocol and 'wwpns' in connector: for wwpn in connector['wwpns']: ports.append('-hbawwpn %s' % wwpn) self._driver_assert(len(ports), '_create_host: No connector ports') port1 = ports.pop(0) arg_name, arg_val = port1.split() ssh_cmd = ['svctask', 'mkhost', '-force', arg_name, arg_val, '-name', '""%s""' % host_name] out, err = self._ssh(ssh_cmd) self._assert_ssh_return('successfully created' in out, '_create_host', ssh_cmd, out, err) for port in ports: arg_name, arg_val = port.split() ssh_cmd = ['svctask', 'addhostport', '-force', arg_name, arg_val, host_name] out, err = self._ssh(ssh_cmd) self._assert_ssh_return((len(out.strip()) == 0), '_create_host', ssh_cmd, out, err) LOG.debug('leave: _create_host: host %(host)s - %(host_name)s' % {'host': connector['host'], 'host_name': host_name}) return host_name def _create_vdisk(self, name, size, unit, opts): """"""Create a new vdisk."""""" LOG.debug('enter: _create_vdisk: vdisk %s ' % name) model_update = None ssh_cmd = ['svctask', 'mkvdisk', '-name', name, '-mdiskgrp', FLASHSYSTEM_VOLPOOL_NAME, '-iogrp', str(FLASHSYSTEM_VOL_IOGRP), '-size', size, '-unit', unit] out, err = self._ssh(ssh_cmd) self._assert_ssh_return(len(out.strip()), '_create_vdisk', ssh_cmd, out, err) # Ensure that the output is as expected match_obj = re.search('Virtual Disk, id \[([0-9]+)\], ' 'successfully created', out) self._driver_assert(match_obj is not None, ('_create_vdisk %(name)s - did not find ' 'success message in CLI output.\n ' 'stdout: %(out)s\n stderr: %(err)s') % {'name': name, 'out': str(out), 'err': str(err)}) LOG.debug('leave: _create_vdisk: vdisk %s ' % name) def _delete_host(self, host_name): """"""Delete a host on the storage system."""""" LOG.debug('enter: _delete_host: host %s ' % host_name) ssh_cmd = ['svctask', 'rmhost', host_name] out, err = self._ssh(ssh_cmd) # No output should be returned from rmhost self._assert_ssh_return(len(out.strip()) == 0, '_delete_host', ssh_cmd, out, err) LOG.debug('leave: _delete_host: host %s ' % host_name) def _delete_vdisk(self, name, force): """"""Deletes existing vdisks."""""" LOG.debug('enter: _delete_vdisk: vdisk %s' % name) # Try to delete volume only if found on the storage vdisk_defined = self._is_vdisk_defined(name) if not vdisk_defined: LOG.info(('warning: Tried to delete vdisk %s but ' 'it does not exist.') % name) return ssh_cmd = ['svctask', 'rmvdisk', '-force', name] if not force: ssh_cmd.remove('-force') out, err = self._ssh(ssh_cmd) # No output should be returned from rmvdisk self._assert_ssh_return(len(out.strip()) == 0, ('_delete_vdisk %(name)s') % {'name': name}, ssh_cmd, out, err) LOG.debug('leave: _delete_vdisk: vdisk %s' % name) def _driver_assert(self, assert_condition, exception_message): """"""Internal assertion mechanism for CLI output."""""" if not assert_condition: LOG.error(exception_message) raise exception.VolumeBackendAPIException(data=exception_message) def _execute_command_and_parse_attributes(self, ssh_cmd): """"""Execute command on the FlashSystem and parse attributes. Exception is raised if the information from the system can not be obtained. """""" LOG.debug('enter: _execute_command_and_parse_attributes: ' 'command %s' % str(ssh_cmd)) try: out, err = self._ssh(ssh_cmd) except exception.VolumeBackendAPIException as e: msg = ('_execute_command_and_parse_attributes: Failed to ' 'run command %s' % str(ssh_cmd)) LOG.warn(msg) # Does not raise exception when command encounters error. # Only return and the upper logic decides what to do. return None self._assert_ssh_return(len(out), '_execute_command_and_parse_attributes', ssh_cmd, out, err) attributes = {} for attrib_line in out.split('\n'): # If '!' not found, return the string and two empty strings attrib_name, foo, attrib_value = attrib_line.partition('!') if attrib_name is not None and len(attrib_name.strip()): self._append_dict(attributes, attrib_name, attrib_value) LOG.debug('leave: _execute_command_and_parse_attributes: ' 'command: %(cmd)s attributes: %(attr)s' % {'cmd': str(ssh_cmd), 'attr': str(attributes)}) return attributes def _find_host_exhaustive(self, connector, hosts): for host in hosts: ssh_cmd = ['svcinfo', 'lshost', '-delim', '!', host] out, err = self._ssh(ssh_cmd) self._assert_ssh_return(len(out.strip()), '_find_host_exhaustive', ssh_cmd, out, err) for attr_line in out.split('\n'): # If '!' not found, return the string and two empty strings attr_name, foo, attr_val = attr_line.partition('!') if (attr_name == 'iscsi_name' and 'initiator' in connector and attr_val == connector['initiator']): return host elif (attr_name == 'WWPN' and 'wwpns' in connector and attr_val.lower() in map(str.lower, map(str, connector['wwpns']))): return host return None def _get_hdr_dic(self, header, row, delim): """"""Return CLI row data as a dictionary indexed by names from header. string. The strings are converted to columns using the delimiter in delim. """""" attributes = header.split(delim) values = row.split(delim) self._driver_assert(len(values) == len(attributes), ('_get_hdr_dic: attribute headers and values do not match.\n ' 'Headers: %(header)s\n Values: %(row)s') % {'header': str(header), 'row': str(row)}) dic = dict((a, v) for a, v in map(None, attributes, values)) return dic def _get_conn_fc_wwpns(self): wwpns = [] cmd = ['svcinfo', 'lsportfc'] generator = self._port_conf_generator(cmd) header = next(generator, None) if not header: return wwpns for port_data in generator: try: if port_data['status'] == 'active': wwpns.append(port_data['WWPN']) except KeyError as e: self._handle_keyerror('lsportfc', header) return wwpns def _get_fc_wwpns(self): for key in self._storage_nodes: node = self._storage_nodes[key] ssh_cmd = ['svcinfo', 'lsnode', '-delim', '!', node['id']] attributes = self._execute_command_and_parse_attributes(ssh_cmd) wwpns = set(node['WWPN']) for i, s in zip(attributes['port_id'], attributes['port_status']): if 'unconfigured' != s: wwpns.add(i) node['WWPN'] = list(wwpns) LOG.info('WWPN on node %(node)s: %(wwpn)s' % {'node': node['id'], 'wwpn': node['WWPN']}) def _get_host_from_connector(self, connector): """"""List the hosts defined in the storage. Return the host name with the given connection info, or None if there is no host fitting that information. """""" LOG.debug('enter: _get_host_from_connector: %s' % str(connector)) # Get list of host in the storage ssh_cmd = ['svcinfo', 'lshost', '-delim', '!'] out, err = self._ssh(ssh_cmd) if not len(out.strip()): return None # If we have FC information, we have a faster lookup option hostname = None host_lines = out.strip().split('\n') self._assert_ssh_return(len(host_lines), '_get_host_from_connector', ssh_cmd, out, err) header = host_lines.pop(0).split('!') self._assert_ssh_return('name' in header, '_get_host_from_connector', ssh_cmd, out, err) name_index = header.index('name') hosts = map(lambda x: x.split('!')[name_index], host_lines) hostname = self._find_host_exhaustive(connector, hosts) LOG.debug('leave: _get_host_from_connector: host %s' % hostname) return hostname def _get_hostvdisk_mappings(self, host_name): """"""Return the defined storage mappings for a host."""""" return_data = {} ssh_cmd = ['svcinfo', 'lshostvdiskmap', '-delim', '!', host_name] out, err = self._ssh(ssh_cmd) mappings = out.strip().split('\n') if len(mappings): header = mappings.pop(0) for mapping_line in mappings: mapping_data = self._get_hdr_dic(header, mapping_line, '!') return_data[mapping_data['vdisk_name']] = mapping_data return return_data def _get_iscsi_ip_addrs(self): cmd = ['svcinfo', 'lsportip'] generator = self._port_conf_generator(cmd) header = next(generator, None) if not header: return for port_data in generator: try: port_node_id = port_data['node_id'] port_ipv4 = port_data['IP_address'] port_ipv6 = port_data['IP_address_6'] state = port_data['state'] speed = port_data['speed'] except KeyError: self._handle_keyerror('lsportip', header) if port_node_id in self._storage_nodes and ( state == 'configured' or state == 'online') and ( speed != 'NONE'): node = self._storage_nodes[port_node_id] if len(port_ipv4): node['ipv4'].append(port_ipv4) if len(port_ipv6): node['ipv6'].append(port_ipv6) def _get_vdisk_attributes(self, vdisk_name): """"""Return vdisk attributes Exception is raised if the information from system can not be parsed/matched to a single vdisk. """""" ssh_cmd = ['svcinfo', 'lsvdisk', '-gui', '-bytes', '-delim', '!', vdisk_name] return self._execute_command_and_parse_attributes(ssh_cmd) def _get_vdiskhost_mappings(self, vdisk_name): """"""Return the defined storage mappings for a vdisk."""""" return_data = {} ssh_cmd = ['svcinfo', 'lsvdiskhostmap', '-delim', '!', vdisk_name] out, err = self._ssh(ssh_cmd) mappings = out.strip().split('\n') if len(mappings): header = mappings.pop(0) for mapping_line in mappings: mapping_data = self._get_hdr_dic(header, mapping_line, '!') return_data[mapping_data['host_name']] = mapping_data return return_data def _get_vdisk_map_properties(self, connector, lun_id, vdisk_name, vdisk_id, vdisk_params): """"""Get the map properties of vdisk."""""" LOG.debug('enter: _get_vdisk_map_properties: vdisk ' '%(vdisk_name)s' % {'vdisk_name': vdisk_name}) preferred_node = '0' IO_group = '0' # Get preferred node and other nodes in I/O group preferred_node_entry = None io_group_nodes = [] for k, node in self._storage_nodes.iteritems(): if vdisk_params['protocol'] != node['protocol']: continue if node['id'] == preferred_node: preferred_node_entry = node if node['IO_group'] == IO_group: io_group_nodes.append(node) if not len(io_group_nodes): exception_msg = ('_get_vdisk_map_properties: No node found in ' 'I/O group %(gid)s for volume %(vol)s' % {'gid': IO_group, 'vol': vdisk_name}) LOG.error(exception_msg) raise exception.VolumeBackendAPIException(data=exception_msg) if not preferred_node_entry and not vdisk_params['multipath']: # Get 1st node in I/O group preferred_node_entry = io_group_nodes[0] LOG.warn('_get_vdisk_map_properties: Did not find a preferred ' 'node for vdisk %s' % vdisk_name) properties = {} properties['target_discovered'] = False properties['target_lun'] = lun_id properties['volume_id'] = vdisk_id if vdisk_params['protocol'] == 'iSCSI': type_str = 'iscsi' if len(preferred_node_entry['ipv4']): ipaddr = preferred_node_entry['ipv4'][0] else: ipaddr = preferred_node_entry['ipv6'][0] properties['target_portal'] = '%s:%s' % (ipaddr, '3260') properties['target_iqn'] = preferred_node_entry['iscsi_name'] else: type_str = 'fibre_channel' conn_wwpns = self._get_conn_fc_wwpns() if len(conn_wwpns) == 0: msg = ('_get_vdisk_map_properties: Could not get FC ' 'connection information for the host-volume connection. ' 'Is the host configured properly for FC connections?') LOG.error(msg) raise exception.VolumeBackendAPIException(data=msg) properties['target_wwn'] = conn_wwpns if ""zvm_fcp"" in connector: properties['zvm_fcp'] = connector['zvm_fcp'] LOG.debug('leave: _get_vdisk_map_properties: vdisk ' '%(vdisk_name)s' % {'vdisk_name': vdisk_name}) return {'driver_volume_type': type_str, 'data': properties} def _get_vdisk_params(self, type_id): params = self._build_default_params() if type_id: ctxt = context.get_admin_context() volume_type = volume_types.get_volume_type(ctxt, type_id) specs = volume_type.get('extra_specs') for k, value in specs.iteritems(): # Get the scope, if using scope format key_split = k.split(':') if len(key_split) == 1: scope = None key = key_split[0] else: scope = key_split[0] key = key_split[1] # We generally do not look at capabilities in the driver, but # protocol is a special case where the user asks for a given # protocol and we want both the scheduler and the driver to act # on the value. if ((not scope or scope == 'capabilities') and key == 'storage_protocol'): scope = None key = 'protocol' # Anything keys that the driver should look at should have the # 'drivers' scope. if scope and scope != ""drivers"": continue if key in params: this_type = type(params[key]).__name__ if this_type == 'int': value = int(value) elif this_type == 'bool': value = strutils.bool_from_string(value) params[key] = value self._check_vdisk_params(params) return params def _handle_keyerror(self, function, header): msg = (('Did not find expected column in %(fun)s: %(hdr)s') % {'fun': function, 'hdr': header}) LOG.error(msg) raise exception.VolumeBackendAPIException( data=msg) def _is_vdisk_defined(self, vdisk_name): """"""Check if vdisk is defined."""""" LOG.debug('enter: _is_vdisk_defined: vdisk %s ' % vdisk_name) vdisk_attributes = self._get_vdisk_attributes(vdisk_name) LOG.debug('leave: _is_vdisk_defined: vdisk %(vol)s with %(str)s ' % {'vol': vdisk_name, 'str': vdisk_attributes is not None}) if vdisk_attributes is None: return False else: return True def _is_vdisk_copy_in_progress(self, vdisk_name): if vdisk_name not in self._vdisk_copy_in_progress: LOG.debug('_is_vdisk_copy_in_progress: %s' % str(self._vdisk_copy_in_progress)) raise loopingcall.LoopingCallDone(retvalue=True) def _is_vdisk_map(self, vdisk_name, connector): """"""Check if vdisk is mapped. If map, return True and lun id. If not map, return False and expected lun id. """""" LOG.debug('enter: _is_vdisk_map: %(src)s.' % {'src': vdisk_name}) map_flag = False result_lun = '-1' host_name = self._get_host_from_connector(connector) if host_name is None: return (map_flag, int(result_lun)) mapping_data = self._get_hostvdisk_mappings(host_name) if vdisk_name in mapping_data: map_flag = True result_lun = mapping_data[vdisk_name]['SCSI_id'] else: lun_used = [int(v['SCSI_id']) for v in mapping_data.values()] lun_used.sort() # Start from 1 due to problems with lun id being 0. result_lun = 1 for lun_id in lun_used: if result_lun < lun_id: break elif result_lun == lun_id: result_lun += 1 LOG.debug('leave: _is_vdisk_map: %(src)s' 'mapped %(map_flag)s %(result_lun)s.' % {'src': vdisk_name, 'map_flag': str(map_flag), 'result_lun': result_lun}) return (map_flag, int(result_lun)) def _log_cli_output_error(self, function, cmd, out, err): LOG.error(('%(fun)s: Failed with unexpected CLI output.\n ' 'Command: %(cmd)s\nstdout: %(out)s\nstderr: %(err)s\n') % {'fun': function, 'cmd': cmd, 'out': str(out), 'err': str(err)}) def _map_vdisk_to_host(self, vdisk_name, connector): """"""Create a mapping between a vdisk to a host."""""" LOG.debug('enter: _map_vdisk_to_host: vdisk %(vdisk_name)s to ' 'host %(host)s' % {'vdisk_name': vdisk_name, 'host': str(connector)}) # Check if a host object is defined for this host name host_name = self._get_host_from_connector(connector) if host_name is None: # Host does not exist - add a new host to FlashSystem host_name = self._create_host(connector) # Verify that create_new_host succeeded self._driver_assert(host_name is not None, ('_create_host failed to return the host name.')) (map_flag, result_lun) = self._is_vdisk_map(vdisk_name, connector) # Volume is not mapped to host, create a new LUN if not map_flag: ssh_cmd = ['svctask', 'mkvdiskhostmap', '-host', host_name, '-scsi', str(result_lun), vdisk_name] out, err = self._ssh(ssh_cmd, check_exit_code=False) if err and err.startswith('CMMVC6071E'): if not self.configuration.flashsystem_multihostmap_enabled: LOG.error(('flashsystem_multihostmap_enabled is set ' 'to False, Not allow multi host mapping')) exception_msg = 'CMMVC6071E The VDisk-to-host mapping '\ 'was not created because the VDisk is '\ 'already mapped to a host.\n""' raise exception.CinderException(data=exception_msg) for i in range(len(ssh_cmd)): if ssh_cmd[i] == 'mkvdiskhostmap': ssh_cmd.insert(i + 1, '-force') # try to map one volume to multiple hosts out, err = self._ssh(ssh_cmd) LOG.warn('volume %s mapping to multi host' % vdisk_name) self._assert_ssh_return('successfully created' in out, '_map_vdisk_to_host', ssh_cmd, out, err) else: self._assert_ssh_return('successfully created' in out, '_map_vdisk_to_host', ssh_cmd, out, err) LOG.debug(('leave: _map_vdisk_to_host: LUN %(result_lun)s, vdisk ' '%(vdisk_name)s, host %(host_name)s') % {'result_lun': result_lun, 'vdisk_name': vdisk_name, 'host_name': host_name}) return int(result_lun) def _port_conf_generator(self, cmd): ssh_cmd = cmd + ['-delim', '!'] out, err = self._ssh(ssh_cmd) if not len(out.strip()): return port_lines = out.strip().split('\n') if not len(port_lines): return header = port_lines.pop(0) yield header for portip_line in port_lines: try: port_data = self._get_hdr_dic(header, portip_line, '!') except exception.VolumeBackendAPIException: with excutils.save_and_reraise_exception(): self._log_cli_output_error('_port_conf_generator', ssh_cmd, out, err) yield port_data def _remove_device(self, properties, device): LOG.debug('enter: _remove_device') use_multipath = self.configuration.use_multipath_for_image_xfer device_scan_attempts = self.configuration.num_volume_device_scan_tries protocol = properties['driver_volume_type'] connector = utils.brick_get_connector(protocol, use_multipath=use_multipath, device_scan_attempts= device_scan_attempts, conn=properties) connector.disconnect_volume(properties['data'], device) LOG.debug('leave: _remove_device') def _scan_device(self, properties): LOG.debug('enter: _scan_device') use_multipath = self.configuration.use_multipath_for_image_xfer device_scan_attempts = self.configuration.num_volume_device_scan_tries protocol = properties['driver_volume_type'] connector = utils.brick_get_connector(protocol, use_multipath=use_multipath, device_scan_attempts= device_scan_attempts, conn=properties) device = connector.connect_volume(properties['data']) host_device = device['path'] if not connector.check_valid_device(host_device): raise exception.DeviceUnavailable(path=host_device, reason=((""Unable to access the backend storage "" ""via the path %(path)s."") % {'path': host_device})) return device LOG.debug('leave: _scan_device') def _unmap_vdisk_from_host(self, vdisk_name, vdisk_params, connector): if 'host' in connector: host_name = self._get_host_from_connector(connector) self._driver_assert(host_name is not None, ('_get_host_from_connector failed to return the host name ' 'for connector')) else: host_name = None # Check if vdisk-host mapping exists, remove if it does. If no host # name was given, but only one mapping exists, we can use that. mapping_data = self._get_vdiskhost_mappings(vdisk_name) if len(mapping_data) == 0: LOG.warning(('_unmap_vdisk_from_host: No mapping of volume ' '%(vol_name)s to any host found.') % {'vol_name': vdisk_name}) return if host_name is None: if len(mapping_data) > 1: LOG.warning(('_unmap_vdisk_from_host: Multiple mappings of ' 'volume %(vdisk_name)s found, no host ' 'specified.') % {'vdisk_name': vdisk_name}) return else: host_name = mapping_data.keys()[0] else: if host_name not in mapping_data: LOG.error(('_unmap_vdisk_from_host: No mapping of volume ' '%(vol_name)s to host %(host_name)s found') % {'vol_name': vdisk_name, 'host_name': host_name}) return # We have a valid host_name now ssh_cmd = ['svctask', 'rmvdiskhostmap', '-host', host_name, vdisk_name] out, err = self._ssh(ssh_cmd) # Verify CLI behaviour - no output is returned from rmvdiskhostmap self._assert_ssh_return(len(out.strip()) == 0, '_unmap_vdisk_from_host', ssh_cmd, out, err) # If this host has no more mappings, delete it mapping_data = self._get_hostvdisk_mappings(host_name) if not mapping_data: self._delete_host(host_name) def _update_volume_stats(self): """"""Retrieve stats info from volume group."""""" LOG.debug(""Updating volume stats"") data = {} data['vendor_name'] = 'IBM' data['driver_version'] = self.VERSION data['storage_protocol'] = self._protocol data['total_capacity_gb'] = 0 data['free_capacity_gb'] = 0 data['reserved_percentage'] = self.configuration.reserved_percentage data['QoS_support'] = False pool = FLASHSYSTEM_VOLPOOL_NAME backend_name = self.configuration.safe_get('volume_backend_name') if not backend_name: backend_name = '%s_%s' % (self._system_name, pool) data['volume_backend_name'] = backend_name ssh_cmd = ['svcinfo', 'lsmdiskgrp', '-gui', '-bytes', '-delim', '!', pool] attributes = self._execute_command_and_parse_attributes(ssh_cmd) if not attributes: LOG.error('Could not get pool data from the storage') exception_message = ('_update_volume_stats: ' 'Could not get storage pool data') raise exception.VolumeBackendAPIException(data=exception_message) data['total_capacity_gb'] = (float(attributes['capacity']) /units.Gi) data['free_capacity_gb'] = (float(attributes['free_capacity']) /units.Gi) data['easytier_support'] = False # Do not support easy tier data['location_info'] = ('FlashSystemDriver:%(sys_id)s:%(pool)s' % {'sys_id': self._system_id, 'pool': pool}) self._stats = data def _set_vdisk_copy_in_progress(self, vdisk_list): get_lock = True self._vdisk_copy_lock.acquire() for vdisk in vdisk_list: if vdisk in self._vdisk_copy_in_progress: get_lock = False break if get_lock: self._vdisk_copy_in_progress.update(vdisk_list) self._vdisk_copy_lock.release() if get_lock: LOG.debug('_set_vdisk_copy_in_progress: %s' % str(self._vdisk_copy_in_progress)) raise loopingcall.LoopingCallDone(retvalue=True) def _unset_vdisk_copy_in_progress(self, vdisk_list): self._vdisk_copy_lock.acquire() for vdisk in vdisk_list: if vdisk in self._vdisk_copy_in_progress: self._vdisk_copy_in_progress.remove(vdisk) LOG.debug('_unset_vdisk_copy_in_progress: %s' % str(self._vdisk_copy_in_progress)) self._vdisk_copy_lock.release() def _wait_vdisk_copy_completed(self, vdisk_name): timer = loopingcall.FixedIntervalLoopingCall( self._is_vdisk_copy_in_progress, vdisk_name) timer.start(interval=self._check_lock_interval).wait() timer.stop() def do_setup(self, ctxt): """"""Check that we have all configuration details from the storage."""""" LOG.debug('enter: do_setup') self._context = ctxt # Get storage system name and id ssh_cmd = ['svcinfo', 'lssystem', '-delim', '!'] attributes = self._execute_command_and_parse_attributes(ssh_cmd) if not attributes or not attributes.has_key('name'): msg = ('do_setup: Could not get system name') LOG.error(msg) raise exception.VolumeBackendAPIException(data=msg) self._system_name = attributes['name'] self._system_id = attributes['id'] # Validate value of open_access_enabled flag, by now only # support when open_access_enabled is off if not attributes or not attributes.has_key('open_access_enabled') or ( attributes['open_access_enabled'] != 'off'): msg = ('do_setup: open_access_enabled is not off.') LOG.error(msg) raise exception.VolumeBackendAPIException(data=msg) # Validate that the array exists pool = FLASHSYSTEM_VOLPOOL_NAME ssh_cmd = ['svcinfo', 'lsmdiskgrp', '-gui', '-bytes', '-delim', '!', pool] attributes = self._execute_command_and_parse_attributes(ssh_cmd) if not attributes or not attributes.has_key('status') or ( attributes['status'] == 'offline'): msg = ('do_setup: Array does not exist') LOG.error(msg) raise exception.InvalidInput(reason=msg) # Get the iSCSI and FC names of the FlashSystem nodes ssh_cmd = ['svcinfo', 'lsnode', '-delim', '!'] out, err = self._ssh(ssh_cmd) self._assert_ssh_return(len(out.strip()), 'do_setup', ssh_cmd, out, err) nodes = out.strip().splitlines() self._assert_ssh_return(len(nodes), 'do_setup', ssh_cmd, out, err) header = nodes.pop(0) for node_line in nodes: try: node_data = self._get_hdr_dic(header, node_line, '!') except exception.VolumeBackendAPIException: with excutils.save_and_reraise_exception(): self._log_cli_output_error('do_setup', ssh_cmd, out, err) node = {} try: node['id'] = node_data['id'] node['name'] = node_data['name'] node['IO_group'] = node_data['IO_group_id'] node['iscsi_name'] = node_data['iscsi_name'] node['WWNN'] = node_data['WWNN'] node['status'] = node_data['status'] node['WWPN'] = [] node['ipv4'] = [] node['ipv6'] = [] node['protocol'] = None if node['status'] == 'online': self._storage_nodes[node['id']] = node except KeyError: self._handle_keyerror('lsnode', header) # Get the iSCSI IP addresses and WWPNs of the FlashSystem nodes self._get_iscsi_ip_addrs() self._get_fc_wwpns() # For each node, check what connection modes it supports. Delete any # nodes that do not support any types (may be partially configured). to_delete = [] enabled_protocols = set() for k, node in self._storage_nodes.iteritems(): if ((len(node['ipv4']) or len(node['ipv6'])) and len(node['iscsi_name'])): enabled_protocols.add('iSCSI') if len(node['WWPN']): enabled_protocols.add('FC') if not len(enabled_protocols): to_delete.append(k) for delkey in to_delete: del self._storage_nodes[delkey] # Make sure we have at least one node configured self._driver_assert(len(self._storage_nodes), 'do_setup: No configured nodes') # Make sure we have only one protocol self._driver_assert((len(enabled_protocols) == 1), 'do_setup: Enabled protocols should only be one') self._protocol = node['protocol'] = list(enabled_protocols)[0] # Set for vdisk synchronization self._vdisk_copy_in_progress = set() self._vdisk_copy_lock = thread.allocate_lock() self._check_lock_interval = 5 LOG.debug('leave: do_setup') def check_for_setup_error(self): """"""Ensure that the flags are set properly."""""" LOG.debug('enter: check_for_setup_error') # Check that we have the system ID information if self._system_name is None: exception_msg = ('check_for_setup_error: Unable ' 'to determine system name') raise exception.VolumeBackendAPIException(data=exception_msg) if self._system_id is None: exception_msg = ('check_for_setup_error: Unable ' 'to determine system id') raise exception.VolumeBackendAPIException(data=exception_msg) required_flags = ['san_ip', 'san_ssh_port', 'san_login'] for flag in required_flags: if not self.configuration.safe_get(flag): raise exception.InvalidInput(reason=('%s is not set' % flag)) # Ensure that either password or keyfile were set if not (self.configuration.san_password or self.configuration.san_private_key): raise exception.InvalidInput( reason=('check_for_setup_error: Password or SSH private key ' 'is required for authentication: set either san_password ' 'or san_private_key option')) params = self._build_default_params() self._check_vdisk_params(params) LOG.debug('leave: check_for_setup_error') def validate_connector(self, connector): """"""Check connector for at least one enabled protocol (iSCSI/FC)."""""" valid = False if 'iSCSI' == self._protocol and 'initiator' in connector: valid = True if 'FC' == self._protocol and 'wwpns' in connector: valid = True if not valid: err_msg = ('The connector does not contain the required information.') LOG.error(err_msg) raise exception.VolumeDriverException(data=err_msg) def create_volume(self, volume): """"""Create volume."""""" vdisk_name = volume['name'] vdisk_params = self._get_vdisk_params(volume['volume_type_id']) vdisk_size = str(volume['size']) return self._create_vdisk(vdisk_name, vdisk_size, 'gb', vdisk_params) def delete_volume(self, volume): """"""Delete volume."""""" vdisk_name = volume['name'] self._wait_vdisk_copy_completed(vdisk_name) self._delete_vdisk(vdisk_name, False) def extend_volume(self, volume, new_size): """"""Extend volume."""""" LOG.debug('enter: extend_volume: volume %s' % volume['name']) vdisk_name = volume['name'] self._wait_vdisk_copy_completed(vdisk_name) extend_amt = int(new_size) - volume['size'] ssh_cmd = (['svctask', 'expandvdisksize', '-size', str(extend_amt), '-unit', 'gb', vdisk_name]) out, err = self._ssh(ssh_cmd) # No output should be returned from expandvdisksize self._assert_ssh_return(len(out.strip()) == 0, 'extend_volume', ssh_cmd, out, err) LOG.debug('leave: extend_volume: volume %s' % volume['name']) def initialize_connection(self, volume, connector): """"""Perform the necessary work so that an iSCSI/FC connection can be made. To be able to create an iSCSI/FC connection from a given host to a volume, we must: 1. Translate the given iSCSI name or WWNN to a host name 2. Create new host on the storage system if it does not yet exist 3. Map the volume to the host if it is not already done 4. Return the connection information for relevant nodes (in the proper I/O group) """""" LOG.debug('enter: initialize_connection: volume %(vol)s with ' 'connector %(conn)s' % {'vol': str(volume), 'conn': str(connector)}) vdisk_name = volume['name'] vdisk_id = volume['id'] vdisk_params = self._get_vdisk_params(volume['volume_type_id']) self._wait_vdisk_copy_completed(vdisk_name) self._driver_assert(self._is_vdisk_defined(vdisk_name), 'initialize_connection: vdisk %s is not defined.' % vdisk_name) lun_id = self._map_vdisk_to_host(vdisk_name, connector) properties = {} try: properties = self._get_vdisk_map_properties(connector, lun_id, vdisk_name, vdisk_id, vdisk_params) except Exception: with excutils.save_and_reraise_exception(): self.terminate_connection(volume, connector) LOG.error('initialize_connection: Failed to collect return ' 'properties for volume %(vol)s and connector %(conn)s.\n' % {'vol': str(volume), 'conn': str(connector)}) LOG.debug('leave: initialize_connection:\n volume: %(vol)s\n connector ' '%(conn)s\n properties: %(prop)s' % {'vol': str(volume), 'conn': str(connector), 'prop': str(properties)}) return properties def terminate_connection(self, volume, connector, **kwargs): """"""Cleanup after an iSCSI connection has been terminated. When we clean up a terminated connection between a given connector and volume, we: 1. Translate the given connector to a host name 2. Remove the volume-to-host mapping if it exists 3. Delete the host if it has no more mappings (hosts are created automatically by this driver when mappings are created) """""" LOG.debug('enter: terminate_connection: volume %(vol)s with ' 'connector %(conn)s' % {'vol': str(volume), 'conn': str(connector)}) vdisk_name = volume['name'] vdisk_params = self._get_vdisk_params(volume['volume_type_id']) self._wait_vdisk_copy_completed(vdisk_name) self._unmap_vdisk_from_host(vdisk_name, vdisk_params, connector) LOG.debug('leave: terminate_connection: volume %(vol)s with ' 'connector %(conn)s' % {'vol': str(volume), 'conn': str(connector)}) def create_snapshot(self, snapshot): """"""Create snapshot from volume."""""" LOG.debug('enter: create_snapshot: create %(snap)s from %(vol)s' % {'snap': snapshot['name'], 'vol': snapshot['volume']['name']}) status = snapshot['volume']['status'] if status not in ['available', 'in-use']: msg = ('create_snapshot: Volume status must be ""available"" or ' '""in-use"" for snapshot. The invalid status is %s' % status) raise exception.InvaidVolume(msg) vdisk_name = snapshot['volume']['name'] vdisk_id = snapshot['volume']['id'] vdisk_attr = self._get_vdisk_attributes(vdisk_name) self._driver_assert(vdisk_attr is not None, 'create_snapshot: Failed to get attributes for vdisk %s' % vdisk_name) snap_vdisk_name = snapshot['name'] snap_vdisk_id = snapshot['id'] self._create_vdisk(snap_vdisk_name, vdisk_attr['capacity'], 'b', None) # create a timer to lock vdisk that will be used to data copy timer = loopingcall.FixedIntervalLoopingCall( self._set_vdisk_copy_in_progress, [vdisk_name, snap_vdisk_name]) timer.start(interval=self._check_lock_interval).wait() timer.stop() try: self._copy_vdisk_data(vdisk_name, vdisk_id, snap_vdisk_name, snap_vdisk_id) except Exception: LOG.error('create_snapshot: Can not create snapshot for volume %s' % vdisk_name) exception_msg = ('create_snapshot: Failed to create snapshot for ' 'volume %s' % vdisk_name) raise exception.VolumeBackendAPIException(data=exception_msg) finally: self._unset_vdisk_copy_in_progress([vdisk_name, snap_vdisk_name]) LOG.debug('leave: create_snapshot: create %(snap)s from %(vol)s' % {'snap': snapshot['name'], 'vol': snapshot['volume']['name']}) def delete_snapshot(self, snapshot): """"""Delete snapshot."""""" LOG.debug('enter: delete_snapshot: delete %(snap)s' % {'snap': snapshot['name']}) self._wait_vdisk_copy_completed(snapshot['name']) self._delete_vdisk(snapshot['name'], False) LOG.debug('leave: delete_snapshot: delete %(snap)s' % {'snap': snapshot['name']}) def create_volume_from_snapshot(self, volume, snapshot): """"""Create volume from snapshot."""""" LOG.debug('enter: create_volume_from_snapshot: create %(vol)s from ' '%(snap)s' % {'vol': volume['name'], 'snap': snapshot['name']}) if volume['size'] != snapshot['volume_size']: msg = ('create_volume_from_snapshot: Volume size is different from' 'snapshot based volume') LOG.error(msg) raise exception.VolumeDriverException(message=msg) status = snapshot['status'] if status != 'available': msg = ('create_volume_from_snapshot: Snapshot status ' 'must be ""available"" for creating volume. ' 'The invalid status is %s' % status) raise exception.InvaidSnapshot(msg) snap_name = snapshot['name'] snap_id = snapshot['id'] snap_attr = self._get_vdisk_attributes(snap_name) self._driver_assert(snap_attr is not None, 'create_volume_from_snapshot: Failed to get ' 'attributes for snapshot %s' % snap_name) vdisk_name = volume['name'] vdisk_id = volume['id'] self._create_vdisk(vdisk_name, snap_attr['capacity'], 'b', None) # create a timer to lock vdisk that will be used to data copy timer = loopingcall.FixedIntervalLoopingCall( self._set_vdisk_copy_in_progress, [snap_name, vdisk_name]) timer.start(interval=self._check_lock_interval).wait() timer.stop() try: self._copy_vdisk_data(snap_name, snap_id, vdisk_name, vdisk_id) except Exception: LOG.error('create_volume_from_snapshot: Can not create volume ' 'from snapshot %s' % snap_name) exception_msg = ('create_volume_from_snapshot: Failed to ' 'create volume from snapshot %s' % snap_name) raise exception.VolumeBackendAPIException(data=exception_msg) finally: self._unset_vdisk_copy_in_progress([snap_name, vdisk_name]) LOG.debug('leave: create_volume_from_snapshot: create %(vol)s from ' '%(snap)s' % {'vol': volume['name'], 'snap': snapshot['name']}) def create_cloned_volume(self, volume, src_volume): """"""Create volume from a source volume."""""" LOG.debug('enter: create_cloned_volume: create %(vol)s from %(src)s' % {'src': src_volume['name'], 'vol': volume['name']}) if src_volume['size'] != volume['size']: msg = ('create_cloned_volume: Source and destination ' 'size differ.') LOG.error(msg) raise exception.VolumeDriverException(message=msg) src_vdisk_name = src_volume['name'] src_vdisk_id = src_volume['id'] src_vdisk_attrs = self._get_vdisk_attributes(src_volume['name']) self._driver_assert(src_vdisk_attrs is not None, 'create_cloned_volume: Failed to get attributes' ' for %s' % src_volume['name']) vdisk_name = volume['name'] vdisk_id = volume['id'] vdisk_params = self._get_vdisk_params(volume['volume_type_id']) self._create_vdisk(vdisk_name, src_vdisk_attrs['capacity'], 'b', vdisk_params) # create a timer to lock vdisk that will be used to data copy timer = loopingcall.FixedIntervalLoopingCall( self._set_vdisk_copy_in_progress, [src_vdisk_name, vdisk_name]) timer.start(interval=self._check_lock_interval).wait() timer.stop() try: self._copy_vdisk_data(src_vdisk_name, src_vdisk_id, vdisk_name, vdisk_id) except Exception: LOG.error('create_cloned_volume: Can not create cloned ' 'volume %s' % volume['name']) exception_msg = ('create_cloned_volume: Failed to create cloned ' 'volume %s' % volume['name']) raise exception.VolumeBackendAPIException(data=exception_msg) finally: self._unset_vdisk_copy_in_progress([src_vdisk_name, vdisk_name]) LOG.debug('leave: create_cloned_volume: create %(vol)s from %(src)s' % {'src': src_volume['name'], 'vol': volume['name']}) def get_volume_stats(self, refresh=False): """"""Get volume stats. If we haven't gotten stats yet or 'refresh' is True, run update the stats first. """""" if not self._stats or refresh: self._update_volume_stats() return self._stats ",,1401,0
openstack%2Fcinder~master~I3fe57e5cc20a97d32c8b8c6089ab1e812257da45,openstack/cinder,master,I3fe57e5cc20a97d32c8b8c6089ab1e812257da45,Get the 'consumer' in a correct way for retyping with qos-specs,MERGED,2014-11-25 08:22:42.000000000,2014-12-06 04:10:55.000000000,2014-11-26 15:54:03.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 2759}, {'_account_id': 7219}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}]","[{'number': 1, 'created': '2014-11-25 08:22:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c13fa6ff567a60638ee5a9c02ea4891e98bf959a', 'message': 'Get the \'consumer\' in a correct way for retyping with qos-specs\n\nThe qos specs returned from the database does not contain the\nproperty ""qos-spec"". Intead, the property ""consumer"" can be accessed\ndirectly.\n\nChange-Id: I3fe57e5cc20a97d32c8b8c6089ab1e812257da45\ncloses-bug: #1389519\n'}, {'number': 2, 'created': '2014-11-25 08:25:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/d6b40b7dccd32c18d28832391cf1a895caea52e8', 'message': 'Get the \'consumer\' in a correct way for retyping with qos-specs\n\nThe qos specs returned from the database does not contain the\nproperty ""qos-specs"". Intead, the property ""consumer"" can be accessed\ndirectly.\n\nChange-Id: I3fe57e5cc20a97d32c8b8c6089ab1e812257da45\ncloses-bug: #1389519\n'}, {'number': 3, 'created': '2014-11-25 13:38:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/85927d108e9146b74c71281a1b9f146b3916cf54', 'message': 'Get the \'consumer\' in a correct way for retyping with qos-specs\n\nThe qos specs returned from the database does not contain the\nproperty ""qos-specs"". Intead, the property ""consumer"" can be accessed\ndirectly.\n\nChange-Id: I3fe57e5cc20a97d32c8b8c6089ab1e812257da45\ncloses-bug: #1389519\n'}, {'number': 4, 'created': '2014-11-26 02:07:27.000000000', 'files': ['cinder/tests/api/contrib/test_volume_actions.py', 'cinder/volume/api.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/ef59e5f8c42365f7d55ea84d4afb364241a388c3', 'message': 'Get the \'consumer\' in a correct way for retyping with qos-specs\n\nThe qos specs returned from the database does not contain the\nproperty ""qos-specs"". Intead, the property ""consumer"" can be accessed\ndirectly.\n\nChange-Id: I3fe57e5cc20a97d32c8b8c6089ab1e812257da45\ncloses-bug: #1389519\n'}]",2,136989,ef59e5f8c42365f7d55ea84d4afb364241a388c3,24,9,4,2861,,,0,"Get the 'consumer' in a correct way for retyping with qos-specs

The qos specs returned from the database does not contain the
property ""qos-specs"". Intead, the property ""consumer"" can be accessed
directly.

Change-Id: I3fe57e5cc20a97d32c8b8c6089ab1e812257da45
closes-bug: #1389519
",git fetch https://review.opendev.org/openstack/cinder refs/changes/89/136989/3 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/api/contrib/test_volume_actions.py', 'cinder/volume/api.py']",2,c13fa6ff567a60638ee5a9c02ea4891e98bf959a,Bug1389519, if specs['consumer'] != 'back-end':, if specs['qos_specs']['consumer'] != 'back-end':,5,5
openstack%2Fproject-config~master~Icc080c6bb5943a69167bc70b12dc2b123097512e,openstack/project-config,master,Icc080c6bb5943a69167bc70b12dc2b123097512e,Enable gate/check (with voting) for taskflow-python34,MERGED,2014-11-27 18:58:35.000000000,2014-12-06 03:56:06.000000000,2014-12-06 03:56:05.000000000,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 4146}, {'_account_id': 6316}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-11-27 18:58:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/e62f37010a9f03912d3847782fdef77cde082b3e', 'message': ""Enable gate/check (with voting) for taskflow-python34\n\nThe non-voting check has been running for about a week\nnow without any problems so I think it's fair to turn\nit into a voting check so that the taskflow team continues\nto maintain the python34 compatibility on an ongoing basis.\n\nChange-Id: Icc080c6bb5943a69167bc70b12dc2b123097512e\n""}, {'number': 2, 'created': '2014-12-04 19:08:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/50e227d030f647c97ba1a32d2f20d77b9fd8b38f', 'message': ""Enable gate/check (with voting) for taskflow-python34\n\nThe non-voting check has been running for about a week\nnow without any problems so I think it's fair to turn\nit into a voting check so that the taskflow team continues\nto maintain the python34 compatibility on an ongoing basis.\n\nChange-Id: Icc080c6bb5943a69167bc70b12dc2b123097512e\n""}, {'number': 3, 'created': '2014-12-06 01:06:05.000000000', 'files': ['zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/9acd6c54651a0dfb9cd0a60d458cc0e70f09d115', 'message': ""Enable gate/check (with voting) for taskflow-python34\n\nThe non-voting check has been running for about a week\nnow without any problems so I think it's fair to turn\nit into a voting check so that the taskflow team continues\nto maintain the python34 compatibility on an ongoing basis.\n\nChange-Id: Icc080c6bb5943a69167bc70b12dc2b123097512e\n""}]",0,137693,9acd6c54651a0dfb9cd0a60d458cc0e70f09d115,16,5,3,1297,,,0,"Enable gate/check (with voting) for taskflow-python34

The non-voting check has been running for about a week
now without any problems so I think it's fair to turn
it into a voting check so that the taskflow team continues
to maintain the python34 compatibility on an ongoing basis.

Change-Id: Icc080c6bb5943a69167bc70b12dc2b123097512e
",git fetch https://review.opendev.org/openstack/project-config refs/changes/93/137693/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul/layout.yaml'],1,e62f37010a9f03912d3847782fdef77cde082b3e,, - gate-taskflow-python34, - name: gate-taskflow-python34 voting: false,1,2
openstack%2Fproject-config~master~Ib74840a0d79732565f1af039ec91df89e0f04e95,openstack/project-config,master,Ib74840a0d79732565f1af039ec91df89e0f04e95,Remove DEVSTACK_GATE_FULL,MERGED,2014-12-04 20:48:50.000000000,2014-12-06 03:52:23.000000000,2014-12-06 03:52:23.000000000,"[{'_account_id': 3}, {'_account_id': 6133}, {'_account_id': 6316}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-12-04 20:48:50.000000000', 'files': ['jenkins/jobs/devstack-gate.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/947cbc4c443218ecd98b75542a5a3f0b26365f39', 'message': ""Remove DEVSTACK_GATE_FULL\n\nI can't seem to find what it does pickaxe in this repo shows no other\nuses of it ever.\n\nChange-Id: Ib74840a0d79732565f1af039ec91df89e0f04e95\n""}]",0,139190,947cbc4c443218ecd98b75542a5a3f0b26365f39,8,4,1,1849,,,0,"Remove DEVSTACK_GATE_FULL

I can't seem to find what it does pickaxe in this repo shows no other
uses of it ever.

Change-Id: Ib74840a0d79732565f1af039ec91df89e0f04e95
",git fetch https://review.opendev.org/openstack/project-config refs/changes/90/139190/1 && git format-patch -1 --stdout FETCH_HEAD,['jenkins/jobs/devstack-gate.yaml'],1,947cbc4c443218ecd98b75542a5a3f0b26365f39,GATE_FULL,, export DEVSTACK_GATE_FULL=1 export DEVSTACK_GATE_FULL=1,0,2
openstack%2Fcinder~master~I2a830e43c876433da4cdb50eabe4fb66b7eb6faf,openstack/cinder,master,I2a830e43c876433da4cdb50eabe4fb66b7eb6faf,PureISCSIDriver:Handle delete called on already deleted volume,MERGED,2014-11-25 03:16:25.000000000,2014-12-06 03:51:40.000000000,2014-11-26 22:47:41.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 1207}, {'_account_id': 4355}, {'_account_id': 6491}, {'_account_id': 7634}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12779}, {'_account_id': 12924}, {'_account_id': 13461}, {'_account_id': 13527}, {'_account_id': 13868}]","[{'number': 1, 'created': '2014-11-25 03:16:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2e6ef30b7501c4e9e752a96a6389c98e8d0e5ef4', 'message': ""Handle the case of delete called on already deleted volume\n\nIf the delete_volume method gets an error from Purity\nsaying volume does not exist, don't proceed with attempt\nat deletion\n\nChange-Id: I2a830e43c876433da4cdb50eabe4fb66b7eb6faf\nCloses-Bug: #1395826\n""}, {'number': 2, 'created': '2014-11-25 18:05:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/645daaff1d1bac056b77b3624c10db6dc5825408', 'message': ""Handle the case of delete called on already deleted volume\n\nIf the delete_volume method gets an error from Purity\nsaying volume does not exist, don't proceed with attempt\nat deletion\n\nChange-Id: I2a830e43c876433da4cdb50eabe4fb66b7eb6faf\nCloses-Bug: #1395826\n""}, {'number': 3, 'created': '2014-11-25 18:58:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c39597b97dc6f4f17430388715b2537309454798', 'message': ""PureISCSIDriver:Handle the case of delete called on already deleted volume\n\nIf the delete_volume method gets an error from Purity\nsaying volume does not exist, don't proceed with attempt\nat deletion\n\nChange-Id: I2a830e43c876433da4cdb50eabe4fb66b7eb6faf\nCloses-Bug: #1395826\n""}, {'number': 4, 'created': '2014-11-25 20:12:24.000000000', 'files': ['cinder/volume/drivers/pure.py', 'cinder/tests/test_pure.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/0d47799beaad6fb390c4761fef8d84d665f9b189', 'message': ""PureISCSIDriver:Handle delete called on already deleted volume\n\nIf the delete_volume method gets an error from Purity\nsaying volume does not exist, don't proceed with attempt\nat deletion\n\nChange-Id: I2a830e43c876433da4cdb50eabe4fb66b7eb6faf\nCloses-Bug: #1395826\n""}]",7,136954,0d47799beaad6fb390c4761fef8d84d665f9b189,38,16,4,13461,,,0,"PureISCSIDriver:Handle delete called on already deleted volume

If the delete_volume method gets an error from Purity
saying volume does not exist, don't proceed with attempt
at deletion

Change-Id: I2a830e43c876433da4cdb50eabe4fb66b7eb6faf
Closes-Bug: #1395826
",git fetch https://review.opendev.org/openstack/cinder refs/changes/54/136954/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/pure.py', 'cinder/tests/test_pure.py']",2,2e6ef30b7501c4e9e752a96a6389c98e8d0e5ef4,bug/1395826," def test_delete_volume_already_deleted(self): self.array.list_volume_hosts.side_effect = exception.PureAPIException( code=400, reason=""Volume does not exist"") self.driver.delete_volume(VOLUME) self.assertFalse(self.array.destroy_volume.called) self.array.list_volume_hosts.side_effect = None self.assert_error_propagates([self.array.destroy_volume], self.driver.delete_volume, VOLUME) ",,15,7
openstack%2Fcinder~master~Ib9edf83eb3ade1e6b2dcf1121a16a6f2e68753e1,openstack/cinder,master,Ib9edf83eb3ade1e6b2dcf1121a16a6f2e68753e1,Add limited retype support for rbd,MERGED,2014-11-20 07:50:30.000000000,2014-12-06 03:32:16.000000000,2014-11-26 05:27:42.000000000,"[{'_account_id': 3}, {'_account_id': 136}, {'_account_id': 170}, {'_account_id': 1207}, {'_account_id': 2759}, {'_account_id': 6491}, {'_account_id': 6962}, {'_account_id': 7219}, {'_account_id': 8871}, {'_account_id': 8978}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11292}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12780}, {'_account_id': 12924}]","[{'number': 1, 'created': '2014-11-20 07:50:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/275ab4e40af464f46489c9769cb1732cb9a643a2', 'message': 'Add QoS retype support for rbd\n\nThis patch enables QoS retype support in rbd.\n\nMigration, change of encryption or additional changes related to\nvolume retyping are left for later patches.\n\nImplements: blueprint Ceph rbd support retype volume\nChange-Id: Ib9edf83eb3ade1e6b2dcf1121a16a6f2e68753e1\n'}, {'number': 2, 'created': '2014-11-20 12:46:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/96ede337ac08f8b24fe2f6714e56ac67e7856d18', 'message': 'Add QoS retype support for rbd\n\nThis patch enables QoS retype support in rbd.\n\nMigration, change of encryption or additional changes related to\nvolume retyping are left for later patches.\n\nImplements: blueprint Ceph rbd support retype volume\nChange-Id: Ib9edf83eb3ade1e6b2dcf1121a16a6f2e68753e1\n'}, {'number': 3, 'created': '2014-11-20 16:33:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/efc4961baeeccddac1f6b6a486439b2b71caeae0', 'message': 'Add QoS retype support for rbd\n\nThis patch enables QoS retype support in rbd.\n\nMigration, change of encryption or additional changes related to\nvolume retyping are left for later patches.\n\nImplements: blueprint Ceph rbd support retype volume\nChange-Id: Ib9edf83eb3ade1e6b2dcf1121a16a6f2e68753e1\n'}, {'number': 4, 'created': '2014-11-24 07:54:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/89baa500ff07bea0f6581d2aaf464d482b77c84a', 'message': 'Add QoS retype support for rbd\n\nThis patch enables QoS retype support in rbd.\n\nMigration, change of encryption or additional changes related to\nvolume retyping are left for later patches.\n\nImplements: blueprint Ceph rbd support retype volume\nChange-Id: Ib9edf83eb3ade1e6b2dcf1121a16a6f2e68753e1\n'}, {'number': 5, 'created': '2014-11-24 07:56:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b394e75fa48b50e1b5f2e92f6b7e89ce6908efab', 'message': 'Add QoS retype support for rbd\n\nThis patch enables QoS retype support in rbd.\n\nMigration, change of encryption or additional changes related to\nvolume retyping are left for later patches.\n\nImplements: blueprint Ceph rbd support retype volume\nChange-Id: Ib9edf83eb3ade1e6b2dcf1121a16a6f2e68753e1\n'}, {'number': 6, 'created': '2014-11-24 16:23:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/af9e55275f2e8a76a30b3e3c820a93fcb5461749', 'message': 'Add QoS retype support for rbd\n\nThis patch enables QoS retype support in rbd.\n\nMigration, change of encryption or additional changes related to\nvolume retyping are left for later patches.\n\nImplements: blueprint Ceph rbd support retype volume\nChange-Id: Ib9edf83eb3ade1e6b2dcf1121a16a6f2e68753e1\n'}, {'number': 7, 'created': '2014-11-25 08:44:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/90bfa3981ea9a06953bf253047ae28c3c0553822', 'message': 'Add limited retype support for rbd\n\nThis patch enables limited retype support for rbd. In addition to\nbasic retyping between volume types that only differ in their name,\nretyping between volume types that imply a change of the qos_specs\nare supported. This hence allows to adapt the quality-of-service\nsettings of a volume after its creation.\n\nAll other changes, such as host migration, change of encryption or\nchanges of settings as defined by the extra_specs are not supported\nby this commit and left for later patches.\n\nImplements: blueprint Ceph rbd support retype volume\nChange-Id: Ib9edf83eb3ade1e6b2dcf1121a16a6f2e68753e1\n'}, {'number': 8, 'created': '2014-11-25 10:11:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/51ba51171f0bde2e40ce62c422efd2c905583224', 'message': 'Add limited retype support for rbd\n\nThis patch enables limited retype support for rbd. In addition to\nbasic retyping between volume types that only differ in their name,\nretyping between volume types that imply a change of the qos_specs\nare supported. This hence allows to adapt the quality-of-service\nsettings of a volume after its creation.\n\nAll other changes, such as host migration, change of encryption or\nchanges of settings as defined by the extra_specs are not supported\nby this commit and left for later patches.\n\nImplements: blueprint Ceph rbd support retype volume\nChange-Id: Ib9edf83eb3ade1e6b2dcf1121a16a6f2e68753e1\n'}, {'number': 9, 'created': '2014-11-25 19:06:28.000000000', 'files': ['cinder/volume/drivers/rbd.py', 'cinder/tests/test_rbd.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/2fb9b07ea6646609e4bdf9b08613b3c1d587378e', 'message': 'Add limited retype support for rbd\n\nThis patch enables limited retype support for rbd. In addition to\nbasic retyping between volume types that only differ in their name,\nretyping between volume types that imply a change of the qos_specs\nare supported. This hence allows to adapt the quality-of-service\nsettings of a volume after its creation.\n\nAll other changes, such as host migration, change of encryption or\nchanges of settings as defined by the extra_specs are not supported\nby this commit and left for later patches.\n\nImplements: blueprint ceph-rbd-support-retype\nChange-Id: Ib9edf83eb3ade1e6b2dcf1121a16a6f2e68753e1\n'}]",3,135874,2fb9b07ea6646609e4bdf9b08613b3c1d587378e,72,19,9,11292,,,0,"Add limited retype support for rbd

This patch enables limited retype support for rbd. In addition to
basic retyping between volume types that only differ in their name,
retyping between volume types that imply a change of the qos_specs
are supported. This hence allows to adapt the quality-of-service
settings of a volume after its creation.

All other changes, such as host migration, change of encryption or
changes of settings as defined by the extra_specs are not supported
by this commit and left for later patches.

Implements: blueprint ceph-rbd-support-retype
Change-Id: Ib9edf83eb3ade1e6b2dcf1121a16a6f2e68753e1
",git fetch https://review.opendev.org/openstack/cinder refs/changes/74/135874/8 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/drivers/rbd.py'],1,275ab4e40af464f46489c9769cb1732cb9a643a2,bp/ceph-rbd-support-retype," def retype(self, ctxt, volume, new_type, diff, host): """"""Retypes a volume, allows QoS change only."""""" LOG.debug('Retype volume request %(vol)s to be %(type)s ' '(host: %(host)s), diff %(diff)s.' % {'vol': volume['name'], 'type': new_type, 'host': host, 'diff': diff}) if volume['host'] != host['host']: LOG.error('Retype with host migration not supported') return False if diff['encryption']: LOG.error('Retype of encryption type not supported') return False if diff['extra_specs']: LOG.error('Retype of extra_specs not supported') return False return True ",,23,0
openstack%2Fproject-config~master~Ieb64b9c6546bdd7ad3c24c958fc9ac01ec0bee66,openstack/project-config,master,Ieb64b9c6546bdd7ad3c24c958fc9ac01ec0bee66,Set check-ansible-aio-build to use devstack-trusty image,MERGED,2014-12-05 16:25:56.000000000,2014-12-06 03:21:23.000000000,2014-12-06 03:21:22.000000000,"[{'_account_id': 3}, {'_account_id': 4146}, {'_account_id': 5263}, {'_account_id': 6316}]","[{'number': 1, 'created': '2014-12-05 16:25:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/0b8dbeed69a4353c59cb284235f97c7aee6a8e95', 'message': 'Set check-ansible-aio-build to use devstack-trusty image\n\nSet the check job to use the devstack-trusty image instead of the bare-trusty\nimage as the bare-trusty image has conflicting packages installed on it.\n\nChange-Id: Ieb64b9c6546bdd7ad3c24c958fc9ac01ec0bee66\n'}, {'number': 2, 'created': '2014-12-05 16:47:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/509abec6ca853d91fbab8c1e6ac75b933e74edf6', 'message': 'Set check-ansible-aio-build to use devstack-trusty image\n\nSet the check job to use the devstack-trusty image instead of the bare-trusty\nimage as the bare-trusty image has conflicting packages installed on it.\n\nChange-Id: Ieb64b9c6546bdd7ad3c24c958fc9ac01ec0bee66\n'}, {'number': 3, 'created': '2014-12-05 17:09:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/ca4c9eb9c6d9cdc6ed7f7651c3a8db4d839028d6', 'message': 'Set check-ansible-aio-build to use devstack-trusty image\n\nSet the check job to use the devstack-trusty image instead of the bare-trusty\nimage as the bare-trusty image has conflicting packages installed on it.\n\nChange-Id: Ieb64b9c6546bdd7ad3c24c958fc9ac01ec0bee66\n'}, {'number': 4, 'created': '2014-12-05 17:25:01.000000000', 'files': ['jenkins/jobs/projects.yaml', 'jenkins/jobs/os-ansible-jobs.yaml', 'zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/47a7e83a2dfffdff94e7bd0aedea65549d8c0746', 'message': 'Set check-ansible-aio-build to use devstack-trusty image\n\nSet the check job to use the devstack-trusty image instead of the bare-trusty\nimage as the bare-trusty image has conflicting packages installed on it.\n\nChange-Id: Ieb64b9c6546bdd7ad3c24c958fc9ac01ec0bee66\n'}]",1,139676,47a7e83a2dfffdff94e7bd0aedea65549d8c0746,12,4,4,6816,,,0,"Set check-ansible-aio-build to use devstack-trusty image

Set the check job to use the devstack-trusty image instead of the bare-trusty
image as the bare-trusty image has conflicting packages installed on it.

Change-Id: Ieb64b9c6546bdd7ad3c24c958fc9ac01ec0bee66
",git fetch https://review.opendev.org/openstack/project-config refs/changes/76/139676/1 && git format-patch -1 --stdout FETCH_HEAD,['jenkins/jobs/projects.yaml'],1,0b8dbeed69a4353c59cb284235f97c7aee6a8e95,os-ansible-deployment-aio, node: 'devstack-trusty',,1,0
openstack%2Fcinder~master~If02db137f936dc5b509fc81ca3c29ae4f87f1cb2,openstack/cinder,master,If02db137f936dc5b509fc81ca3c29ae4f87f1cb2,Add iSCSI Target objects as independent objects,MERGED,2014-10-29 19:35:24.000000000,2014-12-06 03:12:31.000000000,2014-11-25 22:47:30.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 1736}, {'_account_id': 2243}, {'_account_id': 6491}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12924}]","[{'number': 1, 'created': '2014-10-29 19:35:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/dd7ca87ba51e7e2e3e2a1a356b33f375da06af57', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and working on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 2, 'created': '2014-10-29 19:41:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/d4d5417d12f4f905072484eb7c6bb450342a3c3c', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and working on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 3, 'created': '2014-10-29 20:57:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/460ae828e14b5e7d52545352195aa0d81f59b9fe', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and working on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 4, 'created': '2014-11-17 21:05:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9381019f86369303b20c969a55962a85c97b8db2', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and continued work on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 5, 'created': '2014-11-17 23:32:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/e2527c1a69f02b49b46c019fa5ea0f207940d669', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and continued work on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 6, 'created': '2014-11-18 14:20:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/02bf38466cc802fe8432bcf67369db07046a282a', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and continued work on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 7, 'created': '2014-11-18 19:47:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/49ab13c857b5e8d38de75dde6e955ece2c8e995d', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and continued work on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 8, 'created': '2014-11-19 22:08:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/5a3d13923277f74362343ad4089e78800a2a3a71', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and continued work on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 9, 'created': '2014-11-19 22:14:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/417bd2741fd2674fa6febfae565d0673f2839b5e', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and continued work on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 10, 'created': '2014-11-24 16:05:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9c815ddc8573ea92881628510ceacd3e299c9a37', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and continued work on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 11, 'created': '2014-11-25 03:10:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/531b2e91e9c86745665f3904ed000dc3a69d9025', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and continued work on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 12, 'created': '2014-11-25 03:15:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6df5449bf6e626e6439f3f131fe6ee759adfbede', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and continued work on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 13, 'created': '2014-11-25 15:58:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/0595990c0d46db1eafd7e24d38b6cd5c0eedb736', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and continued work on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}, {'number': 14, 'created': '2014-11-25 16:09:07.000000000', 'files': ['cinder/volume/targets/driver.py', 'cinder/tests/targets/test_base_iscsi_driver.py', 'cinder/tests/targets/__init__.py', 'cinder/volume/targets/fake.py', 'cinder/volume/targets/tgt.py', 'cinder/volume/targets/iet.py', 'cinder/volume/targets/iser.py', 'cinder/volume/targets/lio.py', 'cinder/volume/targets/iscsi.py', 'cinder/exception.py', 'cinder/volume/targets/__init__.py', 'cinder/tests/targets/test_tgt_driver.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/c505cb8046a2d49a327838d32126c41852f2f8da', 'message': 'Add iSCSI Target objects as independent objects\n\nThis patch is a step in decoupling the target\nmethods and the Volume Driver\'s Control methods.\n\nThis adds the targets directory and the new target objects\nthat we use with the exception of IET (follow up for that later).\nTgtAdm and LIO drivers have been tested with the new LVM object.\n\nAll existing drivers are still able to be specified and use the\nsame objects and code-path they were using previously.  New\nconnector objects are only used when specifying the new driver.\n\nNext step will be mapping current ref LVM driver to the new\nLVM object and target model and continued work on the unit-tests.\n\nAfter that mark the ""old"" methods and objects as deprecated\nand we can then begin working on some other improvements.\n\nChange-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2\nPartial-Bug: #1329139\n'}]",14,131860,c505cb8046a2d49a327838d32126c41852f2f8da,76,12,14,2243,,,0,"Add iSCSI Target objects as independent objects

This patch is a step in decoupling the target
methods and the Volume Driver's Control methods.

This adds the targets directory and the new target objects
that we use with the exception of IET (follow up for that later).
TgtAdm and LIO drivers have been tested with the new LVM object.

All existing drivers are still able to be specified and use the
same objects and code-path they were using previously.  New
connector objects are only used when specifying the new driver.

Next step will be mapping current ref LVM driver to the new
LVM object and target model and continued work on the unit-tests.

After that mark the ""old"" methods and objects as deprecated
and we can then begin working on some other improvements.

Change-Id: If02db137f936dc5b509fc81ca3c29ae4f87f1cb2
Partial-Bug: #1329139
",git fetch https://review.opendev.org/openstack/cinder refs/changes/60/131860/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/targets/driver.py', 'cinder/volume/targets/fake.py', 'cinder/volume/targets/__init__.py', 'cinder/volume/targets/tgt.py', 'cinder/volume/targets/iet.py', 'cinder/volume/targets/iser.py', 'cinder/volume/targets/lio.py', 'cinder/volume/targets/iscsi.py']",8,dd7ca87ba51e7e2e3e2a1a356b33f375da06af57,transition_lvmdriver_to_new_tgt_objects,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from cinder import exception from cinder.i18n import _, _LW, _LE from cinder.openstack.common import log as logging from cinder.openstack.common import processutils from cinder.volume.targets import driver LOG = logging.getLogger(__name__) class ISCSITarget(driver.Target): """"""Target object for block storage devices. Base class for target object, where target is data transport mechanism (target) specific calls. This includes things like create targets, attach, detach etc. """""" def __init__(self, *args, **kwargs): super(ISCSITarget, self).__init__(*args, **kwargs) self.iscsi_target_prefix = \ self.configuration.safe_get('iscsi_target_prefix') self.protocol = 'iSCSI' def _get_iscsi_properties(self, volume): """"""Gets iscsi configuration We ideally get saved information in the volume entity, but fall back to discovery if need be. Discovery may be completely removed in the future. The properties are: :target_discovered: boolean indicating whether discovery was used :target_iqn: the IQN of the iSCSI target :target_portal: the portal of the iSCSI target :target_lun: the lun of the iSCSI target :volume_id: the uuid of the volume :auth_method:, :auth_username:, :auth_password: the authentication details. Right now, either auth_method is not present meaning no authentication, or auth_method == `CHAP` meaning use CHAP with the specified credentials. :access_mode: the volume access mode allow client used ('rw' or 'ro' currently supported) """""" properties = {} location = volume['provider_location'] if location: # provider_location is the same format as iSCSI discovery output properties['target_discovered'] = False else: location = self._do_iscsi_discovery(volume) if not location: msg = (_(""Could not find iSCSI export for volume %s"") % (volume['name'])) raise exception.InvalidVolume(reason=msg) LOG.debug((""ISCSI Discovery: Found %s"") % (location)) properties['target_discovered'] = True results = location.split("" "") properties['target_portal'] = results[0].split("","")[0] properties['target_iqn'] = results[1] try: properties['target_lun'] = int(results[2]) except (IndexError, ValueError): # NOTE(jdg): The following is carried over from the existing # code. The trick here is that different targets use different # default lun numbers, the base driver with tgtadm uses 1 # others like LIO use 0. if (self.configuration.volume_driver in ['cinder.volume.drivers.lvm.LVMISCSIDriver', 'cinder.volume.drivers.lvm.ThinLVMVolumeDriver'] and self.configuration.iscsi_helper == 'tgtadm'): properties['target_lun'] = 1 else: properties['target_lun'] = 0 properties['volume_id'] = volume['id'] auth = volume['provider_auth'] if auth: (auth_method, auth_username, auth_secret) = auth.split() properties['auth_method'] = auth_method properties['auth_username'] = auth_username properties['auth_password'] = auth_secret geometry = volume.get('provider_geometry', None) if geometry: (physical_block_size, logical_block_size) = geometry.split() properties['physical_block_size'] = physical_block_size properties['logical_block_size'] = logical_block_size encryption_key_id = volume.get('encryption_key_id', None) properties['encrypted'] = encryption_key_id is not None return properties def _iscsi_authentication(self, chap, name, password): return ""%s %s %s"" % (chap, name, password) def _do_iscsi_discovery(self, volume): # TODO(justinsb): Deprecate discovery and use stored info # NOTE(justinsb): Discovery won't work with CHAP-secured targets (?) LOG.warn(_LW(""ISCSI provider_location not stored, using discovery"")) volume_name = volume['name'] try: # NOTE(griff) We're doing the split straight away which should be # safe since using '@' in hostname is considered invalid (out, _err) = self._execute('iscsiadm', '-m', 'discovery', '-t', 'sendtargets', '-p', volume['host'].split('@')[0], run_as_root=True) except processutils.ProcessExecutionError as ex: LOG.error(_LE(""ISCSI discovery attempt failed for:%s"") % volume['host'].split('@')[0]) LOG.debug((""Error from iscsiadm -m discovery: %s"") % ex.stderr) return None for target in out.splitlines(): if (self.configuration.safe_get('iscsi_ip_address') in target and volume_name in target): return target return None def _get_target_chap_auth(self, volume_id): """"""Get the current chap auth username and password."""""" return None def detach_volume(self, context, volume): self._get_iscsi_properties(volume) def initialize_connection(self, volume, **kwargs): """"""Initializes the connection and returns connection info. The iscsi driver returns a driver_volume_type of 'iscsi'. The format of the driver data is defined in _get_iscsi_properties. Example return value:: { 'driver_volume_type': 'iscsi' 'data': { 'target_discovered': True, 'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001', 'target_portal': '127.0.0.0.1:3260', 'volume_id': '9a0d35d0-175a-11e4-8c21-0800200c9a66', 'access_mode': 'rw' } } """""" iscsi_properties = self._get_iscsi_properties(volume) return { 'driver_volume_type': 'iscsi', 'data': iscsi_properties } def validate_connector(self, connector): # NOTE(jdg): api passes in connector which is initiator info if 'initiator' not in connector: err_msg = (_LE('The volume driver requires the iSCSI initiator ' 'name in the connector.')) LOG.error(err_msg) raise exception.VolumeBackendAPIException(data=err_msg) ",,985,0
openstack%2Fcinder~master~I47f5e3ba30c7d8950862bc0a0d67d33d90c7eecf,openstack/cinder,master,I47f5e3ba30c7d8950862bc0a0d67d33d90c7eecf,Remove Python 2.6 classifier,MERGED,2014-11-25 15:55:25.000000000,2014-12-06 02:53:17.000000000,2014-11-25 21:43:00.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 1736}, {'_account_id': 2243}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12369}]","[{'number': 1, 'created': '2014-11-25 15:55:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/17d140f89e69ce3ff55760a94ec3057b1d99be40', 'message': 'Remove Python 2.6 classifier\n\nCinder does not support Python 2.6 anymore starting with Kilo and might\nnot work correctly with it, so remove the classifier.\n\nChange-Id: I47f5e3ba30c7d8950862bc0a0d67d33d90c7eecf\n'}, {'number': 2, 'created': '2014-11-25 16:00:03.000000000', 'files': ['setup.cfg', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/cinder/commit/c6c0f4d0f4342b2018ff00a01772aaf0e720d4ef', 'message': 'Remove Python 2.6 classifier\n\nCinder does not support Python 2.6 anymore starting with Kilo and might\nnot work correctly with it, so remove the classifier.\n\nChange-Id: I47f5e3ba30c7d8950862bc0a0d67d33d90c7eecf\n'}]",0,137111,c6c0f4d0f4342b2018ff00a01772aaf0e720d4ef,16,9,2,1669,,,0,"Remove Python 2.6 classifier

Cinder does not support Python 2.6 anymore starting with Kilo and might
not work correctly with it, so remove the classifier.

Change-Id: I47f5e3ba30c7d8950862bc0a0d67d33d90c7eecf
",git fetch https://review.opendev.org/openstack/cinder refs/changes/11/137111/2 && git format-patch -1 --stdout FETCH_HEAD,['setup.cfg'],1,17d140f89e69ce3ff55760a94ec3057b1d99be40,jd/py26,, Programming Language :: Python :: 2.6,0,1
openstack%2Fproject-config~master~I262e51b9cbdc02f30bae2d39fe9f638179965b96,openstack/project-config,master,I262e51b9cbdc02f30bae2d39fe9f638179965b96,Adjust wording for new Workflow URLs,MERGED,2014-12-05 19:48:25.000000000,2014-12-06 02:51:43.000000000,2014-12-06 02:51:42.000000000,"[{'_account_id': 3}, {'_account_id': 6316}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-12-05 19:48:25.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/project-config/commit/665739c76f61b3201cff16d6387c615b2bd9fa65', 'message': ""Adjust wording for new Workflow URLs\n\nWorkflow documentation links were updated, but the wording doesn't\nquite make sense now because the first link includes the second link.\n\nMade adjustments to the text so that it is clear which link to use.\n\nChange-Id: I262e51b9cbdc02f30bae2d39fe9f638179965b96\n""}]",0,139718,665739c76f61b3201cff16d6387c615b2bd9fa65,7,3,1,6609,,,0,"Adjust wording for new Workflow URLs

Workflow documentation links were updated, but the wording doesn't
quite make sense now because the first link includes the second link.

Made adjustments to the text so that it is clear which link to use.

Change-Id: I262e51b9cbdc02f30bae2d39fe9f638179965b96
",git fetch https://review.opendev.org/openstack/project-config refs/changes/18/139718/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,665739c76f61b3201cff16d6387c615b2bd9fa65,,"If you already have a good understanding of how the system works and your OpenStack accounts are set up, you can skip to the development workflow section of this documentation to learn how changes to OpenStack should be submitted for review via the Gerrit tool:","Once those steps have been completed, changes to OpenStack should be submitted for review via the Gerrit tool, following the workflow documented at:",4,3
openstack%2Fcinder~master~I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3,openstack/cinder,master,I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3,Implementing the use of _L’x’/i18n markers,MERGED,2014-11-04 16:47:00.000000000,2014-12-06 02:34:26.000000000,2014-12-01 14:59:39.000000000,"[{'_account_id': 3}, {'_account_id': 842}, {'_account_id': 1207}, {'_account_id': 1773}, {'_account_id': 2243}, {'_account_id': 6491}, {'_account_id': 7198}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10503}, {'_account_id': 10621}, {'_account_id': 10622}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12369}, {'_account_id': 12716}, {'_account_id': 12780}, {'_account_id': 12924}]","[{'number': 1, 'created': '2014-11-04 16:47:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/825d9b67fc8ccd81eebaa8a5b9948b84eb7c8b7d', 'message': 'Implementing the use of _L’x’/i18n markers\n\nPlacing the _Lx markers back into the code.  No other cleaner solution has\nhas been implemented. Patches will be submitted in a series of sub\ndirectories and in a fashion that is manageable.\n\nChange-Id: I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3\nPartial-Bug: #1384312\n'}, {'number': 2, 'created': '2014-11-04 17:20:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/bcddf70120a125d2c47f40b898309afcb28c08fa', 'message': 'Implementing the use of _L’x’/i18n markers\n\nPlacing the _Lx markers back into the code.  No other cleaner solution has\nhas been implemented. Patches will be submitted in a series of sub\ndirectories and in a fashion that is manageable.\n\nChange-Id: I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3\nPartial-Bug: #1384312\n'}, {'number': 3, 'created': '2014-11-05 11:32:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/07412aaa7ddb56844f22b605269ccd978e21c19a', 'message': 'Implementing the use of _L’x’/i18n markers\n\nPlacing the _Lx markers back into the code.  No other cleaner solution has\nhas been implemented. Patches will be submitted in a series of sub\ndirectories and in a fashion that is manageable.\n\nChange-Id: I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3\nPartial-Bug: #1384312\n'}, {'number': 4, 'created': '2014-11-05 13:04:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/52f96eebd8c0746e787a7b97e4fa7010a1f83487', 'message': 'Implementing the use of _L’x’/i18n markers\n\nPlacing the _Lx markers back into the code.  No other cleaner solution has\nhas been implemented. Patches will be submitted in a series of sub\ndirectories and in a fashion that is manageable.\n\nChange-Id: I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3\nPartial-Bug: #1384312\n'}, {'number': 5, 'created': '2014-11-05 14:31:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/59e23b480c7712f4e71d3d0e0f2703cf0e3c7002', 'message': 'Implementing the use of _L’x’/i18n markers\n\nPlacing the _Lx markers back into the code.  No other cleaner solution has\nhas been implemented. Patches will be submitted in a series of sub\ndirectories and in a fashion that is manageable.\n\nChange-Id: I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3\nPartial-Bug: #1384312\n'}, {'number': 6, 'created': '2014-11-06 13:26:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6b2e5cc64855831458dc985519171d42464b69a0', 'message': 'Implementing the use of _L’x’/i18n markers\n\nPlacing the _Lx markers back into the code.  No other cleaner solution has\nhas been implemented. Patches will be submitted in a series of sub\ndirectories and in a fashion that is manageable.\n\nChange-Id: I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3\nPartial-Bug: #1384312\n'}, {'number': 7, 'created': '2014-11-20 09:01:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/0ed0f74a4ac9677e28e838476ce8eb12d0024936', 'message': 'Implementing the use of _L’x’/i18n markers\n\nPlacing the _Lx markers back into the code.  No other cleaner solution has\nhas been implemented. Patches will be submitted in a series of sub\ndirectories and in a fashion that is manageable.\n\nChange-Id: I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3\nPartial-Bug: #1384312\n'}, {'number': 8, 'created': '2014-11-20 10:26:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/08e1b4ff20d04c7426a8ce59f741451470b55ceb', 'message': 'Implementing the use of _L’x’/i18n markers\n\nPlacing the _Lx markers back into the code.  No other cleaner solution has\nhas been implemented. Patches will be submitted in a series of sub\ndirectories and in a fashion that is manageable.\n\nChange-Id: I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3\nPartial-Bug: #1384312\n'}, {'number': 9, 'created': '2014-11-21 14:43:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/7029b0de22137f93ac45aa9051cee8248a3ffb32', 'message': 'Implementing the use of _L’x’/i18n markers\n\nPlacing the _Lx markers back into the code.  No other cleaner solution has\nhas been implemented. Patches will be submitted in a series of sub\ndirectories and in a fashion that is manageable.\n\nChange-Id: I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3\nPartial-Bug: #1384312\n'}, {'number': 10, 'created': '2014-11-25 10:13:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b152422fc6dca0131cc55e76018187ffec810437', 'message': 'Implementing the use of _L’x’/i18n markers\n\nPlacing the _Lx markers back into the code.  No other cleaner solution has\nhas been implemented. Patches will be submitted in a series of sub\ndirectories and in a fashion that is manageable.\n\nChange-Id: I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3\nPartial-Bug: #1384312\n'}, {'number': 11, 'created': '2014-11-25 15:59:56.000000000', 'files': ['cinder/scheduler/filter_scheduler.py', 'cinder/zonemanager/utils.py', 'cinder/zonemanager/drivers/brocade/brcd_fc_zone_driver.py', 'cinder/zonemanager/drivers/cisco/cisco_fc_san_lookup_service.py', 'cinder/scheduler/flows/create_volume.py', 'cinder/scheduler/manager.py', 'cinder/scheduler/scheduler_options.py', 'cinder/transfer/api.py', 'cinder/zonemanager/drivers/brocade/brcd_fc_san_lookup_service.py', 'cinder/db/sqlalchemy/api.py', 'cinder/zonemanager/drivers/cisco/cisco_fc_zone_driver.py', 'cinder/zonemanager/fc_zone_manager.py', 'cinder/image/glance.py', 'cinder/scheduler/host_manager.py', 'cinder/zonemanager/drivers/cisco/cisco_fc_zone_client_cli.py', 'cinder/scheduler/filters/capacity_filter.py', 'cinder/zonemanager/drivers/brocade/brcd_fc_zone_client_cli.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/f69c40ebb40714c15bcb983643b93a6ba2f5424c', 'message': 'Implementing the use of _L’x’/i18n markers\n\nPlacing the _Lx markers back into the code. No other cleaner solution has\nhas been implemented. Patches will be submitted in a series of sub\ndirectories and in a fashion that is manageable.\n\nPartial-Bug: #1384312\n\nChange-Id: I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3\n'}]",6,132698,f69c40ebb40714c15bcb983643b93a6ba2f5424c,89,19,11,12716,,,0,"Implementing the use of _L’x’/i18n markers

Placing the _Lx markers back into the code. No other cleaner solution has
has been implemented. Patches will be submitted in a series of sub
directories and in a fashion that is manageable.

Partial-Bug: #1384312

Change-Id: I3974b58bd9b8b9e3c34d5a609228e30c6a08a3c3
",git fetch https://review.opendev.org/openstack/cinder refs/changes/98/132698/10 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/scheduler/filter_scheduler.py', 'cinder/zonemanager/utils.py', 'cinder/db/sqlalchemy/migrate_repo/versions/017_add_encryption_information.py', 'cinder/zonemanager/drivers/brocade/brcd_fc_zone_driver.py', 'cinder/zonemanager/drivers/cisco/cisco_fc_san_lookup_service.py', 'cinder/scheduler/flows/create_volume.py', 'cinder/scheduler/manager.py', 'cinder/scheduler/scheduler_options.py', 'cinder/db/sqlalchemy/migrate_repo/versions/026_add_consistencygroup_quota_class.py', 'cinder/db/sqlalchemy/api.py', 'cinder/openstack/common/request_utils.py', 'cinder/zonemanager/drivers/cisco/cisco_fc_zone_driver.py', 'cinder/zonemanager/fc_zone_manager.py', 'cinder/db/sqlalchemy/migrate_repo/versions/001_cinder_init.py', 'cinder/scheduler/host_manager.py', 'cinder/db/sqlalchemy/migrate_repo/versions/003_glance_metadata.py', 'cinder/db/sqlalchemy/migrate_repo/versions/020_add_volume_admin_metadata_table.py', 'cinder/db/sqlalchemy/migrate_repo/versions/009_add_snapshot_metadata_table.py', 'cinder/db/sqlalchemy/migrate_repo/versions/016_drop_sm_tables.py', 'cinder/db/sqlalchemy/migrate_repo/versions/025_add_consistencygroup.py', 'cinder/db/sqlalchemy/migrate_repo/versions/008_add_backup.py', 'cinder/db/sqlalchemy/migrate_repo/versions/023_add_expire_reservations_index.py', 'cinder/transfer/api.py', 'cinder/zonemanager/drivers/brocade/brcd_fc_san_lookup_service.py', 'cinder/db/sqlalchemy/migrate_repo/versions/010_add_transfers_table.py', 'cinder/image/glance.py', 'cinder/db/sqlalchemy/migrate_repo/versions/018_add_qos_specs.py', 'cinder/openstack/common/lockutils.py', 'cinder/zonemanager/drivers/cisco/cisco_fc_zone_client_cli.py', 'cinder/scheduler/filters/capacity_filter.py', 'cinder/zonemanager/drivers/brocade/brcd_fc_zone_client_cli.py']",31,825d9b67fc8ccd81eebaa8a5b9948b84eb7c8b7d,bug/1384312,"from cinder.i18n import _LE LOG.error(_LE(""Failed getting active zone set "" ""from fabric %s""), self.switch_ip) LOG.error(_LE(""Deleting zone failed %s""), zone) LOG.error(_LE(""Failed collecting nsshow "" ""info for fabric %s""), self.switch_ip) LOG.error(_LE(""No CLI output for firmware version check"")) LOG.error(_LE(""Error running SSH command: %s"") % command) LOG.error(_LE(""Error executing command via ssh: %s""), e)","from cinder.i18n import _ LOG.error(_(""Failed getting active zone set "" ""from fabric %s""), self.switch_ip) LOG.error(_(""Deleting zone failed %s""), zone) LOG.error(_(""Failed collecting nsshow "" ""info for fabric %s""), self.switch_ip) LOG.error(_(""No CLI output for firmware version check"")) LOG.error(_(""Error running SSH command: %s"") % command) LOG.error(_(""Error executing command via ssh: %s""), e)",171,169
openstack%2Fcinder~master~I035d71cb3b81f0c8bfd83ed81d8426cb0df31c90,openstack/cinder,master,I035d71cb3b81f0c8bfd83ed81d8426cb0df31c90,Match mock.patch decorator with appropriate param,MERGED,2014-11-25 14:36:39.000000000,2014-12-06 02:12:34.000000000,2014-11-25 17:33:05.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 4523}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 12016}]","[{'number': 1, 'created': '2014-11-25 14:36:39.000000000', 'files': ['cinder/tests/test_nimble.py', 'cinder/tests/test_volume.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/65c1528b1c7d038778840d298a3da33f220c466e', 'message': ""Match mock.patch decorator with appropriate param\n\nmock.patch and mock.patch.object can be used as decorators for mocking\nwithin the scope of the function they decorate. When there are multiple\ndecorators it is important the function parameters relate to the\ncorresponding patch objects i.e. that the parameter order matches the\ndecorator order.\nIt is easiest to explain this with an example:\n\n@mock.patch.object(Foo, 'bar')\n@mock.patch.object(SomeClass, 'some_method', some_function)\n@mock.patch.object(AClass, 'a_method')\ndef test_some_stuff(self, mock_a_method, mock_bar):\n    pass\n\nSo the decorator closest to the function definition must correspond to\nthe first (left-most) patch parameter. Note, if the decorator is given a\nthird argument, the kwarg new, then the decorated function is not\npassed an extra argument by that decorator.\n\nChange-Id: I035d71cb3b81f0c8bfd83ed81d8426cb0df31c90\n""}]",0,137085,65c1528b1c7d038778840d298a3da33f220c466e,10,6,1,7219,,,0,"Match mock.patch decorator with appropriate param

mock.patch and mock.patch.object can be used as decorators for mocking
within the scope of the function they decorate. When there are multiple
decorators it is important the function parameters relate to the
corresponding patch objects i.e. that the parameter order matches the
decorator order.
It is easiest to explain this with an example:

@mock.patch.object(Foo, 'bar')
@mock.patch.object(SomeClass, 'some_method', some_function)
@mock.patch.object(AClass, 'a_method')
def test_some_stuff(self, mock_a_method, mock_bar):
    pass

So the decorator closest to the function definition must correspond to
the first (left-most) patch parameter. Note, if the decorator is given a
third argument, the kwarg new, then the decorated function is not
passed an extra argument by that decorator.

Change-Id: I035d71cb3b81f0c8bfd83ed81d8426cb0df31c90
",git fetch https://review.opendev.org/openstack/cinder refs/changes/85/137085/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/test_nimble.py', 'cinder/tests/test_volume.py']",2,65c1528b1c7d038778840d298a3da33f220c466e,mock_patch_order," @mock.patch.object(QUOTAS, 'commit') @mock.patch.object(QUOTAS, 'reserve') _mock_volume_get,"," @mock.patch.object(QUOTAS, 'reserve') @mock.patch.object(QUOTAS, 'commit') _mock_volume_get,",5,5
openstack%2Fironic-python-agent~master~I7f7196b740006b8a2d7123ad6e781e35718da07d,openstack/ironic-python-agent,master,I7f7196b740006b8a2d7123ad6e781e35718da07d,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:41:50.000000000,2014-12-06 02:08:02.000000000,2014-12-06 02:08:00.000000000,"[{'_account_id': 3}, {'_account_id': 3099}, {'_account_id': 10342}, {'_account_id': 10343}, {'_account_id': 11739}]","[{'number': 1, 'created': '2014-12-05 03:41:50.000000000', 'files': ['README.rst'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/f9a7ad2db6643efea9e60a6632b5c0daf2d15931', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I7f7196b740006b8a2d7123ad6e781e35718da07d\n'}]",0,139330,f9a7ad2db6643efea9e60a6632b5c0daf2d15931,9,5,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I7f7196b740006b8a2d7123ad6e781e35718da07d
",git fetch https://review.opendev.org/openstack/ironic-python-agent refs/changes/30/139330/1 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,f9a7ad2db6643efea9e60a6632b5c0daf2d15931,infra-manual,contribute <http://docs.openstack.org/infra/manual/developers.html>`__.,contribute <https://wiki.openstack.org/wiki/How_To_Contribute>`__.,1,1
openstack%2Fhorizon~master~Idf0c9b14712bc1c433cb6a5aef602d1820af915c,openstack/horizon,master,Idf0c9b14712bc1c433cb6a5aef602d1820af915c,Refactor horizon/common/_modal_form.html template,MERGED,2014-09-18 12:23:28.000000000,2014-12-06 02:03:46.000000000,2014-12-06 02:03:45.000000000,"[{'_account_id': 3}, {'_account_id': 5623}, {'_account_id': 8040}, {'_account_id': 8871}, {'_account_id': 9576}, {'_account_id': 10295}, {'_account_id': 11592}, {'_account_id': 13325}]","[{'number': 1, 'created': '2014-09-18 12:23:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/949984aa17aec8f79ceac10f93761da5977249b0', 'message': 'Refactor horizon/common/_modal_form.html template\n\nTemplate _modal_form.html has an identical header with\n_modal.html - thus it makes sense to inherit _modal_form.html\nfrom _modal.html to reduce duplication.\n\nChange-Id: Idf0c9b14712bc1c433cb6a5aef602d1820af915c\n'}, {'number': 2, 'created': '2014-09-22 12:19:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/d5fe64ecefda182bc917585713758938a2478cb2', 'message': 'Refactor horizon/common/_modal_form.html template\n\nTemplate _modal_form.html has an identical header with\n_modal.html - thus it makes sense to inherit _modal_form.html\nfrom _modal.html to reduce duplication.\n\nChange-Id: Idf0c9b14712bc1c433cb6a5aef602d1820af915c\n'}, {'number': 3, 'created': '2014-09-23 07:16:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/dd4795c8f6ae29e2d484cfe2e890557ec55c414f', 'message': 'Refactor horizon/common/_modal_form.html template\n\nTemplate _modal_form.html has an identical header with\n_modal.html - thus it makes sense to inherit _modal_form.html\nfrom _modal.html to reduce duplication.\n\nChange-Id: Idf0c9b14712bc1c433cb6a5aef602d1820af915c\n'}, {'number': 4, 'created': '2014-09-24 11:39:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/41d7087d6736670c64a11df951f7f5e9229c1fce', 'message': 'Refactor horizon/common/_modal_form.html template\n\nTemplate _modal_form.html has an identical header with\n_modal.html - thus it makes sense to inherit _modal_form.html\nfrom _modal.html to reduce duplication.\n\nChange-Id: Idf0c9b14712bc1c433cb6a5aef602d1820af915c\n'}, {'number': 5, 'created': '2014-09-26 10:58:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/adb5c7630e6eb1cb49bdec23277658d08d3d76fc', 'message': 'Refactor horizon/common/_modal_form.html template\n\nTemplate _modal_form.html has an identical header with\n_modal.html - thus it makes sense to inherit _modal_form.html\nfrom _modal.html to reduce duplication.\n\nChange-Id: Idf0c9b14712bc1c433cb6a5aef602d1820af915c\n'}, {'number': 6, 'created': '2014-11-23 17:30:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/93f2ffd55ebe6a39e91783ae91c2962be1c96fee', 'message': 'Refactor horizon/common/_modal_form.html template\n\nTemplate _modal_form.html has an identical header with\n_modal.html - thus it makes sense to inherit _modal_form.html\nfrom _modal.html to reduce duplication.\n\nImplements: blueprint form-template-to-view\nChange-Id: Idf0c9b14712bc1c433cb6a5aef602d1820af915c\n'}, {'number': 7, 'created': '2014-11-23 17:39:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/dab956cc3d70c12829b352398eb15fd2f2e5b9e8', 'message': 'Refactor horizon/common/_modal_form.html template\n\nTemplate _modal_form.html has an identical header with\n_modal.html - thus it makes sense to inherit _modal_form.html\nfrom _modal.html to reduce duplication.\n\nImplements: blueprint form-template-to-view\nChange-Id: Idf0c9b14712bc1c433cb6a5aef602d1820af915c\n'}, {'number': 8, 'created': '2014-11-23 17:42:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/8812dc54fa3d76758d6ced85f7cbb0df48b8cd32', 'message': 'Refactor horizon/common/_modal_form.html template\n\nTemplate _modal_form.html has an identical header with\n_modal.html - thus it makes sense to inherit _modal_form.html\nfrom _modal.html to reduce duplication.\n\nImplements: blueprint form-template-to-view\nChange-Id: Idf0c9b14712bc1c433cb6a5aef602d1820af915c\n'}, {'number': 9, 'created': '2014-12-05 12:32:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/b4ae9e6a7a881b730b96c0741ade819cada1930e', 'message': 'Refactor horizon/common/_modal_form.html template\n\nTemplate _modal_form.html has an identical header with\n_modal.html - thus it makes sense to inherit _modal_form.html\nfrom _modal.html to reduce duplication.\n\nImplements: blueprint form-template-to-view\nChange-Id: Idf0c9b14712bc1c433cb6a5aef602d1820af915c\n'}, {'number': 10, 'created': '2014-12-05 21:02:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/0452fc43b680224e17a9cdd9513f7c0cd846f151', 'message': 'Refactor horizon/common/_modal_form.html template\n\nTemplate _modal_form.html has an identical header with\n_modal.html - thus it makes sense to inherit _modal_form.html\nfrom _modal.html to reduce duplication.\n\nImplements: blueprint form-template-to-view\nChange-Id: Idf0c9b14712bc1c433cb6a5aef602d1820af915c\n'}, {'number': 11, 'created': '2014-12-06 00:37:46.000000000', 'files': ['horizon/templates/horizon/common/_modal_form.html', 'horizon/templates/horizon/common/_modal.html'], 'web_link': 'https://opendev.org/openstack/horizon/commit/40ca91417364ea8a13076f5c3bf64d7058083670', 'message': 'Refactor horizon/common/_modal_form.html template\n\nTemplate _modal_form.html has an identical header with\n_modal.html - thus it makes sense to inherit _modal_form.html\nfrom _modal.html to reduce duplication.\n\nPartially Implements: blueprint form-template-to-view\nChange-Id: Idf0c9b14712bc1c433cb6a5aef602d1820af915c\n'}]",9,122398,40ca91417364ea8a13076f5c3bf64d7058083670,54,8,11,8040,,,0,"Refactor horizon/common/_modal_form.html template

Template _modal_form.html has an identical header with
_modal.html - thus it makes sense to inherit _modal_form.html
from _modal.html to reduce duplication.

Partially Implements: blueprint form-template-to-view
Change-Id: Idf0c9b14712bc1c433cb6a5aef602d1820af915c
",git fetch https://review.opendev.org/openstack/horizon refs/changes/98/122398/8 && git format-patch -1 --stdout FETCH_HEAD,"['horizon/templates/horizon/common/_modal_form.html', 'horizon/templates/horizon/common/_modal.html']",2,949984aa17aec8f79ceac10f93761da5977249b0,bp/form-template-to-view, {% block content %} {% endblock %}{% block modal-js %} {% endblock %},,35,39
openstack%2Ftaskflow~master~Ib61b34b83203f5999f92b6e8616efd90cb259f81,openstack/taskflow,master,Ib61b34b83203f5999f92b6e8616efd90cb259f81,Update listeners to ensure they correctly handle all atoms,MERGED,2014-11-25 03:51:39.000000000,2014-12-06 02:02:50.000000000,2014-12-06 02:02:49.000000000,"[{'_account_id': 3}, {'_account_id': 1297}]","[{'number': 1, 'created': '2014-11-25 03:51:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/640a5d7f63f7b63c351b213c81843ee3d69bc073', 'message': 'Update listeners to ensure they correctly handle all atoms\n\nInstead of looking for keys that are task specific (as well\nas using the deprecated task_notifier) we need to update the\nlisteners to be agnostic to atoms (retry or task) that are\nsent to them so that key errors do not occur when extracting\nany data sent along with the event notification.\n\nFixes bug 1395966\n\nChange-Id: Ib61b34b83203f5999f92b6e8616efd90cb259f81\n'}, {'number': 2, 'created': '2014-11-25 03:52:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/993bc17bc08721bbb5859bc3b23e4e3806469500', 'message': ""Update listeners to ensure they correctly handle all atoms\n\nInstead of looking for keys that are task specific (as well\nas using the deprecated 'task_notifier') we need to update the\nlisteners to be agnostic to atoms (retry or task) that are\nsent to them so that key errors do not occur when extracting\nany data sent along with the event notification.\n\nFixes bug 1395966\n\nChange-Id: Ib61b34b83203f5999f92b6e8616efd90cb259f81\n""}, {'number': 3, 'created': '2014-11-25 03:54:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/a69296a546cac55a61f49b2789f7e73fa0b1b555', 'message': ""Update listeners to ensure they correctly handle all atoms\n\nInstead of looking for keys that are task specific (as well\nas using the deprecated 'task_notifier') we need to update the\nlisteners to be agnostic to atoms (retry or task) that are\nsent to them so that key errors do not occur when extracting\nany data sent along with the event notification.\n\nFixes bug 1395966\n\nChange-Id: Ib61b34b83203f5999f92b6e8616efd90cb259f81\n""}, {'number': 4, 'created': '2014-11-25 05:59:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/cffe6f3324932b992f82245666bf3be46d26e06a', 'message': ""Update listeners to ensure they correctly handle all atoms\n\nInstead of looking for keys that are task specific (as well\nas using the deprecated 'task_notifier') we need to update the\nlisteners to be agnostic to atoms (retry or task) that are\nsent to them so that key errors do not occur when extracting\nany data sent along with the event notification.\n\nFixes bug 1395966\n\nChange-Id: Ib61b34b83203f5999f92b6e8616efd90cb259f81\n""}, {'number': 5, 'created': '2014-11-25 07:37:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/91294858c4334bbf32cc6ad2b48f59f6de2670d3', 'message': ""Update listeners to ensure they correctly handle all atoms\n\nInstead of looking for keys that are task specific (as well\nas using the deprecated 'task_notifier') we need to update the\nlisteners to be agnostic to atoms (retry or task) that are\nsent to them so that key errors do not occur when extracting\nany data sent along with the event notification.\n\nFixes bug 1395966\n\nChange-Id: Ib61b34b83203f5999f92b6e8616efd90cb259f81\n""}, {'number': 6, 'created': '2014-11-25 19:57:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/9143c9f3af2c6231bb81988eff3f5991446469a8', 'message': ""Update listeners to ensure they correctly handle all atoms\n\nInstead of looking for keys that are task specific (as well\nas using the deprecated 'task_notifier') we need to update the\nlisteners to be agnostic to atoms (retry or task) that are\nsent to them so that key errors do not occur when extracting\nany data sent along with the event notification.\n\nFixes bug 1395966\n\nChange-Id: Ib61b34b83203f5999f92b6e8616efd90cb259f81\n""}, {'number': 7, 'created': '2014-11-25 22:36:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/4ef1f872e90cda9f2d11148c7108685d4dee008f', 'message': ""Update listeners to ensure they correctly handle all atoms\n\nInstead of looking for keys that are task specific (as well\nas using the deprecated 'task_notifier') we need to update the\nlisteners to be agnostic to atoms (retry or task) that are\nsent to them so that key errors do not occur when extracting\nany data sent along with the event notification.\n\nFixes bug 1395966\n\nChange-Id: Ib61b34b83203f5999f92b6e8616efd90cb259f81\n""}, {'number': 8, 'created': '2014-12-02 02:23:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/91f6d3a5b0e47e76a4e2594864413b13774c6237', 'message': ""Update listeners to ensure they correctly handle all atoms\n\nInstead of looking for keys that are task specific (as well\nas using the deprecated 'task_notifier') we need to update the\nlisteners to be agnostic to atoms (retry or task) that are\nsent to them so that key errors do not occur when extracting\nany data sent along with the event notification.\n\nFixes bug 1395966\n\nChange-Id: Ib61b34b83203f5999f92b6e8616efd90cb259f81\n""}, {'number': 9, 'created': '2014-12-02 03:43:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/09e238b083677d4eca076759fb2d5f3326c7b17a', 'message': ""Update listeners to ensure they correctly handle all atoms\n\nInstead of looking for keys that are task specific (as well\nas using the deprecated 'task_notifier') we need to update the\nlisteners to be agnostic to atoms (retry or task) that are\nsent to them so that key errors do not occur when extracting\nany data sent along with the event notification.\n\nFixes bug 1395966\n\nChange-Id: Ib61b34b83203f5999f92b6e8616efd90cb259f81\n""}, {'number': 10, 'created': '2014-12-02 05:10:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/c52150209b4a3f8afa8078832d3ba43177039430', 'message': ""Update listeners to ensure they correctly handle all atoms\n\nInstead of looking for keys that are task specific (as well\nas using the deprecated 'task_notifier') we need to update the\nlisteners to be agnostic to atoms (retry or task) that are\nsent to them so that key errors do not occur when extracting\nany data sent along with the event notification.\n\nFixes bug 1395966\n\nChange-Id: Ib61b34b83203f5999f92b6e8616efd90cb259f81\n""}, {'number': 11, 'created': '2014-12-04 19:04:14.000000000', 'files': ['taskflow/listeners/logging.py', 'taskflow/listeners/base.py', 'taskflow/listeners/printing.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/b8e975e885e4f5aaab52edcd23fee99d450273ad', 'message': ""Update listeners to ensure they correctly handle all atoms\n\nInstead of looking for keys that are task specific (as well\nas using the deprecated 'task_notifier') we need to update the\nlisteners to be agnostic to atoms (retry or task) that are\nsent to them so that key errors do not occur when extracting\nany data sent along with the event notification.\n\nFixes bug 1395966\n\nChange-Id: Ib61b34b83203f5999f92b6e8616efd90cb259f81\n""}]",0,136958,b8e975e885e4f5aaab52edcd23fee99d450273ad,31,2,11,1297,,,0,"Update listeners to ensure they correctly handle all atoms

Instead of looking for keys that are task specific (as well
as using the deprecated 'task_notifier') we need to update the
listeners to be agnostic to atoms (retry or task) that are
sent to them so that key errors do not occur when extracting
any data sent along with the event notification.

Fixes bug 1395966

Change-Id: Ib61b34b83203f5999f92b6e8616efd90cb259f81
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/58/136958/1 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/listeners/timing.py', 'taskflow/examples/delayed_return.py', 'taskflow/listeners/logging.py', 'taskflow/listeners/base.py', 'taskflow/tests/unit/test_suspend_flow.py', 'taskflow/listeners/printing.py']",6,640a5d7f63f7b63c351b213c81843ee3d69bc073,bug/1395966," atom_listen_for=(notifier.Notifier.ANY,), atom_listen_for=atom_listen_for,"," task_listen_for=(notifier.Notifier.ANY,), task_listen_for=task_listen_for,",31,22
openstack%2Fnova~master~Idb2e851109501c3f212c22d55f699194fac10f9a,openstack/nova,master,Idb2e851109501c3f212c22d55f699194fac10f9a,WIP: Nuke XML code/tests in api - Phase 3,ABANDONED,2014-12-05 22:48:40.000000000,2014-12-06 01:59:15.000000000,,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 5170}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-12-05 22:48:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/fd79dfa651d3e6bfcb57ec7233176348a62e91dc', 'message': 'WIP: Nuke XML code/tests in api - Phase 3\n\nChange-Id: Idb2e851109501c3f212c22d55f699194fac10f9a\n'}, {'number': 2, 'created': '2014-12-05 22:59:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a443b4b7da90bf7bb81ba79a010bb890e23e2669', 'message': 'WIP: Nuke XML code/tests in api - Phase 3\n\nChange-Id: Idb2e851109501c3f212c22d55f699194fac10f9a\n'}, {'number': 3, 'created': '2014-12-05 23:28:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/5f28c059c711da34f531f05ad2c529749fcd5233', 'message': 'WIP: Nuke XML code/tests in api - Phase 3\n\nChange-Id: Idb2e851109501c3f212c22d55f699194fac10f9a\n'}, {'number': 4, 'created': '2014-12-06 00:06:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a65c428fb4297994367d3f6274a42d919022e51d', 'message': 'WIP: Nuke XML code/tests in api - Phase 3\n\nChange-Id: Idb2e851109501c3f212c22d55f699194fac10f9a\n'}, {'number': 5, 'created': '2014-12-06 00:26:42.000000000', 'files': ['nova/api/openstack/compute/contrib/hypervisors.py', 'nova/api/openstack/compute/contrib/floating_ips.py', 'nova/api/openstack/compute/contrib/quota_classes.py', 'nova/api/openstack/compute/contrib/cells.py', 'nova/api/openstack/common.py', 'nova/api/openstack/compute/contrib/virtual_interfaces.py', 'nova/tests/unit/api/openstack/compute/contrib/test_flavor_access.py', 'nova/api/openstack/compute/contrib/config_drive.py', 'nova/api/openstack/compute/contrib/server_usage.py', 'nova/tests/unit/api/openstack/compute/contrib/test_instance_actions.py', 'nova/tests/unit/api/openstack/compute/contrib/test_volumes.py', 'nova/api/openstack/compute/contrib/disk_config.py', 'nova/api/openstack/compute/contrib/extended_availability_zone.py', 'nova/api/openstack/compute/image_metadata.py', 'nova/api/openstack/xmlutil.py', 'nova/api/openstack/compute/contrib/baremetal_nodes.py', 'nova/api/openstack/compute/versions.py', 'nova/api/openstack/compute/contrib/assisted_volume_snapshots.py', 'nova/api/openstack/compute/contrib/migrations.py', 'nova/api/openstack/compute/contrib/extended_status.py', 'nova/api/openstack/compute/server_metadata.py', 'nova/api/openstack/compute/contrib/extended_volumes.py', 'nova/api/openstack/compute/contrib/flavorextraspecs.py', 'nova/tests/unit/api/openstack/compute/contrib/test_simple_tenant_usage.py', 'nova/api/openstack/compute/contrib/volumes.py', 'nova/tests/unit/api/openstack/compute/contrib/test_floating_ip_dns.py', 'nova/api/openstack/compute/contrib/used_limits.py', 'nova/api/openstack/compute/contrib/quotas.py', 'nova/api/openstack/compute/plugins/v3/baremetal_nodes.py', 'nova/api/openstack/compute/contrib/extended_ips.py', 'nova/api/openstack/compute/contrib/floating_ip_dns.py', 'nova/api/openstack/compute/contrib/services.py', 'nova/api/openstack/compute/limits.py', 'nova/api/openstack/compute/contrib/hosts.py', 'nova/tests/unit/api/openstack/compute/contrib/test_hypervisors.py', 'nova/api/openstack/compute/contrib/extended_server_attributes.py', 'nova/api/openstack/compute/contrib/simple_tenant_usage.py', 'nova/api/openstack/extensions.py', 'nova/api/openstack/compute/contrib/flavorextradata.py', 'nova/tests/unit/api/openstack/compute/contrib/test_createserverext.py', 'nova/tests/unit/api/openstack/test_xmlutil.py', 'nova/api/openstack/compute/contrib/flavor_access.py', 'nova/tests/unit/api/openstack/compute/contrib/test_virtual_interfaces.py', 'nova/api/openstack/compute/contrib/server_groups.py', 'nova/api/openstack/compute/ips.py', 'nova/tests/unit/api/openstack/compute/contrib/test_floating_ips.py', 'nova/api/openstack/compute/flavors.py', 'nova/tests/unit/api/openstack/compute/contrib/test_certificates.py', 'nova/api/openstack/compute/contrib/flavor_swap.py', 'nova/api/openstack/compute/contrib/floating_ip_pools.py', 'nova/api/openstack/compute/contrib/server_diagnostics.py', 'nova/tests/unit/api/openstack/compute/test_versions.py', 'nova/tests/unit/api/openstack/compute/contrib/test_snapshots.py', 'nova/api/openstack/compute/contrib/cloudpipe.py', 'nova/api/openstack/compute/contrib/instance_actions.py', 'nova/tests/unit/api/openstack/compute/contrib/test_hosts.py', 'nova/api/openstack/compute/contrib/availability_zone.py', 'nova/api/openstack/compute/contrib/extended_ips_mac.py', 'nova/api/openstack/compute/contrib/flavor_disabled.py', 'nova/api/openstack/compute/contrib/keypairs.py', 'nova/api/openstack/compute/contrib/security_groups.py', 'nova/api/openstack/compute/contrib/server_external_events.py', 'nova/api/openstack/compute/images.py', 'nova/api/openstack/compute/contrib/certificates.py', 'nova/api/openstack/compute/contrib/flavormanage.py', 'nova/api/openstack/compute/servers.py', 'nova/api/openstack/wsgi.py', 'nova/api/openstack/compute/contrib/flavor_rxtx.py', 'nova/tests/unit/api/openstack/compute/contrib/test_availability_zone.py', 'nova/api/openstack/compute/contrib/agents.py', 'nova/api/openstack/compute/contrib/image_size.py', 'nova/tests/unit/api/openstack/compute/contrib/test_floating_ip_pools.py', 'nova/api/openstack/compute/contrib/security_group_default_rules.py', 'nova/api/openstack/compute/contrib/server_password.py', 'nova/api/openstack/compute/consoles.py', 'nova/tests/unit/api/ec2/test_apirequest.py', 'nova/api/openstack/compute/contrib/server_group_quotas.py', 'nova/api/openstack/compute/contrib/extended_virtual_interfaces_net.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/bb495ec0f890af0617255b6d1fc5c4f26f6e8637', 'message': 'WIP: Nuke XML code/tests in api - Phase 3\n\nChange-Id: Idb2e851109501c3f212c22d55f699194fac10f9a\n'}]",0,139744,bb495ec0f890af0617255b6d1fc5c4f26f6e8637,13,6,5,5638,,,0,"WIP: Nuke XML code/tests in api - Phase 3

Change-Id: Idb2e851109501c3f212c22d55f699194fac10f9a
",git fetch https://review.opendev.org/openstack/nova refs/changes/44/139744/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/api/openstack/compute/contrib/hypervisors.py', 'nova/api/openstack/compute/contrib/floating_ips.py', 'nova/api/openstack/compute/contrib/quota_classes.py', 'nova/api/openstack/compute/contrib/cells.py', 'nova/api/openstack/common.py', 'nova/api/openstack/compute/contrib/virtual_interfaces.py', 'nova/tests/unit/api/openstack/compute/contrib/test_flavor_access.py', 'nova/api/openstack/compute/contrib/config_drive.py', 'nova/api/openstack/compute/contrib/server_usage.py', 'nova/tests/unit/api/openstack/compute/contrib/test_instance_actions.py', 'nova/tests/unit/api/openstack/compute/contrib/test_volumes.py', 'nova/api/openstack/compute/contrib/disk_config.py', 'nova/api/openstack/compute/contrib/extended_availability_zone.py', 'nova/api/openstack/compute/image_metadata.py', 'nova/api/openstack/xmlutil.py', 'nova/api/openstack/compute/contrib/baremetal_nodes.py', 'nova/api/openstack/compute/versions.py', 'nova/api/openstack/compute/contrib/assisted_volume_snapshots.py', 'nova/api/openstack/compute/contrib/migrations.py', 'nova/api/openstack/compute/contrib/extended_status.py', 'nova/api/openstack/compute/server_metadata.py', 'nova/api/openstack/compute/contrib/extended_volumes.py', 'nova/api/openstack/compute/contrib/flavorextraspecs.py', 'nova/tests/unit/api/openstack/compute/contrib/test_simple_tenant_usage.py', 'nova/api/openstack/compute/contrib/volumes.py', 'nova/tests/unit/api/openstack/compute/contrib/test_floating_ip_dns.py', 'nova/api/openstack/compute/contrib/used_limits.py', 'nova/api/openstack/compute/contrib/quotas.py', 'nova/api/openstack/compute/plugins/v3/baremetal_nodes.py', 'nova/api/openstack/compute/contrib/extended_ips.py', 'nova/api/openstack/compute/contrib/floating_ip_dns.py', 'nova/api/openstack/compute/contrib/services.py', 'nova/api/openstack/compute/limits.py', 'nova/api/openstack/compute/contrib/hosts.py', 'nova/api/openstack/compute/contrib/extended_server_attributes.py', 'nova/api/openstack/compute/contrib/simple_tenant_usage.py', 'nova/api/openstack/extensions.py', 'nova/api/openstack/compute/contrib/flavorextradata.py', 'nova/tests/unit/api/openstack/test_xmlutil.py', 'nova/api/openstack/compute/contrib/flavor_access.py', 'nova/tests/unit/api/openstack/compute/contrib/test_virtual_interfaces.py', 'nova/api/openstack/compute/contrib/server_groups.py', 'nova/api/openstack/compute/ips.py', 'nova/tests/unit/api/openstack/compute/contrib/test_floating_ips.py', 'nova/api/openstack/compute/flavors.py', 'nova/tests/unit/api/openstack/compute/contrib/test_certificates.py', 'nova/api/openstack/compute/contrib/flavor_swap.py', 'nova/api/openstack/compute/contrib/floating_ip_pools.py', 'nova/api/openstack/compute/contrib/server_diagnostics.py', 'nova/tests/unit/api/openstack/compute/test_versions.py', 'nova/tests/unit/api/openstack/compute/contrib/test_snapshots.py', 'nova/api/openstack/compute/contrib/cloudpipe.py', 'nova/api/openstack/compute/contrib/instance_actions.py', 'nova/tests/unit/api/openstack/compute/contrib/test_hosts.py', 'nova/api/openstack/compute/contrib/availability_zone.py', 'nova/api/openstack/compute/contrib/extended_ips_mac.py', 'nova/api/openstack/compute/contrib/flavor_disabled.py', 'nova/api/openstack/compute/contrib/keypairs.py', 'nova/api/openstack/compute/contrib/security_groups.py', 'nova/api/openstack/compute/contrib/server_external_events.py', 'nova/api/openstack/compute/images.py', 'nova/api/openstack/compute/contrib/certificates.py', 'nova/api/openstack/compute/contrib/flavormanage.py', 'nova/api/openstack/compute/servers.py', 'nova/api/openstack/wsgi.py', 'nova/api/openstack/compute/contrib/flavor_rxtx.py', 'nova/tests/unit/api/openstack/compute/contrib/test_availability_zone.py', 'nova/api/openstack/compute/contrib/agents.py', 'nova/api/openstack/compute/contrib/image_size.py', 'nova/tests/unit/api/openstack/compute/contrib/test_floating_ip_pools.py', 'nova/api/openstack/compute/contrib/security_group_default_rules.py', 'nova/api/openstack/compute/contrib/server_password.py', 'nova/api/openstack/compute/consoles.py', 'nova/api/openstack/compute/contrib/server_group_quotas.py', 'nova/api/openstack/compute/contrib/extended_virtual_interfaces_net.py']",75,fd79dfa651d3e6bfcb57ec7233176348a62e91dc,nuke-xml, resp_obj.attach(),"def make_vif(elem): elem.set('{%s}net_id' % Extended_virtual_interfaces_net.namespace, '%s:net_id' % Extended_virtual_interfaces_net.alias) class ExtendedVirtualInterfaceNetTemplate(xmlutil.TemplateBuilder): def construct(self): root = xmlutil.TemplateElement('virtual_interfaces', selector='virtual_interfaces') elem = xmlutil.SubTemplateElement(root, 'virtual_interface', selector='virtual_interfaces') make_vif(elem) return xmlutil.SlaveTemplate(root, 1, nsmap={Extended_virtual_interfaces_net.alias: Extended_virtual_interfaces_net.namespace}) resp_obj.attach(xml=ExtendedVirtualInterfaceNetTemplate())",71,5279
openstack%2Fcinder~master~Ic9ec34952dc43232c29b1dc8f6baec05f804ce25,openstack/cinder,master,Ic9ec34952dc43232c29b1dc8f6baec05f804ce25,Correct misspelled words,MERGED,2014-11-25 14:23:19.000000000,2014-12-06 01:53:16.000000000,2014-11-25 17:32:47.000000000,"[{'_account_id': 3}, {'_account_id': 170}, {'_account_id': 1207}, {'_account_id': 7198}, {'_account_id': 10621}, {'_account_id': 11811}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12369}]","[{'number': 1, 'created': '2014-11-25 14:23:19.000000000', 'files': ['cinder/volume/drivers/vmware/vmdk.py', 'cinder/volume/drivers/hitachi/hbsd_snm2.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/402b78ca1fca91d2ef5e6782a798fdff23a6d621', 'message': 'Correct misspelled words\n\nIn some files I found misspelled words like :\n\nbegining -> beginning\noccured -> ocurred\n\nChange-Id: Ic9ec34952dc43232c29b1dc8f6baec05f804ce25\n'}]",1,137078,402b78ca1fca91d2ef5e6782a798fdff23a6d621,14,9,1,14093,,,0,"Correct misspelled words

In some files I found misspelled words like :

begining -> beginning
occured -> ocurred

Change-Id: Ic9ec34952dc43232c29b1dc8f6baec05f804ce25
",git fetch https://review.opendev.org/openstack/cinder refs/changes/78/137078/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/vmware/vmdk.py', 'cinder/volume/drivers/hitachi/hbsd_snm2.py']",2,402b78ca1fca91d2ef5e6782a798fdff23a6d621,misspellings, # Don't move 'import pexpect' to the beginning of the file so that, # Don't move 'import pexpect' to the begining of the file so that,2,2
openstack%2Fneutron-specs~master~Iec6205f0e4d958f2031b460d87dc7bbfee4cd2bf,openstack/neutron-specs,master,Iec6205f0e4d958f2031b460d87dc7bbfee4cd2bf,Re-propose Neutron LBaaS TLS for Kilo,ABANDONED,2014-12-06 01:21:33.000000000,2014-12-06 01:33:18.000000000,,[{'_account_id': 3}],"[{'number': 1, 'created': '2014-12-06 01:21:33.000000000', 'files': ['specs/kilo/lbaas-tls.rst'], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/45d7756359dfa40ac610b0d7d04809ae22793b35', 'message': 'Re-propose Neutron LBaaS TLS for Kilo\n\nChange-Id: Iec6205f0e4d958f2031b460d87dc7bbfee4cd2bf\nCo-Authored-By: Evgeny Fedoruk <evgenyf@radware.com>\n'}]",0,139771,45d7756359dfa40ac610b0d7d04809ae22793b35,3,1,1,10980,,,0,"Re-propose Neutron LBaaS TLS for Kilo

Change-Id: Iec6205f0e4d958f2031b460d87dc7bbfee4cd2bf
Co-Authored-By: Evgeny Fedoruk <evgenyf@radware.com>
",git fetch https://review.opendev.org/openstack/neutron-specs refs/changes/71/139771/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/kilo/lbaas-tls.rst'],1,45d7756359dfa40ac610b0d7d04809ae22793b35,bp/lbaas-tls,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ================================== Neutron LBaaS TLS - Specification ================================== BP https://blueprints.launchpad.net/neutron/+spec/lbaas-ssl-termination Terminating TLS connections on the load balancer is a capability expected from modern load balancers and incorporated into many applications. This capability enables better certificate management and improved application based load balancing e.g. cookie-based persistency, L7 Policies appliance, etc. Problem Description =================== No TLS offloading capability is available for Neutron LBaaS Proposed Change =============== *Note: This document is referencing to the new LBaaS objects model proposed at https://review.openstack.org/#/c/89903* *Note: This document does not consider flavors framework proposed at https://review.openstack.org/#/c/90070 Before the flavors framework is in place, specific back-end driver which does not support TLS capabilities should throw an exception stating a lack of TLS support once it gets request for listener with TLS configuration. This document specifies a ""core"" feature set that every back-end implementing TLS capabilities must comply. TLS capabilities of various back-end implementations may differ in future releases, thus flavors aspect should definitely be part of TLS capabilities specification* *Note: Horizon project aspect is not a part of this specification.* * Tenant will manage his TLS certificates using Barbican. Certificates will be stored in Barbican secure containers. * Barbican is in charge of containers life cycle management, containers classification and validation. LBaaS TLS requires a specific container type (TLS). Only container of this type will be listed to the tenant for selection while configuring listener's TLS containers to use. Any invalid container usage will raise an error. * Barbican will also manage list of interested consumers for each container. See spec at https://review.openstack.org/#/c/99516 Neutron LBaaS (a consumer, according to Barbican's terminology) will not use a regular GET request for container resource in order to get the container. Instead, it will use a POST request to container's consumers resource (http://admin-api/v1/containers/{container-uuid}/consumers) with following info: { ""type"": ""LBaaS"", ""URL"": ""https://lbaas.myurl.net/loadbalancers/<lb_id>/"" } **Note:** We might want to use specific Listener URL instead of Loadbalancer's one, like ""https://lbaas.myurl.net/lbaas/listeners/<listener_id>"" As a response, it will get containers data like container's resource GET request was used. Barbican, in its turn will store consumer's (LBaaS instance URL) data in its database so this info can be used for getting all consumers of a specific TLS container. As a result, Neutron LBaaS TLS implementation is required to: * Use only POST request to container's consumers resource in order to get the container's data. * Perform DELETE request to container's consumers resource when stop using a container. * Barbican TLS container will contain PEM encoded data. Specific back-end implementation might convert the certificates data to other format such as DER, if needed. * In addition to existing HTTP, HTTPS and TCP, new protocol, TERMINATED_HTTPS will be added for listener creation * For tenant, creating listener with TERMINATED_HTTPS as a protocol means desire to offload incoming encrypted traffic. New configuration options will be available for listener's configuration including: * Default TLS container id for TLS termination * TLS containers list for SNI * In case when specific back-end is not able to support TLS capabilities, its driver should throw an exception. The exception message should state that this specific back-end (provider) does not support listeners with TLS offloading. The clear sign of listener's requirement for TLS offloading capabilities is its TERMINATED_HTTPS protocol. * New module will be developed in Neutron LBaaS for Barbican TLS containers interactions. The module will be used by Neutron LBaaS front-end API code and providers' driver code. The module will be used for validation and data extraction from default TLS container and SNI containers. This module represents the only legitimate API for Barbican TLS containers interactions. See 'SNI certificates list management' section for detailed module specification. * When creating listener with TERMINATED_HTTPS protocol: * Front-end TLS offloading is always enabled - hard coded as a default behaviour for listener with TERMINATED_HTTPS protocol * Tenant must supply default TLS container for front-end offloading. Not supplying a container is an invalid configuration. * TLS supported protocols and cipher suites for termination will be set to sane values by each back-end's specific code * SNI certificates list is optional and not mandatory to specify * SNI certificates list specifying is described in ""SNI certificates list management"" section * Certificate intermediate chain will be stored as a part of Barbican's TLS container * Back-end re-encryption will not be supported in first phase * Front-end client authentication and back-end server authentication will not be supported in first phase * When updating listener with TERMINATED_HTTPS protocol: * In TLS configuration domain, default TLS container ID for front-end offloading and SNI container IDs list are values that may be changed * In case when defaut TLS container ID is replaced for the listener, back-end implementation should ensure a lack of a downtime on LB appliance. * Same for changing SNI container IDs list, back-end should avoid LB downtime. * HA-Proxy LBaaS implementaion and other LBaaS implementations should be modified to support this specification. **With stated above, following is a description of a basic tenant use case - creating listener with TLS offloading:** * Tenant cteates Barbican TLS container with a certificate. * Tenant creates listener with TERMINATED_HTTPS as a listener protocol and specifies the Barbican TLS container ID as a default TLS container for front-end offloading * As a result, listener created, offloading encrypted traffic on front-end with default tenant's TLS certificate, not re-encrypting traffic to the back-end. Requirements from Barbican -------------------------- * Tenant should be able to create and delete TLS containers using Barbican * Ability to store TLS certificates in Barbican containers that contain the TLS certificate itself, its private key and optionaly, intermediate chain * Creating TLS container with: * Certificate : PEM text field * Private_key: PEM text_field * (extracted) Private_key_pass_phrase : text field * Intermediates: PEM text field (optional) This field is a concatination of PEM encoded certificate blocks in specific order * Delete TLS certificate **optional:** Check if certificate is in use by any consumer and warn before deleting. Barbican's BP discussing this feature: https://review.openstack.org/#/c/99516/ * Get TLS container, including private key in PEM encoded PKCS1 or PKCS7 formats, by container id * Get TLS certificate in pem encoded x509 format, by container id Restrictions ------------ * TLS settings are only available for listeners having TERMINATED_HTTPS as a protocol. In other cases TLS settings will be disabled and have None or empty values. There should be a meaningfull error message to a user explaining the exact reason of a failure in case of an invalid configuration. * Listener protocol is immutable. Changing the protocol will require radical re-configuration of provider's back-end system, which seems to be not justified for this use case. Tenant should create new listener. * While updating existing TLS certificate, name and description are only values allowed to be modified. Creating new TLS container and using it instead of the old one will be easier option than re-configuring LBaaS back-end with modified container, at least in first phase. SNI certificates list management -------------------------------- For SNI functionality, tenant will supply list of TLS containers in specific order. In case when specific back-end is not able to support SNI capabilities, its driver should throw an exception. The exception message should state that this specific back-end (provider) does not support SNI capability. The clear sign of listener's requirement for SNI capability is a none empty SNI container ids list. However, reference implementation must support SNI capability. New separate module will be developed in Neutron LBaaS for Barbican TLS containers interactions. The module will use service account for Barbican API interation. The module will have API for: * Ensuring Barbican TLS container existence (used by LBaaS front-end API) * Validating Barbican TLS container (used by LBaaS front-end API) This API will also ""register"" LBaaS as a container's consumer in Barbican's repository. * Extracting SubjectCommonName and SubjectAltName information from certificates’ X509 (used by LBaaS front-end API) As for now, only dNSName and directoryName types will be extracted from SubjectAltName sequence, while directoryName type usage is an issue for further discussion. * Extracting certificate’s data from Barbican TLS container (used by provider/driver code) * Unregistering LBaaS as a consumer of the container when container is not used by any listener any more (used by LBaaS front-end API) The module will use pyOpenSSL and PyASN1 packages. Only this new common module should be used by Neutron LBaaS code for Barbican containers interactions. Front-end LBaaS API (plugin) code will use a new developed module for validating Barbican TLS containers. Driver, in its turn, can extract SubjectCommonName and SubjectAltName information from certificates’ X509 via the common module API and use it for its specific SNI implementation. **Note:** **Specific back-end driver does not have to use SubjectAltName information. Furthermore, specific driver may throw an exception saying SubjectAltName is not supported by its provider** Any specific driver implementation may extract host names info from certificates using the mentioned above common module API only, if needed. **SNI conflicts** Employing the order of certificates list is not a common requirement for all back-end implementations. The order of SNI containers list may be used by specific back-end code, like Radware's, for specifying priorities among certificates. Order is meant to be a hint to resolve conflicts when 2 or more certificates match the DNS name requested in the SNI client hello. Specific backends might choose to ignore this order and might employ their own mechanisms to choose one among the clashing certificates. For ex. NetScaler employs the best match algorithm and does not require order for conflict resolution. It's also possible that specific driver throws an exception saying there is a collision and this specific SNI setup will not be supported by the back-end. Data Model Impact ----------------- **Data model changes** * *lbaas_listeners* table will be modified with new * default_tls_container_id (nullable string 36) - Barbican's TLS container id * New *lbaas_sni* table will be created for storing ordered list of TLS containers associated to a listener for SNI capabilities. Association objects is composed of: * id (immutable string 36) - generated object id * listener_id (string 36) - associated listener id * tls_container_id (string 36) - associated Barbican TLS container id * position - (integer) index for preserving the order **Required database migration** * add new columns to *lbaas_isteners* table * create new *lbaas_sni* table **New data initial set** * New columns for *lbaas_listeners* table's existing entries will be set to defaults REST API Impact --------------- **Listener Attributes** +-------------+-------+---------+---------+------------+----------------------+ |Attribute |Type |Access |Default |Validation/ |Description | |Name | | |Value |Conversion | | +=============+=======+=========+=========+============+======================+ |default-tls- |UUID |RW,tenant|NULL |UUID |default TLS cert id | |container-id | | | | |to use for offloading | +-------------+-------+---------+---------+------------+----------------------+ |sni_container|UUID |RW,tenant|NULL |UUID list |ordered list of | |_ids |list | | | |TLS containers to use | | | | | | |for SNI .| +-------------+-------+---------+---------+------------+----------------------+ **Functions** * create_listener * Creates new listener * Request *POST /v2.0/lbaas/listeners Accept: application/json { ""listener"":{ <...usual listener parameters>, ""protocol"": ""TERMINATED_HTTPS"" ""default_tls_container_id"": ""7804a0de-7f6b-409a-a47c-a1cc7bc77b4j"", ""sni_container_ids"": None } }* * Response *{ ""listener"":{ ""id"": ""8604a0de-7f6b-409a-a47c-a1cc7bc77b2e"" <...usual listener parameters>, ""default_tls_container_id"": ""7804a0de-7f6b-409a-a47c-a1cc7bc77b4c"", ""sni_container_ids"": None ""tenant_id"":""6b96ff0cb17a4b859e1e575d221683d3"" } }* * create_listener (with SNI list) * Creates new listener * Request *POST /v2.0/lbaas/listeners Accept: application/json { ""listener"":{ <...usual listener parameters>, ""protocol"": ""TERMINATED_HTTPS"" ""default_tls_container_id"": ""7804a0de-7f6b-409a-a47c-a1cc7bc77b4j"", ""sni_container_ids"": [5404a0de-7f6b-409a-a47c-a1ccgbc77b3j, 1206a0de-7f6b-409a-a47c-a1ccgbc7bgf3] } }* * Response *{ ""listener"":{ ""id"": ""8604a0de-7f6b-409a-a47c-a1cc7bc77b2e"" <...usual listener parameters>, ""default_tls_container_id"": ""7804a0de-7f6b-409a-a47c-a1cc7bc77b4c"", ""sni_container_ids"":[5404a0de-7f6b-409a-a47c-a1ccgbc77b3j, 1206a0de-7f6b-409a-a47c-a1ccgbc7bgf3] ""tenant_id"":""6b96ff0cb17a4b859e1e575d221683d3"" } }* * update_listener * Updates VIP listener * Request *PUT /v2.0/lbaas/listeners/<listener-id> Accept: application/json { ""listener"":{ <...usual listener parameters>, ""protocol"": ""TERMINATED_HTTPS"" ""default_tls_container_id"": ""7804a0de-7f6b-409a-a47c-a1cc7bc77b4c"", ""sni_container_ids"": None } }* * Response *{ ""listener"":{ ""id"": ""8604a0de-7f6b-409a-a47c-a1cc7bc77b2e"" <...usual listener parameters>, ""default_tls_container_id"": ""7804a0de-7f6b-409a-a47c-a1cc7bc77b4c"", ""sni_container_ids"": None ""tenant_id"":""6b96ff0cb17a4b859e1e575d221683d3"" } }* Security Impact --------------- Following are security requirements: * Retrieving TLS container from Barbican to LBaaS plugin/driver must be secured * Sending TLS container contents from driver to back-end system must be secured * Storing secrets on neutron server is prohibited * Back-end systems may need to ensure secured store for secrets to meet certain security compliance requirements Notifications Impact -------------------- None CLI Impact --------------------- * Listener creation with TERMINATED_HTTPS protocol (default behavior) *lb-listener-create* **--protocol TERMINATED_HTTPS** *--protocol-port 443* **--default_tls_container_id 9a96ff0cb17a4b859e1e575d2216cd23** *...<usual CLI options>* * Listener creation with TERMINATED_HTTPS protocol and SNI certificates list *lb-listener-create* **--protocol TERMINATED_HTTPS** *--protocol-port 443* **--default_tls_container_id 9a96ff0cb17a4b859e1e575d2216cd23 --sni_container_ids list=true 6b96ff0cb17a4b859e1e575d221683d3 4596ff0cb17a4b859e1e575d22168ba1** *...<usual CLI options>* Other End User Impact --------------------- None Performance Impact ------------------ * When updating listener without modifying TLS settings (default container id or SNI list) - Barbican API should not be used for retrieving container content which was not actually changed. This will prevent unnecessary resources consumption when, for example, members are added to the pool used by listener. It means that each Barbican TLS container will be validated only once for a listener while it's still in use by this listener. IPv6 Impact ----------- None Other Deployer Impact --------------------- * Barbican is required to be deployed and functional in order this feature to work. * New dependencies are added for neutron, pyOpenSSL and PyASN1. These are required by new module for Barbican TLS containers interactions. Developer Impact ---------------- None Community Impact ---------------- This change has been in review since Juno. Much discussion has taken place over IRC and the mailing list. Alternatives ------------ None Implementation ============== Assignee(s) ----------- Primary assignee: https://launchpad.net/~evgenyf Other contributors: Barbican TLS containers interactions module - https://launchpad.net/~carlos-garza Work Items ---------- * Develop new module for BArbican TLS containers interactions using pyOpenSSL and PyASN1 packages. * Implement changes in LBaaS DB schema v2 * Implement changes in LBaaS extension v2 * Implement all required CLI changes * Implement all required unit testing * Implement all required tempest testing * Make integration with Barbican certificates storage API Detailed specificatio of how Barbican's API for containers should be used is at https://review.openstack.org/#/c/99516 * Modifying LBaaS HA-Proxy driver to support TLS capability Detailed specification of this work item is at https://review.openstack.org/#/c/100931 * Use HA-Proxy version 1.5 * Implement horizon part of this spec, not as part of Juno release. Dependencies ============ * Barbican API requirements * Neutron LBaaS API v2 with new listener object implemented * New dependencies will be added for neutron, pyOpenSSL and PyASN1. These are required by new module for Barbican TLS containers interactions. Testing ======= Tempest Tests ------------- **listener unit testing domains** * REST API and attributes validation tests * DB mixin and schema tests * LBaaS Plugin with mocked driver end-to-end tests * Specific driver tests for each existing driver supporting TLS offloading * Tempest tests * CLI tests * New listener creation with TERMINATED_HTTPS as a protocol * No default TLS container for termination supplied. Check error generation * Default TLS container for termination supplied. Test expected default configuration took place. * Default TLS container supplied. SNI TLS containers list was supplied Test expected configuration took place * Update existing listener with TERMINATED_HTTPS as a protocole * Change default TLS container. Test expected configuration * Add/Modify SNI containers list. Test expected configuration CLI tests should test inconsistency issues such as: * No default offloading TLS container specified when creating listener with TERMINATES_HTTPS protocole Functional Tests ---------------- As above. API Tests ---------- As above. Documentation Impact ==================== User Documentation ------------------ * Neutron CLI should be modified with updated listener commands with TLS options Developer Documentation ----------------------- * Neutron API should be modified with new listener TLS attributes References ========== * TLS RFC http://tools.ietf.org/html/rfc2818 ",,594,0
openstack%2Ftaskflow~master~If03005c253c3f540d2c33faf7a7474a5fde9dcdd,openstack/taskflow,master,If03005c253c3f540d2c33faf7a7474a5fde9dcdd,Allow for the notifier to provide a 'details_filter',MERGED,2014-11-25 05:37:21.000000000,2014-12-06 01:22:17.000000000,2014-12-06 01:22:16.000000000,"[{'_account_id': 3}, {'_account_id': 1297}]","[{'number': 1, 'created': '2014-11-25 05:37:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/d7e79393e43bc65d5384b9c9c9962ecaca2b6b3f', 'message': 'Allow for the notifier to provide a details_filter\n\nWhen a notifier is being used for retry atoms and for\ntask atoms it is useful to allow a filter callback to\nbe provided that will skip notification depending on\nwhether the details matches or not (so that task or\nretry notifications can proceed).\n\nPart of fix for bug 1395966\n\nChange-Id: If03005c253c3f540d2c33faf7a7474a5fde9dcdd\n'}, {'number': 2, 'created': '2014-11-25 19:57:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/eafe440a62b023b8e026324d26273041fe312d87', 'message': ""Allow for the notifier to provide a 'details_filter'\n\nWhen a notifier is being used for retry atoms and for\ntask atoms it is useful to allow a filter callback to\nbe provided that will skip notification depending on\nwhether the details matches or not (so that task or\nretry notifications can proceed).\n\nPart of fix for bug 1395966\n\nChange-Id: If03005c253c3f540d2c33faf7a7474a5fde9dcdd\n""}, {'number': 3, 'created': '2014-11-25 22:36:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/475d66a180d0cf59a11daa34b59aa45f7b2880aa', 'message': ""Allow for the notifier to provide a 'details_filter'\n\nWhen a notifier is being used for retry atoms and for\ntask atoms it is useful to allow a filter callback to\nbe provided that will skip notification depending on\nwhether the details matches or not (so that task or\nretry notifications can proceed).\n\nPart of fix for bug 1395966\n\nChange-Id: If03005c253c3f540d2c33faf7a7474a5fde9dcdd\n""}, {'number': 4, 'created': '2014-12-02 02:23:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/93c787aa597d6b34ecd17ff52be47cb9c57a4694', 'message': ""Allow for the notifier to provide a 'details_filter'\n\nWhen a notifier is being used for retry atoms and for\ntask atoms it is useful to allow a filter callback to\nbe provided that will skip notification depending on\nwhether the details matches or not (so that task or\nretry notifications can proceed).\n\nPart of fix for bug 1395966\n\nChange-Id: If03005c253c3f540d2c33faf7a7474a5fde9dcdd\n""}, {'number': 5, 'created': '2014-12-04 19:04:04.000000000', 'files': ['taskflow/types/notifier.py', 'taskflow/tests/unit/test_notifier.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/cf45a7045921b1109cdb11dd2f7d374791d20a37', 'message': ""Allow for the notifier to provide a 'details_filter'\n\nWhen a notifier is being used for retry atoms and for\ntask atoms it is useful to allow a filter callback to\nbe provided that will skip notification depending on\nwhether the details matches or not (so that task or\nretry notifications can proceed).\n\nPart of fix for bug 1395966\n\nChange-Id: If03005c253c3f540d2c33faf7a7474a5fde9dcdd\n""}]",0,136969,cf45a7045921b1109cdb11dd2f7d374791d20a37,16,2,5,1297,,,0,"Allow for the notifier to provide a 'details_filter'

When a notifier is being used for retry atoms and for
task atoms it is useful to allow a filter callback to
be provided that will skip notification depending on
whether the details matches or not (so that task or
retry notifications can proceed).

Part of fix for bug 1395966

Change-Id: If03005c253c3f540d2c33faf7a7474a5fde9dcdd
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/69/136969/1 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/types/notifier.py', 'taskflow/tests/unit/test_notifier.py']",2,d7e79393e43bc65d5384b9c9c9962ecaca2b6b3f,bug/1395966," def test_details_filter(self): call_counts = collections.defaultdict(list) def call_me_on(registered_state, state, details): call_counts[registered_state].append((state, details)) def when_red(details): if details.get('color') == 'red': return True return False notifier = nt.Notifier() notifier.register(states.SUCCESS, functools.partial(call_me_on, states.SUCCESS), details_filter=when_red) notifier.notify(states.SUCCESS, {}) self.assertEqual(0, len(call_counts[states.SUCCESS])) notifier.notify(states.SUCCESS, {'color': 'red'}) self.assertEqual(1, len(call_counts[states.SUCCESS]))",,72,27
openstack%2Fnova~master~I95e509b2526e56f49341009f5f89e71b2f308532,openstack/nova,master,I95e509b2526e56f49341009f5f89e71b2f308532,WIP: Nuke XML tests - Phase 2,ABANDONED,2014-12-05 19:30:19.000000000,2014-12-06 00:48:04.000000000,,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 5170}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-12-05 19:30:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c6188899de375e4ab5fca0851d9338e70ec16960', 'message': 'WIP: Nuke XML tests - Phase 2\n\nChange-Id: I95e509b2526e56f49341009f5f89e71b2f308532\n'}, {'number': 2, 'created': '2014-12-05 20:03:11.000000000', 'files': ['nova/tests/unit/api/openstack/compute/contrib/test_cloudpipe.py', 'nova/tests/unit/api/openstack/test_common.py', 'nova/tests/unit/api/openstack/compute/contrib/test_block_device_mapping_v1.py', 'nova/tests/unit/api/openstack/compute/contrib/test_security_group_default_rules.py', 'nova/tests/unit/api/openstack/compute/contrib/test_server_diagnostics.py', 'nova/tests/unit/api/openstack/compute/contrib/test_flavors_extra_specs.py', 'nova/tests/unit/api/openstack/compute/test_consoles.py', 'nova/tests/unit/api/openstack/compute/contrib/test_keypairs.py', 'nova/tests/unit/api/openstack/compute/contrib/test_neutron_security_groups.py', 'nova/tests/unit/api/openstack/compute/contrib/test_quota_classes.py', 'nova/tests/unit/api/openstack/compute/contrib/test_server_groups.py', 'nova/tests/unit/api/openstack/compute/contrib/test_cells.py', 'nova/tests/unit/api/openstack/compute/contrib/test_migrations.py', 'nova/tests/unit/api/openstack/compute/contrib/test_security_groups.py', 'nova/tests/unit/api/openstack/compute/contrib/test_volumes.py', 'nova/tests/unit/api/openstack/compute/contrib/test_used_limits.py', 'nova/tests/unit/api/openstack/test_faults.py', 'nova/tests/unit/api/openstack/compute/contrib/test_quotas.py', 'nova/tests/unit/api/openstack/compute/test_servers.py', 'nova/tests/unit/api/openstack/test_wsgi.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/4dd85071c01fb109ce1e8a65e9b16ae663eb43d5', 'message': 'WIP: Nuke XML tests - Phase 2\n\nChange-Id: I95e509b2526e56f49341009f5f89e71b2f308532\n'}]",0,139716,4dd85071c01fb109ce1e8a65e9b16ae663eb43d5,9,6,2,5638,,,0,"WIP: Nuke XML tests - Phase 2

Change-Id: I95e509b2526e56f49341009f5f89e71b2f308532
",git fetch https://review.opendev.org/openstack/nova refs/changes/16/139716/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/api/openstack/compute/contrib/test_cloudpipe.py', 'nova/tests/unit/api/openstack/test_common.py', 'nova/tests/unit/api/openstack/compute/contrib/test_block_device_mapping_v1.py', 'nova/tests/unit/api/openstack/compute/contrib/test_security_group_default_rules.py', 'nova/tests/unit/api/openstack/compute/contrib/test_server_diagnostics.py', 'nova/tests/unit/api/openstack/compute/contrib/test_flavors_extra_specs.py', 'nova/tests/unit/api/openstack/compute/test_consoles.py', 'nova/tests/unit/api/openstack/compute/contrib/test_keypairs.py', 'nova/tests/unit/api/openstack/compute/contrib/test_neutron_security_groups.py', 'nova/tests/unit/api/openstack/compute/contrib/test_quota_classes.py', 'nova/tests/unit/api/openstack/compute/contrib/test_server_groups.py', 'nova/tests/unit/api/openstack/compute/contrib/test_cells.py', 'nova/tests/unit/api/openstack/compute/contrib/test_migrations.py', 'nova/tests/unit/api/openstack/compute/contrib/test_security_groups.py', 'nova/tests/unit/api/openstack/compute/contrib/test_volumes.py', 'nova/tests/unit/api/openstack/compute/contrib/test_used_limits.py', 'nova/tests/unit/api/openstack/test_faults.py', 'nova/tests/unit/api/openstack/compute/contrib/test_quotas.py', 'nova/tests/unit/api/openstack/compute/test_servers.py', 'nova/tests/unit/api/openstack/test_wsgi.py']",20,c6188899de375e4ab5fca0851d9338e70ec16960,nuke-xml, class Controller(object):,"class XMLDictSerializerTest(test.NoDBTestCase): def test_xml(self): input_dict = dict(servers=dict(a=(2, 3))) expected_xml = '<serversxmlns=""asdf""><a>(2,3)</a></servers>' serializer = wsgi.XMLDictSerializer(xmlns=""asdf"") result = serializer.serialize(input_dict) result = result.replace('\n', '').replace(' ', '') self.assertEqual(result, expected_xml) def test_xml_contains_unicode(self): input_dict = dict(test=u'\u89e3\u7801') expected_xml = '<test>\xe8\xa7\xa3\xe7\xa0\x81</test>' serializer = wsgi.XMLDictSerializer() result = serializer.serialize(input_dict) result = result.replace('\n', '').replace(' ', '') self.assertEqual(expected_xml, result) class XMLDeserializerTest(test.NoDBTestCase): def test_xml(self): xml = """""" <a a1=""1"" a2=""2""> <bs><b>1</b><b>2</b><b>3</b><b><c c1=""1""/></b></bs> <d><e>1</e></d> <f>1</f> </a> """""".strip() as_dict = { 'body': { 'a': { 'a1': '1', 'a2': '2', 'bs': ['1', '2', '3', {'c': {'c1': '1'}}], 'd': {'e': '1'}, 'f': '1', }, }, } metadata = {'plurals': {'bs': 'b', 'ts': 't'}} deserializer = wsgi.XMLDeserializer(metadata=metadata) self.assertEqual(deserializer.deserialize(xml), as_dict) def test_xml_empty(self): xml = '<a></a>' as_dict = {""body"": {""a"": {}}} deserializer = wsgi.XMLDeserializer() self.assertEqual(deserializer.deserialize(xml), as_dict) def test_xml_valid_utf8(self): xml = """""" <a><name>\xe6\xa6\x82\xe5\xbf\xb5</name></a> """""" deserializer = wsgi.XMLDeserializer() as_dict = {'body': {u'a': {u'name': u'\u6982\u5ff5'}}} self.assertEqual(deserializer.deserialize(xml), as_dict) def test_xml_invalid_utf8(self): """"""Send invalid utf-8 to XMLDeserializer."""""" xml = """""" <a><name>\xf0\x28\x8c\x28</name></a> """""" deserializer = wsgi.XMLDeserializer() self.assertRaises(exception.MalformedRequestBody, deserializer.deserialize, xml) class XMLDeserializer(object): def deserialize(self, body): return 'xml' class Controller(object): @wsgi.deserializers(xml=XMLDeserializer) class XMLSerializer(object): def serialize(self, obj): return 'xml' xml=XMLSerializer,",12,2070
openstack%2Fpython-keystoneclient~master~If72295590483bb52fcf5a0d59cf95f3e49ea69c8,openstack/python-keystoneclient,master,If72295590483bb52fcf5a0d59cf95f3e49ea69c8,Pass all adapter parameters through to adapter,MERGED,2014-12-02 02:29:24.000000000,2014-12-06 00:37:30.000000000,2014-12-06 00:37:30.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 792}, {'_account_id': 1916}, {'_account_id': 2218}, {'_account_id': 2903}, {'_account_id': 5046}, {'_account_id': 5707}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 7191}, {'_account_id': 7725}]","[{'number': 1, 'created': '2014-12-02 02:29:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/2537f2c5f92050318b67131af30f91fc12faafce', 'message': ""Pass all adapter parameters through to adapter\n\nThis is how all the other clients should work. It means we'll have the\nsame options like auth= that the other clients have. It also means that\nthe keystoneclient options will be kept up to date with others.\n\nChange-Id: If72295590483bb52fcf5a0d59cf95f3e49ea69c8\n""}, {'number': 2, 'created': '2014-12-02 19:37:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/f1594589bc9ac6855380f02b1a9a7daaafaac8fc', 'message': ""Pass all adapter parameters through to adapter\n\nThis is how all the other clients should work. It means we'll have the\nsame options like auth= that the other clients have. It also means that\nthe keystoneclient options will be kept up to date with others.\n\nChange-Id: If72295590483bb52fcf5a0d59cf95f3e49ea69c8\n""}, {'number': 3, 'created': '2014-12-02 19:40:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/ab119708d43d1ea0651f284044539d897ef19f88', 'message': ""Pass all adapter parameters through to adapter\n\nWe can't simply pass through kwargs to the adapter for compatibility\nreasons however make sure that we accept the appropriate arguments and\npass them to adapter.\n\nAlso add some notes to try and keep them up to date.\n\nChange-Id: If72295590483bb52fcf5a0d59cf95f3e49ea69c8\n""}, {'number': 4, 'created': '2014-12-04 02:25:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/ac46c5c8f3b3cecb7ebee4535a60b529b187fda9', 'message': ""Pass all adapter parameters through to adapter\n\nWe can't simply pass through kwargs to the adapter for compatibility\nreasons however make sure that we accept the appropriate arguments and\npass them to adapter.\n\nAlso add some notes to try and keep them up to date.\n\nChange-Id: If72295590483bb52fcf5a0d59cf95f3e49ea69c8\n""}, {'number': 5, 'created': '2014-12-04 02:30:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/d7d89d50985365a7c662269a21fc6bd0f8ac2901', 'message': ""Pass all adapter parameters through to adapter\n\nWe can't simply pass through kwargs to the adapter for compatibility\nreasons however make sure that we accept the appropriate arguments and\npass them to adapter.\n\nAlso add some notes to try and keep them up to date.\n\nChange-Id: If72295590483bb52fcf5a0d59cf95f3e49ea69c8\n""}, {'number': 6, 'created': '2014-12-04 08:35:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/6adddd126b38765c2fa50f6a2a6fca5572fd0027', 'message': ""Pass all adapter parameters through to adapter\n\nWe can't simply pass through kwargs to the adapter for compatibility\nreasons however make sure that we accept the appropriate arguments and\npass them to adapter.\n\nAlso add some notes to try and keep them up to date.\n\nChange-Id: If72295590483bb52fcf5a0d59cf95f3e49ea69c8\n""}, {'number': 7, 'created': '2014-12-05 01:12:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/40949ff469ee1930685bb1c2860a25c23b1a32de', 'message': ""Pass all adapter parameters through to adapter\n\nWe can't simply pass through kwargs to the adapter for compatibility\nreasons however make sure that we accept the appropriate arguments and\npass them to adapter.\n\nAlso add some notes to try and keep them up to date.\n\nChange-Id: If72295590483bb52fcf5a0d59cf95f3e49ea69c8\n""}, {'number': 8, 'created': '2014-12-05 01:38:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/0cc5f1c500402af7000bdeb9bea498e454b0fda0', 'message': ""Pass all adapter parameters through to adapter\n\nWe can't simply pass through kwargs to the adapter for compatibility\nreasons however make sure that we accept the appropriate arguments and\npass them to adapter.\n\nAlso add some notes to try and keep them up to date.\n\nCloses-Bug: #1399492\nChange-Id: If72295590483bb52fcf5a0d59cf95f3e49ea69c8\n""}, {'number': 9, 'created': '2014-12-05 04:21:56.000000000', 'files': ['keystoneclient/adapter.py', 'keystoneclient/tests/v2_0/test_client.py', 'keystoneclient/tests/v3/test_client.py', 'keystoneclient/httpclient.py'], 'web_link': 'https://opendev.org/openstack/python-keystoneclient/commit/28ea0a8e36e56f5420314e38eb980c4d9b38dfe5', 'message': ""Pass all adapter parameters through to adapter\n\nWe can't simply pass through kwargs to the adapter for compatibility\nreasons however make sure that we accept the appropriate arguments and\npass them to adapter.\n\nAlso add some notes to try and keep them up to date.\n\nCloses-Bug: #1399492\nChange-Id: If72295590483bb52fcf5a0d59cf95f3e49ea69c8\n""}]",25,138228,28ea0a8e36e56f5420314e38eb980c4d9b38dfe5,37,12,9,7191,,,0,"Pass all adapter parameters through to adapter

We can't simply pass through kwargs to the adapter for compatibility
reasons however make sure that we accept the appropriate arguments and
pass them to adapter.

Also add some notes to try and keep them up to date.

Closes-Bug: #1399492
Change-Id: If72295590483bb52fcf5a0d59cf95f3e49ea69c8
",git fetch https://review.opendev.org/openstack/python-keystoneclient refs/changes/28/138228/2 && git format-patch -1 --stdout FETCH_HEAD,['keystoneclient/httpclient.py'],1,2537f2c5f92050318b67131af30f91fc12faafce,doc," # NOTE(jamielennox): For now we don't allow users to override these kwargs['service_type'] = 'identity' kwargs['interface'] = 'admin' version=self.version, **kwargs)"," service_type='identity', interface='admin', version=self.version)",6,3
openstack%2Fnova~master~Ie1697577bc14038faccd50caf2fea20cac342388,openstack/nova,master,Ie1697577bc14038faccd50caf2fea20cac342388,Fix long hostname in dnsmasq,MERGED,2014-11-21 20:07:12.000000000,2014-12-06 00:24:09.000000000,2014-12-05 23:46:45.000000000,"[{'_account_id': 3}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 3189}, {'_account_id': 5170}, {'_account_id': 6794}, {'_account_id': 9008}, {'_account_id': 9569}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 11531}, {'_account_id': 12898}]","[{'number': 1, 'created': '2014-11-21 20:07:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/26823ac000355fea353700f821f16f17dab6152e', 'message': 'Fix long hostname in dnsmaq\n\ndnsmasq only supports 63 character hostnames. Longer names cause\ndnsmasq to ignore the entry and not hand out a DHCP address.\n\nTrunacte hostnames longer then 63 characters to the last 63\ncharacters in order to make DHCP work.\n\nCloses-Bug: #1238910\nChange-Id: Ie1697577bc14038faccd50caf2fea20cac342388\n'}, {'number': 2, 'created': '2014-11-21 21:57:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8bd0905405e36022ae8c5bcbb903257afd615a14', 'message': 'Fix long hostname in dnsmasq\n\ndnsmasq only supports 63 character hostnames. Longer names cause\ndnsmasq to ignore the entry and not hand out a DHCP address.\n\nTrunacte hostnames longer then 63 characters to the last 63\ncharacters in order to make DHCP work.\n\nCloses-Bug: #1238910\nChange-Id: Ie1697577bc14038faccd50caf2fea20cac342388\n'}, {'number': 3, 'created': '2014-11-24 18:30:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/87cc22dcdeaeb6415dfc120c0329724eef81dd2c', 'message': 'Fix long hostname in dnsmasq\n\ndnsmasq only supports 63 character hostnames. Longer names cause\ndnsmasq to ignore the entry and not hand out a DHCP address.\n\nTrunacte hostnames longer then 63 characters in order to make\nDHCP work.\n\nCloses-Bug: #1238910\nChange-Id: Ie1697577bc14038faccd50caf2fea20cac342388\n'}, {'number': 4, 'created': '2014-12-05 02:23:01.000000000', 'files': ['nova/tests/unit/network/test_linux_net.py', 'nova/network/linux_net.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/f58e1fd6c701132297c01af4eb239284ef4c2e7d', 'message': 'Fix long hostname in dnsmasq\n\ndnsmasq only supports 63 character hostnames. Longer names cause\ndnsmasq to ignore the entry and not hand out a DHCP address.\n\nTrunacte hostnames longer then 63 characters in order to make\nDHCP work.\n\nCloses-Bug: #1238910\nChange-Id: Ie1697577bc14038faccd50caf2fea20cac342388\n'}]",13,136477,f58e1fd6c701132297c01af4eb239284ef4c2e7d,46,13,4,3189,,,0,"Fix long hostname in dnsmasq

dnsmasq only supports 63 character hostnames. Longer names cause
dnsmasq to ignore the entry and not hand out a DHCP address.

Trunacte hostnames longer then 63 characters in order to make
DHCP work.

Closes-Bug: #1238910
Change-Id: Ie1697577bc14038faccd50caf2fea20cac342388
",git fetch https://review.opendev.org/openstack/nova refs/changes/77/136477/4 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/network/test_linux_net.py', 'nova/network/linux_net.py']",2,26823ac000355fea353700f821f16f17dab6152e,bug/1238910," # NOTE(cfb): dnsmasq on linux only supports 64 characters in the hostname # field (LP #1238910). Since the . counts as a character we need # to truncate the hostname to only 63 characters. hostname = fixedip.instance.hostname[-63:] hostname, hostname,"," fixedip.instance.hostname, fixedip.instance.hostname,",71,4
openstack%2Fhorizon~master~I607a0e428833e7478b073eec4d9ef9e1e856ae12,openstack/horizon,master,I607a0e428833e7478b073eec4d9ef9e1e856ae12,POST instantiation for Django-Angular,MERGED,2014-11-22 07:57:46.000000000,2014-12-06 00:18:51.000000000,2014-12-06 00:18:50.000000000,"[{'_account_id': 3}, {'_account_id': 2455}, {'_account_id': 5623}, {'_account_id': 8871}, {'_account_id': 9622}, {'_account_id': 9981}, {'_account_id': 10761}, {'_account_id': 10881}, {'_account_id': 11902}, {'_account_id': 12071}, {'_account_id': 12826}, {'_account_id': 13325}, {'_account_id': 14064}, {'_account_id': 14105}]","[{'number': 1, 'created': '2014-11-22 07:57:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/4b3cced30326bdf8c37758e2c43410180e6ecd95', 'message': ""POST instantiation for Django-Angular\n\nWe currently need to include a CSRF token in our form if we want to POST\nto a Django view. Furthermore, we need to change the content-type to\n'application/x-www-form-urlencoded' before we can POST. This is\ncumbersome and should really be a global setting.\n\nChange-Id: I607a0e428833e7478b073eec4d9ef9e1e856ae12\nCloses-Bug: #1395270\n""}, {'number': 2, 'created': '2014-11-25 07:56:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/5f8002e4755baeb549bc36e76dddc6d69af0a86d', 'message': ""POST instantiation for Django-Angular\n\nWe currently need to include a CSRF token in our form if we want to POST\nto a Django view. Furthermore, we need to change the content-type to\n'application/x-www-form-urlencoded' before we can POST. This is\ncumbersome and should really be a global setting.\n\nChange-Id: I607a0e428833e7478b073eec4d9ef9e1e856ae12\nCloses-Bug: #1395270\n""}, {'number': 3, 'created': '2014-11-26 22:51:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/9485983d922846f4dcf4887119789dae66df8ca7', 'message': ""POST instantiation for Django-Angular\n\nWe currently need to include a CSRF token in our form if we want to POST\nto a Django view. Furthermore, we need to change the content-type to\n'application/x-www-form-urlencoded' before we can POST. This is\ncumbersome and should really be a global setting.\n\nChange-Id: I607a0e428833e7478b073eec4d9ef9e1e856ae12\nCloses-Bug: #1395270\n""}, {'number': 4, 'created': '2014-12-01 14:48:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/155ccbd4e32ddd3f6faf38a358ac946e0d4d2e09', 'message': 'POST instantiation for Django-Angular\n\nWe currently need to include a CSRF token in our form if we want to POST\nto a Django view.  This is cumbersome and should really be a global setting.\n\nChange-Id: I607a0e428833e7478b073eec4d9ef9e1e856ae12\nCloses-Bug: #1395270'}, {'number': 5, 'created': '2014-12-02 01:15:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/1160a176f555853da4b3fdeecae9f6d19a26ecb7', 'message': 'POST instantiation for Django-Angular\n\nWe currently need to include a CSRF token in our form if we want to POST\nto a Django view.  This is cumbersome and should really be a global setting.\n\nChange-Id: I607a0e428833e7478b073eec4d9ef9e1e856ae12\nCloses-Bug: #1395270\n'}, {'number': 6, 'created': '2014-12-02 20:41:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/37efaa2fcce206d04694b47a225574eacc6bac36', 'message': 'POST instantiation for Django-Angular\n\nWe currently need to include a CSRF token in our form if we want to POST\nto a Django view.  This is cumbersome and should really be a global setting.\n\nChange-Id: I607a0e428833e7478b073eec4d9ef9e1e856ae12\nCloses-Bug: #1395270\n'}, {'number': 7, 'created': '2014-12-02 21:37:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/aee2c114b6aa4a1f2b422c91a6323ac0ff7cc22c', 'message': 'POST instantiation for Django-Angular\n\nWe currently need to include a CSRF token in our form if we want to POST\nto a Django view.  This is cumbersome and should really be a global setting.\n\nChange-Id: I607a0e428833e7478b073eec4d9ef9e1e856ae12\nCloses-Bug: #1395270\n'}, {'number': 8, 'created': '2014-12-05 21:59:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/819b24152ac653d5f73b76f43379387cd59864f8', 'message': 'POST instantiation for Django-Angular\n\nWe currently need to include a CSRF token in our form if we want to POST\nto a Django view.  This is cumbersome and should really be a global setting.\n\nChange-Id: I607a0e428833e7478b073eec4d9ef9e1e856ae12\nCloses-Bug: #139527'}, {'number': 9, 'created': '2014-12-05 21:59:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/d60d7fbfd1133effcf450a321a082bd2976ac748', 'message': 'POST instantiation for Django-Angular\n\nWe currently need to include a CSRF token in our form if we want to POST\nto a Django view.  This is cumbersome and should really be a global setting.\n\nChange-Id: I607a0e428833e7478b073eec4d9ef9e1e856ae12\nCloses-Bug: #139527\n'}, {'number': 10, 'created': '2014-12-05 22:03:24.000000000', 'files': ['horizon/static/horizon/js/angular/horizon.js'], 'web_link': 'https://opendev.org/openstack/horizon/commit/04ce53bcb3896fbb6f251eb57458edbc845f791f', 'message': 'POST instantiation for Django-Angular\n\nWe currently need to include a CSRF token in our form if we want to POST\nto a Django view.  This is cumbersome and should really be a global setting.\n\nChange-Id: I607a0e428833e7478b073eec4d9ef9e1e856ae12\nCloses-Bug: #1395270\n'}]",0,136556,04ce53bcb3896fbb6f251eb57458edbc845f791f,40,14,10,9576,,,0,"POST instantiation for Django-Angular

We currently need to include a CSRF token in our form if we want to POST
to a Django view.  This is cumbersome and should really be a global setting.

Change-Id: I607a0e428833e7478b073eec4d9ef9e1e856ae12
Closes-Bug: #1395270
",git fetch https://review.opendev.org/openstack/horizon refs/changes/56/136556/1 && git format-patch -1 --stdout FETCH_HEAD,['horizon/static/horizon/js/angular/horizon.js'],1,4b3cced30326bdf8c37758e2c43410180e6ecd95,bug/angular-django-csrf," $httpProvider.defaults.headers.post['Content-Type'] = 'application/x-www-form-urlencoded'; .run(['hzConfig', 'hzUtils', '$cookieStore', '$http', '$cookies', function (hzConfig, hzUtils, $cookieStore, $http, $cookies) { $http.defaults.headers.post['X-CSRFToken'] = $cookies.csrftoken;"," .run(['hzConfig', 'hzUtils', '$cookieStore', function (hzConfig, hzUtils, $cookieStore) {",4,2
openstack%2Fhorizon~stable%2Fjuno~I011b6e01eca3e04e5ef2528732b9bde3782b4eba,openstack/horizon,stable/juno,I011b6e01eca3e04e5ef2528732b9bde3782b4eba,Gracefully handle failures to retrieve a flavor,MERGED,2014-11-27 15:57:38.000000000,2014-12-06 00:18:40.000000000,2014-12-06 00:18:38.000000000,"[{'_account_id': 3}, {'_account_id': 1955}, {'_account_id': 4264}, {'_account_id': 4978}, {'_account_id': 5623}, {'_account_id': 6610}, {'_account_id': 6914}, {'_account_id': 8040}, {'_account_id': 8090}, {'_account_id': 9317}, {'_account_id': 12071}, {'_account_id': 12355}, {'_account_id': 13325}]","[{'number': 1, 'created': '2014-11-27 15:57:38.000000000', 'files': ['openstack_dashboard/dashboards/project/instances/templates/instances/_detail_overview.html', 'openstack_dashboard/dashboards/project/instances/tests.py', 'openstack_dashboard/dashboards/project/instances/tables.py', 'openstack_dashboard/dashboards/project/instances/views.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/bcac1a89b991b41c34f011b7578fceac7320cf80', 'message': ""Gracefully handle failures to retrieve a flavor\n\nIn some cases, a flavor in use can become unaccessible, for instance\nafter deleting a flavor with access restrictions.\n\n * On the Instance details page, make the try/except blocks for API\n   calls more fine-grained. Some specific failures shouldn't cause the\n   whole page to fail to load (and e.g. prevent Console access).\n\n * Fix the Resize Instance form to display even if information about\n   the current flavor cannot be retrieved.\n\n * Gracefully handle failures to retrieve flavor information on AJAX\n   calls for row update, so that the row doesn't disappear from the\n   table.\n\nConflicts:\n\topenstack_dashboard/dashboards/project/instances/tests.py\n\nChange-Id: I011b6e01eca3e04e5ef2528732b9bde3782b4eba\nCloses-Bug: #1366166\n(cherry picked from commit b50413ca0afa456dbbbb6ec554db0de67bb7b3ff)\n""}]",0,137667,bcac1a89b991b41c34f011b7578fceac7320cf80,27,13,1,4978,,,0,"Gracefully handle failures to retrieve a flavor

In some cases, a flavor in use can become unaccessible, for instance
after deleting a flavor with access restrictions.

 * On the Instance details page, make the try/except blocks for API
   calls more fine-grained. Some specific failures shouldn't cause the
   whole page to fail to load (and e.g. prevent Console access).

 * Fix the Resize Instance form to display even if information about
   the current flavor cannot be retrieved.

 * Gracefully handle failures to retrieve flavor information on AJAX
   calls for row update, so that the row doesn't disappear from the
   table.

Conflicts:
	openstack_dashboard/dashboards/project/instances/tests.py

Change-Id: I011b6e01eca3e04e5ef2528732b9bde3782b4eba
Closes-Bug: #1366166
(cherry picked from commit b50413ca0afa456dbbbb6ec554db0de67bb7b3ff)
",git fetch https://review.opendev.org/openstack/horizon refs/changes/67/137667/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/instances/templates/instances/_detail_overview.html', 'openstack_dashboard/dashboards/project/instances/tests.py', 'openstack_dashboard/dashboards/project/instances/tables.py', 'openstack_dashboard/dashboards/project/instances/views.py']",4,bcac1a89b991b41c34f011b7578fceac7320cf80,bug/1366166," instance_id = self.kwargs['instance_id'] try: status_label = [label for (value, label) in project_tables.STATUS_DISPLAY_CHOICES if value.lower() == (instance.status or '').lower()] if status_label: instance.status_label = status_label[0] else: instance.status_label = instance.status try: instance.volumes = api.nova.instance_volumes_list(self.request, instance_id) # Sort by device name instance.volumes.sort(key=lambda vol: vol.device) except Exception: msg = _('Unable to retrieve volume list for instance ' '""%s"".') % instance_id exceptions.handle(self.request, msg, ignore=True) try: instance.full_flavor = api.nova.flavor_get( self.request, instance.flavor[""id""]) except Exception: msg = _('Unable to retrieve flavor information for instance ' '""%s"".') % instance_id, exceptions.handle(self.request, msg, ignore=True) try: instance.security_groups = api.network.server_security_groups( self.request, instance_id) except Exception: msg = _('Unable to retrieve security groups for instance ' '""%s"".') % instance_id exceptions.handle(self.request, msg, ignore=True) flavor_id = instance.flavor['id'] flavors = self.get_flavors() if flavor_id in flavors: instance.flavor_name = flavors[flavor_id].name else: try: flavor = api.nova.flavor_get(self.request, flavor_id) instance.flavor_name = flavor.name except Exception: msg = _('Unable to retrieve flavor information for instance ' '""%s"".') % instance_id exceptions.handle(self.request, msg, ignore=True) instance.flavor_name = _(""Not available"")"," try: instance_id = self.kwargs['instance_id'] status_label = [label for (value, label) in project_tables.STATUS_DISPLAY_CHOICES if value.lower() == (instance.status or '').lower()] if status_label: instance.status_label = status_label[0] else: instance.status_label = instance.status instance.volumes = api.nova.instance_volumes_list(self.request, instance_id) # Sort by device name instance.volumes.sort(key=lambda vol: vol.device) instance.full_flavor = api.nova.flavor_get( self.request, instance.flavor[""id""]) instance.security_groups = api.network.server_security_groups( self.request, instance_id) flavor_id = instance.flavor['id'] flavors = self.get_flavors() if flavor_id in flavors: instance.flavor_name = flavors[flavor_id].name else: flavor = api.nova.flavor_get(self.request, flavor_id) instance.flavor_name = flavor.name",147,39
openstack%2Fneutron~master~I8046aa1126a6b7899f6a5cea893ad9caf9563d38,openstack/neutron,master,I8046aa1126a6b7899f6a5cea893ad9caf9563d38,Flavor Framework implementation,ABANDONED,2014-07-10 06:26:50.000000000,2014-12-06 00:17:35.000000000,,"[{'_account_id': 3}, {'_account_id': 162}, {'_account_id': 1653}, {'_account_id': 5170}, {'_account_id': 6072}, {'_account_id': 6659}, {'_account_id': 6854}, {'_account_id': 7249}, {'_account_id': 7576}, {'_account_id': 8645}, {'_account_id': 9008}, {'_account_id': 9681}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9885}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10980}, {'_account_id': 11208}, {'_account_id': 11685}]","[{'number': 1, 'created': '2014-07-10 06:26:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/f3254d21294681186579bc13d26524d6edc7e080', 'message': 'in progress\n\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 2, 'created': '2014-07-10 06:27:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/d9f5abe2a91a098fb732b8dfa9f0ab6ff3219dbb', 'message': 'in progress\n\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 3, 'created': '2014-07-11 15:22:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ece96ce24292b2751ed859dd804b3f44bd9aa598', 'message': 'in progress\n\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 4, 'created': '2014-07-15 13:27:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/266fcec145f50c9d6038c1a85569dd893d9913d9', 'message': 'in progress\n\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 5, 'created': '2014-07-15 18:42:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/728bc79c9a11f74155f9c214ca95b3274d50f60b', 'message': 'in progress\n\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 6, 'created': '2014-07-16 12:55:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/d3bc4c58a36b05502720911a62769812611ce896', 'message': 'in progress\n\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 7, 'created': '2014-07-16 12:59:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/5b88ae3bf383d95f3177e4f1ed03c9ca9e0a2852', 'message': 'in progress\n\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 8, 'created': '2014-07-16 14:51:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9a1cfd39e6cdb16d3887540d70e31b52f6800186', 'message': 'in progress\n\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 9, 'created': '2014-07-17 12:01:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/57329e7856446462c0a8a24b4cfcefa56f54d6a2', 'message': 'Flavor Framework implementation\n\nThis patch introduces API and DB plugin for flavor framework.\nAPI adds Flavors and Service Profiles which are resources\navailable only for admins to operate.\n\nThis framework then should be leveraged by advanced services\n\nImplements blueprint neutron-flavor-framework\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 10, 'created': '2014-07-17 12:07:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/85c8262f121699dc93efb4d3c639ed87c9f62474', 'message': 'Flavor Framework implementation\n\nThis patch introduces API and DB plugin for flavor framework.\nAPI adds Flavors and Service Profiles which are resources\navailable only for admins to operate.\n\nThis framework then should be leveraged by advanced services\n\nImplements blueprint neutron-flavor-framework\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 11, 'created': '2014-07-25 12:04:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/4487e79257b1cd28f4b855e6af2b8fb00a638f1b', 'message': 'Flavor Framework implementation\n\nThis patch introduces API and DB plugin for flavor framework.\nAPI adds Flavors and Service Profiles which are resources\navailable only for admins to operate.\n\nThis framework then should be leveraged by advanced services\n\nImplements blueprint neutron-flavor-framework\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 12, 'created': '2014-07-30 05:28:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ff75bfd4605e5999d6369dcbc074cd11e57bc71d', 'message': 'Flavor Framework implementation\n\nThis patch introduces API and DB plugin for flavor framework.\nAPI adds Flavors and Service Profiles which are resources\navailable only for admins to operate.\n\nThis framework then should be leveraged by advanced services\n\nImplements blueprint neutron-flavor-framework\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 13, 'created': '2014-07-31 08:46:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3f3db558adebcb4383f1357c1b0605682836e029', 'message': 'Flavor Framework implementation\n\nThis patch introduces API and DB plugin for flavor framework.\nAPI adds Flavors and Service Profiles which are resources\navailable only for admins to operate.\n\nThis framework then should be leveraged by advanced services\n\nImplements blueprint neutron-flavor-framework\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 14, 'created': '2014-07-31 11:55:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e8f3c209cf09f8b7dec4ce9cbf6ba6ae28606fbe', 'message': 'Flavor Framework implementation\n\nThis patch introduces API and DB plugin for flavor framework.\nAPI adds Flavors and Service Profiles which are resources\navailable only for admins to operate.\n\nThis framework then should be leveraged by advanced services\n\nImplements blueprint neutron-flavor-framework\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}, {'number': 15, 'created': '2014-08-10 18:19:58.000000000', 'files': ['neutron/db/migration/models/head.py', 'neutron/api/v2/base.py', 'neutron/tests/unit/test_flavors.py', 'neutron/plugins/common/constants.py', 'neutron/db/migration/alembic_migrations/versions/31337ec0ffee_flavors.py', 'neutron/manager.py', 'neutron/extensions/flavors.py', 'neutron/db/migration/alembic_migrations/versions/HEAD', 'neutron/db/flavors_db.py', 'neutron/tests/unit/test_neutron_manager.py', 'etc/policy.json'], 'web_link': 'https://opendev.org/openstack/neutron/commit/2d6df8df550dcde59e3369aea956427dbbc85a4d', 'message': 'Flavor Framework implementation\n\nThis patch introduces API and DB plugin for flavor framework.\nAPI adds Flavors and Service Profiles which are resources\navailable only for admins to operate.\n\nThis framework then should be leveraged by advanced services\n\nImplements blueprint neutron-flavor-framework\nChange-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38\n'}]",80,105982,2d6df8df550dcde59e3369aea956427dbbc85a4d,313,31,15,6072,,,0,"Flavor Framework implementation

This patch introduces API and DB plugin for flavor framework.
API adds Flavors and Service Profiles which are resources
available only for admins to operate.

This framework then should be leveraged by advanced services

Implements blueprint neutron-flavor-framework
Change-Id: I8046aa1126a6b7899f6a5cea893ad9caf9563d38
",git fetch https://review.opendev.org/openstack/neutron refs/changes/82/105982/3 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/test_flavors.py', 'neutron/db/common_mixin.py', 'neutron/plugins/common/constants.py', 'neutron/db/db_base_plugin_v2.py', 'neutron/manager.py', 'neutron/extensions/flavors.py', 'neutron/db/flavors_db.py']",7,f3254d21294681186579bc13d26524d6edc7e080,bp/neutron-flavor-framework,"# All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from oslo.config import cfg import sqlalchemy as sa from sqlalchemy import orm from sqlalchemy.orm import exc as sa_exc from neutron.common import exceptions as qexception from neutron.plugins.common import constants from neutron.db import common_mixin from neutron.db import model_base from neutron.db import models_v2 #from neutron import manager from neutron.openstack.common import log as logging from neutron.openstack.common import uuidutils LOG = logging.getLogger(__name__) # Flavor Exceptions class FlavorNotFound(qexception.NotFound): message = _(""Flavor %(flavor_id)s could not be found"") class FlavorInUse(qexception.InUse): message = _(""Flavor %(flavor_id)s is used by some service instance"") class ServiceProfileNotFound(qexception.NotFound): message = _(""Service Profile %(sp_id)s could not be found"") class ServiceProfileInUse(qexception.InUse): message = _(""Service Profile %(sp_id)s is used by some service instance"") class Flavor(model_base.BASEV2, models_v2.HasId): name = sa.Column(sa.String(255)) description = sa.Column(sa.String(1024)) enabled = sa.Column(sa.Boolean, nullable=False, default=True) service_type = sa.Column(sa.String(36), nullable=False) #service_profiles service_profiles = orm.relationship(""FlavorServiceProfileBinding"", cascade=""all, delete-orphan"") class ServiceProfile(model_base.BASEV2, models_v2.HasId): description = sa.Column(sa.String(1024)) # TODO (enikanorov): entrypoint or provider? driver = sa.Column(sa.String(1024), nullable=False) enabled = sa.Column(sa.Boolean, nullable=False, default=True) meta_data = sa.Column(sa.Text(4096)) class FlavorServiceProfileBinding(model_base.BASEV2): flavor_id = sa.Column(sa.String(36), #sa.ForeignKey(""flavors.id"", # ondelete=""CASCADE""), nullable=False, primary_key = True) service_profile_id = sa.Column(sa.String(36), nullable=False, primary_key = True) class InstanceFlavorBinding(model_base.BASEV2): flavor_id = sa.Column(sa.String(36), sa.ForeignKey(""flavors.id"", ondelete=""CASCADE""), nullable=False, primary_key = True) # no foreign key because it can reference different tables instance_id = sa.Column(sa.String(36), nullable=False, primary_key = True) class InstanceServiceProfileBinding(model_base.BASEV2): service_profile_id = sa.Column(sa.String(36), nullable=False, primary_key = True) instance_id = sa.Column(sa.String(36), nullable=False, primary_key = True) class FlavorManager(common_mixin.CommonDbMixin): """"""Class to support flavors and service profiles."""""" def get_plugin_name(self): return constants.FLAVORS def get_plugin_type(self): return constants.FLAVORS def _get_flavor(self, context, flavor_id): try: return self._get_by_id(context, Flavor, id) except sa_exc.NoResultFound: raise FlavorNotFound(flavor_id=flavor_id) def _get_service_profile(self, context, sp_id): try: return self._get_by_id(context, ServiceProfile, id) except sa_exc.NoResultFound: raise ServiceProfileNotFound(sp_id=sp_id) def _make_flavor_dict(self, flavor_db, fields=None): res = {'id': flavor_db['id'], 'name': flavor_db['name'], 'description': flavor_db['description'], 'enabled': flavor_db['enabled'], 'service_profiles': []} if flavor_db.service_profiles: res['service_profiles'] = [sp['id'] for sp in flavor_db.service_profiles] return self._fields(res, fields) def _make_service_profile_dict(self, sp_db, fields=None): res = {'id': sp_db['id'], 'description': sp_db['description'], 'driver': sp_db['driver'], 'enabled': sp_db['enabled'], 'metadata': sp_db['metadata']} return self._fields(res, fields) def _ensure_flavor_not_in_use(self, context, flavor_id): """""" Checks that flavor is not associated with service instance."""""" # TODO (enikanorov): check that there is no binding to instances return False def _ensure_service_profile_not_in_use(self, context, sp_id): # TODO (enikanorov): check that there is no binding to instances return False def create_flavor(self, context, flavor): fl = flavor['flavor'] with context.session.begin(subtransactions=True): fl_db = Flavor(id=uuidutils.generate_uuid(), name=fl['name'], description=fl['description'], enabled=fl['enabled']) context.session.add(fl_db) return self._make_flavor_dict(fl_db) def update_flavor(self, context, flavor_id, flavor): fl = flavor['flavor'] self._ensure_flavor_not_in_use(context, flavor_id) with context.session.begin(subtransactions=True): fl_db = self._get_flavor(context, flavor_id) fl_db.update(fl) return self._make_flavor_dict(fl_db) def get_flavor(self, context, flavor_id, fields=None): fl = self._get_flavor(context, flavor_id) return self._make_flavor_dict(fl, fields) def delete_flavor(self, context, flavor_id): self._ensure_flavor_not_in_use(context, flavor_id) with context.session.begin(subtransactions=True): fl_db = self._get_flavor(context, flavor_id) context.session.delete(fl_db) def get_flavors(self, context, filters=None, fields=None, sorts=None, limit=None, marker=None, page_reverse=False): return self._get_collection(context, Flavor, self._make_flavor_dict, filters=filters, fields=fields, sorts=sorts, limit=limit, marker=marker, page_reverse=page_reverse) def _load_driver(self, profile): pass def create_service_profile(self, context, service_profile): sp = service_profile['service_profile'] with context.session.begin(subtransactions=True): sp_db = Flavor(id=uuidutils.generate_uuid(), description=sp['description'], driver=sp['driver'], enabled=sp['enabled'], metadata=sp['metadata']) context.session.add(sp_db) try: driver_inst = self._load_driver(sp) svc_type = driver_inst.get_service_type() #plugin = manager.NeutronManager.get_service_plugins()[svc_type] #plugin.driver_loaded(driver_inst, ) except Exception: # TODO (enikanorov): raise proper exception self.delete_service_profile(context, sp_db['id']) raise return self._make_flavor_dict(sp_db) def update_service_profile(self, context, sp_id, service_profile): sp = service_profile['service_profile'] self._ensure_service_profile_not_in_use(context, sp_id) with context.session.begin(subtransactions=True): sp_db = self._get_service_profile(context, sp_id) sp_db.update(sp) return self._make_service_profile_dict(sp_db) def get_service_profile(self, context, sp_id, fields=None): sp_db = self._get_service_profile(context, sp_id) return self._make_service_profile_dict(sp_db, fields) def delete_service_profile(self, context, sp_id): self._ensure_flavor_not_in_use(context, sp_id) with context.session.begin(subtransactions=True): fl_db = self._get_flavor(context, sp_id) context.session.delete(fl_db) def get_service_profiles(self, context, filters=None, fields=None, sorts=None, limit=None, marker=None, page_reverse=False): return self._get_collection(context, ServiceProfile, self._make_flavor_dict, filters=filters, fields=fields, sorts=sorts, limit=limit, marker=marker, page_reverse=page_reverse) ",,699,4
openstack%2Fpython-openstackclient~master~Id09c9bdf8804c1ed90e49606e76ffbff1d96a7c2,openstack/python-openstackclient,master,Id09c9bdf8804c1ed90e49606e76ffbff1d96a7c2,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:51:42.000000000,2014-12-06 00:15:59.000000000,2014-12-06 00:15:58.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 6482}, {'_account_id': 8736}]","[{'number': 1, 'created': '2014-12-05 03:51:42.000000000', 'files': ['doc/source/index.rst'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/1dc0e2b5529dd144443e948ad2b8e4a128c2ab9c', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Id09c9bdf8804c1ed90e49606e76ffbff1d96a7c2\n'}]",0,139382,1dc0e2b5529dd144443e948ad2b8e4a128c2ab9c,12,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Id09c9bdf8804c1ed90e49606e76ffbff1d96a7c2
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/82/139382/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/index.rst'],1,1dc0e2b5529dd144443e948ad2b8e4a128c2ab9c,infra-manual,.. _Gerrit: http://docs.openstack.org/infra/manual/developers.html#development-workflow,.. _Gerrit: http://wiki.openstack.org/GerritWorkflow,1,1
openstack%2Fsolum~master~I60c82c0ae8d4fb7bddabd61d755049402da07126,openstack/solum,master,I60c82c0ae8d4fb7bddabd61d755049402da07126,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:58:35.000000000,2014-12-06 00:08:11.000000000,2014-12-06 00:08:10.000000000,"[{'_account_id': 3}, {'_account_id': 1375}, {'_account_id': 2506}, {'_account_id': 9095}]","[{'number': 1, 'created': '2014-12-05 03:58:35.000000000', 'files': ['CONTRIBUTING.rst', 'contrib/devstack/setup_devstack.sh'], 'web_link': 'https://opendev.org/openstack/solum/commit/74d8fb6c6cabd5681819a1ebb0306a966a46915f', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I60c82c0ae8d4fb7bddabd61d755049402da07126\n'}]",0,139501,74d8fb6c6cabd5681819a1ebb0306a966a46915f,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I60c82c0ae8d4fb7bddabd61d755049402da07126
",git fetch https://review.opendev.org/openstack/solum refs/changes/01/139501/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'contrib/devstack/setup_devstack.sh']",2,74d8fb6c6cabd5681819a1ebb0306a966a46915f,infra-manual,git clone https://git.openstack.org/cgit/openstack-dev/devstack.git ${DEVSTACK_DIR},git clone https://github.com/openstack-dev/devstack.git ${DEVSTACK_DIR},3,3
openstack%2Fsolum-infra-guestagent~master~I50243208ee0ac4e20dad72dc170be86a592a8a4e,openstack/solum-infra-guestagent,master,I50243208ee0ac4e20dad72dc170be86a592a8a4e,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:58:38.000000000,2014-12-06 00:02:00.000000000,2014-12-06 00:01:59.000000000,"[{'_account_id': 3}, {'_account_id': 1375}, {'_account_id': 2506}, {'_account_id': 9095}]","[{'number': 1, 'created': '2014-12-05 03:58:38.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/solum-infra-guestagent/commit/e80ee69f0c62009b1f7e9d726b86e4757f0bc00b', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I50243208ee0ac4e20dad72dc170be86a592a8a4e\n'}]",0,139502,e80ee69f0c62009b1f7e9d726b86e4757f0bc00b,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I50243208ee0ac4e20dad72dc170be86a592a8a4e
",git fetch https://review.opendev.org/openstack/solum-infra-guestagent refs/changes/02/139502/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,e80ee69f0c62009b1f7e9d726b86e4757f0bc00b,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Frequirements~master~I62a58a41fa2485b199cd29836e2b1fb46632dab7,openstack/requirements,master,I62a58a41fa2485b199cd29836e2b1fb46632dab7,"Bump oslo.concurrency, as keystone is now importing oslo_concurrency",MERGED,2014-12-03 18:18:40.000000000,2014-12-05 23:58:40.000000000,2014-12-05 23:58:38.000000000,"[{'_account_id': 3}, {'_account_id': 24}, {'_account_id': 475}, {'_account_id': 2472}, {'_account_id': 2592}, {'_account_id': 8090}, {'_account_id': 8871}, {'_account_id': 9656}, {'_account_id': 10980}]","[{'number': 1, 'created': '2014-12-03 18:18:40.000000000', 'files': ['global-requirements.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/26f731a52e7626e6223b1bdbf0fdf11c98dbc990', 'message': 'Bump oslo.concurrency, as keystone is now importing oslo_concurrency\n\nChange-Id: I62a58a41fa2485b199cd29836e2b1fb46632dab7\n'}]",0,138813,26f731a52e7626e6223b1bdbf0fdf11c98dbc990,17,9,1,10980,,,0,"Bump oslo.concurrency, as keystone is now importing oslo_concurrency

Change-Id: I62a58a41fa2485b199cd29836e2b1fb46632dab7
",git fetch https://review.opendev.org/openstack/requirements refs/changes/13/138813/1 && git format-patch -1 --stdout FETCH_HEAD,['global-requirements.txt'],1,26f731a52e7626e6223b1bdbf0fdf11c98dbc990,,oslo.concurrency>=0.3.0 # Apache-2.0,oslo.concurrency>=0.1.0 # Apache-2.0,1,1
openstack%2Fpython-solumclient~master~I8d4b74e1e61eb7eb228986ac4eb4f7ad4fe3fe08,openstack/python-solumclient,master,I8d4b74e1e61eb7eb228986ac4eb4f7ad4fe3fe08,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:58:18.000000000,2014-12-05 23:56:10.000000000,2014-12-05 23:56:09.000000000,"[{'_account_id': 3}, {'_account_id': 1375}, {'_account_id': 2506}, {'_account_id': 9095}]","[{'number': 1, 'created': '2014-12-05 03:58:18.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/f35ff260376613ceed3c7af8eb1d276b6905d25b', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I8d4b74e1e61eb7eb228986ac4eb4f7ad4fe3fe08\n'}]",0,139497,f35ff260376613ceed3c7af8eb1d276b6905d25b,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I8d4b74e1e61eb7eb228986ac4eb4f7ad4fe3fe08
",git fetch https://review.opendev.org/openstack/python-solumclient refs/changes/97/139497/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,f35ff260376613ceed3c7af8eb1d276b6905d25b,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Fcongress~master~I96ff596c15e0b6d47f3d996f856c85187d66777c,openstack/congress,master,I96ff596c15e0b6d47f3d996f856c85187d66777c,Enable: E713 test for membership should be ‘not in',MERGED,2014-12-05 07:29:50.000000000,2014-12-05 23:54:39.000000000,2014-12-05 23:17:18.000000000,"[{'_account_id': 3}, {'_account_id': 4395}, {'_account_id': 13050}]","[{'number': 1, 'created': '2014-12-05 07:29:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/congress/commit/a89e45c4236241b498c1e6bd15f6fb6029e042e5', 'message': ""Enable: E713 test for membership should be ‘not in'\n\nChange-Id: I96ff596c15e0b6d47f3d996f856c85187d66777c\nCloses-Bug: #1398545\n""}, {'number': 2, 'created': '2014-12-05 18:04:41.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/congress/commit/1bd869c3df8d6431043d8bd2f0ae3351bc5f4636', 'message': ""Enable: E713 test for membership should be ‘not in'\n\nChange-Id: I96ff596c15e0b6d47f3d996f856c85187d66777c\nCloses-Bug: #1398545\n""}]",0,139551,1bd869c3df8d6431043d8bd2f0ae3351bc5f4636,20,3,2,12256,,,0,"Enable: E713 test for membership should be ‘not in'

Change-Id: I96ff596c15e0b6d47f3d996f856c85187d66777c
Closes-Bug: #1398545
",git fetch https://review.opendev.org/openstack/congress refs/changes/51/139551/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,a89e45c4236241b498c1e6bd15f6fb6029e042e5,bug/1398545,"ignore = E128,E129,E251,F402,F811,F812,H237,H305,H307,H401,H404,H405,H904,H231,E122,H302","# E713 test for membership should be ‘not in’ignore = E128,E129,E251,E713,F402,F811,F812,H237,H305,H307,H401,H404,H405,H904,H231,E122,H302",1,2
openstack%2Fsolum-specs~master~I43a3bc89dfa967b94dc639b306bf0403a4dce96f,openstack/solum-specs,master,I43a3bc89dfa967b94dc639b306bf0403a4dce96f,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:58:42.000000000,2014-12-05 23:52:30.000000000,2014-12-05 23:52:30.000000000,"[{'_account_id': 3}, {'_account_id': 1375}, {'_account_id': 2506}, {'_account_id': 9095}]","[{'number': 1, 'created': '2014-12-05 03:58:42.000000000', 'files': ['README.rst'], 'web_link': 'https://opendev.org/openstack/solum-specs/commit/1297c5ed373503571a7e018b62c455f638c7fc2e', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I43a3bc89dfa967b94dc639b306bf0403a4dce96f\n'}]",0,139503,1297c5ed373503571a7e018b62c455f638c7fc2e,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I43a3bc89dfa967b94dc639b306bf0403a4dce96f
",git fetch https://review.opendev.org/openstack/solum-specs refs/changes/03/139503/1 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,1297c5ed373503571a7e018b62c455f638c7fc2e,infra-manual, http://docs.openstack.org/infra/manual/developers.html#development-workflow, https://wiki.openstack.org/wiki/Gerrit_Workflow,1,1
openstack%2Fpython-openstackclient~master~I5d4beb9d1fa6914fef5e4c7b459cdd967e614b24,openstack/python-openstackclient,master,I5d4beb9d1fa6914fef5e4c7b459cdd967e614b24,Fix ec2 credentials commands for new auth,MERGED,2014-12-05 20:20:25.000000000,2014-12-05 23:47:10.000000000,2014-12-05 23:47:10.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 6482}, {'_account_id': 8736}]","[{'number': 1, 'created': '2014-12-05 20:20:25.000000000', 'files': ['openstackclient/identity/v2_0/ec2creds.py'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/62a2083a785bcfac25b2b7e409e1c7e066f9edff', 'message': 'Fix ec2 credentials commands for new auth\n\nThese commands were not updated for the new authentication model.\n\nCloses-Bug: 1399757\n\nChange-Id: I5d4beb9d1fa6914fef5e4c7b459cdd967e614b24\n'}]",0,139729,62a2083a785bcfac25b2b7e409e1c7e066f9edff,8,4,1,970,,,0,"Fix ec2 credentials commands for new auth

These commands were not updated for the new authentication model.

Closes-Bug: 1399757

Change-Id: I5d4beb9d1fa6914fef5e4c7b459cdd967e614b24
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/29/139729/1 && git format-patch -1 --stdout FETCH_HEAD,['openstackclient/identity/v2_0/ec2creds.py'],1,62a2083a785bcfac25b2b7e409e1c7e066f9edff,bug/1399757, project = self.app.client_manager.auth_ref.project_id user = self.app.client_manager.auth_ref.user_id user = self.app.client_manager.auth_ref.user_id user = self.app.client_manager.auth_ref.user_id user = self.app.client_manager.auth_ref.user_id, project = identity_client.auth_tenant_id user = identity_client.auth_user_id user = identity_client.auth_user_id user = identity_client.auth_user_id user = identity_client.auth_user_id,5,5
openstack%2Fnova~stable%2Fjuno~I983feea4ac26e34032fa66a4b55f0ce42699ba6a,openstack/nova,stable/juno,I983feea4ac26e34032fa66a4b55f0ce42699ba6a,Sync strutils from oslo-incubator for mask_password fix,MERGED,2014-10-23 21:58:41.000000000,2014-12-05 23:37:33.000000000,2014-12-05 23:37:30.000000000,"[{'_account_id': 3}, {'_account_id': 979}, {'_account_id': 1955}, {'_account_id': 5170}, {'_account_id': 6873}, {'_account_id': 10118}]","[{'number': 1, 'created': '2014-10-23 21:58:41.000000000', 'files': ['nova/openstack/common/strutils.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/0bad3b14abe54926b73acdab8e9b31cae4a87e47', 'message': 'Sync strutils from oslo-incubator for mask_password fix\n\nThis sync pulls in:\n   1131b56 Enable mask_password to handle byte code strings\n\nThis is needed because Nova commands are hitting the same\nproblem Cinder was hitting in bug 1368527 which was fixed\nby this strutils update in bug 1366189.\n\nThis is not needed in kilo because in kilo Nova has moved\nto using the oslo.utils library, which has this fix in it.\n\nCloses-bug: #1366189\nChange-Id: I983feea4ac26e34032fa66a4b55f0ce42699ba6a\n'}]",0,130644,0bad3b14abe54926b73acdab8e9b31cae4a87e47,14,6,1,6601,,,0,"Sync strutils from oslo-incubator for mask_password fix

This sync pulls in:
   1131b56 Enable mask_password to handle byte code strings

This is needed because Nova commands are hitting the same
problem Cinder was hitting in bug 1368527 which was fixed
by this strutils update in bug 1366189.

This is not needed in kilo because in kilo Nova has moved
to using the oslo.utils library, which has this fix in it.

Closes-bug: #1366189
Change-Id: I983feea4ac26e34032fa66a4b55f0ce42699ba6a
",git fetch https://review.opendev.org/openstack/nova refs/changes/44/130644/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/openstack/common/strutils.py'],1,0bad3b14abe54926b73acdab8e9b31cae4a87e47,bug/1368527, try: message = six.text_type(message) except UnicodeDecodeError: # NOTE(jecarey): Temporary fix to handle cases where message is a # byte string. A better solution will be provided in Kilo. pass, message = six.text_type(message),6,1
openstack%2Fbarbican~master~I2d859aba74a7f6c55435f2d56e8676e6bf9c4430,openstack/barbican,master,I2d859aba74a7f6c55435f2d56e8676e6bf9c4430,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:40:29.000000000,2014-12-05 23:07:29.000000000,2014-12-05 23:07:28.000000000,"[{'_account_id': 3}, {'_account_id': 7136}, {'_account_id': 7262}, {'_account_id': 7973}]","[{'number': 1, 'created': '2014-12-05 03:40:29.000000000', 'files': ['doc/source/setup/devstack.rst', 'doc/source/contribute/getting_involved.rst'], 'web_link': 'https://opendev.org/openstack/barbican/commit/b8ab9433348156c336b0185056fb9760f1398e8d', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I2d859aba74a7f6c55435f2d56e8676e6bf9c4430\n'}]",0,139309,b8ab9433348156c336b0185056fb9760f1398e8d,9,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I2d859aba74a7f6c55435f2d56e8676e6bf9c4430
",git fetch https://review.opendev.org/openstack/barbican refs/changes/09/139309/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/setup/devstack.rst', 'doc/source/contribute/getting_involved.rst']",2,b8ab9433348156c336b0185056fb9760f1398e8d,infra-manual,.. _`Gerrit Workflow`: http://docs.openstack.org/infra/manual/developers.html#development-workflow,.. _`Gerrit Workflow`: https://wiki.openstack.org/wiki/Gerrit_Workflow,2,2
openstack%2Fheat~master~I1c43e5cf2a7ab6fa388b91fd420392996c368e72,openstack/heat,master,I1c43e5cf2a7ab6fa388b91fd420392996c368e72,Remove heat.config.sample file,MERGED,2014-12-03 17:45:53.000000000,2014-12-05 23:05:41.000000000,2014-12-05 23:05:40.000000000,"[{'_account_id': 3}, {'_account_id': 3098}, {'_account_id': 4328}, {'_account_id': 7256}, {'_account_id': 9542}, {'_account_id': 13009}, {'_account_id': 13323}]","[{'number': 1, 'created': '2014-12-03 17:45:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/ea8817b4138d5a7eed3455b00f655b79a99585b7', 'message': 'Remove heat.config.sample file\n\nThis file keeps breaking our gate, because libraries we depend on\nrelease and the generated content changes, thus breaking our gate.\n\nTo avoid this, move to the same model used by other projects, e.g\nnova, and include a README which shows how to use the (existing)\ntox genconfig target to build the sample file.\n\nChange-Id: I1c43e5cf2a7ab6fa388b91fd420392996c368e72\n'}, {'number': 2, 'created': '2014-12-03 17:48:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/heat/commit/0a895d9a2a60c6a1eef5ad33f851b34c6970cdf0', 'message': 'Remove heat.config.sample file\n\nThis file keeps breaking our gate, because libraries we depend on\nrelease and the generated content changes, thus breaking our gate.\n\nTo avoid this, move to the same model used by other projects, e.g\nnova, and include a README which shows how to use the (existing)\ntox genconfig target to build the sample file.\n\nChange-Id: I1c43e5cf2a7ab6fa388b91fd420392996c368e72\n'}, {'number': 3, 'created': '2014-12-03 20:36:39.000000000', 'files': ['etc/heat/heat.conf.sample', 'etc/heat/README-heat.conf.txt'], 'web_link': 'https://opendev.org/openstack/heat/commit/cf0b59953cedb6aeb5d182aa567e2cb928843ca6', 'message': 'Remove heat.config.sample file\n\nThis file keeps breaking our gate, because libraries we depend on\nrelease and the generated content changes.\n\nThe check has been disabled, but to avoid the sample config becoming\ntotally stale, remove it and replace with a README which says\nhow to generate the current sample config.\n\nChange-Id: I1c43e5cf2a7ab6fa388b91fd420392996c368e72\n'}]",0,138800,cf0b59953cedb6aeb5d182aa567e2cb928843ca6,24,7,3,4328,,,0,"Remove heat.config.sample file

This file keeps breaking our gate, because libraries we depend on
release and the generated content changes.

The check has been disabled, but to avoid the sample config becoming
totally stale, remove it and replace with a README which says
how to generate the current sample config.

Change-Id: I1c43e5cf2a7ab6fa388b91fd420392996c368e72
",git fetch https://review.opendev.org/openstack/heat refs/changes/00/138800/2 && git format-patch -1 --stdout FETCH_HEAD,"['etc/heat/heat.conf.sample', 'etc/heat/README-heat.conf.txt', 'tox.ini']",3,ea8817b4138d5a7eed3455b00f655b79a99585b7,no_sample_config2,, {toxinidir}/tools/config/check_uptodate.sh,4,1320
openstack%2Fhorizon~master~I54d07233d2003904468ef317c352126eb6bf5f70,openstack/horizon,master,I54d07233d2003904468ef317c352126eb6bf5f70,Create titles without concatenation,MERGED,2014-11-20 16:48:24.000000000,2014-12-05 23:05:30.000000000,2014-12-05 23:05:28.000000000,"[{'_account_id': 3}, {'_account_id': 1941}, {'_account_id': 2455}, {'_account_id': 5623}, {'_account_id': 6637}, {'_account_id': 7012}, {'_account_id': 7179}, {'_account_id': 7634}, {'_account_id': 9317}, {'_account_id': 9576}, {'_account_id': 9622}, {'_account_id': 9981}, {'_account_id': 11592}, {'_account_id': 12826}, {'_account_id': 13325}, {'_account_id': 13747}]","[{'number': 1, 'created': '2014-11-20 16:48:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/3b4bc719fd3940d1f29a957bd432e107c7e98de1', 'message': 'Create titles without concatenation.\n\nSeveral panels use the template filter ""add"" to concatenate data\nand translatable text.  These need to be updated to use string\nformatting.\n\nChange-Id: I54d07233d2003904468ef317c352126eb6bf5f70\nCloses-bug: #1394573\n'}, {'number': 2, 'created': '2014-11-20 16:48:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/141a68cdcdf3a2fb9095e2921e352939bd5d42b7', 'message': 'Create titles without concatenation\n\nSeveral panels use the template filter ""add"" to concatenate data\nand translatable text.  These need to be updated to use string\nformatting.\n\nChange-Id: I54d07233d2003904468ef317c352126eb6bf5f70\nCloses-bug: #1394573\n'}, {'number': 3, 'created': '2014-11-24 21:32:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/e61bcc9c41deb69d977a458ab4d29a1ffcf2aefb', 'message': 'Create titles without concatenation\n\nSeveral panels use the template filter ""add"" to concatenate data\nand translatable text.  These need to be updated to use string\nformatting.  This are obscure because it\'s not clear when looking\nat the translations if any text follows the translatable segment.\n\nChange-Id: I54d07233d2003904468ef317c352126eb6bf5f70\nCloses-bug: #1394573\n'}, {'number': 4, 'created': '2014-12-02 13:09:30.000000000', 'files': ['openstack_dashboard/dashboards/project/database_backups/views.py', 'openstack_dashboard/dashboards/admin/volumes/volumes/views.py', 'openstack_dashboard/dashboards/project/access_and_security/security_groups/views.py', 'openstack_dashboard/dashboards/project/images/templates/images/images/detail.html', 'openstack_dashboard/dashboards/project/stacks/views.py', 'openstack_dashboard/dashboards/project/volumes/templates/volumes/volumes/detail.html', 'openstack_dashboard/dashboards/project/volumes/snapshots/views.py', 'openstack_dashboard/dashboards/project/databases/views.py', 'openstack_dashboard/dashboards/project/firewalls/templates/firewalls/updaterule.html', 'openstack_dashboard/dashboards/project/firewalls/templates/firewalls/updatepolicy.html', 'openstack_dashboard/dashboards/project/instances/templates/instances/detail.html', 'openstack_dashboard/dashboards/project/networks/templates/networks/detail.html', 'openstack_dashboard/dashboards/project/volumes/backups/views.py', 'openstack_dashboard/dashboards/project/images/images/views.py', 'openstack_dashboard/dashboards/project/volumes/templates/volumes/snapshots/detail.html', 'openstack_dashboard/dashboards/project/databases/templates/databases/detail.html', 'openstack_dashboard/dashboards/project/instances/views.py', 'openstack_dashboard/dashboards/project/volumes/volumes/views.py', 'openstack_dashboard/dashboards/project/stacks/templates/stacks/detail.html', 'openstack_dashboard/dashboards/project/stacks/templates/stacks/resource.html', 'openstack_dashboard/dashboards/project/firewalls/views.py', 'openstack_dashboard/dashboards/project/database_backups/templates/database_backups/details.html', 'openstack_dashboard/dashboards/project/volumes/templates/volumes/backups/detail.html', 'openstack_dashboard/dashboards/project/networks/views.py', 'openstack_dashboard/dashboards/identity/groups/templates/groups/manage.html', 'openstack_dashboard/dashboards/admin/volumes/templates/volumes/volumes/detail.html', 'openstack_dashboard/dashboards/project/access_and_security/templates/access_and_security/security_groups/detail.html', 'openstack_dashboard/dashboards/identity/groups/views.py', 'openstack_dashboard/dashboards/project/firewalls/templates/firewalls/updatefirewall.html'], 'web_link': 'https://opendev.org/openstack/horizon/commit/de20d7271aea8a3e47bd0afec2d3737035813701', 'message': 'Create titles without concatenation\n\nSeveral panels use the template filter ""add"" to concatenate data\nand translatable text, for example\ntitle=_(""Volume Details: "")|add:volume.name|default:_(""Volume Details:"")\n\nThese need to be updated to use string formatting.  This is needed\nbecause it\'s not clear when looking at the translations if any text\nfollows the translatable segment.  Dict based substitution has been\nused to further clarify the messages for translators.\n\nChange-Id: I54d07233d2003904468ef317c352126eb6bf5f70\nCloses-bug: #1394573\n'}]",2,136056,de20d7271aea8a3e47bd0afec2d3737035813701,45,16,4,9981,,,0,"Create titles without concatenation

Several panels use the template filter ""add"" to concatenate data
and translatable text, for example
title=_(""Volume Details: "")|add:volume.name|default:_(""Volume Details:"")

These need to be updated to use string formatting.  This is needed
because it's not clear when looking at the translations if any text
follows the translatable segment.  Dict based substitution has been
used to further clarify the messages for translators.

Change-Id: I54d07233d2003904468ef317c352126eb6bf5f70
Closes-bug: #1394573
",git fetch https://review.opendev.org/openstack/horizon refs/changes/56/136056/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack_dashboard/dashboards/project/database_backups/views.py', 'openstack_dashboard/dashboards/admin/volumes/volumes/views.py', 'openstack_dashboard/dashboards/project/access_and_security/security_groups/views.py', 'openstack_dashboard/dashboards/project/images/templates/images/images/detail.html', 'openstack_dashboard/dashboards/project/stacks/views.py', 'openstack_dashboard/dashboards/project/volumes/templates/volumes/volumes/detail.html', 'openstack_dashboard/dashboards/project/volumes/snapshots/views.py', 'openstack_dashboard/dashboards/project/databases/views.py', 'openstack_dashboard/dashboards/project/firewalls/templates/firewalls/updaterule.html', 'openstack_dashboard/dashboards/project/firewalls/templates/firewalls/updatepolicy.html', 'openstack_dashboard/dashboards/project/instances/templates/instances/detail.html', 'openstack_dashboard/dashboards/project/networks/templates/networks/detail.html', 'openstack_dashboard/dashboards/project/volumes/backups/views.py', 'openstack_dashboard/dashboards/project/images/images/views.py', 'openstack_dashboard/dashboards/project/volumes/templates/volumes/snapshots/detail.html', 'openstack_dashboard/dashboards/project/databases/templates/databases/detail.html', 'openstack_dashboard/dashboards/project/instances/views.py', 'openstack_dashboard/dashboards/project/volumes/volumes/views.py', 'openstack_dashboard/dashboards/project/stacks/templates/stacks/detail.html', 'openstack_dashboard/dashboards/project/stacks/templates/stacks/resource.html', 'openstack_dashboard/dashboards/project/firewalls/views.py', 'openstack_dashboard/dashboards/project/database_backups/templates/database_backups/details.html', 'openstack_dashboard/dashboards/project/volumes/templates/volumes/backups/detail.html', 'openstack_dashboard/dashboards/project/networks/views.py', 'openstack_dashboard/dashboards/identity/groups/templates/groups/manage.html', 'openstack_dashboard/dashboards/admin/volumes/templates/volumes/volumes/detail.html', 'openstack_dashboard/dashboards/project/access_and_security/templates/access_and_security/security_groups/detail.html', 'openstack_dashboard/dashboards/identity/groups/views.py', 'openstack_dashboard/dashboards/project/firewalls/templates/firewalls/updatefirewall.html']",29,3b4bc719fd3940d1f29a957bd432e107c7e98de1,bug/1394573," {% include ""horizon/common/_page_header.html"" with title=page_title %}"," {% include ""horizon/common/_page_header.html"" with title=_(""Edit Firewall "")|add:name %}",49,19
openstack%2Fswift~master~I060e5f6869fd302a47a54556f31763b5ab668012,openstack/swift,master,I060e5f6869fd302a47a54556f31763b5ab668012,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:52:21.000000000,2014-12-05 23:05:21.000000000,2014-12-05 23:05:20.000000000,"[{'_account_id': 3}, {'_account_id': 1179}, {'_account_id': 7233}, {'_account_id': 7847}, {'_account_id': 9625}, {'_account_id': 13052}]","[{'number': 1, 'created': '2014-12-05 03:52:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/b8eba8a96c0551c4f04e31390a14bef70e520b78', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I060e5f6869fd302a47a54556f31763b5ab668012\n'}, {'number': 2, 'created': '2014-12-05 04:30:38.000000000', 'files': ['CONTRIBUTING.md'], 'web_link': 'https://opendev.org/openstack/swift/commit/a3b192614dbff709237b6269932980e4f04017a2', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I060e5f6869fd302a47a54556f31763b5ab668012\n'}]",1,139393,a3b192614dbff709237b6269932980e4f04017a2,14,6,2,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I060e5f6869fd302a47a54556f31763b5ab668012
",git fetch https://review.opendev.org/openstack/swift refs/changes/93/139393/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.md'],1,b8eba8a96c0551c4f04e31390a14bef70e520b78,infra-manual,you must follow the steps in this page: section of this page: [http://docs.openstack.org/infra/manual/developers.html](http://docs.openstack.org/infra/manual/developers.html#development-workflow)the workflow documented at [http://docs.openstack.org/infra/manual/developers.html#development-workflow](http://docs.openstack.org/infra/manual/developers.html#development-workflow).,"you must follow the steps in the ""If you're a developer"" section of this page: [http://wiki.openstack.org/HowToContribute](http://wiki.openstack.org/HowToContribute#If_you.27re_a_developer)the workflow documented at [http://wiki.openstack.org/GerritWorkflow](http://wiki.openstack.org/GerritWorkflow).",3,3
openstack%2Ftrove~master~Ic33174d053274ab980c14da03b8394bfaf5375d3,openstack/trove,master,Ic33174d053274ab980c14da03b8394bfaf5375d3,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:52:47.000000000,2014-12-05 23:05:18.000000000,2014-12-05 23:05:17.000000000,"[{'_account_id': 3}, {'_account_id': 1925}, {'_account_id': 5293}]","[{'number': 1, 'created': '2014-12-05 03:52:47.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/trove/commit/f7b4e8f13d0baaf731f5308e8a5b98ee1d6da6aa', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Ic33174d053274ab980c14da03b8394bfaf5375d3\n'}]",0,139400,f7b4e8f13d0baaf731f5308e8a5b98ee1d6da6aa,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Ic33174d053274ab980c14da03b8394bfaf5375d3
",git fetch https://review.opendev.org/openstack/trove refs/changes/00/139400/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,f7b4e8f13d0baaf731f5308e8a5b98ee1d6da6aa,infra-manual, http://docs.openstack.org/infra/manual/developers.html#development-workflow http://docs.openstack.org/infra/manual/developers.html#development-workflow - http://docs.openstack.org/infra/manual/developers.html, http://wiki.openstack.org/HowToContribute#If_you.27re_a_developer http://wiki.openstack.org/GerritWorkflow - https://wiki.openstack.org/wiki/How_To_Contribute,3,3
openstack%2Fkeystone-specs~master~I622d5a61054f487fc9583f04a4f5930af17fa6a4,openstack/keystone-specs,master,I622d5a61054f487fc9583f04a4f5930af17fa6a4,Fix 'heirarchy' typo on 'Get project',MERGED,2014-12-05 06:25:37.000000000,2014-12-05 23:04:41.000000000,2014-12-05 23:04:41.000000000,"[{'_account_id': 3}, {'_account_id': 1916}, {'_account_id': 6486}]","[{'number': 1, 'created': '2014-12-05 06:25:37.000000000', 'files': ['api/v3/identity-api-v3.rst'], 'web_link': 'https://opendev.org/openstack/keystone-specs/commit/1ea325eac66f0d23f91741740b2252c24fdc0575', 'message': ""Fix 'heirarchy' typo on 'Get project'\n\nChange-Id: I622d5a61054f487fc9583f04a4f5930af17fa6a4\n""}]",0,139536,1ea325eac66f0d23f91741740b2252c24fdc0575,7,3,1,9142,,,0,"Fix 'heirarchy' typo on 'Get project'

Change-Id: I622d5a61054f487fc9583f04a4f5930af17fa6a4
",git fetch https://review.opendev.org/openstack/keystone-specs refs/changes/36/139536/1 && git format-patch -1 --stdout FETCH_HEAD,['api/v3/identity-api-v3.rst'],1,1ea325eac66f0d23f91741740b2252c24fdc0575,get-project-typo,"If additional information about the project's hierarchy is required, this API","If additional information about the project's heirarchy is required, this API",1,1
openstack%2Fneutron-specs~master~I11d8c37b63338afcc72054dbce93acd0998dfb2e,openstack/neutron-specs,master,I11d8c37b63338afcc72054dbce93acd0998dfb2e,Brocade Vyatta FWaaS device driver,ABANDONED,2014-12-03 23:55:56.000000000,2014-12-05 23:03:43.000000000,,"[{'_account_id': 3}, {'_account_id': 162}]","[{'number': 1, 'created': '2014-12-03 23:55:56.000000000', 'files': ['specs/kilo/brocade-vyatta-fwaas-plugin.rst'], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/c3c5da1ac8319e7c9d32f642646930e0c83316ed', 'message': 'Brocade Vyatta FWaaS device driver\n\nChange-Id: I11d8c37b63338afcc72054dbce93acd0998dfb2e\n'}]",0,138908,c3c5da1ac8319e7c9d32f642646930e0c83316ed,3,2,1,12525,,,0,"Brocade Vyatta FWaaS device driver

Change-Id: I11d8c37b63338afcc72054dbce93acd0998dfb2e
",git fetch https://review.opendev.org/openstack/neutron-specs refs/changes/08/138908/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/kilo/brocade-vyatta-fwaas-plugin.rst'],1,c3c5da1ac8319e7c9d32f642646930e0c83316ed,bp/brocade-vyatta-fwaas-plugin,"Introduce the Brocade Vyatta Firewall device driver to provide FWaaS solution using Vyatta vRouter VM running as a Neutron router. The driver implementsBrocade Vyatta vRouter is a multi-service product that provides various L3 and L4 services like Routing, NAT, Firewall, VPN, etc. While basic neutron router L3 functions are available using Brocade Vyatta L3 plugin [1] vRouter's Firewall functionality is currently not configurable through existing Neutron FWaaS APIs.This blueprint proposes a new vendor device-driver for Neutron FWaaS agent. There is no change proposed in the FWaaS service plugin side as existing reference FWaaS plugin is sufficient.>> | >> | >> | REST API >> | >> +--------v---------+ >> | | >> | | >> | Vyatta vRouter | >> | | >> | | >> | | >> | | >> +------------------+ Vyatta FWaaS device driver will invoke the Vyatta vRouter REST APIs for theGiven the device driver interface is planned to be kept as-is the changesThis effort is part of a wider set of blueprints to offer Neutron L3 and L4 services using Vyatta vRouter VM:None.The device driver will use a common RESTapi client library that uses basic-auth authentication to connect to Vyatta vRouter.When tenants create Firewall using Neutron API it will be created on the carrier-grade Vyatta vRouter.None. IPv6 Impact ----------- None.1. Operators should first configure Brocade Vyatta L3 plugin as described in [1]. 2. Edit the controller node neutron configuration file ‘/etc/neutron/neutron.conf’ to specify loading the FirewallPlugin in addition to Vyatta vRouter L3 plugin 4. Edit the network node configuration file ‘/etc/init/neutron-l3-agent.conf’ to specify that the config file ‘/etc/neutron/plugins.brocade/vyatta/vrouter.ini’ be loaded at l3-agent service startup 5. Edit the network node configuration file ‘/etc/neutron/fwaas_driver.ini’ file with below parameters to specify Vyatta FWaaS driverOnce configured, Vyatta FWaaS driver will be invoked for the firewall CRUD operation on tenant router* Brocade Vyatta L3 Plugin [1]functionality in Neutron and the fwaas_device_driver configuration required to enable it.* [3] https://blueprints.launchpad.net/neutron/+spec/restructure-l3-agent","Introduce the Brocade Vyatta Firewall device driver to provide FWaaS solution using Vyatta vRouter VM running as a Neutron router. The driver implements Brocade Vyatta vRouter is a multi-service product that provides various L3 and L4 services like Routing, NAT, Firewall, VPN, etc. While basic neutron router L3 functions are available using Brocade Vyatta L3 plugin [1] vRouter's Firewall functionality is currently not configurable through existing Neutron FWaaS APIs. This blueprint proposes a new vendor device-driver for Neutron FWaaS agent. There is no change proposed in the FWaaS service plugin side as existing reference FWaaS plugin is sufficient. >> | >> | >> | REST API >> | >> +--------v---------+ >> | | >> | | >> | Vyatta vRouter | >> | | >> | | >> | | >> | | >> +------------------+ Vyatta FWaaS device driver will invoke the Vyatta vRouter REST APIs for the Given the device driver interface is planned to be kept as-is the changes This effort is part of a wider set of blueprints to offer Neutron L3 and L4 services using Vyatta vRouter VM:None. The device driver will use a common RESTapi client library that uses basic-auth authentication to connect to Vyatta vRouter. When tenants create Firewall using Neutron API it will be created on the carrier-grade Vyatta vRouter. None. 1. Operators should first configure Brocade Vyatta L3 plugin as described in [1]. 2. Edit the controller node neutron configuration file ‘/etc/neutron/neutron.conf’ to specify loading the FirewallPlugin in addition to Vyatta vRouter L3 plugin 4. Edit the network node configuration file ‘/etc/init/neutron-l3-agent.conf’ to specify that the config file ‘/etc/neutron/plugins.brocade/vyatta/vrouter.ini’ be loaded at l3-agent service startup 5. Edit the network node configuration file ‘/etc/neutron/fwaas_driver.ini’ file with below parameters to specify Vyatta FWaaS driverOnce configured, Vyatta FWaaS driver will be invoked for the firewall CRUD operation on tenant router* Brocade Vyatta L3 Plugin [1] functionality in Neutron and the fwaas_device_driver configuration required to enable it.* [3] https://blueprints.launchpad.net/neutron/+spec/restructure-l3-agent ",57,50
openstack%2Fkeystone-specs~master~I9eeeefca78b1a2cc6cd867fe5fb77c1463283ff3,openstack/keystone-specs,master,I9eeeefca78b1a2cc6cd867fe5fb77c1463283ff3,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:42:02.000000000,2014-12-05 23:03:29.000000000,2014-12-05 23:03:29.000000000,"[{'_account_id': 3}, {'_account_id': 6482}, {'_account_id': 6486}]","[{'number': 1, 'created': '2014-12-05 03:42:02.000000000', 'files': ['README.rst'], 'web_link': 'https://opendev.org/openstack/keystone-specs/commit/303ef7464cd5878322d1c20009bd4492da715afe', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I9eeeefca78b1a2cc6cd867fe5fb77c1463283ff3\n'}]",0,139333,303ef7464cd5878322d1c20009bd4492da715afe,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I9eeeefca78b1a2cc6cd867fe5fb77c1463283ff3
",git fetch https://review.opendev.org/openstack/keystone-specs refs/changes/33/139333/1 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,303ef7464cd5878322d1c20009bd4492da715afe,infra-manual, http://docs.openstack.org/infra/manual/developers.html#development-workflow, https://wiki.openstack.org/wiki/Gerrit_Workflow,1,1
openstack%2Fbarbican~master~Iadf53bdca09a26f0e29bb2025a73f22529943e4e,openstack/barbican,master,Iadf53bdca09a26f0e29bb2025a73f22529943e4e,Add functional tests for order,MERGED,2014-11-20 22:31:24.000000000,2014-12-05 23:02:11.000000000,2014-12-05 23:02:10.000000000,"[{'_account_id': 3}, {'_account_id': 994}, {'_account_id': 7136}, {'_account_id': 7789}, {'_account_id': 8004}, {'_account_id': 8623}, {'_account_id': 9234}, {'_account_id': 10873}, {'_account_id': 13510}]","[{'number': 1, 'created': '2014-11-20 22:31:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/786c9d5d5d7e6848254d3793b0e3a103b8d6cf5f', 'message': 'Add functional tests for order\n\nAdded all current functional tests for orders.\nThis does not include paging tests.\n\nChange-Id: Iadf53bdca09a26f0e29bb2025a73f22529943e4e\n'}, {'number': 2, 'created': '2014-11-20 23:40:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/7a77844b02368ae28eceffd2f74fca06e68e8c72', 'message': 'Add functional tests for order\n\nAdded all current functional tests for orders.\nThis does not include paging tests.\n\nChange-Id: Iadf53bdca09a26f0e29bb2025a73f22529943e4e\n'}, {'number': 3, 'created': '2014-11-21 14:26:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/c6b98698697333e9614c0ca1a443a8e815181f21', 'message': 'Add functional tests for order\n\nAdded all current functional tests for orders.\nThis does not include paging tests.\n\nChange-Id: Iadf53bdca09a26f0e29bb2025a73f22529943e4e\n'}, {'number': 4, 'created': '2014-11-21 15:12:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/911b5a13a570c1c43c7f0668da4f94bfd0ae5376', 'message': 'Add functional tests for order\n\nAdded all current functional tests for orders.\nThis does not include paging tests.\n\nChange-Id: Iadf53bdca09a26f0e29bb2025a73f22529943e4e\n'}, {'number': 5, 'created': '2014-11-25 16:51:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/096f81c92c3ef200e9e8ab3f7375fc9e8c83dce3', 'message': 'Add functional tests for order\n\nAdded all current functional tests for orders.\nThis does not include paging tests.\n\nChange-Id: Iadf53bdca09a26f0e29bb2025a73f22529943e4e\n'}, {'number': 6, 'created': '2014-12-01 15:24:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/54cf5c0fbbde8128fe75e3949f32556eb234e0fb', 'message': 'Add functional tests for order\n\nAdded all current functional tests for orders.\nThis does not include paging tests.\n\nChange-Id: Iadf53bdca09a26f0e29bb2025a73f22529943e4e\n'}, {'number': 7, 'created': '2014-12-01 20:36:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/a108441877db23f0abd01156cf2b230befbe5ab3', 'message': 'Add functional tests for order\n\nAdded all current functional tests for orders.\nThis does not include paging tests.\n\nChange-Id: Iadf53bdca09a26f0e29bb2025a73f22529943e4e\n'}, {'number': 8, 'created': '2014-12-02 16:34:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/3fd2aeddcfdc1abb02f148c9a68fd54c483c2564', 'message': 'Add functional tests for order\n\nAdded all current functional tests for orders.\nThis does not include paging tests.\n\nChange-Id: Iadf53bdca09a26f0e29bb2025a73f22529943e4e\n'}, {'number': 9, 'created': '2014-12-02 17:19:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/0af4e9fe03ff996304bf513b825a3c6c65f5bdc0', 'message': 'Add functional tests for order\n\nAdded all current functional tests for orders.\nThis does not include paging tests.\n\nChange-Id: Iadf53bdca09a26f0e29bb2025a73f22529943e4e\n'}, {'number': 10, 'created': '2014-12-02 17:39:12.000000000', 'files': ['functionaltests/api/base.py', 'functionaltests/api/v1/functional/test_secrets.py', 'functionaltests/api/v1/functional/test_orders.py'], 'web_link': 'https://opendev.org/openstack/barbican/commit/c5c84a3e186047ddaa57c6f701649fba5b960b51', 'message': 'Add functional tests for order\n\nAdded all current functional tests for orders.\nThis does not include paging tests.\n\nChange-Id: Iadf53bdca09a26f0e29bb2025a73f22529943e4e\n'}]",12,136155,c5c84a3e186047ddaa57c6f701649fba5b960b51,40,9,10,13510,,,0,"Add functional tests for order

Added all current functional tests for orders.
This does not include paging tests.

Change-Id: Iadf53bdca09a26f0e29bb2025a73f22529943e4e
",git fetch https://review.opendev.org/openstack/barbican refs/changes/55/136155/10 && git format-patch -1 --stdout FETCH_HEAD,['functionaltests/api/v1/functional/test_orders.py'],1,786c9d5d5d7e6848254d3793b0e3a103b8d6cf5f,orders_functional,"import copy import json import sys max_allowed_payload_in_bytes = 10000 oversized = bytearray().zfill(max_allowed_payload_in_bytes + 1) len_255_string = str(bytearray().zfill(255)) self.default_data = copy.deepcopy(order_create_defaults_data) self.nones_data = copy.deepcopy(order_create_nones_data) test_model = order_models.OrderModel(**self.default_data) test_model.name = None test_model = order_models.OrderModel(**self.default_data) test_model.name = """" @testcase.attr('positive') def test_create_order_defaults_payload_content_type_none(self): """"""Covers creating orders with various valid payload content types."""""" test_model = order_models.OrderModel(**self.default_data) del test_model.meta['payload_content_type'] self.assertEqual(create_resp.status_code, 202) self.assertIsNotNone(order_ref) test_model = order_models.OrderModel(**self.default_data) test_model.meta['name'] = """" @testcase.attr('positive') def test_order_and_secret_metadata_same(self): """"""Checks that metadata from secret GET and order GET are the same. Covers checking that secret metadata from a get on the order and secret metadata from a get on the secret are the same. Assumes that the order status will be active and not pending. """""" test_model = order_models.OrderModel(**self.default_data) resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(resp.status_code, 202) order_resp = self.behaviors.get_order(order_ref) secret_ref = order_resp.model.secret_ref secret_resp = self.secret_behaviors.get_secret_metadata(secret_ref) self.assertEqual(order_resp.model.meta['name'], secret_resp.model.name, 'Names were not the same') self.assertEqual(order_resp.model.meta['algorithm'], secret_resp.model.algorithm, 'Algorithms were not the same') self.assertEqual(order_resp.model.meta['bit_length'], secret_resp.model.bit_length, 'Bit lengths were not the same') self.assertEqual(order_resp.model.meta['expiration'], secret_resp.model.expiration, 'Expirations were not the same') self.assertEqual(order_resp.model.meta['mode'], secret_resp.model.mode, 'Cypher types were not the same') @testcase.attr('negative') def test_get_order_defaults_that_doesnt_exist(self): """"""Covers case of getting a non-existent order."""""" # try to get a non-existent order order_resp = self.behaviors.get_order(""a ref that does not exist"") # verify that the order get failed self.assertEqual(order_resp.status_code, 404) @testcase.attr('negative') def test_create_order_defaults_w_invalid_content_type(self): """"""Covers creating order with invalid content-type header."""""" test_model = order_models.OrderModel(**self.default_data) extra_headers = {""Content-Type"": ""crypto/boom""} create_resp, order_ref = self.behaviors.create_order( test_model, extra_headers=extra_headers) self.assertEqual(create_resp.status_code, 415) self.assertIsNone(order_ref) @testcase.attr('negative') def test_create_order_nones(self): """"""Covers order creation with empty JSON."""""" test_model = order_models.OrderModel(**self.nones_data) create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 400) self.assertIsNone(order_ref) @testcase.attr('negative') def test_create_order_empty_entries(self): """"""Covers order creation with empty JSON."""""" test_model = order_models.OrderModel(**self.nones_data) test_model.meta['name'] = """" test_model.meta['algorithm'] = """" test_model.meta['mode'] = """" test_model.meta['bit_length'] = """" test_model.meta['payload_content_type'] = """" create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 400) self.assertIsNone(order_ref) @testcase.attr('negative') def test_create_order_defaults_oversized_strings(self): """"""Covers order creation with empty JSON."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['name'] = str(oversized) test_model.meta['algorithm'] = str(oversized) test_model.meta['mode'] = str(oversized) create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 400) self.assertIsNone(order_ref) @testcase.attr('negative') def test_create_order_defaults_error_message_on_invalid_order_create(self): """"""Related Launchpad issue: 1269594."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['payload_content_encoding'] = ""blarg!"" resp, order_ref = self.behaviors.create_order(test_model) print(resp.content) # Make sure we actually get a message back error_msg = json.loads(resp.content).get('title') self.assertEqual(resp.status_code, 400) self.assertIsNotNone(error_msg) self.assertNotEqual(error_msg, 'None') @testcase.skip @testcase.attr('negative') def test_create_order_defaults_wout_mode(self): """"""Create an order without the mode attribute. #Launchpad bug 1376902 """""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['mode'] = None create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 400) @testcase.skip @testcase.attr('negative') def test_create_order_defaults_wout_algorithm(self): """"""Create an order without the algorithm attribute. Launchpad bug 1376902 """""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['algorithm'] = None create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 400) @utils.parameterized_dataset({ '8': [8], '64': [64], '128': [128], '192': [192], '256': [256], '16M_plus_256': [16777472] }) @testcase.attr('positive') def test_create_order_defaults_valid_bit_length(self, bit_length): """"""Covers creating orders with various valid bit lengths."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['bit_length'] = bit_length create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 202) self.assertIsNotNone(order_ref) @utils.parameterized_dataset({ 'negative_maxint': [-sys.maxint], 'negative_7': [-7], 'negative_1': [-1], '0': [0], '1': [1], '7': [7], '129': [129], 'none': [None], 'empty': [''], 'space': [' '] }) @testcase.attr('negative') def test_create_order_defaults_invalid_bit_length(self, bit_length): """"""Covers creating orders with various invalid bit lengths."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['bit_length'] = bit_length create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 400) @utils.parameterized_dataset({ 'array': [['array']], 'int': [123], 'oversized_payload': [str(oversized)], 'standard_payload': ['standard payload'], 'empty': [''] }) @testcase.attr('negative') def test_create_order_defaults_invalid_payload(self, payload): """"""Covers creating orders with various invalid payloads."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['payload'] = payload create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 400) @utils.parameterized_dataset({ 'alphanumeric': ['1f34ds'], 'len_255': [len_255_string], 'uuid': ['54262d9d-4bc7-4821-8df0-dc2ca8e112bb'], 'punctuation': ['~!@#$%^&*()_+`-={}[]|:;<>,.?'], 'empty': [""""] }) @testcase.attr('positive') def test_create_order_defaults_valid_name(self, name): """"""Covers creating orders with various valid names."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['name'] = name create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 202) self.assertIsNotNone(order_ref) @utils.parameterized_dataset({ 'int': [123] }) @testcase.attr('negative') def test_create_order_defaults_invalid_name(self, name): """"""Covers creating orders with various invalid names."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['name'] = name create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 400) @utils.parameterized_dataset({ 'cbc': ['cbc'] }) @testcase.attr('positive') def test_create_order_defaults_valid_mode(self, mode): """"""Covers creating orders with various valid modes."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['mode'] = mode create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 202) self.assertIsNotNone(order_ref) @utils.parameterized_dataset({ 'int': [123] }) @testcase.attr('negative') def test_create_order_defaults_invalid_mode(self, mode): """"""Covers creating orders with various invalid modes."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['mode'] = mode create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 400) @utils.parameterized_dataset({ 'aes': ['aes'] }) @testcase.attr('positive') def test_create_order_defaults_valid_algorithm(self, algorithm): """"""Covers creating orders with various valid algorithms."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['algorithm'] = algorithm create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 202) self.assertIsNotNone(order_ref) @utils.parameterized_dataset({ 'int': [123] }) @testcase.attr('negative') def test_create_order_defaults_invalid_algorithm(self, algorithm): """"""Covers creating orders with various invalid algorithms."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['algorithm'] = algorithm create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 400) @utils.parameterized_dataset({ 'empty': [''], 'invalid': ['invalid'], 'text': ['text'], 'text/plain': ['text/plain'], 'text_plain_space_charset_utf8': ['text/plain; charset=utf-8'], 'text_slash_with_no_subtype': ['text/'] }) @testcase.attr('positive') def test_create_order_defaults_valid_payload_content_type(self, pct): """"""Covers order creation with various valid payload content types."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['payload_content_type'] = pct create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 202) self.assertIsNotNone(order_ref) @utils.parameterized_dataset({ 'int': [123], 'oversized_string': [str(oversized)] }) @testcase.attr('negative') def test_create_order_defaults_invalid_payload_content_type(self, pct): """"""Covers order creation with various invalid payload content types."""""" test_model = order_models.OrderModel(**self.default_data) test_model.meta['payload_content_type'] = pct create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 400) @utils.parameterized_dataset({ 'negative_five_long_expire': { 'timezone': '-05:00', 'days': 5}, 'positive_five_long_expire': { 'timezone': '+05:00', 'days': 5}, 'negative_one_short_expire': { 'timezone': '-01', 'days': 1}, 'positive_one_short_expire': { 'timezone': '+01', 'days': 1} }) @testcase.attr('positive') def test_create_order_defaults_valid_expiration(self, **kwargs): """"""Covers creating orders with various valid expiration data."""""" timestamp = utils.create_timestamp_w_tz_and_offset(**kwargs) test_model = order_models.OrderModel(**self.default_data) test_model.meta['expiration'] = timestamp create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 202) self.assertIsNotNone(order_ref) @utils.parameterized_dataset({ 'malformed_timezone': { 'timezone': '-5:00', 'days': 5}, }) @testcase.attr('negative') def test_create_order_defaults_invalid_expiration(self, **kwargs): """"""Covers creating orders with various invalid expiration data."""""" timestamp = utils.create_timestamp_w_tz_and_offset(**kwargs) test_model = order_models.OrderModel(**self.default_data) test_model.meta['expiration'] = timestamp create_resp, order_ref = self.behaviors.create_order(test_model) self.assertEqual(create_resp.status_code, 400)"," # create order with no name test_model = order_models.OrderModel(**order_create_defaults_data) overrides = {""name"": None} test_model.override_values(**overrides) # verify that the order was created successfully # create order with empty name test_model = order_models.OrderModel(**order_create_defaults_data) overrides = {""name"": """"} test_model.override_values(**overrides) # verify that the order was created successfully @testcase.attr('negative') def test_get_order_defaults_that_doesnt_exist(self): """"""Covers case of getting a non-existent order."""""" # try to get a non-existent order order_resp = self.behaviors.get_order(""a ref that does not exist"") # verify that the order get failed self.assertEqual(order_resp.status_code, 404) @testcase.attr('negative') def test_create_order_defaults_w_invalid_content_type(self): """"""Covers creating order with invalid content-type header."""""" # create order with empty name test_model = order_models.OrderModel(**order_create_defaults_data) extra_headers = {""Content-Type"": ""crypto/boom""} create_resp, order_ref = self.behaviors.create_order( test_model, extra_headers=extra_headers) # verify that the order creation failed self.assertEqual(create_resp.status_code, 415) self.assertIsNone(order_ref) @testcase.attr('negative') def test_create_order_nones(self): """"""Covers order creation with empty JSON."""""" # create an order with empty data test_model = order_models.OrderModel(**order_create_nones_data) # verify that the order creation failed self.assertEqual(create_resp.status_code, 400) self.assertIsNone(order_ref) @testcase.attr('negative') def test_create_order_empty_entries(self): """"""Covers order creation with empty JSON."""""" # create an order with empty data test_model = order_models.OrderModel() overrides = {""name"": """", ""algorithm"": """", ""mode"": """", ""bit_length"": """", ""payload_content_type"": """"} test_model.override_values(**overrides) create_resp, order_ref = self.behaviors.create_order(test_model) # verify that the order creation failed self.assertEqual(create_resp.status_code, 400) self.assertIsNone(order_ref) test_model = order_models.OrderModel(**order_create_defaults_data) overrides = {""meta"": {""name"": """", ""algorithm"": ""aes"", ""bit_length"": 256, ""mode"": ""cbc"", ""payload_content_type"": ""text/plain""}} test_model.override_values(**overrides)",374,64
openstack%2Fpython-troveclient~master~Ie62438c60d2087fa067e967a5e2b8ea8ee392e48,openstack/python-troveclient,master,Ie62438c60d2087fa067e967a5e2b8ea8ee392e48,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:51:52.000000000,2014-12-05 22:57:36.000000000,2014-12-05 22:57:35.000000000,"[{'_account_id': 3}, {'_account_id': 1925}, {'_account_id': 5293}]","[{'number': 1, 'created': '2014-12-05 03:51:52.000000000', 'files': ['CONTRIBUTING.rst', 'README.rst'], 'web_link': 'https://opendev.org/openstack/python-troveclient/commit/8d19bdad5bc2de47dc99c807b164cfb31fc66471', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Ie62438c60d2087fa067e967a5e2b8ea8ee392e48\n'}]",0,139385,8d19bdad5bc2de47dc99c807b164cfb31fc66471,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Ie62438c60d2087fa067e967a5e2b8ea8ee392e48
",git fetch https://review.opendev.org/openstack/python-troveclient refs/changes/85/139385/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'README.rst']",2,8d19bdad5bc2de47dc99c807b164cfb31fc66471,infra-manual,.. _Gerrit: http://docs.openstack.org/infra/manual/developers.html#development-workflow,.. _Gerrit: http://wiki.openstack.org/GerritWorkflow,3,3
openstack%2Fneutron-specs~master~Iba0aec1b4bfda16f36f8de08ed93346bcae665f0,openstack/neutron-specs,master,Iba0aec1b4bfda16f36f8de08ed93346bcae665f0,Layer 3 Service Plugin for Cisco Nexus Switches,ABANDONED,2014-12-04 16:31:01.000000000,2014-12-05 22:55:41.000000000,,"[{'_account_id': 3}, {'_account_id': 162}, {'_account_id': 10980}]","[{'number': 1, 'created': '2014-12-04 16:31:01.000000000', 'files': ['specs/kilo/cisco-nexus-l3-service-plugin.rst'], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/42fc02e55a0fef7388fc846e8830f4f4dcdca4ff', 'message': 'Layer 3 Service Plugin for Cisco Nexus Switches\n\nThis spec describes the work being done by the blueprint\nhttps://blueprints.launchpad.net/neutron/+spec/ml2-cisco-nexus-mechdriver-svi\n(NB: blueprint description needs to be updated to cover all L3 features)\nwhich implements Layer 3 features for the Cisco Nexus switches.\n\nChange-Id: Iba0aec1b4bfda16f36f8de08ed93346bcae665f0\n'}]",1,139104,42fc02e55a0fef7388fc846e8830f4f4dcdca4ff,4,3,1,6694,,,0,"Layer 3 Service Plugin for Cisco Nexus Switches

This spec describes the work being done by the blueprint
https://blueprints.launchpad.net/neutron/+spec/ml2-cisco-nexus-mechdriver-svi
(NB: blueprint description needs to be updated to cover all L3 features)
which implements Layer 3 features for the Cisco Nexus switches.

Change-Id: Iba0aec1b4bfda16f36f8de08ed93346bcae665f0
",git fetch https://review.opendev.org/openstack/neutron-specs refs/changes/04/139104/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/kilo/cisco-nexus-l3-service-plugin.rst'],1,42fc02e55a0fef7388fc846e8830f4f4dcdca4ff,bp/https,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode =============================================== Layer 3 Service Plugin for Cisco Nexus Switches =============================================== https://blueprints.launchpad.net/neutron/+spec/ml2-cisco-nexus-mechdriver-svi This blueprint is to implement Layer 3 features for the Cisco 3K, 5K, 7K and 9K Nexus switches. Problem Description =================== The core Cisco plugin supports both L2 and L3 features. This core plugin is being deprecated in favor of the ML2 core plugin however ML2 only supports L2 features. To support L3 features this service plugin is being created. Switched Virtual Interface (SVI) support under the Cisco core plugin will be ported over to this L3 service plugin. Additional L3 features (router, floating IP L3 APIs) will also be supported by this service plugin. Proposed Change =============== This proposal is to introduce a new Layer 3 service plugin that communicates with Cisco Nexus switches. This plugin will support full L3 routing functionality. See [3] for details. This plugin will use the existing ML2 cisco_nexus mechanism driver module, ml2/drivers/cisco/nexus/nexus_network_driver.py, for programming the Nexus switch using netconf and new L3 based XML snippets. The SVI methods (and corresponding XML snippets) will be ported over from the core cisco plugin codebase and added to this module. New methods and XML snippets will be created to support router and floating IP functionality. Although not necessary for this L3 service plugin in run successfully, it is assumed that the ML2 cisco_nexus MD is running (or at least configured) along with this Cisco Nexus service plugin and therefore the correct nexus switch information will be available to this plugin. Data Model Impact ----------------- The existing data model used by the ML2 cisco_nexus MD will also be used to save L3 information. Additional columns will be added to support these features. REST API Impact --------------- None Security Impact --------------- None Notifications Impact -------------------- None Other End User Impact --------------------- None Performance Impact ------------------ The service plugin is triggered instead of polled, there are no changes to any existing code patterns. The potential bottleneck for this plugin would be the link between neutron and the cisco nexus switches (netconf). Other Deployer Impact --------------------- There are no config options specific to the Layer 3 plugin, it relies on the configuration options of the ML2 cisco_nexus mechanism driver. Developer Impact ---------------- None Community Impact ---------------- None Alternatives ------------ In the long term it is hoped there will be agreement to adopt the new flavor framework [4] also for routing and also that the L3 DB code for router ports can be reworked to adopt a new service port construct. With that, the different vendor router plugins can be drivers in a single router service plugin. Implementation ============== Assignee(s) ----------- Arvind Somya <asomya@cisco.com> Rich Curran <rcurran@cisco.com> Work Items ---------- Single work item for the L3 Cisco Nexus service plugin. Dependencies ============ None Testing ======= Complete unit test coverage of the code is included. Tempest Tests ------------- For tempest test coverage, third party testing is provided. The Cisco CI reports on all changes affecting this driver. The testing is run in a setup with an OpenStack deployment (devstack) connected to a Cisco Nexus 3K physical switch. Functional Tests ---------------- None API Tests --------- N/A Documentation Impact ==================== User Documentation ------------------ Deployment documentation on how to configure and deploy this service plugin will be documented in the Openstack wiki. Developer Documentation ----------------------- N/A References ========== [1] ML2 Cisco Nexus WIKI: https://wiki.openstack.org/wiki/Neutron/ML2/MechCiscoNexus [2] Bug review used to commit SVI support for the core Cisco plugin: https://review.openstack.org/30153 [3] http://docs.openstack.org/api/openstack-network/2.0/content/router_ext.html [4] Flavors Framework: https://review.openstack.org/90070 ",,155,0
openstack%2Fnova~stable%2Fjuno~I6d7d7612c67c420079e7dd3feb2dddee21e0428d,openstack/nova,stable/juno,I6d7d7612c67c420079e7dd3feb2dddee21e0428d,we download image to esx rather than vCenter in order to increase speed,ABANDONED,2014-11-17 15:14:12.000000000,2014-12-05 22:48:39.000000000,,"[{'_account_id': 3}, {'_account_id': 979}, {'_account_id': 1955}, {'_account_id': 5170}, {'_account_id': 8871}, {'_account_id': 10118}]","[{'number': 1, 'created': '2014-11-17 15:14:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/28e97bafbf39a2d1fbf7ef7730b45554cc55e75f', 'message': 'we download image to esx rather than vCenter in order to increase speed\n\nChange-Id: I6d7d7612c67c420079e7dd3feb2dddee21e0428d\nCloses-Bug: 1390111\n'}, {'number': 2, 'created': '2014-11-18 15:27:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ce738bacfc1ce7115570d590b7b607e5fdab5baf', 'message': 'we download image to esx rather than vCenter in order to increase speed\n\nChange-Id: I6d7d7612c67c420079e7dd3feb2dddee21e0428d\nCloses-Bug: 1390111\n'}, {'number': 3, 'created': '2014-11-18 23:44:18.000000000', 'files': ['nova/virt/vmwareapi/vmops.py', 'nova/virt/vmwareapi/read_write_util.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/0131c6e3d5ff350428eacf55b68db49d3e3053dc', 'message': 'we download image to esx rather than vCenter in order to increase speed\n\nChange-Id: I6d7d7612c67c420079e7dd3feb2dddee21e0428d\nCloses-Bug: 1390111\n'}]",0,134965,0131c6e3d5ff350428eacf55b68db49d3e3053dc,23,6,3,11672,,,0,"we download image to esx rather than vCenter in order to increase speed

Change-Id: I6d7d7612c67c420079e7dd3feb2dddee21e0428d
Closes-Bug: 1390111
",git fetch https://review.opendev.org/openstack/nova refs/changes/65/134965/3 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/vmwareapi/vmops.py', 'nova/virt/vmwareapi/read_write_util.py']",2,28e97bafbf39a2d1fbf7ef7730b45554cc55e75f,bug/1390111," @staticmethod def get_url(host, data_center_name, datastore_name, file_path, scheme=""https""): if utils.is_valid_ipv6(host): base_url = ""%s://[%s]/folder/%s"" % (scheme, host, file_path) else: base_url = ""%s://%s/folder/%s"" % (scheme, host, file_path) param_list = {""dcPath"": data_center_name, ""dsName"": datastore_name} url = base_url + ""?"" + urllib.urlencode(param_list) return url",,76,1
openstack%2Fneutron~master~Ic0a352cd61131637c471931e3e73995e2143675f,openstack/neutron,master,Ic0a352cd61131637c471931e3e73995e2143675f,Update i18n translation for NEC plugin log msg's,MERGED,2014-12-02 13:16:49.000000000,2014-12-05 22:32:17.000000000,2014-12-05 22:32:15.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 1653}, {'_account_id': 2592}, {'_account_id': 5170}, {'_account_id': 6659}, {'_account_id': 6854}, {'_account_id': 8873}, {'_account_id': 9008}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10980}, {'_account_id': 12040}, {'_account_id': 13051}]","[{'number': 1, 'created': '2014-12-02 13:16:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/eab4cb74559e249c6db29e6fe696a19534531241', 'message': ""Update i18n translation for NEC plugin log msg's\n\nAll the log messages now have the required hints. In addition to\nthis debug messages are not translated. This is done for the\nplugins/nec directory.\n\nChange-Id: Ic0a352cd61131637c471931e3e73995e2143675f\n""}, {'number': 2, 'created': '2014-12-03 20:21:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e7617faab423b8fa557c656dbbdda9f86df6610e', 'message': ""Update i18n translation for NEC plugin log msg's\n\nAll the log messages now have the required hints. In addition to\nthis debug messages are not translated. This is done for the\nplugins/nec directory.\n\nChange-Id: Ic0a352cd61131637c471931e3e73995e2143675f\n""}, {'number': 3, 'created': '2014-12-05 08:00:51.000000000', 'files': ['neutron/plugins/nec/common/ofc_client.py', 'neutron/plugins/nec/agent/nec_neutron_agent.py', 'neutron/plugins/nec/nec_router.py', 'neutron/plugins/nec/router_drivers.py', 'neutron/plugins/nec/packet_filter.py', 'neutron/plugins/nec/nec_plugin.py', 'neutron/plugins/nec/drivers/__init__.py', 'neutron/hacking/checks.py', 'neutron/plugins/nec/db/router.py', 'neutron/plugins/nec/db/api.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/50d702899b1a6ccd3f61dd0298788a71d854967d', 'message': ""Update i18n translation for NEC plugin log msg's\n\nAll the log messages now have the required hints. In addition to\nthis debug messages are not translated. This is done for the\nplugins/nec directory.\n\nChange-Id: Ic0a352cd61131637c471931e3e73995e2143675f\n""}]",4,138358,50d702899b1a6ccd3f61dd0298788a71d854967d,80,25,3,1653,,,0,"Update i18n translation for NEC plugin log msg's

All the log messages now have the required hints. In addition to
this debug messages are not translated. This is done for the
plugins/nec directory.

Change-Id: Ic0a352cd61131637c471931e3e73995e2143675f
",git fetch https://review.opendev.org/openstack/neutron refs/changes/58/138358/3 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/nec/agent/nec_neutron_agent.py', 'neutron/plugins/nec/common/ofc_client.py', 'neutron/plugins/nec/nec_router.py', 'neutron/plugins/nec/router_drivers.py', 'neutron/plugins/nec/packet_filter.py', 'neutron/plugins/nec/drivers/__init__.py', 'neutron/plugins/nec/nec_plugin.py', 'neutron/hacking/checks.py', 'neutron/plugins/nec/db/router.py', 'neutron/plugins/nec/db/api.py']",10,eab4cb74559e249c6db29e6fe696a19534531241,debug-nec,"from neutron.i18n import _LW LOG.warning(_LW(""del_ofc_item(): NotFound item "" LOG.warning(_LW(""del_portinfo(): NotFound portinfo for "" LOG.debug(""get_port_with_securitygroups() called:port_id=%s"", port_id)"," LOG.warning(_(""del_ofc_item(): NotFound item "" LOG.warning(_(""del_portinfo(): NotFound portinfo for "" LOG.debug(_(""get_port_with_securitygroups() called:port_id=%s""), port_id)",126,120
openstack%2Fneutron~stable%2Fjuno~I8dca1fce9fbc83e75ba7e4ce948531427bf7e88b,openstack/neutron,stable/juno,I8dca1fce9fbc83e75ba7e4ce948531427bf7e88b,Fix enable_metadata_network flag,MERGED,2014-12-05 13:07:18.000000000,2014-12-05 22:32:03.000000000,2014-12-05 22:32:02.000000000,"[{'_account_id': 3}, {'_account_id': 2592}, {'_account_id': 4395}, {'_account_id': 5170}, {'_account_id': 8873}, {'_account_id': 9656}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9846}, {'_account_id': 10119}, {'_account_id': 10153}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 14208}]","[{'number': 1, 'created': '2014-12-05 13:07:18.000000000', 'files': ['neutron/agent/dhcp_agent.py', 'neutron/agent/linux/dhcp.py', 'neutron/tests/unit/test_linux_dhcp.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/6068320cbd76a8c70e754acdffb48e8bcf28ff09', 'message': ""Fix enable_metadata_network flag\n\nThe following patch: 9569b2fe broke the desired functionality of\nthe enable_metadata_network flag, by not allowing the metadata\nproxy to be spawn for 'metadata networks', which are used for\naccessing the metadata service when the logical router is not\nimplemented through the l3 agent.\n\nThis patch enables spawning of the metadata proxy for metadata\nnetworks when the appropriate flag is set to True.\n\nThe patch also adds rather pedant unit test coverage for the\nshould_enable_metadata method which previously had no unit test.\n\nChange-Id: I8dca1fce9fbc83e75ba7e4ce948531427bf7e88b\nCloses-bug: 1394020\n(cherry picked from commit c45842af38da322b93d1200451a4a254abfcaed1)\n""}]",0,139617,6068320cbd76a8c70e754acdffb48e8bcf28ff09,19,15,1,8655,,,0,"Fix enable_metadata_network flag

The following patch: 9569b2fe broke the desired functionality of
the enable_metadata_network flag, by not allowing the metadata
proxy to be spawn for 'metadata networks', which are used for
accessing the metadata service when the logical router is not
implemented through the l3 agent.

This patch enables spawning of the metadata proxy for metadata
networks when the appropriate flag is set to True.

The patch also adds rather pedant unit test coverage for the
should_enable_metadata method which previously had no unit test.

Change-Id: I8dca1fce9fbc83e75ba7e4ce948531427bf7e88b
Closes-bug: 1394020
(cherry picked from commit c45842af38da322b93d1200451a4a254abfcaed1)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/17/139617/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/agent/dhcp_agent.py', 'neutron/agent/linux/dhcp.py', 'neutron/tests/unit/test_linux_dhcp.py']",3,6068320cbd76a8c70e754acdffb48e8bcf28ff09,bug/1394020," def __init__(self, dev_owner=constants.DEVICE_OWNER_ROUTER_INTF, ip_address='192.168.0.1'): self.fixed_ips = [FakeIPAllocation( ip_address, 'dddddddd-dddd-dddd-dddd-dddddddddddd')]class FakeV4MetadataSubnet: id = 'dddddddd-dddd-dddd-dddd-dddddddddddd' ip_version = 4 cidr = '169.254.169.254/30' gateway_ip = '169.254.169.253' enable_dhcp = True host_routes = [] dns_nameservers = [] class FakeV4MetadataNetwork: id = 'cccccccc-cccc-cccc-cccc-cccccccccccc' subnets = [FakeV4MetadataSubnet()] ports = [FakeRouterPort(ip_address='169.254.169.253')] config.register_use_namespaces_opts_helper(self.conf) self.conf.register_opt(cfg.BoolOpt('enable_metadata_network', default=False)) def test_should_enable_metadata_namespaces_disabled_returns_false(self): self.conf.set_override('use_namespaces', False) self.assertFalse(dhcp.Dnsmasq.should_enable_metadata(self.conf, mock.ANY)) def test_should_enable_metadata_isolated_network_returns_true(self): self.assertTrue(dhcp.Dnsmasq.should_enable_metadata( self.conf, FakeV4NetworkNoRouter())) def test_should_enable_metadata_non_isolated_network_returns_false(self): self.assertFalse(dhcp.Dnsmasq.should_enable_metadata( self.conf, FakeV4NetworkDistRouter())) def test_should_enable_metadata_isolated_meta_disabled_returns_false(self): self.conf.set_override('enable_isolated_metadata', False) self.assertFalse(dhcp.Dnsmasq.should_enable_metadata(self.conf, mock.ANY)) def test_should_enable_metadata_with_metadata_network_returns_true(self): self.conf.set_override('enable_metadata_network', True) self.assertTrue(dhcp.Dnsmasq.should_enable_metadata( self.conf, FakeV4MetadataNetwork()))"," fixed_ips = [FakeIPAllocation('192.168.0.1', 'dddddddd-dddd-dddd-dddd-dddddddddddd')] def __init__(self, dev_owner=constants.DEVICE_OWNER_ROUTER_INTF): self.conf.use_namespaces = True",67,10
openstack%2Fneutron~stable%2Fjuno~I3ff3bb85105b8215b36535983016d8c0ff3d8cb7,openstack/neutron,stable/juno,I3ff3bb85105b8215b36535983016d8c0ff3d8cb7,Teach DHCP Agent about DVR router interfaces,MERGED,2014-12-05 13:04:48.000000000,2014-12-05 22:31:48.000000000,2014-12-05 22:31:47.000000000,"[{'_account_id': 3}, {'_account_id': 1131}, {'_account_id': 2592}, {'_account_id': 5170}, {'_account_id': 8655}, {'_account_id': 8873}, {'_account_id': 9656}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9846}, {'_account_id': 10119}, {'_account_id': 10153}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 14208}]","[{'number': 1, 'created': '2014-12-05 13:04:48.000000000', 'files': ['neutron/agent/linux/dhcp.py', 'neutron/tests/unit/test_linux_dhcp.py', 'neutron/tests/unit/test_dhcp_agent.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/c96217a8b3db8d0e2d2621e0c9e61e42a7997029', 'message': ""Teach DHCP Agent about DVR router interfaces\n\nWhen DVR is enabled and enable_isolated_metadata=True,\nthe DHCP agent should only inject a metadata host route\nwhen there is no port with the gateway IP address configured\non the subnet.  Add a check for DEVICE_OWNER_DVR_INTERFACE\nwhen we look at each port's device_owner field, otherwise\nit will always add this route to the opts file when DVR\nis enabled.\n\nChange-Id: I3ff3bb85105b8215b36535983016d8c0ff3d8cb7\nCloses-bug: #1377307\n(cherry picked from commit 29250949012e9c0a60b0ddb56ddbf18d7b68106b)\n""}]",0,139616,c96217a8b3db8d0e2d2621e0c9e61e42a7997029,20,16,1,8655,,,0,"Teach DHCP Agent about DVR router interfaces

When DVR is enabled and enable_isolated_metadata=True,
the DHCP agent should only inject a metadata host route
when there is no port with the gateway IP address configured
on the subnet.  Add a check for DEVICE_OWNER_DVR_INTERFACE
when we look at each port's device_owner field, otherwise
it will always add this route to the opts file when DVR
is enabled.

Change-Id: I3ff3bb85105b8215b36535983016d8c0ff3d8cb7
Closes-bug: #1377307
(cherry picked from commit 29250949012e9c0a60b0ddb56ddbf18d7b68106b)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/16/139616/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/agent/linux/dhcp.py', 'neutron/tests/unit/test_linux_dhcp.py', 'neutron/tests/unit/test_dhcp_agent.py']",3,c96217a8b3db8d0e2d2621e0c9e61e42a7997029,bug/1377307,"fake_fixed_ip2 = dhcp.DictModel(dict(id='', subnet_id=fake_subnet1.id, ip_address='172.9.9.10')) fixed_ips=[fake_fixed_ip2]))fake_dist_port = dhcp.DictModel(dict(id='12345678-1234-aaaa-1234567890ab', mac_address='aa:bb:cc:dd:ee:ff', network_id='12345678-1234-5678-1234567890ab', device_owner=const.DEVICE_OWNER_DVR_INTERFACE, device_id='forzanapoli', fixed_ips=[fake_meta_fixed_ip])) nonisolated_dist_network = dhcp.NetModel( True, dict( id='12345678-1234-5678-1234567890ab', tenant_id='aaaaaaaa-aaaa-aaaa-aaaaaaaaaaaa', admin_state_up=True, subnets=[fake_subnet1], ports=[fake_port1, fake_port2])) fake_dist_network = dhcp.NetModel( True, dict(id='12345678-1234-5678-1234567890ab', tenant_id='aaaaaaaa-aaaa-aaaa-aaaaaaaaaaaa', admin_state_up=True, subnets=[fake_meta_subnet], ports=[fake_meta_port, fake_dist_port])) nonisolated_network.ports[0].device_owner = ( const.DEVICE_OWNER_ROUTER_INTF) def test_enable_dhcp_helper_enable_metadata_nonisolated_dist_network(self): nonisolated_dist_network.ports[0].device_owner = ( const.DEVICE_OWNER_ROUTER_INTF) nonisolated_dist_network.ports[0].fixed_ips[0].ip_address = '172.9.9.1' nonisolated_dist_network.ports[1].device_owner = ( const.DEVICE_OWNER_DVR_INTERFACE) nonisolated_dist_network.ports[1].fixed_ips[0].ip_address = '172.9.9.1' self._enable_dhcp_helper(nonisolated_dist_network, enable_isolated_metadata=True, is_isolated_network=False) def _test_metadata_network(self, network): self.dhcp.enable_isolated_metadata_proxy(network) network.id)], addl_env=None) def test_enable_isolated_metadata_proxy_with_metadata_network(self): self._test_metadata_network(fake_meta_network) def test_enable_isolated_metadata_proxy_with_dist_network(self): self._test_metadata_network(fake_dist_network) "," fixed_ips=[])) nonisolated_network.ports[0].device_owner = ""network:router_interface"" def test_enable_isolated_metadata_proxy_with_metadata_network(self): self.dhcp.enable_isolated_metadata_proxy(fake_meta_network) fake_meta_network.id)], addl_env=None)",79,7
openstack%2Fproject-config~master~Idd3cf25612aaed4f1c28bfaba94bba413827b378,openstack/project-config,master,Idd3cf25612aaed4f1c28bfaba94bba413827b378,Make agent_ssh check job voting for IPA,MERGED,2014-11-14 03:48:45.000000000,2014-12-05 22:29:13.000000000,2014-12-05 22:29:13.000000000,"[{'_account_id': 3}, {'_account_id': 1420}, {'_account_id': 2889}, {'_account_id': 5263}, {'_account_id': 6316}, {'_account_id': 6547}, {'_account_id': 6554}, {'_account_id': 6773}, {'_account_id': 8106}, {'_account_id': 10342}, {'_account_id': 10343}]","[{'number': 1, 'created': '2014-11-14 03:48:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/5c0078eb423b4dde402e7d18ace0418395247c04', 'message': 'Make agent_ssh check job voting for Ironic\n\nThis job recently started passing reliably and should be reliable enough\nto vote in the check pipeline.\n\nChange-Id: Idd3cf25612aaed4f1c28bfaba94bba413827b378\n'}, {'number': 2, 'created': '2014-11-15 00:50:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/e2af1e46d943a64c75bf31c43259785564dc1a38', 'message': 'Make agent_ssh check job voting for Ironic & IPA\n\nThis job recently started passing reliably and should be reliable enough\nto vote in the check pipeline.\n\nChange-Id: Idd3cf25612aaed4f1c28bfaba94bba413827b378\n'}, {'number': 3, 'created': '2014-11-15 00:53:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/85c5c3e7e7360085d3017635293f34c97498c088', 'message': 'Make agent_ssh check job voting for Ironic & IPA\n\nThis job recently started passing reliably and should be reliable enough\nto vote in the check pipeline.\n\nChange-Id: Idd3cf25612aaed4f1c28bfaba94bba413827b378\n'}, {'number': 4, 'created': '2014-11-19 00:12:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/7d5ebc9dfabc5f22c901bdd43cecfc5e2fc56568', 'message': ""Make agent_ssh check job voting for Ironic & IPA\n\nThe agent_ssh job has been passing for weeks and should be reliable\nenough to vote. The agent_ssh-src job just recently began passing, but\nwe'd like IPA to vote on it immediately to prevent it breaking.\n\nI've also removed the buildimage-coreos jobs from the check/gate for\nIPA, as the agent_ssh-src job will not pass unless it is able to\nsuccessfully build a working IPA image. However, it remains in post so\nit can continue publishing to tarballs.o.o.\n\nChange-Id: Idd3cf25612aaed4f1c28bfaba94bba413827b378\n""}, {'number': 5, 'created': '2014-11-19 00:30:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/9533099ef95656f5446fa32c055b59a19cf81e25', 'message': ""Make agent_ssh check job voting for Ironic & IPA\n\nThe agent_ssh job has been passing for weeks and should be reliable\nenough to vote. The agent_ssh-src job just recently began passing, but\nwe'd like IPA to vote on it immediately to prevent it breaking.\n\nI've also removed the buildimage-coreos jobs from the check/gate for\nIPA, as the agent_ssh-src job will not pass unless it is able to\nsuccessfully build a working IPA image. However, it remains in post so\nit can continue publishing to tarballs.o.o.\n\nChange-Id: Idd3cf25612aaed4f1c28bfaba94bba413827b378\n""}, {'number': 6, 'created': '2014-11-26 01:37:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/c347e5bdeddf7e21c5e4fefdf57ca83f8fb9bb9f', 'message': ""Make agent_ssh check job voting for IPA\n\nThe agent_ssh-src job has been passing for a week and should be reliable\nenough to vote. We'd like this job to vote on IPA changes to prevent\nbreaking changes from merging.\n\nI've also removed the buildimage-coreos jobs from the check/gate for\nIPA, as the agent_ssh-src job will not pass unless it is able to\nsuccessfully build a working IPA image. However, it remains in post so\nit can continue publishing to tarballs.o.o.\n\nAs a note; this does mean that Ironic can break IPA's gate (because\nIronic isn't voting on agent tempest jobs), but this is a small risk as\nthe core teams between the projects overlap significantly.\n\nChange-Id: Idd3cf25612aaed4f1c28bfaba94bba413827b378\n""}, {'number': 7, 'created': '2014-11-26 16:34:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/adf07a990e98aef10235d192f9e273d67edb20e1', 'message': ""Make agent_ssh check job voting for IPA\n\nThe agent_ssh-src job has been passing for a week and should be reliable\nenough to vote. We'd like this job to vote on IPA changes to prevent\nbreaking changes from merging.\n\nI've also removed the buildimage-coreos jobs from the check/gate for\nIPA, as the agent_ssh-src job will not pass unless it is able to\nsuccessfully build a working IPA image. However, it remains in post so\nit can continue publishing to tarballs.o.o.\n\nAs a note; this does mean that Ironic can break IPA's gate (because\nIronic isn't voting on agent tempest jobs), but this is a small risk as\nthe core teams between the projects overlap significantly.\n\nChange-Id: Idd3cf25612aaed4f1c28bfaba94bba413827b378\n""}, {'number': 8, 'created': '2014-11-26 16:35:33.000000000', 'files': ['jenkins/jobs/projects.yaml', 'zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/84173e06f1565ee3ecba4d97bea9a4f2d40658af', 'message': ""Make agent_ssh check job voting for IPA\n\nThe agent_ssh-src job has been passing for a week and should be reliable\nenough to vote. We'd like this job to vote on IPA changes to prevent\nbreaking changes from merging.\n\nI've also removed the buildimage-coreos jobs from the check/gate for\nIPA, as the agent_ssh-src job will not pass unless it is able to\nsuccessfully build a working IPA image. However, it remains in post so\nit can continue publishing to tarballs.o.o.\n\nAs a note; this does mean that Ironic can break IPA's gate (because\nIronic isn't voting on agent tempest jobs), but this is a small risk as\nthe core teams between the projects overlap significantly.\n\nChange-Id: Idd3cf25612aaed4f1c28bfaba94bba413827b378\n""}]",10,134436,84173e06f1565ee3ecba4d97bea9a4f2d40658af,42,11,8,10342,,,0,"Make agent_ssh check job voting for IPA

The agent_ssh-src job has been passing for a week and should be reliable
enough to vote. We'd like this job to vote on IPA changes to prevent
breaking changes from merging.

I've also removed the buildimage-coreos jobs from the check/gate for
IPA, as the agent_ssh-src job will not pass unless it is able to
successfully build a working IPA image. However, it remains in post so
it can continue publishing to tarballs.o.o.

As a note; this does mean that Ironic can break IPA's gate (because
Ironic isn't voting on agent tempest jobs), but this is a small risk as
the core teams between the projects overlap significantly.

Change-Id: Idd3cf25612aaed4f1c28bfaba94bba413827b378
",git fetch https://review.opendev.org/openstack/project-config refs/changes/36/134436/4 && git format-patch -1 --stdout FETCH_HEAD,['jenkins/jobs/projects.yaml'],1,5c0078eb423b4dde402e7d18ace0418395247c04,ironic, branch-designator: '', branch-designator: '-nv',1,1
openstack%2Fneutron~master~I77aa4152a903a0f1f47de20bb1a006d29f5a1bf2,openstack/neutron,master,I77aa4152a903a0f1f47de20bb1a006d29f5a1bf2,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:42:22.000000000,2014-12-05 22:26:46.000000000,2014-12-05 22:26:44.000000000,"[{'_account_id': 3}, {'_account_id': 748}, {'_account_id': 2592}, {'_account_id': 5170}, {'_account_id': 6854}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 12040}, {'_account_id': 13051}]","[{'number': 1, 'created': '2014-12-05 03:42:22.000000000', 'files': ['CONTRIBUTING.rst', 'doc/source/devref/development.environment.rst', 'TESTING.rst'], 'web_link': 'https://opendev.org/openstack/neutron/commit/25ef9258a85dfc0899343d80da8078601c4d51d3', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I77aa4152a903a0f1f47de20bb1a006d29f5a1bf2\n'}]",1,139338,25ef9258a85dfc0899343d80da8078601c4d51d3,26,20,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I77aa4152a903a0f1f47de20bb1a006d29f5a1bf2
",git fetch https://review.opendev.org/openstack/neutron refs/changes/38/139338/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'doc/source/devref/development.environment.rst', 'TESTING.rst']",3,25ef9258a85dfc0899343d80da8078601c4d51d3,infra-manual,"Testing OpenStack projects, including Neutron, is made easier with `DevStack <https://git.openstack.org/cgit/openstack-dev/devstack>`_.","Testing OpenStack projects, including Neutron, is made easier with `DevStack <https://github.com/openstack-dev/devstack>`_.",7,7
openstack%2Fmonasca-ui~master~I5d04654d01415c6cd9862aee1ab104ee55541643,openstack/monasca-ui,master,I5d04654d01415c6cd9862aee1ab104ee55541643,Adding option to select WEBHOOK notification,MERGED,2014-11-24 17:43:44.000000000,2014-12-05 22:25:21.000000000,2014-12-05 22:25:20.000000000,"[{'_account_id': 3}, {'_account_id': 2419}, {'_account_id': 10046}, {'_account_id': 11809}]","[{'number': 1, 'created': '2014-11-24 17:43:44.000000000', 'files': ['monitoring/notifications/forms.py', 'monitoring/notifications/constants.py'], 'web_link': 'https://opendev.org/openstack/monasca-ui/commit/c6f0556c86e9955b961e196e8a643f27e8dcfe8a', 'message': 'Adding option to select WEBHOOK notification\n\nChange-Id: I5d04654d01415c6cd9862aee1ab104ee55541643\n'}]",0,136844,c6f0556c86e9955b961e196e8a643f27e8dcfe8a,23,4,1,10046,,,0,"Adding option to select WEBHOOK notification

Change-Id: I5d04654d01415c6cd9862aee1ab104ee55541643
",git fetch https://review.opendev.org/openstack/monasca-ui refs/changes/44/136844/1 && git format-patch -1 --stdout FETCH_HEAD,"['monitoring/notifications/forms.py', 'monitoring/notifications/constants.py']",2,c6f0556c86e9955b961e196e8a643f27e8dcfe8a,," WEBHOOK = ""WEBHOOK"" CHOICES = [(EMAIL, _(""Email"")), (WEBHOOK, _(""Webhook"")),]WEBHOOK_VALIDATOR = validators.URLValidator( message=_(""Address must contain a valid URL address.""))"," CHOICES = [(EMAIL, _(""Email"")),]",6,1
openstack%2Fneutron~stable%2Fjuno~I71039d213a8946b503e9dbf41ebf551dde023375,openstack/neutron,stable/juno,I71039d213a8946b503e9dbf41ebf551dde023375,Cisco N1kv:Allow n/w delete if subnets are around,ABANDONED,2014-11-24 19:37:29.000000000,2014-12-05 22:24:28.000000000,,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 490}, {'_account_id': 1955}, {'_account_id': 5170}, {'_account_id': 7018}, {'_account_id': 7787}, {'_account_id': 8940}, {'_account_id': 9656}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 10119}, {'_account_id': 10153}, {'_account_id': 11757}, {'_account_id': 12040}, {'_account_id': 12749}, {'_account_id': 13660}, {'_account_id': 14101}]","[{'number': 1, 'created': '2014-11-24 19:37:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/30fa4e4c1304183314fd477cbe4eac5af3cc6903', 'message': 'Allow network delete even if subnets are around\n\nPermit networks with underlying subnets to be deleted by cleaning up\nthose subnets. Similarly, cleanup all the DHCP ports for the subnets\nbefore removing them.\n\nChange-Id: I71039d213a8946b503e9dbf41ebf551dde023375\nCloses-Bug: 1379583\n'}, {'number': 2, 'created': '2014-11-25 00:09:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8c579c27977f14b45c1987f3d4521769aeaebe89', 'message': 'Allow network delete if subnets are around in N1kv\n\nPermit networks with underlying subnets to be deleted in N1kv, by\ncleaning up those subnets. Similarly, cleanup all the DHCP ports for the\nsubnets before removing them.\n\nChange-Id: I71039d213a8946b503e9dbf41ebf551dde023375\nCloses-Bug: 1379583\n'}, {'number': 3, 'created': '2014-11-25 00:13:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/d474584bf2c186597ed18787a7a6e9d341b21c09', 'message': 'Allow network delete if subnets are around in N1kv\n\nPermit networks with underlying subnets to be deleted in N1kv, by\ncleaning up those subnets. Similarly, cleanup all the DHCP ports for the\nsubnets before removing them.\n\nChange-Id: I71039d213a8946b503e9dbf41ebf551dde023375\nCloses-Bug: 1379583\n'}, {'number': 4, 'created': '2014-11-25 01:40:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9083b57d19c105caa5e75a8444b65a96ef77fc42', 'message': 'Allow network delete if subnets are around in N1kv\n\nPermit networks with underlying subnets to be deleted in N1kv, by\ncleaning up those subnets. Similarly, cleanup all the DHCP ports for the\nsubnets before removing them.\n\nChange-Id: I71039d213a8946b503e9dbf41ebf551dde023375\nCloses-Bug: 1379583\n'}, {'number': 5, 'created': '2014-11-25 22:12:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/50c4c9470bd84c72d6cb0e7533ac22769faf7f9f', 'message': 'Cisco N1kv:Allow n/w delete if subnets are around\n\nPermit networks with underlying subnets to be deleted in Cisco N1kv\nplugin, by cleaning up those subnets. Similarly, cleanup all the\nDHCP ports for the subnets before removing them.\n\nChange-Id: I71039d213a8946b503e9dbf41ebf551dde023375\nCloses-Bug: 1379583\n'}, {'number': 6, 'created': '2014-11-25 22:15:36.000000000', 'files': ['neutron/tests/unit/cisco/n1kv/test_n1kv_plugin.py', 'neutron/plugins/cisco/n1kv/n1kv_neutron_plugin.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/f4db5be61a70d8d2ed7f3f56fe58500678fb8613', 'message': 'Cisco N1kv:Allow n/w delete if subnets are around\n\nPermit networks with underlying subnets to be deleted in Cisco N1kv\nplugin, by cleaning up those subnets. Similarly, cleanup all the\nDHCP ports for the subnets before removing them.\n\nChange-Id: I71039d213a8946b503e9dbf41ebf551dde023375\nCloses-Bug: 1379583\n'}]",28,136873,f4db5be61a70d8d2ed7f3f56fe58500678fb8613,96,19,6,12749,,,0,"Cisco N1kv:Allow n/w delete if subnets are around

Permit networks with underlying subnets to be deleted in Cisco N1kv
plugin, by cleaning up those subnets. Similarly, cleanup all the
DHCP ports for the subnets before removing them.

Change-Id: I71039d213a8946b503e9dbf41ebf551dde023375
Closes-Bug: 1379583
",git fetch https://review.opendev.org/openstack/neutron refs/changes/73/136873/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/cisco/n1kv/test_n1kv_plugin.py', 'neutron/plugins/cisco/n1kv/n1kv_neutron_plugin.py']",2,30fa4e4c1304183314fd477cbe4eac5af3cc6903,bug/1379583," for subnet in network['subnets']: self.delete_subnet(context, subnet) msg = _('Removed subnets for network %s') % network['id'] LOG.debug(msg) port_list = self.get_ports(context, {'network_id': [subnet['network_id']]}) for port in port_list: if port['device_owner'] in [constants.DEVICE_OWNER_DHCP]: self.delete_port(context, port['id'])"," if network['subnets']: msg = _(""Cannot delete network '%s', "" ""delete the associated subnet first"") % network['name'] raise n_exc.InvalidInput(error_message=msg)",10,5
openstack%2Fmonasca-agent~master~I920ddbaa9a3433264086eb3b4082ad07187ca386,openstack/monasca-agent,master,I920ddbaa9a3433264086eb3b4082ad07187ca386,Correct number of return values from _load_conf,MERGED,2014-12-04 21:11:12.000000000,2014-12-05 22:15:05.000000000,2014-12-05 22:15:05.000000000,"[{'_account_id': 3}, {'_account_id': 236}, {'_account_id': 688}, {'_account_id': 2419}, {'_account_id': 2860}, {'_account_id': 5387}, {'_account_id': 6230}, {'_account_id': 6460}, {'_account_id': 8126}, {'_account_id': 11094}, {'_account_id': 11155}, {'_account_id': 11809}, {'_account_id': 12108}, {'_account_id': 12133}, {'_account_id': 12443}, {'_account_id': 12512}]","[{'number': 1, 'created': '2014-12-04 21:11:12.000000000', 'files': ['monagent/collector/checks_d/http_check.py'], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/de293bfb18086b2d2b740b7979d9636b0e0fd45b', 'message': 'Correct number of return values from _load_conf\n\nChange-Id: I920ddbaa9a3433264086eb3b4082ad07187ca386\n'}]",0,139198,de293bfb18086b2d2b740b7979d9636b0e0fd45b,26,16,1,8126,,,0,"Correct number of return values from _load_conf

Change-Id: I920ddbaa9a3433264086eb3b4082ad07187ca386
",git fetch https://review.opendev.org/openstack/monasca-agent refs/changes/98/139198/1 && git format-patch -1 --stdout FETCH_HEAD,['monagent/collector/checks_d/http_check.py'],1,de293bfb18086b2d2b740b7979d9636b0e0fd45b,bug/fix_http_check," addr, username, password, timeout, headers, response_time, dimensions, disable_ssl_validation, pattern, use_keystone = self._load_conf("," addr, username, password, timeout, headers, response_time, dimensions, disable_ssl_validation, pattern, use_keystone, token = self._load_conf(",1,1
openstack%2Fopenstack-ansible~master~I2b5c5e692d3d72b603fdd6298475cb76c52c66df,openstack/openstack-ansible,master,I2b5c5e692d3d72b603fdd6298475cb76c52c66df,Enlarge Cinder-Volume container,MERGED,2014-12-04 22:50:33.000000000,2014-12-05 22:07:46.000000000,2014-12-05 22:07:46.000000000,"[{'_account_id': 3}, {'_account_id': 7217}, {'_account_id': 7307}, {'_account_id': 7353}, {'_account_id': 7414}, {'_account_id': 9884}]","[{'number': 1, 'created': '2014-12-04 22:50:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/e66570c4bfe20757379bfcf63817a530f2b8e838', 'message': ""Enlarge Cinder-Volume container\n\nCinder volume needs temporary space to convert images. This patch\nenlarges the default size of the cinder volume container to 105GB. It\nalso exposes the size in the user configuration. This way a user can\nincrease the size if they anticipate converting larger images, or reduce\nit if they don't require image conversion.\n\nChange-Id: I2b5c5e692d3d72b603fdd6298475cb76c52c66df\nRelated: #166\n""}, {'number': 2, 'created': '2014-12-05 10:09:05.000000000', 'files': ['etc/rpc_deploy/rpc_user_config.yml', 'rpc_deployment/inventory/group_vars/cinder_volume.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/a769413d895cd48cec6d8bc0de4cf601e4aabd0c', 'message': ""Enlarge Cinder-Volume container\n\nCinder requires temporary working space to convert images. This patch\nexposes cinder_volume_lv_size_gb to the user config file, so the user\ncan decide how large the cinder volumes container should be based on\navailable space and the size of images that will need to be converted.\n\ncinder_volume_lv_size_gb is used to override container_lvm_fssize in\ngroup_vars/cinder_volume. Simple enough but doesn't work because\ntemplated variables (or indirect variables) are not expanded when\naccessed via hostvars[] see: ansible/ansible#7844. In order to work\naround that, I have eliminated hostvars[] usage from the container\ncreation mechanism. This may have positive speed implications as the\nlimit of container creation parallelism is now forks rather than number\nof hosts. However it does make this change larger than a small bug fix.\n\nAlso note that this patch makes use of delegate_to, so specific ansible\nversions must be used to avoid ansible/ansible#8705. Our requirements\nfile currently specifies a version before this bug was introduced.\n\nThere are two commits in this PR as one is the actual bugfix, the other\nis infrastructure changes required for that bugfix to work. Also only\nthe bugfix may be needed if the upstream bugs are fixed.\n\nCloses-Bug: #1399427\nChange-Id: I2b5c5e692d3d72b603fdd6298475cb76c52c66df\n""}]",0,139241,a769413d895cd48cec6d8bc0de4cf601e4aabd0c,13,6,2,4,,,0,"Enlarge Cinder-Volume container

Cinder requires temporary working space to convert images. This patch
exposes cinder_volume_lv_size_gb to the user config file, so the user
can decide how large the cinder volumes container should be based on
available space and the size of images that will need to be converted.

cinder_volume_lv_size_gb is used to override container_lvm_fssize in
group_vars/cinder_volume. Simple enough but doesn't work because
templated variables (or indirect variables) are not expanded when
accessed via hostvars[] see: ansible/ansible#7844. In order to work
around that, I have eliminated hostvars[] usage from the container
creation mechanism. This may have positive speed implications as the
limit of container creation parallelism is now forks rather than number
of hosts. However it does make this change larger than a small bug fix.

Also note that this patch makes use of delegate_to, so specific ansible
versions must be used to avoid ansible/ansible#8705. Our requirements
file currently specifies a version before this bug was introduced.

There are two commits in this PR as one is the actual bugfix, the other
is infrastructure changes required for that bugfix to work. Also only
the bugfix may be needed if the upstream bugs are fixed.

Closes-Bug: #1399427
Change-Id: I2b5c5e692d3d72b603fdd6298475cb76c52c66df
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/41/139241/1 && git format-patch -1 --stdout FETCH_HEAD,"['etc/rpc_deploy/rpc_user_config.yml', 'rpc_deployment/inventory/group_vars/cinder_volume.yml']",2,e66570c4bfe20757379bfcf63817a530f2b8e838,139241,"--- # Copyright 2014, Rackspace US, Inc. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # The variables file used by the playbooks in the Cinder-api group. # These don't have to be explicitly imported by vars_files: they are autopopulated. # Note, most cinder settings sare set in cinder_all, # this file is just to override the lvm size for the volumes container. # The volumes container needs a larger FS as it must have tmp space for # converting glnace imges to volumes. # https://github.com/rcbops/ansible-lxc-rpc/issues/166 # Default is 5GB (same as other containers). # Space must be added for cinder image conversion to work. # For example, to be able to convert 100GB images, set this to 105GB. cinder_volume_lv_size_gb: 5GB # only used when the lxc vg is present on the target container_lvm_fssize: ""{{cinder_volume_lv_size_gb}}"" ",,39,0
openstack%2Fdevstack~master~If863696efc7ef50ac62f002db6800aaab5f8d0b1,openstack/devstack,master,If863696efc7ef50ac62f002db6800aaab5f8d0b1,Stop installing python from distos,ABANDONED,2014-11-15 20:16:49.000000000,2014-12-05 21:33:16.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 6854}, {'_account_id': 7118}, {'_account_id': 9009}, {'_account_id': 10385}, {'_account_id': 11564}, {'_account_id': 13409}]","[{'number': 1, 'created': '2014-11-15 20:16:49.000000000', 'files': ['files/rpms-suse/n-spice', 'files/rpms-suse/general', 'files/rpms/n-cpu', 'files/rpms/general', 'files/rpms-suse/swift', 'files/rpms/horizon', 'files/rpms-suse/ryu', 'files/rpms/n-api', 'files/rpms/swift', 'files/rpms-suse/horizon', 'files/rpms-suse/ceilometer-collector', 'files/rpms-suse/keystone', 'files/rpms-suse/glance', 'files/rpms-suse/neutron', 'files/apts/horizon', 'files/rpms/qpid', 'files/rpms/keystone', 'files/rpms/ryu', 'files/apts/n-cpu', 'files/rpms/cinder', 'lib/ceilometer', 'files/rpms-suse/ldap', 'files/apts/ryu', 'files/rpms-suse/nova', 'files/apts/n-api', 'files/rpms/ceilometer-collector', 'files/apts/neutron', 'files/apts/nova', 'files/apts/ldap', 'files/apts/postgresql', 'files/apts/general', 'files/rpms/ldap', 'files/apts/zaqar-server', 'files/rpms/nova', 'files/rpms/glance', 'files/rpms/neutron', 'files/rpms/zaqar-server', 'files/apts/ironic', 'files/apts/n-novnc', 'files/rpms-suse/n-api', 'files/apts/swift', 'files/rpms/ironic', 'files/rpms-suse/postgresql', 'files/rpms/postgresql', 'files/apts/keystone', 'files/rpms-suse/cinder', 'files/apts/ceilometer-collector', 'files/apts/glance', 'files/rpms-suse/n-novnc'], 'web_link': 'https://opendev.org/openstack/devstack/commit/024a52ca0a69ff0e0e34d09813f79ecee5700105', 'message': 'Stop installing python from distos\n\nBasically, installing some python from distros and some python from\npip is not considered a good idea by any humans on the planet. Stop\ndoing it.\n\nChange-Id: If863696efc7ef50ac62f002db6800aaab5f8d0b1\n'}]",4,134732,024a52ca0a69ff0e0e34d09813f79ecee5700105,11,8,1,2,,,0,"Stop installing python from distos

Basically, installing some python from distros and some python from
pip is not considered a good idea by any humans on the planet. Stop
doing it.

Change-Id: If863696efc7ef50ac62f002db6800aaab5f8d0b1
",git fetch https://review.opendev.org/openstack/devstack refs/changes/32/134732/1 && git format-patch -1 --stdout FETCH_HEAD,"['files/rpms-suse/n-spice', 'files/rpms-suse/general', 'files/rpms/n-cpu', 'files/rpms/general', 'files/rpms-suse/swift', 'files/rpms/horizon', 'files/rpms-suse/ryu', 'files/rpms/n-api', 'files/rpms/swift', 'files/rpms-suse/horizon', 'files/rpms-suse/ceilometer-collector', 'files/rpms-suse/keystone', 'files/rpms-suse/glance', 'files/rpms-suse/neutron', 'files/apts/horizon', 'files/rpms/qpid', 'files/rpms/keystone', 'files/rpms/ryu', 'files/apts/n-cpu', 'files/rpms/cinder', 'lib/ceilometer', 'files/rpms-suse/ldap', 'files/apts/ryu', 'files/rpms-suse/nova', 'files/apts/n-api', 'files/rpms/ceilometer-collector', 'files/apts/neutron', 'files/apts/nova', 'files/apts/ldap', 'files/apts/postgresql', 'files/apts/general', 'files/rpms/ldap', 'files/apts/zaqar-server', 'files/rpms/nova', 'files/rpms/glance', 'files/rpms/neutron', 'files/rpms/zaqar-server', 'files/apts/ironic', 'files/apts/n-novnc', 'files/rpms-suse/n-api', 'files/apts/swift', 'files/rpms/ironic', 'files/rpms-suse/postgresql', 'files/rpms/postgresql', 'files/apts/keystone', 'files/rpms-suse/cinder', 'files/apts/ceilometer-collector', 'files/apts/glance', 'files/rpms-suse/n-novnc']",49,024a52ca0a69ff0e0e34d09813f79ecee5700105,bug/1393006,,python-numpy ,6,291
openstack%2Fdevstack~master~I0b07f69778a67409b8e3397710edb8231f32bca3,openstack/devstack,master,I0b07f69778a67409b8e3397710edb8231f32bca3,WIP: test breaking unit tests,ABANDONED,2014-11-20 02:07:27.000000000,2014-12-05 21:31:55.000000000,,"[{'_account_id': 3}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-20 02:07:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/52bb6a46a43ee17bef3b3358352b44eabb656e87', 'message': 'WIP: test breaking unit tests\n\nChange-Id: I0b07f69778a67409b8e3397710edb8231f32bca3\n'}, {'number': 2, 'created': '2014-11-20 13:32:33.000000000', 'files': ['stackrc'], 'web_link': 'https://opendev.org/openstack/devstack/commit/ea863912e7c768d2ead1c4f964e6aca4a7c46d59', 'message': 'WIP: test breaking unit tests\n\nChange-Id: I0b07f69778a67409b8e3397710edb8231f32bca3\n'}]",0,135817,ea863912e7c768d2ead1c4f964e6aca4a7c46d59,7,2,2,2750,,,0,"WIP: test breaking unit tests

Change-Id: I0b07f69778a67409b8e3397710edb8231f32bca3
",git fetch https://review.opendev.org/openstack/devstack refs/changes/17/135817/2 && git format-patch -1 --stdout FETCH_HEAD,['stackrc'],1,52bb6a46a43ee17bef3b3358352b44eabb656e87,pypi_unit_tests,"GITREPO[""foo""]=1 ",,2,0
openstack%2Fdevstack-gate~master~I5f17d0587468e2227226e60d2e1ca80d4cd74bdb,openstack/devstack-gate,master,I5f17d0587468e2227226e60d2e1ca80d4cd74bdb,print out log url at the end of jobs,ABANDONED,2014-11-14 15:58:31.000000000,2014-12-05 21:31:44.000000000,,"[{'_account_id': 3}, {'_account_id': 5263}, {'_account_id': 7118}]","[{'number': 1, 'created': '2014-11-14 15:58:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/62d92115f71e0cd93fd75e434f8d84574e14e4f0', 'message': ""print out log url at the end of jobs\n\nTo make it easier to look at logs before zuul has finished all the\njobs, print out the log url in a format that jenkins console will\nturn into a link. It's a simple UX improvement, but saves a few\ncopy / paste / edit url bar actions.\n\nChange-Id: I5f17d0587468e2227226e60d2e1ca80d4cd74bdb\n""}, {'number': 2, 'created': '2014-11-14 21:13:22.000000000', 'files': ['devstack-vm-gate-wrap.sh'], 'web_link': 'https://opendev.org/openstack/devstack-gate/commit/b2be250d18d72a121195ccf195194f7af9f52b36', 'message': ""print out log url at the end of jobs\n\nTo make it easier to look at logs before zuul has finished all the\njobs, print out the log url in a format that jenkins console will\nturn into a link. It's a simple UX improvement, but saves a few\ncopy / paste / edit url bar actions.\n\nChange-Id: I5f17d0587468e2227226e60d2e1ca80d4cd74bdb\n""}]",2,134582,b2be250d18d72a121195ccf195194f7af9f52b36,8,3,2,2750,,,0,"print out log url at the end of jobs

To make it easier to look at logs before zuul has finished all the
jobs, print out the log url in a format that jenkins console will
turn into a link. It's a simple UX improvement, but saves a few
copy / paste / edit url bar actions.

Change-Id: I5f17d0587468e2227226e60d2e1ca80d4cd74bdb
",git fetch https://review.opendev.org/openstack/devstack-gate refs/changes/82/134582/2 && git format-patch -1 --stdout FETCH_HEAD,['devstack-vm-gate-wrap.sh'],1,62d92115f71e0cd93fd75e434f8d84574e14e4f0,log_url,"echo ""Job Finished, logs will be at ${SWIFT_logs_LOGSERVER_PREFIX}logs/${LOG_PATH}"" ",,2,0
openstack%2Frequirements~stable%2Ficehouse~I4d28a986c7ecb9e1382fa95f70ac9f73cac675ab,openstack/requirements,stable/icehouse,I4d28a986c7ecb9e1382fa95f70ac9f73cac675ab,"Revert ""Add a version to pytz.""",ABANDONED,2014-09-28 20:02:21.000000000,2014-12-05 21:31:19.000000000,,"[{'_account_id': 3}, {'_account_id': 308}, {'_account_id': 979}, {'_account_id': 2750}, {'_account_id': 6476}, {'_account_id': 6786}, {'_account_id': 7687}, {'_account_id': 9656}]","[{'number': 1, 'created': '2014-09-28 20:02:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/74691c49490fa8f502b4edf99977eee1a287a747', 'message': 'Switch to using pytz>2013\n\npytz was originally unqualified, but then it was given a version\nbecause of their version scheme being invalid and detected as\na pre-release by pip. However now pytz has PEP 440 valid versions\nand tooling is getting stricter still. This new specifier is\nnow fully PEP 440 compatible.\n\nConflicts:\n\tglobal-requirements.txt\n\nChange-Id: I4d28a986c7ecb9e1382fa95f70ac9f73cac675ab\n(cherry picked from commit 74012c70f0e54b2b299b945e4b427969414079e8)\n'}, {'number': 2, 'created': '2014-09-29 10:57:45.000000000', 'files': ['global-requirements.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/c74223617f9255913b564bd06a51c633f948ef37', 'message': 'Revert ""Add a version to pytz.""\n\nThis reverts commit 09ab86aeb9c1ce0e63c68b374bd0d95ee711ded8.\n\npytz was originally unqualified, but then it was given a version\nbecause of their version scheme being invalid and detected as\na pre-release by pip. However now pytz has PEP 440 valid versions\nand tooling is getting stricter still. This new specifier is\nnow fully PEP 440 compatible.\n\nConflicts:\n\trequirements.txt\n\nChange-Id: I4d28a986c7ecb9e1382fa95f70ac9f73cac675ab\n(cherry picked from commit 4cdcc24fade33823f4da5308b98693acc31ee8a6)\n'}]",0,124648,c74223617f9255913b564bd06a51c633f948ef37,23,8,2,2750,,,0,"Revert ""Add a version to pytz.""

This reverts commit 09ab86aeb9c1ce0e63c68b374bd0d95ee711ded8.

pytz was originally unqualified, but then it was given a version
because of their version scheme being invalid and detected as
a pre-release by pip. However now pytz has PEP 440 valid versions
and tooling is getting stricter still. This new specifier is
now fully PEP 440 compatible.

Conflicts:
	requirements.txt

Change-Id: I4d28a986c7ecb9e1382fa95f70ac9f73cac675ab
(cherry picked from commit 4cdcc24fade33823f4da5308b98693acc31ee8a6)
",git fetch https://review.opendev.org/openstack/requirements refs/changes/48/124648/2 && git format-patch -1 --stdout FETCH_HEAD,['global-requirements.txt'],1,74691c49490fa8f502b4edf99977eee1a287a747,pytz_req_fix,pytz>=2013,pytz>=2010h,1,1
openstack%2Ftrove-specs~master~Ie79f4ef44c721b5847b50864438ee4d8ff8f9e5e,openstack/trove-specs,master,Ie79f4ef44c721b5847b50864438ee4d8ff8f9e5e,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:52:51.000000000,2014-12-05 21:31:02.000000000,2014-12-05 21:31:01.000000000,"[{'_account_id': 3}, {'_account_id': 1925}, {'_account_id': 5293}, {'_account_id': 8415}]","[{'number': 1, 'created': '2014-12-05 03:52:51.000000000', 'files': ['doc/source/contributing.rst', 'CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/trove-specs/commit/080217e9ab145054483565e97065a7422763ccfa', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Ie79f4ef44c721b5847b50864438ee4d8ff8f9e5e\n'}]",0,139401,080217e9ab145054483565e97065a7422763ccfa,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Ie79f4ef44c721b5847b50864438ee4d8ff8f9e5e
",git fetch https://review.opendev.org/openstack/trove-specs refs/changes/01/139401/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'doc/source/contributing.rst']",2,080217e9ab145054483565e97065a7422763ccfa,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",6,8
openstack%2Fproject-config~master~Ib3c1e7e6e1f824b235bb0ac592932883efdd70bd,openstack/project-config,master,Ib3c1e7e6e1f824b235bb0ac592932883efdd70bd,enforce stable requirements,ABANDONED,2014-09-30 15:21:33.000000000,2014-12-05 21:30:52.000000000,,"[{'_account_id': 1}, {'_account_id': 3}, {'_account_id': 2592}, {'_account_id': 2750}, {'_account_id': 4146}, {'_account_id': 5263}, {'_account_id': 6316}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-09-30 15:21:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/afaacd938792ed33494d13f8f8606bb1ea1d8f73', 'message': ""enforce stable requirements\n\nif a project is in projects.txt, and it doesn't have stable/ branches,\nrun the requirements check against all the supported stable\nrequirements branches to make sure that this project won't break all\nof openstack if installed in a supported stable environment.\n\nThis will require us updating SUPPORTED_BRANCHES once new supported\nbranches are cut for requirements, but that shouldn't be too\nproblematic.\n\nChange-Id: Ib3c1e7e6e1f824b235bb0ac592932883efdd70bd\n""}, {'number': 2, 'created': '2014-09-30 16:40:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/793b758db61addda18ff86611d0cff08ef8346dc', 'message': ""enforce stable requirements\n\nif a project is in projects.txt, and it doesn't have stable/ branches,\nrun the requirements check against all the supported stable\nrequirements branches to make sure that this project won't break all\nof openstack if installed in a supported stable environment.\n\nThis will require us updating SUPPORTED_BRANCHES once new supported\nbranches are cut for requirements, but that shouldn't be too\nproblematic.\n\nChange-Id: Ib3c1e7e6e1f824b235bb0ac592932883efdd70bd\n""}, {'number': 3, 'created': '2014-10-28 15:01:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/c1740000d29fdbe0c958d08b1238af9b315bac8b', 'message': ""enforce stable requirements\n\nif a project is in projects.txt, and it doesn't have stable/ branches,\nrun the requirements check against all the supported stable\nrequirements branches to make sure that this project won't break all\nof openstack if installed in a supported stable environment.\n\nUse the list of stable branches in the requirements repository as the\nsource of truth for what our currently supported branches are.\n\nChange-Id: Ib3c1e7e6e1f824b235bb0ac592932883efdd70bd\n""}, {'number': 4, 'created': '2014-10-28 15:20:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/ca0a67b8ea6cdc9c758b74aa7e8ed663d2e21891', 'message': ""enforce stable requirements\n\nif a project is in projects.txt, and it doesn't have stable/ branches,\nrun the requirements check against all the supported stable\nrequirements branches to make sure that this project won't break all\nof openstack if installed in a supported stable environment.\n\nUse the list of stable branches in the requirements repository as the\nsource of truth for what our currently supported branches are.\n\nChange-Id: Ib3c1e7e6e1f824b235bb0ac592932883efdd70bd\n""}, {'number': 5, 'created': '2014-10-28 18:09:01.000000000', 'files': ['jenkins/scripts/project-requirements-change.py'], 'web_link': 'https://opendev.org/openstack/project-config/commit/511f2554e4aa940579c917315f768e5615cf916c', 'message': ""enforce stable requirements\n\nif a project is in projects.txt, and it doesn't have stable/ branches,\nrun the requirements check against all the supported stable\nrequirements branches to make sure that this project won't break all\nof openstack if installed in a supported stable environment.\n\nUse the list of stable branches in the requirements repository as the\nsource of truth for what our currently supported branches are.\n\nChange-Id: Ib3c1e7e6e1f824b235bb0ac592932883efdd70bd\n""}]",10,125095,511f2554e4aa940579c917315f768e5615cf916c,26,8,5,2750,,,0,"enforce stable requirements

if a project is in projects.txt, and it doesn't have stable/ branches,
run the requirements check against all the supported stable
requirements branches to make sure that this project won't break all
of openstack if installed in a supported stable environment.

Use the list of stable branches in the requirements repository as the
source of truth for what our currently supported branches are.

Change-Id: Ib3c1e7e6e1f824b235bb0ac592932883efdd70bd
",git fetch https://review.opendev.org/openstack/project-config refs/changes/95/125095/4 && git format-patch -1 --stdout FETCH_HEAD,['jenkins/scripts/project-requirements-change.py'],1,afaacd938792ed33494d13f8f8606bb1ea1d8f73,enforce_stable_reqs,"SUPPORTED_BRANCHES = ('stable/icehouse', 'master') def supports_stable_branches(project): branches = run_command(""git branch -r -l"").split(""\n"") for required in SUPPORTED_BRANCHES: if not ""origin/%s"" % required in branches: return False return True def check_requirements_for_branch(branch, head, head_reqs): print(""Updated requirements match openstack/requirements in %s."" % branch) def main(): branch = sys.argv[1] # build a list of requirements in the proposed change, # and check them for style violations while doing so head = run_command(""git rev-parse HEAD"").strip() head_reqs = RequirementsList('HEAD') head_reqs.read_all_requirements(strict=True) # build a list of requirements already in the target branch, # so that we can create a diff and identify what's being changed run_command(""git remote update"") run_command(""git checkout remotes/origin/%s"" % branch) if branch == 'master' and not supports_stable_branches(): for required in SUPPORTED_BRANCHES: check_requirements_for_branch(required, head, head_reqs) else: check_requirements_for_branch(branch, head, head_reqs)","def main(): branch = sys.argv[1] # build a list of requirements in the proposed change, # and check them for style violations while doing so head = run_command(""git rev-parse HEAD"").strip() head_reqs = RequirementsList('HEAD') head_reqs.read_all_requirements(strict=True) # build a list of requirements already in the target branch, # so that we can create a diff and identify what's being changed run_command(""git remote update"") run_command(""git checkout remotes/origin/%s"" % branch) print(""Updated requirements match openstack/requirements."")",30,12
openstack%2Fdevstack~master~Ib80db1d6034c01130cba5cee4e2ee7755bf5ac9d,openstack/devstack,master,Ib80db1d6034c01130cba5cee4e2ee7755bf5ac9d,WIP: DON'T MERGE,ABANDONED,2014-11-13 22:17:52.000000000,2014-12-05 21:30:39.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 8871}, {'_account_id': 9009}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-13 22:17:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/06933d03ed90b28285bb225697666d3b522efb36', 'message': ""WIP: DON'T MERGE\n\nTesting python-fooclient libs from git support\n\nChange-Id: Ib80db1d6034c01130cba5cee4e2ee7755bf5ac9d\n""}, {'number': 2, 'created': '2014-11-14 14:33:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/444ed0c71456fd5dc6514786d6e3d054dfc1be34', 'message': ""WIP: DON'T MERGE\n\nTesting python-fooclient libs from git support\n\nChange-Id: Ib80db1d6034c01130cba5cee4e2ee7755bf5ac9d\n""}, {'number': 3, 'created': '2014-11-14 15:40:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/106cd692620d7cc85cb25f92843fd8e30cf3d7ff', 'message': ""WIP: DON'T MERGE\n\nTesting python-fooclient libs from git support\n\nChange-Id: Ib80db1d6034c01130cba5cee4e2ee7755bf5ac9d\n""}, {'number': 4, 'created': '2014-11-14 16:37:14.000000000', 'files': ['stackrc'], 'web_link': 'https://opendev.org/openstack/devstack/commit/9742228bae480bbce6bce5d8141e538fed9d2fc8', 'message': ""WIP: DON'T MERGE\n\nTesting python-fooclient libs from git support\n\nChange-Id: Ib80db1d6034c01130cba5cee4e2ee7755bf5ac9d\n""}]",0,134379,9742228bae480bbce6bce5d8141e538fed9d2fc8,20,5,4,2750,,,0,"WIP: DON'T MERGE

Testing python-fooclient libs from git support

Change-Id: Ib80db1d6034c01130cba5cee4e2ee7755bf5ac9d
",git fetch https://review.opendev.org/openstack/devstack refs/changes/79/134379/3 && git format-patch -1 --stdout FETCH_HEAD,['stackrc'],1,06933d03ed90b28285bb225697666d3b522efb36,pypi_libs,"LIBS_FROM_GIT=python-ceilometerclient,python-cinderclient,python-glanceclient,python-heatclient,python-ironicclient,python-keystoneclient,python-neutronclient,python-novaclient,python-saharaclient,python-swiftclient,python-troveclient,python-openstackclient ",,2,0
openstack%2Frequirements~stable%2Fjuno~I2f8737b44c703c3094d6bbb6580993f86a571934,openstack/requirements,stable/juno,I2f8737b44c703c3094d6bbb6580993f86a571934,block testtools 1.2.0,ABANDONED,2014-11-15 14:20:43.000000000,2014-12-05 21:30:19.000000000,,"[{'_account_id': 3}, {'_account_id': 979}, {'_account_id': 2750}, {'_account_id': 4190}, {'_account_id': 5638}, {'_account_id': 7680}, {'_account_id': 8871}, {'_account_id': 9656}, {'_account_id': 11356}]","[{'number': 1, 'created': '2014-11-15 14:20:43.000000000', 'files': ['global-requirements.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/b8f51330f85d6f9baa01987b6fde79e536f8379c', 'message': ""block testtools 1.2.0\n\nWe're now seeing the following failure in all tempest runs:\n\nsubunit.run discover: error: no such option: --list\n\nThis coincides with testtools 1.2.0 release, so block it to try to get\neverything functioning again.\n\nConflicts:\n\tglobal-requirements.txt\n\nChange-Id: I2f8737b44c703c3094d6bbb6580993f86a571934\n(cherry picked from commit 81d479cd94da651ba2822c57e502544bdc86dfa0)\n""}]",0,134710,b8f51330f85d6f9baa01987b6fde79e536f8379c,34,9,1,2750,,,0,"block testtools 1.2.0

We're now seeing the following failure in all tempest runs:

subunit.run discover: error: no such option: --list

This coincides with testtools 1.2.0 release, so block it to try to get
everything functioning again.

Conflicts:
	global-requirements.txt

Change-Id: I2f8737b44c703c3094d6bbb6580993f86a571934
(cherry picked from commit 81d479cd94da651ba2822c57e502544bdc86dfa0)
",git fetch https://review.opendev.org/openstack/requirements refs/changes/10/134710/1 && git format-patch -1 --stdout FETCH_HEAD,['global-requirements.txt'],1,b8f51330f85d6f9baa01987b6fde79e536f8379c,,"testtools>=0.9.34,!=1.2.0",testtools>=0.9.34,1,1
openstack%2Fpython-swiftclient~master~I3eda9d029fa4fb5b3aedfa29f5d5c5f7ba55a7d5,openstack/python-swiftclient,master,I3eda9d029fa4fb5b3aedfa29f5d5c5f7ba55a7d5,Do not override specified URL or token,ABANDONED,2014-12-05 20:19:44.000000000,2014-12-05 21:23:54.000000000,,"[{'_account_id': 3}, {'_account_id': 1179}, {'_account_id': 11642}]","[{'number': 1, 'created': '2014-12-05 20:19:44.000000000', 'files': ['swiftclient/client.py'], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/06162e23f99f7991766608820b2a0bab5f052d1f', 'message': 'Do not override specified URL or token\n\nIf either the URL or token is provided, but not both, both are set by\nthe results from auth. This might not be desired if what is passed in\nwill differ from what auth provides. This change only sets what is\nmissing instead of overriding inputs that are provided.\n\nChange-Id: I3eda9d029fa4fb5b3aedfa29f5d5c5f7ba55a7d5\n'}]",0,139728,06162e23f99f7991766608820b2a0bab5f052d1f,5,3,1,11642,,,0,"Do not override specified URL or token

If either the URL or token is provided, but not both, both are set by
the results from auth. This might not be desired if what is passed in
will differ from what auth provides. This change only sets what is
missing instead of overriding inputs that are provided.

Change-Id: I3eda9d029fa4fb5b3aedfa29f5d5c5f7ba55a7d5
",git fetch https://review.opendev.org/openstack/python-swiftclient refs/changes/28/139728/1 && git format-patch -1 --stdout FETCH_HEAD,['swiftclient/client.py'],1,06162e23f99f7991766608820b2a0bab5f052d1f,endpoint_config," url, token = self.get_auth() if not self.url: self.url = url if not self.token: self.token = token"," self.url, self.token = self.get_auth()",5,1
openstack%2Fironic-specs~master~If8552ab970dc5ec12041402bc417e9a75fc2f337,openstack/ironic-specs,master,If8552ab970dc5ec12041402bc417e9a75fc2f337,Exposing Hardware Capabilities,MERGED,2014-10-27 21:05:19.000000000,2014-12-05 21:23:44.000000000,2014-12-05 21:23:44.000000000,"[{'_account_id': 3}, {'_account_id': 114}, {'_account_id': 2889}, {'_account_id': 8125}, {'_account_id': 10202}, {'_account_id': 10239}, {'_account_id': 10342}, {'_account_id': 10343}, {'_account_id': 11076}, {'_account_id': 11297}, {'_account_id': 13362}]","[{'number': 1, 'created': '2014-10-27 21:05:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-specs/commit/3e436fb2d2492ca51d6d49a581f44c45c8e22b77', 'message': 'WIP: Exposing Hardware Capabilities\n\nChange-Id: If8552ab970dc5ec12041402bc417e9a75fc2f337\n'}, {'number': 2, 'created': '2014-11-26 00:52:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-specs/commit/66da81ff062634f6ce5c1ef16ff206ec8a04af68', 'message': 'WIP: Exposing Hardware Capabilities\n\nChange-Id: If8552ab970dc5ec12041402bc417e9a75fc2f337\n'}, {'number': 3, 'created': '2014-11-26 00:58:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-specs/commit/1fd19bbbee38b880d874d29b9165e488324ccc4f', 'message': 'Exposing Hardware Capabilities\n\nBacklog spec for exposing hardware capabilities\n\nChange-Id: If8552ab970dc5ec12041402bc417e9a75fc2f337\n'}, {'number': 4, 'created': '2014-11-26 01:07:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-specs/commit/8a51d44f469500bf26e5e0e57a91dcdaa9a9615e', 'message': 'Exposing Hardware Capabilities\n\nBacklog spec for exposing hardware capabilities\n\nChange-Id: If8552ab970dc5ec12041402bc417e9a75fc2f337\n'}, {'number': 5, 'created': '2014-11-26 01:11:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-specs/commit/cc0fcee17d64519f9a06b6c4cd0c70be2fbc0275', 'message': 'Exposing Hardware Capabilities\n\nBacklog spec for exposing hardware capabilities\n\nChange-Id: If8552ab970dc5ec12041402bc417e9a75fc2f337\n'}, {'number': 6, 'created': '2014-12-05 18:47:18.000000000', 'files': ['specs/backlog/hardware-capabilities.rst'], 'web_link': 'https://opendev.org/openstack/ironic-specs/commit/122826d522636375a13470b8883afacd86d9c115', 'message': 'Exposing Hardware Capabilities\n\nBacklog spec for exposing hardware capabilities\n\nChange-Id: If8552ab970dc5ec12041402bc417e9a75fc2f337\n'}]",8,131272,122826d522636375a13470b8883afacd86d9c115,28,11,6,10342,,,0,"Exposing Hardware Capabilities

Backlog spec for exposing hardware capabilities

Change-Id: If8552ab970dc5ec12041402bc417e9a75fc2f337
",git fetch https://review.opendev.org/openstack/ironic-specs refs/changes/72/131272/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/kilo/hardware-capabilities.rst'],1,3e436fb2d2492ca51d6d49a581f44c45c8e22b77,131272,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ============================== Exposting Hardware Capabilties ============================== https://blueprints.launchpad.net/ironic/+spec/exposing-hardware-capabilities Hardware can have a wide range of capabiltiies and potential configurations. Ironic should expose these capabilities in a declarative way, and allow deployers to configure custom Nova flavors that map to specific types of hardware or hardware configurations. Ironic should then ensure these assertions about the hardware are made valid during the deploy process. For instance, a machine booted with capabilities [""vt_on"", ""turbo_mode_off""] would have Ironic properly configure those BIOS settings before deploying an image onto the node. Problem description =================== Deployers of Ironic currently must configure machines exactly as they would like them to be configured when deployed to. For instance, a deployer who wanted two different configurations of a given node to be mapped to two different nova flavors, that deployer would have to split their capacity, configure one half differently than the other, and manually add a capability string to node.properties.capabilities diffierentiating them that Nova could then schedule against. Possible use cases include: * Setting specific BIOS settings at deploy time * Specifying desired firmware versions (and applying that firmware version) at deploy time * Configuring boot mode (uefi vs bios) at deploy time * Configuring a boot disk raid at deploy time Proposed change =============== Ironic management and deploy drivers should be able to expose capabilities for nodes configured in a given way Ironic nodes can expose capabilities generally in mutually exclusive sets, for instance, a node could expose both ""uefi_boot"" and ""bios_boot"", but Ironic would validate that both were not specified typically positive and negative, but can be other things that are muFor instance, vt_on and vt_off might be used to determine if the VT bit should be on or off. Here is where you cover the change you propose to make in detail. How do you propose to solve this problem? If this is one part of a larger effort make it clear where this piece ends. In other words, what is the scope of this effort? If you are unsure whether this proposal is aligned with the project's mission and scope, stop here and get feedback from the ironic-drivers and ironic-core teams before fleshing out all the details below. Alternatives ------------ What other ways could we do this thing? Has someone else done this thing in another project? In another language? Why aren't we using those? This doesn't have to be a full literature review, but it should demonstrate that thought has been put into why the proposed solution is an appropriate one. Data model impact ----------------- Changes which require modifications to the data model often have a wider impact on the system. The community often has strong opinions on how the data model should be evolved, from both a functional and performance perspective. It is therefore important to capture and gain agreement as early as possible on any proposed changes to the data model. Questions which need to be addressed by this section include: * What new data objects and/or database schema changes is this going to require? * What database migrations will accompany this change? * How will the initial set of new data objects be generated? For example, if you need to take into account existing instances, or modify other existing data, describe how that will work. REST API impact --------------- Each API method which is either added or changed should have the following * Specification for the method * A description of what the method does, suitable for use in user documentation. * Method type (POST/PUT/GET/DELETE/PATCH) * Normal http response code(s) * Expected error http response code(s) * A description for each possible error code should be included. Describe semantic errors which can cause it, such as inconsistent parameters supplied to the method, or when a resource is not in an appropriate state for the request to succeed. Errors caused by syntactic problems covered by the JSON schema defintion do not need to be included. * URL for the resource * Parameters which can be passed via the url, including data types * JSON schema definition for the body data if allowed * JSON schema definition for the response data if any * Example use case including typical API samples for both data supplied by the caller and the response * Discuss any policy changes, and discuss what things a deployer needs to think about when defining their policy. * Is a corresponding change in the client library and CLI necessary? * Is this change discoverable by clients? Not all clients will upgrade at the same time, so this change must work with older clients without breaking them. Note that the schema should be defined as restrictively as possible. Parameters which are required should be marked as such and only under exceptional circumstances should additional parameters which are not defined in the schema be permitted. Use of free-form JSON dicts should only be permitted where necessary to allow divergence in the drivers. In such case, the drivers must expose the expected content of the JSON dict and an ability to validate it. Reuse of existing predefined parameter types is highly encouraged. RPC API impact -------------- Changes which affect the RPC API should be listed here. For example: * What are the changes, if any, to existing API calls? * What new API calls are being added? Will these be using cast() or call()? * ironic-api and ironic-conductor services must be upgradable independently. What is the upgrade process for rolling this change out to an existing deployment? Driver API impact ----------------- Changes which affect the driver API have a direct effect on all drivers, and often have a wider impact on the system. There are several things to consider in this section. * Is it a change to a ""core"" or ""common"" API? * Can all drivers support it initially, or is it specific to a particular vendor's hardware? * How will it be tested in the gate and in third-party CI systems? * If adding a new interface, explain the intended scope of the proposed interface, what functionality it enables, why it is needed, and whether it is supported by current drivers. * If adding or changing a method on an existing interface, the impact on existing drivers should be explored. * Will the new interface or method need to be invoked when the hash ring rebalances, for example to rebuild local state on a new conductor service? * How does this affect upgrades? Third-party drivers could be updated independently from this change, and care must be taken not to break backwards-compatibility within our Driver API. Nova driver impact ------------------ Chances are, if this change affects the REST or Driver APIs, it will also affect the Nova driver in some way. If this requires a functional change in Nova, chances are the Nova team will require a spec to discuss the changes to their project as well. Provide a link to that here, or a justification for why that is not needed. Questions which need to be addressed in this section include: * What is the impact on Nova? * If this change is enabling new functionality exposed via Nova, this section should cite the relevant components within other Nova drivers that alraedy implement this. * Ironic and Nova services must be upgradable independently. If the change affects existing functionality of the nova.virt.ironic driver, how will an upgrade be performed? How will it be tested? Security impact --------------- Describe any potential security impact on the system. Some of the items to consider include: * Does this change touch sensitive data such as tokens, keys, or credentials? * Does this change affect the accessibility of hardware managed by Ironic? * Does this change alter the API in a way that may impact security, such as a new way to access sensitive information or a new way to login? * Does this change involve cryptography or hashing? * Does this change require the use of sudo or any elevated privileges? * Does this change involve using or parsing user-provided data? This could be directly at the API level or indirectly such as changes to a cache layer. * Can this change enable a resource exhaustion attack, such as allowing a single API interaction to consume significant server resources? Some examples of this include launching subprocesses for each connection, or entity expansion attacks in XML. For more detailed guidance, please see the OpenStack Security Guidelines as a reference (https://wiki.openstack.org/wiki/Security/Guidelines). These guidelines are a work in progress and are designed to help you identify security best practices. For further information, feel free to reach out to the OpenStack Security Group at openstack-security@lists.openstack.org. Other end user impact --------------------- Aside from the API, are there other ways a user will interact with this feature? * Does this change have an impact on python-ironicclient? What does the user interface there look like? * Will this require changes in the Horizon panel, or any other OpenStack project? Scalability impact ------------------ Describe any potential scalability impact on the system, for example any increase in network, RPC, or database traffic, or whether the feature requires synchronization across multiple services. Examples of things to consider here include: * Additional network calls to internal or external services. * Additional disk or network traffic that will be required by the feature. * Any change in the number of physical nodes which can be managed by each conductor service. Performance Impact ------------------ Describe any potential performance impact on the system, for example how often will new code be called, and is there a major change to the calling pattern of existing code. Examples of things to consider here include: * A periodic task might look like a small addition, but all periodic tasks run in a single thread so a periodic task that takes a long time to run will have an effect on the timing of other periodic tasks. * A small change in a utility function or a commonly used decorator can have a large impact on performance. * Calls which result in one or more database queries (whether in the api or conductor services) can have a profound impact on performance when called in critical sections of the code. * Will the change include any TaskManager locking, and if so what considerations are there on holding the lock? * How will the new code be affected if the hash ring rebalances while it is running? Other deployer impact --------------------- Discuss things that will affect how you deploy and configure OpenStack that have not already been mentioned, such as: * What config options are being added? Should they be more generic than proposed (for example, a flag that other hardware drivers might want to implement as well)? Are the default values appropriate for production? Provide an explanation of why these defaults are reasonable. * Is this a change that takes immediate effect after it's merged, or is it something that has to be explicitly enabled? * If this change adds a new service that deployers will be requried to run, how would it be deployed? Describe the expected topology, for example, what network connectivity the new service would need, what service(s) it would interact with, how many should run relative to the size of the deployment, and so on. * Please state anything that those doing continuous deployment, or those upgrading from the previous release, need to be aware of. Also describe any plans to deprecate configuration values or features. For example, if we were to change the directory that PXE boot files were stored in, how would we update existing boot files created before the change landed? Would we require deployers to manually move them? Is there a special case in the code, which would be removed after some deprecation period? Would we require operators to delete and recreate all instances in order to perform the upgrade? Developer impact ---------------- Discuss things that will affect other developers working on OpenStack, such as: * If the blueprint proposes a change to the driver API, discussion of how other drivers would implement the feature is required. Implementation ============== Assignee(s) ----------- Who is leading the writing of the code? Or is this a blueprint where you're throwing it out there to see who picks it up? If more than one person is working on the implementation, please designate the primary author and contact. Primary assignee: <launchpad-id or None> Other contributors: <launchpad-id or None> Work Items ---------- Work items or tasks -- break the feature up into the things that need to be done to implement it. Those parts might end up being done by different people, but we're mostly trying to understand the timeline for implementation. Dependencies ============ * Include specific references to specs and/or blueprints in Ironic, or in other projects, that this one either depends on or is related to. * If this requires functionality of another project that is not currently used by Ironic, document that fact. * Does this feature require any new library dependencies or code otherwise not included in OpenStack? Or does it depend on a specific version of library? * Does this feature target specific hardware? If so, is it a common standard (eg IPMI) or a vendor-specific implementation (eg iLO)? Testing ======= Please discuss how the change will be tested. We especially want to know what tempest tests will be added. It is assumed that unit test coverage will be added so that doesn't need to be mentioned explicitly, but discussion of why you think unit tests are sufficient and we don't need to add more tempest tests would need to be included. Is this untestable in gate given current limitations (specific hardware / software configurations available)? If so, are there mitigation plans (3rd party testing, gate enhancements, etc)? Upgrades and Backwards Compatibility ==================================== Care must be taken to support our users by not breaking backwards compatibility with either REST API or Driver API changes. * If your proposal includes any changes to the REST API, describe how existing clients will continue to function when interacting with an upgraded API server. * If your proposal includes any changes to the Driver API, describe how existing driver implementations will continue to function when loaded by a conductor running with the new driver base class. * Describe what testing you will be adding to ensure that backwards compatibility is maintained. * If deprecating an existing feature or API, describe the deprecation plan, and for how long compatibility will be maintained. Documentation Impact ==================== What is the impact on the docs team of this change? Some changes might require donating resources to the docs team to have the documentation updated. Don't repeat details discussed above, but please reference them here. References ========== Please add any useful references here. You are not required to have any reference. Moreover, this specification should still make sense when your references are unavailable. Examples of what you could include are: * Links to mailing list or IRC discussions * Links to notes from a summit session * Links to relevant research, if appropriate * Related specifications as appropriate (e.g. if it's an EC2 thing, link the EC2 docs) * Anything else you feel it is worthwhile to refer to ",,432,0
openstack%2Fneutron~master~Ib225f6407faab9f86e36288f30365051d64c9d34,openstack/neutron,master,Ib225f6407faab9f86e36288f30365051d64c9d34,Imported Translations from Transifex,MERGED,2014-12-05 06:09:33.000000000,2014-12-05 21:19:12.000000000,2014-12-05 21:19:11.000000000,"[{'_account_id': 3}, {'_account_id': 2592}, {'_account_id': 5170}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 12040}, {'_account_id': 13051}]","[{'number': 1, 'created': '2014-12-05 06:09:33.000000000', 'files': ['neutron/locale/neutron-log-error.pot', 'neutron/locale/pt_BR/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/ja/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/de/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/zh_CN/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/es/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/neutron-log-warning.pot', 'neutron/locale/fr/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/neutron-log-info.pot', 'neutron/locale/neutron.pot', 'neutron/locale/ko_KR/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/zh_TW/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/it/LC_MESSAGES/neutron-log-info.po'], 'web_link': 'https://opendev.org/openstack/neutron/commit/1d2ebb07e14c3a0a44833df917508b3d004ac810', 'message': 'Imported Translations from Transifex\n\nFor more information about this automatic import see:\nhttps://wiki.openstack.org/wiki/Translations/Infrastructure\n\nChange-Id: Ib225f6407faab9f86e36288f30365051d64c9d34\n'}]",0,139526,1d2ebb07e14c3a0a44833df917508b3d004ac810,22,16,1,11131,,,0,"Imported Translations from Transifex

For more information about this automatic import see:
https://wiki.openstack.org/wiki/Translations/Infrastructure

Change-Id: Ib225f6407faab9f86e36288f30365051d64c9d34
",git fetch https://review.opendev.org/openstack/neutron refs/changes/26/139526/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/locale/neutron-log-error.pot', 'neutron/locale/pt_BR/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/ja/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/de/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/zh_CN/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/es/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/neutron-log-warning.pot', 'neutron/locale/fr/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/neutron-log-info.pot', 'neutron/locale/neutron.pot', 'neutron/locale/ko_KR/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/zh_TW/LC_MESSAGES/neutron-log-info.po', 'neutron/locale/it/LC_MESSAGES/neutron-log-info.po']",13,1d2ebb07e14c3a0a44833df917508b3d004ac810,transifex/translations,"""POT-Creation-Date: 2014-12-05 06:09+0000\n"" ""PO-Revision-Date: 2014-12-05 00:25+0000\n""#: neutron/agent/dhcp_agent.py:109 neutron/agent/dhcp_agent.py:605#: neutron/agent/dhcp_agent.py:161#: neutron/agent/dhcp_agent.py:182#: neutron/agent/dhcp_agent.py:602 neutron/agent/l3_agent.py:1994#: neutron/agent/linux/interface.py:262 neutron/agent/linux/interface.py:317 #: neutron/agent/linux/interface.py:380 neutron/agent/linux/interface.py:427#: neutron/agent/linux/ovs_lib.py:432#: neutron/plugins/brocade/NeutronPlugin.py:306 #, python-format msgid ""Allocated vlan (%d) from the pool"" msgstr ""vlan (%d) allocata dal pool"" #: neutron/plugins/embrane/common/utils.py:44 msgid ""No ip allocation set"" msgstr """" #: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:240#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:265#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:286#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:308#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:325#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:372#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:374#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:401#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:433 #: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:994 #: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:376 #: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:833 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1369 #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:255#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:469 #: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:1035#: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1564 #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:351#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:808 #: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:217 #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:100#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:879 #: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:725 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1119 #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:213#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:912#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:919 #: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:755 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1174 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1191#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:931 #: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:349 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1203 #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:239#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:984#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:1028 #: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:407#: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:308#: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:311#: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:336 #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:227#: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:361#: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:417#: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:448 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:480#: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:498 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:574#: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:566#: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:711 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1113#: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:744 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1143#: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:838 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1391#: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:669#: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:742#: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:883#: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:993#: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1162#: neutron/plugins/plumgrid/drivers/fake_plumlib.py:30 msgid ""Python PLUMgrid Fake Library Started "" msgstr """" #: neutron/plugins/plumgrid/drivers/fake_plumlib.py:35 #, python-format msgid ""Fake Director: %s"" msgstr """" #: neutron/plugins/plumgrid/drivers/plumlib.py:36 msgid ""Python PLUMgrid Library Started "" msgstr """" #: neutron/plugins/plumgrid/plumgrid_plugin/plumgrid_plugin.py:74 msgid ""Neutron PLUMgrid Director: Starting Plugin"" msgstr """" #: neutron/plugins/plumgrid/plumgrid_plugin/plumgrid_plugin.py:92 #, python-format msgid ""Neutron PLUMgrid Director: %s"" msgstr """" #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:195#: neutron/plugins/sriovnicagent/sriov_nic_agent.py:220#: neutron/plugins/sriovnicagent/sriov_nic_agent.py:248#: neutron/plugins/sriovnicagent/sriov_nic_agent.py:337#: neutron/plugins/sriovnicagent/sriov_nic_agent.py:338#: neutron/services/vpn/device_drivers/cisco_ipsec.py:333#: neutron/services/vpn/device_drivers/cisco_ipsec.py:713#: neutron/services/vpn/device_drivers/cisco_ipsec.py:716#: neutron/services/vpn/device_drivers/cisco_ipsec.py:731","""POT-Creation-Date: 2014-12-04 06:14+0000\n"" ""PO-Revision-Date: 2014-12-03 16:22+0000\n""#: neutron/agent/dhcp_agent.py:110 neutron/agent/dhcp_agent.py:607#: neutron/agent/dhcp_agent.py:162#: neutron/agent/dhcp_agent.py:183#: neutron/agent/dhcp_agent.py:604 neutron/agent/l3_agent.py:1994#: neutron/agent/linux/interface.py:265 neutron/agent/linux/interface.py:320 #: neutron/agent/linux/interface.py:383 neutron/agent/linux/interface.py:430#: neutron/agent/linux/ovs_lib.py:423#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:243#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:268#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:289#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:311#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:328#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:375#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:377#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:404#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:436 #: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:995 #: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:379 #: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:834 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1370 #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:256#: neutron/plugins/hyperv/agent/hyperv_neutron_agent.py:472 #: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:1036#: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1565 #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:352#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:809 #: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:220 #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:101#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:880 #: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:726 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1120 #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:214#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:913#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:920 #: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:756 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1175 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1192#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:932 #: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:352 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1204 #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:240#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:985#: neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:1029 #: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:410#: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:311#: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:314#: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:339 #: neutron/plugins/sriovnicagent/sriov_nic_agent.py:228#: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:364#: neutron/plugins/mlnx/agent/eswitch_neutron_agent.py:420#: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:449 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:481#: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:499 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:575#: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:567#: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:712 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1114#: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:745 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1144#: neutron/plugins/ofagent/agent/ofa_neutron_agent.py:839 #: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1392#: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:670#: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:743#: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:884#: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:994#: neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:1163#: neutron/plugins/sriovnicagent/sriov_nic_agent.py:196#: neutron/plugins/sriovnicagent/sriov_nic_agent.py:221#: neutron/plugins/sriovnicagent/sriov_nic_agent.py:249#: neutron/plugins/sriovnicagent/sriov_nic_agent.py:338#: neutron/plugins/sriovnicagent/sriov_nic_agent.py:339#: neutron/services/vpn/device_drivers/cisco_ipsec.py:337#: neutron/services/vpn/device_drivers/cisco_ipsec.py:717#: neutron/services/vpn/device_drivers/cisco_ipsec.py:720#: neutron/services/vpn/device_drivers/cisco_ipsec.py:735",1312,1096
openstack%2Fdevstack~master~Ib7a2f3a26bbdaf650d8426fa3a94ec6e4fe60e54,openstack/devstack,master,Ib7a2f3a26bbdaf650d8426fa3a94ec6e4fe60e54,WIP: code coloring test with gerrit,ABANDONED,2014-12-05 13:08:04.000000000,2014-12-05 21:15:47.000000000,,"[{'_account_id': 3}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-12-05 13:08:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/1d8aa34cdb7fc8a83b061ba4b1102bdca499e1cb', 'message': 'WIP: code coloring test with gerrit\n\nChange-Id: Ib7a2f3a26bbdaf650d8426fa3a94ec6e4fe60e54\n'}, {'number': 2, 'created': '2014-12-05 13:09:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/4042e1138cfdab5aaa7caf540c1457670bf07acb', 'message': 'WIP: code coloring test with gerrit\n\nChange-Id: Ib7a2f3a26bbdaf650d8426fa3a94ec6e4fe60e54\n'}, {'number': 3, 'created': '2014-12-05 13:10:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/1ab0541990a95b2a14ccc10dfad79ec71bf24061', 'message': 'WIP: code coloring test with gerrit\n\nChange-Id: Ib7a2f3a26bbdaf650d8426fa3a94ec6e4fe60e54\n'}, {'number': 4, 'created': '2014-12-05 13:10:37.000000000', 'files': ['lib/nova'], 'web_link': 'https://opendev.org/openstack/devstack/commit/57341b091ded03461958c109baa02558e791a162', 'message': 'WIP: code coloring test with gerrit\n\nChange-Id: Ib7a2f3a26bbdaf650d8426fa3a94ec6e4fe60e54\n'}]",0,139619,57341b091ded03461958c109baa02558e791a162,7,2,4,2750,,,0,"WIP: code coloring test with gerrit

Change-Id: Ib7a2f3a26bbdaf650d8426fa3a94ec6e4fe60e54
",git fetch https://review.opendev.org/openstack/devstack refs/changes/19/139619/4 && git format-patch -1 --stdout FETCH_HEAD,['lib/nova'],1,1d8aa34cdb7fc8a83b061ba4b1102bdca499e1cb,codecoloring,#!/bin/bash #,,2,0
openstack%2Fkeystone~stable%2Ficehouse~I123aadd55770002fe1c9ed9b17669d5212198aa3,openstack/keystone,stable/icehouse,I123aadd55770002fe1c9ed9b17669d5212198aa3,Fix test_provider_token_expiration_validation transient failure,MERGED,2014-12-05 15:14:42.000000000,2014-12-05 21:09:06.000000000,2014-12-05 21:09:05.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 2472}, {'_account_id': 2592}, {'_account_id': 2903}, {'_account_id': 6486}]","[{'number': 1, 'created': '2014-12-05 15:14:42.000000000', 'files': ['keystone/tests/test_token_provider.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/35d937d1e2f4a296f3475479f31e83305f0a9ed1', 'message': ""Fix test_provider_token_expiration_validation transient failure\n\nThe keystone.tests.test_token_provider.TestTokenProvider.\ntest_provider_token_expiration_validation test would fail if the\ntests weren't run before the expiration period passed. This could\nhappen if the system was slow and the previous tests took a long\ntime causing a potential transient failure.\n\nChange-Id: I123aadd55770002fe1c9ed9b17669d5212198aa3\nCloses-Bug: #1278550\n(cherry picked from commit 170faf1468647e9a8246e6f7f157c7e5fd475ed5)\n""}]",0,139657,35d937d1e2f4a296f3475479f31e83305f0a9ed1,8,6,1,8005,,,0,"Fix test_provider_token_expiration_validation transient failure

The keystone.tests.test_token_provider.TestTokenProvider.
test_provider_token_expiration_validation test would fail if the
tests weren't run before the expiration period passed. This could
happen if the system was slow and the previous tests took a long
time causing a potential transient failure.

Change-Id: I123aadd55770002fe1c9ed9b17669d5212198aa3
Closes-Bug: #1278550
(cherry picked from commit 170faf1468647e9a8246e6f7f157c7e5fd475ed5)
",git fetch https://review.opendev.org/openstack/keystone refs/changes/57/139657/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/tests/test_token_provider.py'],1,35d937d1e2f4a296f3475479f31e83305f0a9ed1,bug/1278550," ""expires"": timeutils.isotime(timeutils.utcnow() + FUTURE_DELTA), ""expires_at"": timeutils.isotime(timeutils.utcnow() + FUTURE_DELTA),"," ""expires"": timeutils.isotime(CURRENT_DATE + FUTURE_DELTA), ""expires_at"": timeutils.isotime(CURRENT_DATE + FUTURE_DELTA),",3,2
openstack%2Foslo.config~master~I4b2a5cbbaea16225701879c0c55fc50365f53d2e,openstack/oslo.config,master,I4b2a5cbbaea16225701879c0c55fc50365f53d2e,Move files out of the namespace package,ABANDONED,2014-11-14 22:25:06.000000000,2014-12-05 21:07:34.000000000,,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 6928}]","[{'number': 1, 'created': '2014-11-14 22:25:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.config/commit/9dc98896ba1a3667a2689fa0c7cac4aab8af3365', 'message': 'Move files out of the namespace package\n\nMove the public API out of oslo.config to oslo_config. Retain the ability\nto import from the old namespace package for backwards compatibility\nfor this release cycle.\n\nbp/drop-namespace-packages\n\nChange-Id: I4b2a5cbbaea16225701879c0c55fc50365f53d2e\n'}, {'number': 2, 'created': '2014-11-14 22:33:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.config/commit/7746cfe9d6c393d95ac7aaf19773729ab75fdd97', 'message': 'Move files out of the namespace package\n\nMove the public API out of oslo.config to oslo_config. Retain the ability\nto import from the old namespace package for backwards compatibility\nfor this release cycle.\n\nbp/drop-namespace-packages\n\nChange-Id: I4b2a5cbbaea16225701879c0c55fc50365f53d2e\n'}, {'number': 3, 'created': '2014-11-14 22:38:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.config/commit/90546720da022955e09044485e6fb2a9e26a3b38', 'message': 'Move files out of the namespace package\n\nMove the public API out of oslo.config to oslo_config. Retain the ability\nto import from the old namespace package for backwards compatibility\nfor this release cycle.\n\nbp/drop-namespace-packages\n\nChange-Id: I4b2a5cbbaea16225701879c0c55fc50365f53d2e\n'}, {'number': 4, 'created': '2014-11-18 18:53:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.config/commit/fffce9baf03f37e5ad6cf8f5bda15e026fab898a', 'message': 'Move files out of the namespace package\n\nMove the public API out of oslo.config to oslo_config. Retain the ability\nto import from the old namespace package for backwards compatibility\nfor this release cycle.\n\nbp/drop-namespace-packages\n\nChange-Id: I4b2a5cbbaea16225701879c0c55fc50365f53d2e\n'}, {'number': 5, 'created': '2014-11-18 22:38:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.config/commit/58a5e5c15b87026de8e2aab9c1a41e32573bbf62', 'message': 'Move files out of the namespace package\n\nMove the public API out of oslo.config to oslo_config. Retain the ability\nto import from the old namespace package for backwards compatibility\nfor this release cycle.\n\nbp/drop-namespace-packages\n\nChange-Id: I4b2a5cbbaea16225701879c0c55fc50365f53d2e\n'}, {'number': 6, 'created': '2014-11-21 20:25:58.000000000', 'files': ['oslo_config/__init__.py', 'oslo/config/cfg.py', '.gitignore', 'oslo/config/__init__.py', 'tests/test_warning.py', 'oslo_config/tests/testmods/fblaa_opt.py', 'oslo_config/tests/test_cfg.py', 'oslo_config/generator.py', 'doc/source/parser.rst', 'doc/source/configopts.rst', 'doc/source/generator.rst', 'doc/source/types.rst', 'oslo_config/tests/testmods/blaa_opt.py', 'oslo_config/tests/testmods/__init__.py', 'doc/source/cfgfilter.rst', 'oslo_config/tests/testmods/baz_qux_opt.py', 'oslo/config/types.py', 'doc/source/exceptions.rst', 'doc/source/opts.rst', 'oslo_config/cfg.py', 'tests/test_cfgfilter.py', 'oslo_config/tests/test_cfgfilter.py', 'oslo_config/tests/testmods/bar_foo_opt.py', 'doc/source/fixture.rst', 'oslo_config/tests/__init__.py', 'oslo/config/iniparser.py', 'oslo_config/tests/test_generator.py', 'oslo/config/cfgfilter.py', 'doc/source/cfg.rst', 'oslo_config/tests/testmods/fbar_foo_opt.py', 'tests/test_iniparser.py', 'tests/test_cfg.py', 'oslo/config/fixture.py', 'oslo_config/cfgfilter.py', 'oslo/config/generator.py', 'oslo_config/tests/test_iniparser.py', 'oslo_config/tests/testmods/fbaar_baa_opt.py', 'tests/test_types.py', 'doc/source/helpers.rst', 'oslo_config/tests/test_fixture.py', 'oslo_config/fixture.py', 'tests/test_generator.py', 'oslo_config/tests/test_types.py', 'tests/test_fixture.py', 'oslo_config/types.py', 'oslo_config/iniparser.py', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/oslo.config/commit/a975fe155a519d44f88484c2062d735ac326ae61', 'message': 'Move files out of the namespace package\n\nMove the public API out of oslo.config to oslo_config. Retain the ability\nto import from the old namespace package for backwards compatibility\nfor this release cycle.\n\nbp/drop-namespace-packages\n\nChange-Id: I4b2a5cbbaea16225701879c0c55fc50365f53d2e\n'}]",1,134660,a975fe155a519d44f88484c2062d735ac326ae61,19,5,6,2472,,,0,"Move files out of the namespace package

Move the public API out of oslo.config to oslo_config. Retain the ability
to import from the old namespace package for backwards compatibility
for this release cycle.

bp/drop-namespace-packages

Change-Id: I4b2a5cbbaea16225701879c0c55fc50365f53d2e
",git fetch https://review.opendev.org/openstack/oslo.config refs/changes/60/134660/6 && git format-patch -1 --stdout FETCH_HEAD,"['oslo_config/__init__.py', '.gitignore', 'oslo/config/__init__.py', 'oslo_config/tests/testmods/fblaa_opt.py', 'oslo_config/tests/test_cfg.py', 'oslo_config/generator.py', 'doc/source/parser.rst', 'doc/source/configopts.rst', 'doc/source/generator.rst', 'doc/source/types.rst', 'oslo_config/tests/testmods/blaa_opt.py', 'oslo_config/tests/testmods/__init__.py', 'doc/source/cfgfilter.rst', 'oslo_config/tests/testmods/baz_qux_opt.py', 'doc/source/exceptions.rst', 'oslo_config/cfg.py', 'tests/test_cfgfilter.py', 'oslo_config/tests/test_cfgfilter.py', 'oslo_config/tests/testmods/bar_foo_opt.py', 'doc/source/fixture.rst', 'oslo_config/tests/__init__.py', 'oslo_config/tests/test_generator.py', 'doc/source/cfg.rst', 'oslo_config/tests/testmods/fbar_foo_opt.py', 'tests/test_iniparser.py', 'tests/test_cfg.py', 'oslo_config/cfgfilter.py', 'oslo_config/tests/test_iniparser.py', 'oslo_config/tests/testmods/fbaar_baa_opt.py', 'tests/test_types.py', 'doc/source/helpers.rst', 'oslo_config/tests/test_fixture.py', 'oslo_config/fixture.py', 'tests/test_generator.py', 'oslo_config/tests/test_types.py', 'tests/test_fixture.py', 'oslo_config/types.py', 'oslo_config/iniparser.py', 'setup.cfg']",39,9dc98896ba1a3667a2689fa0c7cac4aab8af3365,bp/drop-namespace-packages, oslo_config oslo-config-generator = oslo_config.generator:main, oslo-config-generator = oslo.config.generator:main,4971,128
openstack%2Fglance-specs~master~Ic3c865706d8bb5be5d97d9dba25fe9a3df2ae233,openstack/glance-specs,master,Ic3c865706d8bb5be5d97d9dba25fe9a3df2ae233,Allow delete image to be repeatable,ABANDONED,2014-12-04 23:34:30.000000000,2014-12-05 21:06:10.000000000,,[{'_account_id': 3}],"[{'number': 1, 'created': '2014-12-04 23:34:30.000000000', 'files': ['specs/kilo/make-delete-image-repeatable.rst'], 'web_link': 'https://opendev.org/openstack/glance-specs/commit/d08b8cb65471c6272079dcbb2e3d2efaeddfa641', 'message': 'Allow delete image to be repeatable\n\nDelete image operations sometimes fail to remove data from storage\nlocations. My proposal would allow Glance to look for storage that\nhas not been released yet on deleted images when another delete is\nissued.\n\nBlueprint make-delete-image-repeatable\nRelated-bug: #1377012\nChange-Id: Ic3c865706d8bb5be5d97d9dba25fe9a3df2ae233\n'}]",0,139267,d08b8cb65471c6272079dcbb2e3d2efaeddfa641,3,1,1,12807,,,0,"Allow delete image to be repeatable

Delete image operations sometimes fail to remove data from storage
locations. My proposal would allow Glance to look for storage that
has not been released yet on deleted images when another delete is
issued.

Blueprint make-delete-image-repeatable
Related-bug: #1377012
Change-Id: Ic3c865706d8bb5be5d97d9dba25fe9a3df2ae233
",git fetch https://review.opendev.org/openstack/glance-specs refs/changes/67/139267/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/kilo/make-delete-image-repeatable.rst'],1,d08b8cb65471c6272079dcbb2e3d2efaeddfa641,bug/1377012,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ============================ make delete image repeatable ============================ https://blueprints.launchpad.net/glance/+spec/make-delete-image-repeatable Currently, in some error cases deleting an image in Glance may fail to remove the image data from storage. The intention is to ensure that operators have a means of ensuring that image data may be deleted from storage in the case of such a failure. Problem description =================== When an error occurs during removal of image data from storage, Glance still marks the image as deleted and the image becomes unavailable for use but Glance offers operators no way to reattempt to remove image data from backing storage[1] which leaves those locations orphaned and storage unrecoverable. Existing logging provides operators with details of the errors in removing image data from backing storage but the operator is not able to use Glance to clean up that image data. Proposed change =============== I would like to modify the behavior of Glance so that when a delete image operation is triggered for an image that is already marked deleted that all locations are also inspected before a response is delivered. If any location deletes are outstanding for an image already marked deleted, the deletes would be reattempted. Alternatives ------------ I could revise the definition of the 'pending_delete' image status to include images which failed to remove all locations (like an 'error_deleted' status). In the case where the scrubber was running, it would attempt to clean up those locations left behind when a delete fails in the future. This would not allow operators who upgrade to Kilo from prior releases of Glance to easily clean up orphaned locations. I could also introduce an internal-only 'error_deleted' status which could capture any case in which a delete operation fails to remove all locations. That status could then be set to allow reattempts to delete an image, similar to the proposed change. This 'error_deleted' status would need to be serialized in both V1 and V2 APIs as if it was 'deleted' to prevent a modification of existing API schemas and it would add additional complexity for maintenance of Glance. Also, this would not provide a solution for operators who upgrade to Kilo from prior releases. Lastly, the delete operation could be revised to return a 500 status when location deletion fails for any reason, with the image status and deleted flag changes rolled back in the database rather than committed to ensure that the user knows whether an image delete operation succeeded for all storage locations. This again would not provide a solution for operators who upgrade to Kilo from prior releases. In addition an image cannot be made unavailable reliably if any of the storage locations it is stored on are generating errors which could cause additional hardship to user and operators. Data model impact ----------------- None REST API impact --------------- The only change would be to the criteria under which non-error response codes are returned. Currently, any image which was previously marked as deleted would always return a 404 on delete. Under this proposal, when locations remained to be cleaned up the API would return a 204 instead. This should not present a meaningful API impact. Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- None Performance Impact ------------------ Additional work walking image locations will be required when a delete is requested for an image which is already marked as deleted. This is expected to have a low net impact on db load under anticipated frequency of extra image delete operations. Other deployer impact --------------------- None Developer impact ---------------- None Implementation ============== Assignee(s) ----------- Primary assignee: steve-lewis Other contributors: None Reviewers --------- ::Core reviewer(s): <None> ::Other reviewer(s): <icordasc> Work Items ---------- First, the V1 API image delete function will need to be revised to ensure that all locations are deleted before returning a 'Not Found' response for an already-deleted image. The locations should be filtered to ensure each of them are deleted, or that operation can be reattempted. Finally, the implementation under the V2 API will need to be revised to allow an image in deleted status to re-transition to deleted status, and then the locations should be filtered to ensure each of them are deleted, or that operation can be reattempted. Dependencies ============ None Testing ======= Functional and unit tests only. With no change to the API, no change at the boundary with the database, and no change at the boundary with Glance store, isolated functional testing within the Glance project will be enough. Documentation Impact ==================== New feature needs to be mentioned. References ========== [1] https://bugs.launchpad.net/glance/+bug/1377012 ",,168,0
openstack%2Fopenstacksdk~master~Ic5bf05ea68c5b7ac4f4ae5cec70241254839a3c9,openstack/openstacksdk,master,Ic5bf05ea68c5b7ac4f4ae5cec70241254839a3c9,Remove extra GET call when limit provided to list,MERGED,2014-12-03 15:16:38.000000000,2014-12-05 21:04:02.000000000,2014-12-05 21:04:01.000000000,"[{'_account_id': 3}, {'_account_id': 8257}, {'_account_id': 8736}, {'_account_id': 12000}, {'_account_id': 12807}]","[{'number': 1, 'created': '2014-12-03 15:16:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/015dfb0c24da68cdb91b0a9a99c1561c2b1e641a', 'message': ""Have resource list continue if limit reached\n\nServices like neutron don't seem to support paging correctly.  We\nneed some way to handle paging per services since there are so\nmany variations, but this change will work for everything and it\nwill avoid the unnecessary extra GET call.  The case where the\nvalue limit is greater than the quota will fail, but it should be\nthe users responsibility to use a limit less than or equal to the\nquota.\n\nChange-Id: Ic5bf05ea68c5b7ac4f4ae5cec70241254839a3c9\n""}, {'number': 2, 'created': '2014-12-05 20:15:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/0d6c9a84fdc58433b4a031ea9866cd1693a2177d', 'message': 'Remove extra GET call when limit provided to list\n\nChange-Id: Ic5bf05ea68c5b7ac4f4ae5cec70241254839a3c9\n'}, {'number': 3, 'created': '2014-12-05 20:16:56.000000000', 'files': ['openstack/resource.py', 'openstack/tests/test_resource.py'], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/04122cdebb9d08c8e816db94136956d5ac987317', 'message': 'Remove extra GET call when limit provided to list\n\nChange-Id: Ic5bf05ea68c5b7ac4f4ae5cec70241254839a3c9\n'}]",0,138755,04122cdebb9d08c8e816db94136956d5ac987317,13,5,3,8736,,,0,"Remove extra GET call when limit provided to list

Change-Id: Ic5bf05ea68c5b7ac4f4ae5cec70241254839a3c9
",git fetch https://review.opendev.org/openstack/openstacksdk refs/changes/55/138755/2 && git format-patch -1 --stdout FETCH_HEAD,['openstack/resource.py'],1,015dfb0c24da68cdb91b0a9a99c1561c2b1e641a,resource-list-continue," def list(cls, session, limit=10000, marker=None, path_args=None, **params): rendered = 0 rendered = rendered + 1 if rendered < limit: more_data = False "," def list(cls, session, limit=None, marker=None, path_args=None, **params):",6,1
openstack%2Foslo.utils~master~I888d3e2905755ae7833231363d5dab66799a4a48,openstack/oslo.utils,master,I888d3e2905755ae7833231363d5dab66799a4a48,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:44:03.000000000,2014-12-05 20:55:10.000000000,2014-12-05 20:55:10.000000000,"[{'_account_id': 3}, {'_account_id': 1669}, {'_account_id': 2472}, {'_account_id': 7491}]","[{'number': 1, 'created': '2014-12-05 03:44:03.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/oslo.utils/commit/6741748e771df731d2e8c18193d90a4a1d23a070', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I888d3e2905755ae7833231363d5dab66799a4a48\n'}]",0,139362,6741748e771df731d2e8c18193d90a4a1d23a070,11,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I888d3e2905755ae7833231363d5dab66799a4a48
",git fetch https://review.opendev.org/openstack/oslo.utils refs/changes/62/139362/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,6741748e771df731d2e8c18193d90a4a1d23a070,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Foslo.middleware~master~Iaf8f913453df0d8139a661dc731bbcfcbb95cc36,openstack/oslo.middleware,master,Iaf8f913453df0d8139a661dc731bbcfcbb95cc36,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:43:53.000000000,2014-12-05 20:50:14.000000000,2014-12-05 20:50:14.000000000,"[{'_account_id': 3}, {'_account_id': 1669}, {'_account_id': 2472}, {'_account_id': 6537}]","[{'number': 1, 'created': '2014-12-05 03:43:53.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/oslo.middleware/commit/b25e8c56487fbbb3711aee904f024f5155523571', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Iaf8f913453df0d8139a661dc731bbcfcbb95cc36\n'}]",0,139359,b25e8c56487fbbb3711aee904f024f5155523571,13,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Iaf8f913453df0d8139a661dc731bbcfcbb95cc36
",git fetch https://review.opendev.org/openstack/oslo.middleware refs/changes/59/139359/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,b25e8c56487fbbb3711aee904f024f5155523571,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Foslo.serialization~master~I6a12135e219c9feb4e6a64b451844880cc082888,openstack/oslo.serialization,master,I6a12135e219c9feb4e6a64b451844880cc082888,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:44:00.000000000,2014-12-05 20:47:22.000000000,2014-12-05 20:47:22.000000000,"[{'_account_id': 3}, {'_account_id': 1669}, {'_account_id': 2472}]","[{'number': 1, 'created': '2014-12-05 03:44:00.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/oslo.serialization/commit/93f8876f4002ff6ed579bb4ec8adbaad71bf042a', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I6a12135e219c9feb4e6a64b451844880cc082888\n'}]",0,139361,93f8876f4002ff6ed579bb4ec8adbaad71bf042a,9,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I6a12135e219c9feb4e6a64b451844880cc082888
",git fetch https://review.opendev.org/openstack/oslo.serialization refs/changes/61/139361/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,93f8876f4002ff6ed579bb4ec8adbaad71bf042a,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Fceilometer~master~Ia54c5805fe2d2c6bcd2092b435c5242da6ddad4d,openstack/ceilometer,master,Ia54c5805fe2d2c6bcd2092b435c5242da6ddad4d,Specify version of python for pep8,ABANDONED,2014-11-15 15:32:57.000000000,2014-12-05 20:30:19.000000000,,"[{'_account_id': 3}, {'_account_id': 144}, {'_account_id': 3012}, {'_account_id': 6537}, {'_account_id': 8871}]","[{'number': 1, 'created': '2014-11-15 15:32:57.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/50ca3866166a267f5a0d2cedce0bb03a3b40613d', 'message': 'Specify version of python for pep8\n\nWithout this, the version of python is unspecified, and happybase\nmight give the execfile installation error.\n\nChange-Id: Ia54c5805fe2d2c6bcd2092b435c5242da6ddad4d\n'}]",0,134713,50ca3866166a267f5a0d2cedce0bb03a3b40613d,12,5,1,144,,,0,"Specify version of python for pep8

Without this, the version of python is unspecified, and happybase
might give the execfile installation error.

Change-Id: Ia54c5805fe2d2c6bcd2092b435c5242da6ddad4d
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/13/134713/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,50ca3866166a267f5a0d2cedce0bb03a3b40613d,specify_python_for_pep8,basepython = python2.7,,1,0
openstack%2Fswift~master~I9e5db6da85de4188915c51bc401604733f0e1b77,openstack/swift,master,I9e5db6da85de4188915c51bc401604733f0e1b77,Fix the behavior of swift-ring-builder list_parts before rebalance,MERGED,2014-12-05 06:00:53.000000000,2014-12-05 20:29:48.000000000,2014-12-05 20:29:48.000000000,"[{'_account_id': 3}, {'_account_id': 2622}, {'_account_id': 6968}, {'_account_id': 12193}, {'_account_id': 13052}]","[{'number': 1, 'created': '2014-12-05 06:00:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/09a6e0b87ecbe12b30216e4b4927bd90168f7b25', 'message': 'Fix the behavior of swift-ring-builder list_parts before rebalance\n\nThe swift-ring-builder list_parts before rebalance failed abnormally so\nthis patch fix the behavior. After this patch applies the behavior is\nsuccessful completion with ""No matching devices found"" messages.\n\nCloses-Bug: #1399529\nChange-Id: I9e5db6da85de4188915c51bc401604733f0e1b77\n'}, {'number': 2, 'created': '2014-12-05 06:09:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/bfc31794cc8f4ba57d2d1c49b2a91bd20d7424a8', 'message': 'Fix the behavior of swift-ring-builder list_parts before rebalance\n\nThe swift-ring-builder list_parts before rebalance failed abnormally so\nthis patch fix the behavior. After this patch applies the behavior is\ncompletion normally with ""No matching devices found"" messages.\n\nCloses-Bug: #1399529\nChange-Id: I9e5db6da85de4188915c51bc401604733f0e1b77\n'}, {'number': 3, 'created': '2014-12-05 08:52:53.000000000', 'files': ['swift/cli/ringbuilder.py', 'test/unit/cli/test_ringbuilder.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/d742b610df8b9edac97506f165993267db37af3e', 'message': 'Fix the behavior of swift-ring-builder list_parts before rebalance\n\nThe swift-ring-builder list_parts before rebalance failed abnormally so\nthis patch fix the behavior. After this patch applies the behavior is\ncompletion normally with the following messages.\n\nSpecified builder file ""<builder_file>"" is not rebalanced yet.\nPlease rebalance first.\n\nCloses-Bug: #1399529\nChange-Id: I9e5db6da85de4188915c51bc401604733f0e1b77\n'}]",2,139523,d742b610df8b9edac97506f165993267db37af3e,15,5,3,12193,,,0,"Fix the behavior of swift-ring-builder list_parts before rebalance

The swift-ring-builder list_parts before rebalance failed abnormally so
this patch fix the behavior. After this patch applies the behavior is
completion normally with the following messages.

Specified builder file ""<builder_file>"" is not rebalanced yet.
Please rebalance first.

Closes-Bug: #1399529
Change-Id: I9e5db6da85de4188915c51bc401604733f0e1b77
",git fetch https://review.opendev.org/openstack/swift refs/changes/23/139523/3 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/common/ring/test_utils.py', 'swift/common/ring/utils.py']",2,09a6e0b87ecbe12b30216e4b4927bd90168f7b25,bug/1399529, if not devs or not builder._replica2part2dev:, if not devs:,15,1
openstack%2Foslo.context~master~Ief329a52c5d76089b528ddf2f5e6de5e38702bec,openstack/oslo.context,master,Ief329a52c5d76089b528ddf2f5e6de5e38702bec,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:43:36.000000000,2014-12-05 20:27:35.000000000,2014-12-05 20:27:35.000000000,"[{'_account_id': 3}, {'_account_id': 2472}]","[{'number': 1, 'created': '2014-12-05 03:43:36.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/oslo.context/commit/c4384f88ccf1f39687e704cf808ad53b06bd8394', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Ief329a52c5d76089b528ddf2f5e6de5e38702bec\n'}]",0,139354,c4384f88ccf1f39687e704cf808ad53b06bd8394,6,2,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Ief329a52c5d76089b528ddf2f5e6de5e38702bec
",git fetch https://review.opendev.org/openstack/oslo.context refs/changes/54/139354/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,c4384f88ccf1f39687e704cf808ad53b06bd8394,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Foslo.vmware~stable%2Fjuno~I116e92f8942fe9fd353ccaaca0f5a2ea67b448fd,openstack/oslo.vmware,stable/juno,I116e92f8942fe9fd353ccaaca0f5a2ea67b448fd,Updated from global requirements,MERGED,2014-11-27 10:37:55.000000000,2014-12-05 20:26:37.000000000,2014-12-05 20:26:36.000000000,"[{'_account_id': 3}, {'_account_id': 979}, {'_account_id': 1653}, {'_account_id': 1955}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 6786}, {'_account_id': 8759}, {'_account_id': 9171}]","[{'number': 1, 'created': '2014-11-27 10:37:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/77852b46813afa27cd25a8adc27619786ff05a52', 'message': 'Updated from global requirements\n\nChange-Id: I116e92f8942fe9fd353ccaaca0f5a2ea67b448fd\n'}, {'number': 2, 'created': '2014-12-03 21:02:49.000000000', 'files': ['requirements.txt', 'test-requirements.txt'], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/e529e02d20851880e91a738af8592b7f2b64bee2', 'message': 'Updated from global requirements\n\nChange-Id: I116e92f8942fe9fd353ccaaca0f5a2ea67b448fd\n'}]",1,137588,e529e02d20851880e91a738af8592b7f2b64bee2,18,9,2,11131,,,0,"Updated from global requirements

Change-Id: I116e92f8942fe9fd353ccaaca0f5a2ea67b448fd
",git fetch https://review.opendev.org/openstack/oslo.vmware refs/changes/88/137588/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'test-requirements.txt']",2,77852b46813afa27cd25a8adc27619786ff05a52,openstack/requirements,"testtools>=0.9.34,!=1.4.0sphinx>=1.1.2,!=1.2.0,<1.3","testtools>=0.9.36sphinx>=1.1.2,!=1.2.0,!=1.3b1,<1.3",7,7
openstack%2Foslo.vmware~master~Ic52ae9d1d41964dd0e535aa0569df2a7c958f105,openstack/oslo.vmware,master,Ic52ae9d1d41964dd0e535aa0569df2a7c958f105,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:44:10.000000000,2014-12-05 20:26:31.000000000,2014-12-05 20:26:30.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 9008}, {'_account_id': 9171}]","[{'number': 1, 'created': '2014-12-05 03:44:10.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/oslo.vmware/commit/3f7a04ecf91919b6d3846017be5cd7eaf677ee87', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Ic52ae9d1d41964dd0e535aa0569df2a7c958f105\n'}]",0,139364,3f7a04ecf91919b6d3846017be5cd7eaf677ee87,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Ic52ae9d1d41964dd0e535aa0569df2a7c958f105
",git fetch https://review.opendev.org/openstack/oslo.vmware refs/changes/64/139364/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,3f7a04ecf91919b6d3846017be5cd7eaf677ee87,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Ftaskflow~master~I29028bae8800e0e5cc4e3c0ec56756f82f0e6fe3,openstack/taskflow,master,I29028bae8800e0e5cc4e3c0ec56756f82f0e6fe3,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:52:29.000000000,2014-12-05 20:25:14.000000000,2014-12-05 20:25:13.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 2472}]","[{'number': 1, 'created': '2014-12-05 03:52:29.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/2033d011e3cdc320a829ded3288c8d6b7f34aaff', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I29028bae8800e0e5cc4e3c0ec56756f82f0e6fe3\n'}]",0,139395,2033d011e3cdc320a829ded3288c8d6b7f34aaff,8,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I29028bae8800e0e5cc4e3c0ec56756f82f0e6fe3
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/95/139395/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,2033d011e3cdc320a829ded3288c8d6b7f34aaff,infra-manual, http://docs.openstack.org/infra/manual/developers.html#development-workflow http://docs.openstack.org/infra/manual/developers.html#development-workflow, http://wiki.openstack.org/HowToContribute#If_you.27re_a_developer http://wiki.openstack.org/GerritWorkflow,2,2
openstack%2Ftripleo-image-elements~master~I48e0867ef597f637d29585cf666bef05f65c3c43,openstack/tripleo-image-elements,master,I48e0867ef597f637d29585cf666bef05f65c3c43,Fix some hardcoded places in deploy-ci-overcloud,MERGED,2014-11-12 04:15:52.000000000,2014-12-05 20:25:06.000000000,2014-12-05 20:25:06.000000000,"[{'_account_id': 3}, {'_account_id': 360}, {'_account_id': 1926}, {'_account_id': 6488}, {'_account_id': 7471}, {'_account_id': 9369}, {'_account_id': 9453}, {'_account_id': 12459}, {'_account_id': 13762}]","[{'number': 1, 'created': '2014-11-12 04:15:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/5303912444a18db4a93f8a59a996807fc1a491bf', 'message': 'Fix some hardcoded places in deploy-ci-overcloud\n\nChange-Id: I48e0867ef597f637d29585cf666bef05f65c3c43\n'}, {'number': 2, 'created': '2014-11-14 18:37:13.000000000', 'files': ['elements/tripleo-cd/deploy-ci-overcloud'], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/afba7db0efdd52c8a92a73a028160abbd9396d28', 'message': 'Fix some hardcoded places in deploy-ci-overcloud\n\nChange-Id: I48e0867ef597f637d29585cf666bef05f65c3c43\n'}]",1,133882,afba7db0efdd52c8a92a73a028160abbd9396d28,17,9,2,6671,,,0,"Fix some hardcoded places in deploy-ci-overcloud

Change-Id: I48e0867ef597f637d29585cf666bef05f65c3c43
",git fetch https://review.opendev.org/openstack/tripleo-image-elements refs/changes/82/133882/1 && git format-patch -1 --stdout FETCH_HEAD,['elements/tripleo-cd/deploy-ci-overcloud'],1,5303912444a18db4a93f8a59a996807fc1a491bf,fixhardcodedpara,"export OVERCLOUD_PUBLIC_VIRTUAL_INTERFACE=$(jq -r .NeutronPublicInterface $CI_ENV_FILE) devtest_overcloud.sh ""${OVERCLOUD_PUBLIC_VIRTUAL_INTERFACE}"" """" """" """" \ ""${OVERCLOUD_PUBLIC_VLAN_START}"" ""${OVERCLOUD_PUBLIC_VLAN_FINISH}"" \ ""${OVERCLOUD_PUBLIC_VLAN_CIDR}"" \ $TRIPLEO_ROOT/tripleo-image-elements/elements/tripleo-cd/configs/te_${REGION_CODE}rcMSG=$(echo ""************** $STACKNAME $REGION_CODE complete status=$RESULT ************"")"," devtest_overcloud.sh eth2 """" """" """" ""${OVERCLOUD_PUBLIC_VLAN_START}"" \ ""${OVERCLOUD_PUBLIC_VLAN_FINISH}"" ""${OVERCLOUD_PUBLIC_VLAN_CIDR}"" \ $TRIPLEO_ROOT/tripleo-image-elements/elements/tripleo-cd/configs/te_hp1rcMSG=$(echo ""************** $STACKNAME complete status=$RESULT ************"")",6,4
openstack%2Fgrenade~master~I7c8474bd46bad4a8418c6947ea63b4e1ffb1f12e,openstack/grenade,master,I7c8474bd46bad4a8418c6947ea63b4e1ffb1f12e,remove sourcing of nova baremetal driver,MERGED,2014-12-05 15:53:56.000000000,2014-12-05 20:24:53.000000000,2014-12-05 20:24:52.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 1420}, {'_account_id': 1849}]","[{'number': 1, 'created': '2014-12-05 15:53:56.000000000', 'files': ['upgrade-nova', 'upgrade-tempest', 'upgrade-ironic'], 'web_link': 'https://opendev.org/openstack/grenade/commit/180843324dbc29cb76958192ea6ec0837144e8f4', 'message': 'remove sourcing of nova baremetal driver\n\nnova baremetal driver is removed, remove references in grenade so we\ncan pull them from devstack.\n\nChange-Id: I7c8474bd46bad4a8418c6947ea63b4e1ffb1f12e\n'}]",0,139666,180843324dbc29cb76958192ea6ec0837144e8f4,9,4,1,2750,,,0,"remove sourcing of nova baremetal driver

nova baremetal driver is removed, remove references in grenade so we
can pull them from devstack.

Change-Id: I7c8474bd46bad4a8418c6947ea63b4e1ffb1f12e
",git fetch https://review.opendev.org/openstack/grenade refs/changes/66/139666/1 && git format-patch -1 --stdout FETCH_HEAD,"['upgrade-nova', 'upgrade-tempest', 'upgrade-ironic']",3,180843324dbc29cb76958192ea6ec0837144e8f4,remove_bm,,source $TARGET_DEVSTACK_DIR/lib/baremetal,0,3
openstack%2Fcliff~master~I04f0a52fb3f9f166d1eb505ff1736eee896a2558,openstack/cliff,master,I04f0a52fb3f9f166d1eb505ff1736eee896a2558,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:40:56.000000000,2014-12-05 20:23:15.000000000,2014-12-05 20:23:14.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 8736}]","[{'number': 1, 'created': '2014-12-05 03:40:56.000000000', 'files': ['doc/source/developers.rst', 'CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/cliff/commit/b6294db1473838217e077149c868195333287563', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I04f0a52fb3f9f166d1eb505ff1736eee896a2558\n'}]",0,139316,b6294db1473838217e077149c868195333287563,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I04f0a52fb3f9f166d1eb505ff1736eee896a2558
",git fetch https://review.opendev.org/openstack/cliff refs/changes/16/139316/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'doc/source/developers.rst']",2,b6294db1473838217e077149c868195333287563,infra-manual,http://docs.openstack.org/infra/manual/developers.html#development-workflow,http://wiki.openstack.org/GerritWorkflow,2,2
openstack%2Ftripleo-image-elements~master~Ic9dfe88db85fae95e279ab23b31d55be3c3a1233,openstack/tripleo-image-elements,master,Ic9dfe88db85fae95e279ab23b31d55be3c3a1233,Update Ansible version to 1.8.1,MERGED,2014-12-02 00:25:38.000000000,2014-12-05 20:22:15.000000000,2014-12-05 20:22:14.000000000,"[{'_account_id': 3}, {'_account_id': 360}, {'_account_id': 6449}, {'_account_id': 8399}, {'_account_id': 9369}]","[{'number': 1, 'created': '2014-12-02 00:25:38.000000000', 'files': ['elements/ansible/install.d/ansible-source-install/30-ansible-source'], 'web_link': 'https://opendev.org/openstack/tripleo-image-elements/commit/50de293676516beb67e25c44f543c8cde5af6b7e', 'message': 'Update Ansible version to 1.8.1\n\nUpdating the version of Ansible to the current available version\nin order to obtain updated Ansible modules with numerous bug fixes.\n\nChange-Id: Ic9dfe88db85fae95e279ab23b31d55be3c3a1233\n'}]",0,138213,50de293676516beb67e25c44f543c8cde5af6b7e,15,5,1,11655,,,0,"Update Ansible version to 1.8.1

Updating the version of Ansible to the current available version
in order to obtain updated Ansible modules with numerous bug fixes.

Change-Id: Ic9dfe88db85fae95e279ab23b31d55be3c3a1233
",git fetch https://review.opendev.org/openstack/tripleo-image-elements refs/changes/13/138213/1 && git format-patch -1 --stdout FETCH_HEAD,['elements/ansible/install.d/ansible-source-install/30-ansible-source'],1,50de293676516beb67e25c44f543c8cde5af6b7e,fix/ansible-version,$ANSIBLE_VENV_DIR/bin/pip install ansible==1.8.1,$ANSIBLE_VENV_DIR/bin/pip install ansible==1.7.0,1,1
openstack%2Foslo.log~master~I6701205bf33e4632dff2ba721c3106ede87e368e,openstack/oslo.log,master,I6701205bf33e4632dff2ba721c3106ede87e368e,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:43:46.000000000,2014-12-05 20:21:54.000000000,2014-12-05 20:21:54.000000000,"[{'_account_id': 3}, {'_account_id': 2472}]","[{'number': 1, 'created': '2014-12-05 03:43:46.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/oslo.log/commit/084e27e406f1088489b614e5eeb229edff5144b9', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I6701205bf33e4632dff2ba721c3106ede87e368e\n'}]",0,139357,084e27e406f1088489b614e5eeb229edff5144b9,6,2,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I6701205bf33e4632dff2ba721c3106ede87e368e
",git fetch https://review.opendev.org/openstack/oslo.log refs/changes/57/139357/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,084e27e406f1088489b614e5eeb229edff5144b9,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Foslo.i18n~master~I1121751e84a99c7a97beb8ab122494558a7fb71b,openstack/oslo.i18n,master,I1121751e84a99c7a97beb8ab122494558a7fb71b,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:43:43.000000000,2014-12-05 20:20:49.000000000,2014-12-05 20:20:48.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 6601}]","[{'number': 1, 'created': '2014-12-05 03:43:43.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/oslo.i18n/commit/c86629ee47bcd9fbbec46ca44ce036f41979eaea', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I1121751e84a99c7a97beb8ab122494558a7fb71b\n'}]",0,139356,c86629ee47bcd9fbbec46ca44ce036f41979eaea,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I1121751e84a99c7a97beb8ab122494558a7fb71b
",git fetch https://review.opendev.org/openstack/oslo.i18n refs/changes/56/139356/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,c86629ee47bcd9fbbec46ca44ce036f41979eaea,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Foslo.rootwrap~master~I9e7c04ee8d5f0ada64bd49a07cdfe4bc3c12086b,openstack/oslo.rootwrap,master,I9e7c04ee8d5f0ada64bd49a07cdfe4bc3c12086b,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:43:57.000000000,2014-12-05 20:18:43.000000000,2014-12-05 20:18:43.000000000,"[{'_account_id': 3}, {'_account_id': 308}, {'_account_id': 2472}]","[{'number': 1, 'created': '2014-12-05 03:43:57.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/oslo.rootwrap/commit/44aa91f133fecc47a964374a32a33f22ad085b56', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I9e7c04ee8d5f0ada64bd49a07cdfe4bc3c12086b\n'}]",0,139360,44aa91f133fecc47a964374a32a33f22ad085b56,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I9e7c04ee8d5f0ada64bd49a07cdfe4bc3c12086b
",git fetch https://review.opendev.org/openstack/oslo.rootwrap refs/changes/60/139360/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,44aa91f133fecc47a964374a32a33f22ad085b56,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Fpython-solumclient~master~I16f6fb7d7a79fc82a469b4d71d1aa4b9b2ca5bc8,openstack/python-solumclient,master,I16f6fb7d7a79fc82a469b4d71d1aa4b9b2ca5bc8,Allow overriding solum endpoint defined in the service catalog.,MERGED,2014-12-04 23:31:25.000000000,2014-12-05 20:14:00.000000000,2014-12-05 20:14:00.000000000,"[{'_account_id': 3}, {'_account_id': 668}, {'_account_id': 2506}, {'_account_id': 9095}]","[{'number': 1, 'created': '2014-12-04 23:31:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/5078daf7779e20f81e770268e895243b7672804f', 'message': 'Allow user to provide only SOLUM_URL to the CLI\n\nCurrently the CLI does Auth in 2 ways\n\n1) User provides username and password. CLI authenticates against\nkeystone and gets the Solum endpoint from the service catalog.\n2) User provided Auth_Token and Solum_Url. CLI does not authenticate\nagainst keystone in this case. It talks to the the solum endpoint\nprovided and passes the auth_token provided by the user.\n\nI’m adding a new option.\n\n3) User provides Username, Password and Solum endpoint. In this case,\nthe CLI authenticates the user  and gets a auth token from keystone,\nbut uses the solum endpoint provided instead of the one in the service\ncatalog.\n\nChange-Id: I16f6fb7d7a79fc82a469b4d71d1aa4b9b2ca5bc8\n'}, {'number': 2, 'created': '2014-12-04 23:33:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/1375e07da7b504629b38230c9c0d6b5c7ea31eeb', 'message': 'Allow user to provide only SOLUM_URL to the CLI\n\nCurrently the CLI does Auth in 2 ways\n\n1) User provides username and password. CLI authenticates against\nkeystone and gets the Solum endpoint from the service catalog.\n2) User provided Auth_Token and Solum_Url. CLI does not authenticate\nagainst keystone in this case. It talks to the the solum endpoint\nprovided and passes the auth_token provided by the user.\n\nI’m adding a new option.\n\n3) User provides Username, Password and Solum endpoint. In this case,\nthe CLI authenticates the user and gets an auth token from keystone,\nbut uses the solum endpoint provided instead of the one in the service\ncatalog.\n\nChange-Id: I16f6fb7d7a79fc82a469b4d71d1aa4b9b2ca5bc8\n'}, {'number': 3, 'created': '2014-12-05 17:55:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/44bb3959cc8a13d33b65e9fd102924c58a63890f', 'message': 'Allow overriding solum endpoint defined in the service catalog\nwhen provided by user.\n\nWhen authenticating against keystone, the service catalog\nreturned may or may not contain an endpoint for solum url.\nIt should be possible for users to override the solum endpoint\nurl when an environment variable is set (SOLUM_URL).\n\nChange-Id: I16f6fb7d7a79fc82a469b4d71d1aa4b9b2ca5bc8\n'}, {'number': 4, 'created': '2014-12-05 18:04:38.000000000', 'files': ['solumclient/tests/common/test_cliutils.py', 'solumclient/tests/common/test_auth.py', 'solumclient/common/cli_utils.py'], 'web_link': 'https://opendev.org/openstack/python-solumclient/commit/86e732d2c688a1b8ad5b1fdbc245667cc96859bd', 'message': 'Allow overriding solum endpoint defined in the service catalog.\n\nWhen authenticating against keystone, the service catalog\nreturned may or may not contain an endpoint for solum url.\nIt should be possible for users to override the solum endpoint\nurl when an environment variable is set (SOLUM_URL).\n\nChange-Id: I16f6fb7d7a79fc82a469b4d71d1aa4b9b2ca5bc8\n'}]",1,139266,86e732d2c688a1b8ad5b1fdbc245667cc96859bd,15,4,4,9095,,,0,"Allow overriding solum endpoint defined in the service catalog.

When authenticating against keystone, the service catalog
returned may or may not contain an endpoint for solum url.
It should be possible for users to override the solum endpoint
url when an environment variable is set (SOLUM_URL).

Change-Id: I16f6fb7d7a79fc82a469b4d71d1aa4b9b2ca5bc8
",git fetch https://review.opendev.org/openstack/python-solumclient refs/changes/66/139266/2 && git format-patch -1 --stdout FETCH_HEAD,"['solumclient/tests/common/test_cliutils.py', 'solumclient/tests/common/test_auth.py', 'solumclient/common/cli_utils.py']",3,5078daf7779e20f81e770268e895243b7672804f,, if not parsed.os_auth_token:, if not (parsed.os_auth_token and parsed.solum_url):,39,4
openstack%2Fopenstack-manuals~stable%2Fjuno~Ifaf92418552396c992764cb55696da30dd654853,openstack/openstack-manuals,stable/juno,Ifaf92418552396c992764cb55696da30dd654853,Linked Neutron overview content in the Installation Guide,MERGED,2014-12-05 18:31:17.000000000,2014-12-05 20:11:24.000000000,2014-12-05 20:11:23.000000000,"[{'_account_id': 3}, {'_account_id': 6547}, {'_account_id': 6772}, {'_account_id': 10705}, {'_account_id': 14046}]","[{'number': 1, 'created': '2014-12-05 18:31:17.000000000', 'files': ['doc/install-guide/ch_networking.xml'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/f0f0a3f7a8618fdfa0cc0a4a7a746833b4621565', 'message': ""Linked Neutron overview content in the Installation Guide\n\n'xi:include parse' the  neutron overview content in the OpenStack Networking section\n\nbackport: Juno\nCloses-Bug: #1340524\n\nChange-Id: Ifaf92418552396c992764cb55696da30dd654853\n(cherry picked from commit 25c96dd7e552de26b80343066e5cc32a9f5fce50)\n""}]",0,139702,f0f0a3f7a8618fdfa0cc0a4a7a746833b4621565,8,5,1,9515,,,0,"Linked Neutron overview content in the Installation Guide

'xi:include parse' the  neutron overview content in the OpenStack Networking section

backport: Juno
Closes-Bug: #1340524

Change-Id: Ifaf92418552396c992764cb55696da30dd654853
(cherry picked from commit 25c96dd7e552de26b80343066e5cc32a9f5fce50)
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/02/139702/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/install-guide/ch_networking.xml'],1,f0f0a3f7a8618fdfa0cc0a4a7a746833b4621565,juno/backport/139538," <xi:include parse=""xml"" href=""../common/section_getstart_networking.xml""/>",,1,0
openstack%2Foslo-incubator~stable%2Fjuno~Idff5239d264bc44923e4091e86203127db74ebd0,openstack/oslo-incubator,stable/juno,Idff5239d264bc44923e4091e86203127db74ebd0,Updated from global requirements,MERGED,2014-12-03 21:02:33.000000000,2014-12-05 20:11:06.000000000,2014-12-05 20:11:05.000000000,"[{'_account_id': 3}, {'_account_id': 1669}, {'_account_id': 2472}]","[{'number': 1, 'created': '2014-12-03 21:02:33.000000000', 'files': ['requirements.txt', 'test-requirements.txt', 'requirements-py3.txt', 'test-requirements-py3.txt'], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/66a2c0561804a1b5ec1690b60108f6ea1fbf5663', 'message': 'Updated from global requirements\n\nChange-Id: Idff5239d264bc44923e4091e86203127db74ebd0\n'}]",0,138860,66a2c0561804a1b5ec1690b60108f6ea1fbf5663,7,3,1,11131,,,0,"Updated from global requirements

Change-Id: Idff5239d264bc44923e4091e86203127db74ebd0
",git fetch https://review.opendev.org/openstack/oslo-incubator refs/changes/60/138860/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'test-requirements.txt', 'requirements-py3.txt', 'test-requirements-py3.txt']",4,66a2c0561804a1b5ec1690b60108f6ea1fbf5663,openstack/requirements,oslotest>=1.1.0 # Apache-2.0,"oslotest>=1.1.0,<1.4 # Apache-2.0",6,6
openstack%2Fpython-saharaclient~master~Ibb9ff0f82137677827bbe07a7450e21d1bbd52e0,openstack/python-saharaclient,master,Ibb9ff0f82137677827bbe07a7450e21d1bbd52e0,Remove cluster tmpl update function,ABANDONED,2014-11-24 10:02:10.000000000,2014-12-05 20:11:01.000000000,,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 8090}, {'_account_id': 8411}, {'_account_id': 12039}]","[{'number': 1, 'created': '2014-11-24 10:02:10.000000000', 'files': ['saharaclient/api/cluster_templates.py'], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/79965235d6bead3ae87bd112f2662ae9f8197aba', 'message': ""Remove cluster tmpl update function\n\nSahara doesn't support this feature on server side, it was merged\nin time when server side were really near to be done:\nIc9f3c685c5d1e43d9d3331abf5bae5e3a932db1d\n\nIt should be added back only when the server side feature will be\nmerged to the sahara.\n\nChange-Id: Ibb9ff0f82137677827bbe07a7450e21d1bbd52e0\n""}]",0,136727,79965235d6bead3ae87bd112f2662ae9f8197aba,778,6,1,6786,,,0,"Remove cluster tmpl update function

Sahara doesn't support this feature on server side, it was merged
in time when server side were really near to be done:
Ic9f3c685c5d1e43d9d3331abf5bae5e3a932db1d

It should be added back only when the server side feature will be
merged to the sahara.

Change-Id: Ibb9ff0f82137677827bbe07a7450e21d1bbd52e0
",git fetch https://review.opendev.org/openstack/python-saharaclient refs/changes/27/136727/1 && git format-patch -1 --stdout FETCH_HEAD,['saharaclient/api/cluster_templates.py'],1,79965235d6bead3ae87bd112f2662ae9f8197aba,cl-tmpl-upd,," def update(self, cluster_template_id, name, plugin_name, hadoop_version, description=None, cluster_configs=None, node_groups=None, anti_affinity=None, net_id=None, default_image_id=None): data = self._assign_field(name, plugin_name, hadoop_version, description, cluster_configs, node_groups, anti_affinity, net_id, default_image_id) return self._update('/cluster-templates/%s' % cluster_template_id, data, 'cluster_template') ",0,10
openstack%2Fcinder~master~Iebccf7b8f252303f586b36aad33b85945ea5c927,openstack/cinder,master,Iebccf7b8f252303f586b36aad33b85945ea5c927,Brick LVM: LV not found logging and error handling,MERGED,2014-11-21 17:57:10.000000000,2014-12-05 20:01:28.000000000,2014-11-25 22:37:48.000000000,"[{'_account_id': 3}, {'_account_id': 1207}, {'_account_id': 4523}, {'_account_id': 6491}, {'_account_id': 6763}, {'_account_id': 7198}, {'_account_id': 7219}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10621}, {'_account_id': 11737}, {'_account_id': 11811}, {'_account_id': 12016}, {'_account_id': 12369}, {'_account_id': 12780}]","[{'number': 1, 'created': '2014-11-21 17:57:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/38415643ac2db49fa90ffe499762be9483344fcb', 'message': 'Brick LVM: LV not found logging and error handling\n\nFix up two things in get_lv_info/get_volume:\n1. ProcessExecutionError should be handled where we call the\n   command.  We can just return an empty list for this case\n   which makes things simple for callers and consistent with\n   querying a VG.\n2. Not found errors should be logged as info and not warning\n   since this is generally not actionable by the admin (and not\n   a problem).\n\nFix typo in lvm command output for not found test.\n\nAdd test for get_lv_info not found error.\n\nChange-Id: Iebccf7b8f252303f586b36aad33b85945ea5c927\nRelated-Bug: #1390081\n'}, {'number': 2, 'created': '2014-11-25 13:59:30.000000000', 'files': ['cinder/brick/local_dev/lvm.py', 'cinder/tests/brick/test_brick_lvm.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/8a08a6cdd3688e799ae2db4bd94b8b14dc50f2a9', 'message': 'Brick LVM: LV not found logging and error handling\n\nFix up two things in get_lv_info/get_volume:\n1. ProcessExecutionError should be handled where we call the\n   command.  We can just return an empty list for this case\n   which makes things simple for callers and consistent with\n   querying a VG.\n2. Not found errors should be logged as info and not warning\n   since this is generally not actionable by the admin (and not\n   a problem).\n\nFix typo in lvm command output for not found test.\n\nAdd test for get_lv_info not found error.\n\nChange-Id: Iebccf7b8f252303f586b36aad33b85945ea5c927\nRelated-Bug: #1390081\n'}]",12,136438,8a08a6cdd3688e799ae2db4bd94b8b14dc50f2a9,39,15,2,4523,,,0,"Brick LVM: LV not found logging and error handling

Fix up two things in get_lv_info/get_volume:
1. ProcessExecutionError should be handled where we call the
   command.  We can just return an empty list for this case
   which makes things simple for callers and consistent with
   querying a VG.
2. Not found errors should be logged as info and not warning
   since this is generally not actionable by the admin (and not
   a problem).

Fix typo in lvm command output for not found test.

Add test for get_lv_info not found error.

Change-Id: Iebccf7b8f252303f586b36aad33b85945ea5c927
Related-Bug: #1390081
",git fetch https://review.opendev.org/openstack/cinder refs/changes/38/136438/2 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/brick/local_dev/lvm.py', 'cinder/tests/brick/test_brick_lvm.py']",2,38415643ac2db49fa90ffe499762be9483344fcb,lvm_fixup," '--unit=g, -o, vg_name,name,size, --nosuffix, ' 'fake-vg/lv-nothere' in cmd_string): raise processutils.ProcessExecutionError( stderr=""One or more specified logical volume(s) not found."") elif ('env, LC_ALL=C, lvs, --noheadings, ' stderr=""One or more volume(s) not found."" def test_get_lv_info_notfound(self): self.assertEqual( self.vg.get_lv_info( 'sudo', vg_name='fake-vg', lv_name='lv-nothere'), [] ) "," stderr=""One of more volume(s) not found.""",32,20
openstack%2Fnova-specs~master~I31fa0b13e400dd202f5db3e63da7b7c8dbd40e68,openstack/nova-specs,master,I31fa0b13e400dd202f5db3e63da7b7c8dbd40e68,Separate kilo and juno specs,MERGED,2014-11-11 11:50:37.000000000,2014-12-05 20:01:09.000000000,2014-12-05 20:01:08.000000000,"[{'_account_id': 3}, {'_account_id': 782}, {'_account_id': 1779}, {'_account_id': 1849}]","[{'number': 1, 'created': '2014-11-11 11:50:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/69e46e3baef534d792521a24b7987edcfeaf9c0a', 'message': 'Separate kilo and juno specs, add backlog\n\nNeed to split out the different releases so the index page will continue\nto scale past the current two releases.\n\nAt the same time added a placeholder for the backlog specs.\n\nIt required some rework how the specs are symlinked into the doc source\nto allow for the release specific landing pages.\n\nChange-Id: I31fa0b13e400dd202f5db3e63da7b7c8dbd40e68\n'}, {'number': 2, 'created': '2014-11-12 12:18:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/a48a009e566cd518336a4a79cecb5ce203079ad7', 'message': 'Separate kilo and juno specs\n\nNeed to split out the different releases so the index page will continue\nto scale past the current two releases.\n\nIt required some rework how the specs are symlinked into the doc source\nto allow for the release specific landing pages.\n\nChange-Id: I31fa0b13e400dd202f5db3e63da7b7c8dbd40e68\n'}, {'number': 3, 'created': '2014-12-05 19:34:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/44449d340747df4a656c9328675b74d19a12a719', 'message': 'Separate kilo and juno specs\n\nNeed to split out the different releases so the index page will continue\nto scale past the current two releases.\n\nIt required some rework how the specs are symlinked into the doc source\nto allow for the release specific landing pages.\n\nChange-Id: I31fa0b13e400dd202f5db3e63da7b7c8dbd40e68\n'}, {'number': 4, 'created': '2014-12-05 19:46:47.000000000', 'files': ['doc/source/specs/kilo/approved', 'doc/source/index.rst', 'doc/source/specs', 'doc/source/specs/juno/index.rst', 'doc/source/specs/kilo/template.rst', 'doc/source/specs/kilo/index.rst', 'doc/source/conf.py', 'doc/source/specs/juno/redirects', 'doc/source/specs/juno/approved', 'doc/source/specs/juno/implemented', 'doc/source/specs/juno/template.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/297babe3671d20a365be8a3fa77975be29dafe5d', 'message': 'Separate kilo and juno specs\n\nNeed to split out the different releases so the index page will continue\nto scale past the current two releases.\n\nIt required some rework how the specs are symlinked into the doc source\nto allow for the release specific landing pages.\n\nChange-Id: I31fa0b13e400dd202f5db3e63da7b7c8dbd40e68\n'}]",3,133658,297babe3671d20a365be8a3fa77975be29dafe5d,25,4,4,782,,,0,"Separate kilo and juno specs

Need to split out the different releases so the index page will continue
to scale past the current two releases.

It required some rework how the specs are symlinked into the doc source
to allow for the release specific landing pages.

Change-Id: I31fa0b13e400dd202f5db3e63da7b7c8dbd40e68
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/58/133658/4 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/specs/backlog/index.rst', 'doc/source/specs/kilo/approved', 'doc/source/specs', 'doc/source/specs/juno/index.rst', 'doc/source/specs/kilo/template.rst', 'doc/source/conf.py', 'doc/source/specs/juno/redirects', 'doc/source/specs/juno/approved', 'doc/source/specs/juno/implemented', 'doc/source/specs/juno/template.rst', 'doc/source/index.rst', 'doc/source/specs/kilo/index.rst']",12,69e46e3baef534d792521a24b7987edcfeaf9c0a,(detached,=========================== Nova Kilo Specifications =========================== Template: .. toctree:: :maxdepth: 1 Specification Template (Kilo) <template> Kilo approved (but not implemented) specs: .. toctree:: :glob: :maxdepth: 1 approved/* ,,91,25
openstack%2Foslo-cookiecutter~master~Idaeb9e96cf5e07afaf8918e1b3efae5ab623172d,openstack/oslo-cookiecutter,master,Idaeb9e96cf5e07afaf8918e1b3efae5ab623172d,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:53:27.000000000,2014-12-05 19:58:51.000000000,2014-12-05 19:58:50.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5196}]","[{'number': 1, 'created': '2014-12-05 03:53:27.000000000', 'files': ['CONTRIBUTING.rst', 'oslo.{{cookiecutter.module_name}}/CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/oslo-cookiecutter/commit/157e2e5c210c3f679dd1820c5f92d46e76f7a2de', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Idaeb9e96cf5e07afaf8918e1b3efae5ab623172d\n'}]",0,139411,157e2e5c210c3f679dd1820c5f92d46e76f7a2de,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Idaeb9e96cf5e07afaf8918e1b3efae5ab623172d
",git fetch https://review.opendev.org/openstack/oslo-cookiecutter refs/changes/11/139411/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'oslo.{{cookiecutter.module_name}}/CONTRIBUTING.rst']",2,157e2e5c210c3f679dd1820c5f92d46e76f7a2de,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",6,8
openstack%2Fcookiecutter~master~I00283a2391628917deeb12afb8fa093dbf38b53a,openstack/cookiecutter,master,I00283a2391628917deeb12afb8fa093dbf38b53a,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:53:12.000000000,2014-12-05 19:58:45.000000000,2014-12-05 19:58:45.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5196}]","[{'number': 1, 'created': '2014-12-05 03:53:12.000000000', 'files': ['CONTRIBUTING.rst', '{{cookiecutter.repo_name}}/CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/cookiecutter/commit/b05bdac17065e9fc3588dfa032cfcf179a372f5d', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I00283a2391628917deeb12afb8fa093dbf38b53a\n'}]",0,139407,b05bdac17065e9fc3588dfa032cfcf179a372f5d,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I00283a2391628917deeb12afb8fa093dbf38b53a
",git fetch https://review.opendev.org/openstack/cookiecutter refs/changes/07/139407/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', '{{cookiecutter.repo_name}}/CONTRIBUTING.rst']",2,b05bdac17065e9fc3588dfa032cfcf179a372f5d,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",6,8
openstack%2Fspecs-cookiecutter~master~I126eb57446ca3655e8f7da18a19c16240c4efae4,openstack/specs-cookiecutter,master,I126eb57446ca3655e8f7da18a19c16240c4efae4,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:53:34.000000000,2014-12-05 19:58:11.000000000,2014-12-05 19:58:11.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5196}]","[{'number': 1, 'created': '2014-12-05 03:53:34.000000000', 'files': ['CONTRIBUTING.rst', '{{cookiecutter.repo_name}}/CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/specs-cookiecutter/commit/c3d0b3f37ed12c5320c15e433af8721f1ff3594f', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I126eb57446ca3655e8f7da18a19c16240c4efae4\n'}]",0,139413,c3d0b3f37ed12c5320c15e433af8721f1ff3594f,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I126eb57446ca3655e8f7da18a19c16240c4efae4
",git fetch https://review.opendev.org/openstack/specs-cookiecutter refs/changes/13/139413/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', '{{cookiecutter.repo_name}}/CONTRIBUTING.rst']",2,c3d0b3f37ed12c5320c15e433af8721f1ff3594f,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",6,8
openstack%2Fdevstack~stable%2Ficehouse~If56f64f1305a206e5360ad9b5e3e41db4d60d576,openstack/devstack,stable/icehouse,If56f64f1305a206e5360ad9b5e3e41db4d60d576,Fix lib/oslo to only reference things which exist,MERGED,2014-12-05 13:54:56.000000000,2014-12-05 19:50:56.000000000,2014-12-05 19:50:55.000000000,"[{'_account_id': 3}, {'_account_id': 970}, {'_account_id': 2472}, {'_account_id': 2750}, {'_account_id': 5196}]","[{'number': 1, 'created': '2014-12-05 13:54:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/046f2cc8ba8f1a3fe2af6f1ca516e649a4c42a92', 'message': 'Fix lib/oslo to only reference things which exist\n\nThe backport of the install from libs infrastructure left some\nunwanted bits in lib/oslo that cause cryptic build issues. Remove\nthat.\n\nChange-Id: If56f64f1305a206e5360ad9b5e3e41db4d60d576\n'}, {'number': 2, 'created': '2014-12-05 14:07:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/565a89dbcecaa63b83dcbe7d408ea319bc7c0cfc', 'message': 'Fix lib/oslo to only reference things which exist\n\nThe backport of the install from libs infrastructure left some\nunwanted bits in lib/oslo that cause cryptic build issues. Remove\nthat.\n\nChange-Id: If56f64f1305a206e5360ad9b5e3e41db4d60d576\n'}, {'number': 3, 'created': '2014-12-05 15:18:23.000000000', 'files': ['lib/oslo', 'tests/test_libs_from_pypi.sh', 'stackrc'], 'web_link': 'https://opendev.org/openstack/devstack/commit/e5f8d21aae206eebd23f2d551c37b57d12b83932', 'message': 'Fix lib/oslo to only reference things which exist\n\nThe backport of the install from libs infrastructure left some\nunwanted bits in lib/oslo that cause cryptic build issues. Remove\nthat.\n\nChange-Id: If56f64f1305a206e5360ad9b5e3e41db4d60d576\n'}]",0,139631,e5f8d21aae206eebd23f2d551c37b57d12b83932,11,5,3,2750,,,0,"Fix lib/oslo to only reference things which exist

The backport of the install from libs infrastructure left some
unwanted bits in lib/oslo that cause cryptic build issues. Remove
that.

Change-Id: If56f64f1305a206e5360ad9b5e3e41db4d60d576
",git fetch https://review.opendev.org/openstack/devstack refs/changes/31/139631/3 && git format-patch -1 --stdout FETCH_HEAD,['lib/oslo'],1,046f2cc8ba8f1a3fe2af6f1ca516e649a4c42a92,oslo_sync,,"GITDIR[""oslo.concurrency""]=$DEST/oslo.concurrencyGITDIR[""oslo.middleware""]=$DEST/oslo.middlewareGITDIR[""oslo.serialization""]=$DEST/oslo.serialization GITDIR[""oslo.utils""]=$DEST/oslo.utils _do_install_oslo_lib ""oslo.utils"" _do_install_oslo_lib ""oslo.serialization"" _do_install_oslo_lib ""oslo.concurrency"" _do_install_oslo_lib ""oslo.middleware""",0,8
openstack%2Fpycadf~master~I01a3e9af1d709f1f26d96195b224569cc7b4f0c3,openstack/pycadf,master,I01a3e9af1d709f1f26d96195b224569cc7b4f0c3,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:44:20.000000000,2014-12-05 19:49:06.000000000,2014-12-05 19:49:05.000000000,"[{'_account_id': 3}, {'_account_id': 6482}, {'_account_id': 6537}]","[{'number': 1, 'created': '2014-12-05 03:44:20.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/pycadf/commit/5ad22d9544c6bb1dd5739ac228e15e27b2186a42', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I01a3e9af1d709f1f26d96195b224569cc7b4f0c3\n'}]",0,139367,5ad22d9544c6bb1dd5739ac228e15e27b2186a42,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I01a3e9af1d709f1f26d96195b224569cc7b4f0c3
",git fetch https://review.opendev.org/openstack/pycadf refs/changes/67/139367/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,5ad22d9544c6bb1dd5739ac228e15e27b2186a42,infra-manual, http://docs.openstack.org/infra/manual/developers.html#development-workflow http://docs.openstack.org/infra/manual/developers.html#development-workflow, http://wiki.openstack.org/HowToContribute#If_you.27re_a_developer http://wiki.openstack.org/GerritWorkflow,2,2
openstack%2Fproject-config~master~I9dee12e6b91e62dd16c547b300eb90abbbbd1072,openstack/project-config,master,I9dee12e6b91e62dd16c547b300eb90abbbbd1072,Clean up Oslo stable compat jobs,MERGED,2014-12-05 14:04:09.000000000,2014-12-05 19:37:44.000000000,2014-12-05 19:37:43.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 5263}, {'_account_id': 6547}]","[{'number': 1, 'created': '2014-12-05 14:04:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/4adadb44a595bb2981d047ec7710ba6695f3227a', 'message': 'Clean up Oslo stable compat jobs\n\nRename the sahara-stable-compat-jobs to make it more a more template\nfor stable compatibility jobs that only look back one release, to be\nused for new projects that should not be tested for compatibility two\nreleases back.\n\nUse this template for the Oslo libs created during Juno, and for the\nSahara project.\n\nChange-Id: I9dee12e6b91e62dd16c547b300eb90abbbbd1072\n'}, {'number': 2, 'created': '2014-12-05 14:14:44.000000000', 'files': ['zuul/layout.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/ba1fd3884547216989588f4291c6adc97f2e85b2', 'message': 'Clean up Oslo stable compat jobs\n\nRename the sahara-stable-compat-jobs to make it more a more template\nfor stable compatibility jobs that only look back one release, to be\nused for new projects that should not be tested for compatibility two\nreleases back.\n\nUse this template for the Oslo libs created during Juno, and for the\nSahara project.\n\nChange-Id: I9dee12e6b91e62dd16c547b300eb90abbbbd1072\n'}]",0,139634,ba1fd3884547216989588f4291c6adc97f2e85b2,11,4,2,2472,,,0,"Clean up Oslo stable compat jobs

Rename the sahara-stable-compat-jobs to make it more a more template
for stable compatibility jobs that only look back one release, to be
used for new projects that should not be tested for compatibility two
releases back.

Use this template for the Oslo libs created during Juno, and for the
Sahara project.

Change-Id: I9dee12e6b91e62dd16c547b300eb90abbbbd1072
",git fetch https://review.opendev.org/openstack/project-config refs/changes/34/139634/2 && git format-patch -1 --stdout FETCH_HEAD,['zuul/layout.yaml'],1,4adadb44a595bb2981d047ec7710ba6695f3227a,remove-some-oslo-icehouse-jobs, - name: stable-compat-jobs-one-release # a version of stable-compat-jobs that only looks back one release # for new projects that shouldn't be tested back two releases - name: stable-compat-jobs-one-release - name: stable-compat-jobs-one-release - name: stable-compat-jobs-one-release - name: stable-compat-jobs-one-release - name: stable-compat-jobs-one-release, - name: sahara-stable-compat-jobs # sahara stable compat beginning from juno - name: sahara-stable-compat-jobs - name: stable-compat-jobs - name: stable-compat-jobs - name: stable-compat-jobs - name: stable-compat-jobs - name: stable-compat-jobs - name: stable-compat-jobs,8,9
openstack%2Ftripleo-incubator~master~I78c0fa3902c1e480b7ada0055332b14828c3c7f9,openstack/tripleo-incubator,master,I78c0fa3902c1e480b7ada0055332b14828c3c7f9,Add public VLAN support.,MERGED,2014-07-01 06:57:35.000000000,2014-12-05 19:32:40.000000000,2014-12-05 19:32:38.000000000,"[{'_account_id': 3}, {'_account_id': 360}, {'_account_id': 1926}, {'_account_id': 4190}, {'_account_id': 6488}, {'_account_id': 6796}, {'_account_id': 6969}, {'_account_id': 9369}, {'_account_id': 9453}]","[{'number': 1, 'created': '2014-07-01 06:57:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/43c761614f157a39f7d4af3eeda60c154c144a98', 'message': ""WIP: Add VLAN support to the seed.\n\nThis patch adds the ability to bring up a VLAN on the seed, which is\nneeded to communicate with a VLAN if the undercloud public API will be\non a VLAN (and likewise overcloud). I'll be pushing this through to\nthe full stack before taking it out of WIP (in case the schema needs\nto change etc).\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\n""}, {'number': 2, 'created': '2014-07-02 02:16:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/252393a128783ada19a0feeddb9dbf64db46c311', 'message': ""WIP: Add VLAN support to the seed.\n\nThis patch adds the ability to bring up a VLAN on the seed, which is\nneeded to communicate with a VLAN if the undercloud public API will be\non a VLAN (and likewise overcloud). I'll be pushing this through to\nthe full stack before taking it out of WIP (in case the schema needs\nto change etc).\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\n""}, {'number': 3, 'created': '2014-07-02 09:08:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/76195c1457ddf7897d2934b9b25260df8123cc9f', 'message': ""WIP: Add VLAN support to the seed.\n\nThis patch adds the ability to bring up a VLAN on the seed and undercloud,\nwhich is needed to communicate with a VLAN if the undercloud public API will be\non a VLAN (and likewise overcloud). I'll be pushing this through to\nthe full stack before taking it out of WIP (in case the schema needs\nto change etc). Current status is that we need to tweak the IP's we use and\ndocument the impact of multi-homing; then we need to configure setup-neutron\nfor the undercloud, and then finally we get onto the overcloud.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\n""}, {'number': 4, 'created': '2014-07-02 17:03:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/53f820dfc3c3b3ac86deb457f6b01a1b2979e9f4', 'message': ""WIP: Add VLAN support to the seed.\n\nThis patch adds the ability to bring up a VLAN on the seed and undercloud,\nwhich is needed to communicate with a VLAN if the undercloud public API will be\non a VLAN (and likewise overcloud). I'll be pushing this through to\nthe full stack before taking it out of WIP (in case the schema needs\nto change etc). Current status is that we need to tweak the IP's we use and\ndocument the impact of multi-homing; then we need to configure setup-neutron\nfor the undercloud, and then finally we get onto the overcloud.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\n""}, {'number': 5, 'created': '2014-07-06 22:01:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/9deb20a5e864b98a9573ff573e5e7516ff39903e', 'message': ""WIP: Add VLAN support to the seed.\n\nThis patch adds the ability to bring up a VLAN on the seed and undercloud,\nwhich is needed to communicate with a VLAN if the undercloud public API will be\non a VLAN (and likewise overcloud). I'll be pushing this through to\nthe full stack before taking it out of WIP (in case the schema needs\nto change etc). Current status is that we need to tweak the IP's we use and\ndocument the impact of multi-homing; then we need to configure setup-neutron\nfor the undercloud, and then finally we get onto the overcloud.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\n""}, {'number': 6, 'created': '2014-07-06 23:39:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/af2a552f162a925c474292f7e9eeb81b5f128df2', 'message': ""WIP: Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed and undercloud,\nwhich is needed to communicate with a VLAN if the public APIs will be\non a VLAN. I'll be pushing this through to the full stack before taking it out\nof WIP (in case the schema needs to change etc). Current status is that we need\nto apply changes to the overcloud and document the impact of multi-homing on\nrouting in docs.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\n""}, {'number': 7, 'created': '2014-07-07 00:13:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/e273beb01e51a7c8da8afc9760331487b50438fd', 'message': ""WIP: Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed and undercloud,\nwhich is needed to communicate with a VLAN if the public APIs will be\non a VLAN. I'll be pushing this through to the full stack before taking it out\nof WIP (in case the schema needs to change etc). Current status is that we need\nto apply changes to the overcloud and document the impact of multi-homing on\nrouting in docs.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\n""}, {'number': 8, 'created': '2014-07-07 05:16:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/3a40618b66bf959faef3004f781496c203d0c374', 'message': ""WIP: Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed and undercloud,\nwhich is needed to communicate with a VLAN if the public APIs will be\non a VLAN. I'll be pushing this through to the full stack before taking it out\nof WIP (in case the schema needs to change etc). Current status is that we need\nto apply changes to the overcloud and document the impact of multi-homing on\nrouting in docs.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\n""}, {'number': 9, 'created': '2014-07-07 06:50:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/850238a458f315d2b66db718135d8821760b507d', 'message': ""WIP: Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed and undercloud,\nwhich is needed to communicate with a VLAN if the public APIs will be\non a VLAN. I'll be pushing this through to the full stack before taking it out\nof WIP (in case the schema needs to change etc). Current status is that we need\nto apply changes to the overcloud and document the impact of multi-homing on\nrouting in docs.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\n""}, {'number': 10, 'created': '2014-07-08 09:50:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/7acf2d531c8cc96ee289fe3b1f0cfb6a99e70ddf', 'message': ""WIP: Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed and undercloud,\nwhich is needed to communicate with a VLAN if the public APIs will be\non a VLAN. I'll be pushing this through to the full stack before taking it out\nof WIP (in case the schema needs to change etc). Current status is that we need\nto apply changes to the overcloud and document the impact of multi-homing on\nrouting in docs, plus we need to sort out correct routing (as the default route\nis no longer sufficient)\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\n""}, {'number': 11, 'created': '2014-07-08 19:36:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/6d5d33ed5a2c8eda23e631aee64c55a764d5f739', 'message': ""WIP: Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed and undercloud,\nwhich is needed to communicate with a VLAN if the public APIs will be\non a VLAN. I'll be pushing this through to the full stack before taking it out\nof WIP (in case the schema needs to change etc). Current status is that we need\nto apply changes to the overcloud and document the impact of multi-homing on\nrouting / API use in docs.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\n""}, {'number': 12, 'created': '2014-07-08 19:39:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/d0171bc712bd75fd78cfb1a950006a51a7d71491', 'message': ""WIP: Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed and undercloud,\nwhich is needed to communicate with a VLAN if the public APIs will be\non a VLAN. I'll be pushing this through to the full stack before taking it out\nof WIP (in case the schema needs to change etc). Current status is that we need\nto apply changes to the overcloud and document the impact of multi-homing on\nrouting / API use in docs.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCloses-Bug: #1325114\n""}, {'number': 13, 'created': '2014-07-09 03:37:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/cba0743cc60682d991276c20b6baeb4d54e8b30c', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCloses-Bug: #1325114\n'}, {'number': 14, 'created': '2014-07-13 01:36:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/0efcec1614139c569a15b9557b45efb1771bcddb', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCloses-Bug: #1325114\n'}, {'number': 15, 'created': '2014-07-14 22:29:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/cd37c7d593b46bbda46f3f7479760738f9b36470', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCloses-Bug: #1325114\n'}, {'number': 16, 'created': '2014-07-14 22:34:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/a90550607120c5dc9223a60e03372db9a5245f55', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCloses-Bug: #1325114\n'}, {'number': 17, 'created': '2014-07-16 05:56:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/6cfef0ee059de5fa0ce6144c8d60a68595de9caa', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 18, 'created': '2014-07-16 06:29:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/d2fc985e6323019a705eb36497a59599ed7b588e', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 19, 'created': '2014-07-17 05:03:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/25aa56719c28de10f4c418032219404c45145abe', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 20, 'created': '2014-07-17 19:09:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/3d566c9e3fa38b78a028c1b6ebec39b4325078bb', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 21, 'created': '2014-07-17 19:53:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/1e012f4d477412c74f702d5139ed694119b46a15', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 22, 'created': '2014-07-17 19:56:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/d301fd2d28bacc52dfcbcf820a9bbe95c04f6380', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 23, 'created': '2014-07-19 20:17:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/9dd619fdbaaa0a9d3f26b0bc0e2529bec20fb07d', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 24, 'created': '2014-07-19 20:20:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/eb99a39c5df39ada55d54f4ed35ef1dcdd3cce82', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 25, 'created': '2014-07-19 21:04:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/089c31090cb844ec8d3890b0df2c45ae2f9de96b', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 26, 'created': '2014-07-19 22:13:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/2c1d9d022a2cd95d7005e4cfb683a60faac55ac0', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 27, 'created': '2014-07-19 23:04:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/07c4fb7f73a13f1f838378db6f231b01047f74c2', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 28, 'created': '2014-07-20 00:18:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/d90018cbd19c5edd93154d3d79384fe35ca1e3cf', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 29, 'created': '2014-07-22 21:21:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/a563816f94fe95de96035620b3db07e383260aa6', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 30, 'created': '2014-08-01 05:32:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/37d3d453cde569f222d8242da8929685db834de1', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 31, 'created': '2014-08-13 00:34:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/8205486d446307b64cf137f96eb22002026778d9', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 32, 'created': '2014-08-29 05:43:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/637b3a568bc6628f9ca1480353c6d925525677a0', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 33, 'created': '2014-08-29 06:19:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/a52e5fc2c4955ff3a4b798df211485f375d63471', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 34, 'created': '2014-09-02 05:08:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/4badc133cb4c4cbaa4c11991ee18ac2444978899', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 35, 'created': '2014-09-02 06:53:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/0023056bab2c7670282585b04daff5c26cfb353f', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 36, 'created': '2014-09-10 03:15:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/ec59f5e10b00059e320afddee761d1312adac068', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCloses-Bug: #1325114\n'}, {'number': 37, 'created': '2014-11-03 11:57:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/217999fe145e14123301fda198fb4a29cc049001', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCo-Authored-By: Steve Kowalik <steven@wedontsleep.org>\nCloses-Bug: #1325114\n'}, {'number': 38, 'created': '2014-11-19 00:58:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/94b94c40a59756a2158788c3ec4013c804d273ce', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCo-Authored-By: Steve Kowalik <steven@wedontsleep.org>\nCloses-Bug: #1325114\n'}, {'number': 39, 'created': '2014-11-24 17:39:20.000000000', 'files': ['scripts/devtest_seed.sh', 'scripts/devtest_undercloud.sh', 'scripts/install-dependencies', 'scripts/devtest_setup.sh', 'scripts/devtest_overcloud.sh'], 'web_link': 'https://opendev.org/openstack/tripleo-incubator/commit/1abcb5fe3b56666288531a21dd17732d1bd8cba2', 'message': 'Add public VLAN support.\n\nThis patch adds the ability to bring up a VLAN on the seed, undercloud and\novercloud control plane nodes. This feature was sort-of working in the past\nbut the vconfig based configuration broke provider networks due to layering\nunder the bridge device rather than over it. In this revised version we use\nNeutron to manage and allocate IP addresses on the VLAN, though of course\nwe found some limitations in Heat when doing this :).\n\nThis is not really fully documented yet but is otherwise believed to be\nfeature complete.\n\nChange-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9\nCo-Authored-By: Derek Higgins <derekh@redhat.com>\nCo-Authored-By: James Polley <jp@jamezpolley.com>\nCo-Authored-By: Steve Kowalik <steven@wedontsleep.org>\nCloses-Bug: #1325114\n'}]",8,103757,1abcb5fe3b56666288531a21dd17732d1bd8cba2,163,9,39,4190,,,0,"Add public VLAN support.

This patch adds the ability to bring up a VLAN on the seed, undercloud and
overcloud control plane nodes. This feature was sort-of working in the past
but the vconfig based configuration broke provider networks due to layering
under the bridge device rather than over it. In this revised version we use
Neutron to manage and allocate IP addresses on the VLAN, though of course
we found some limitations in Heat when doing this :).

This is not really fully documented yet but is otherwise believed to be
feature complete.

Change-Id: I78c0fa3902c1e480b7ada0055332b14828c3c7f9
Co-Authored-By: Derek Higgins <derekh@redhat.com>
Co-Authored-By: James Polley <jp@jamezpolley.com>
Co-Authored-By: Steve Kowalik <steven@wedontsleep.org>
Closes-Bug: #1325114
",git fetch https://review.opendev.org/openstack/tripleo-incubator refs/changes/57/103757/13 && git format-patch -1 --stdout FETCH_HEAD,"['scripts/devtest_seed.sh', 'scripts/install-dependencies', 'scripts/devtest_setup.sh']",3,43c761614f157a39f7d4af3eeda60c154c144a98,vlan," ## {## ""public_vlan"": (null),## ""public_vlan"": (null) ## } ## } ## The public_vlan keys default to absent, which is suitable for a flat ## networking environment. When exterior access will be on a vlan they ## should be filled out. For instance, if TEST-NET-2 were our exterior ## subnet on VLAN 10, we might have the following as our baremetal network, ## to use a baremetal router on .1, the seed on .2, and a handful of ## addresses for both the seed and the undercloud dhcp pools:: ## { ## ""cidr"": ""192.0.2.0/24"", ## ""gateway-ip"": ""198.51.100.1"", ## ""seed"": { ## ""ip"": ""192.0.2.1"", ## ""range-start"": ""192.0.2.2"", ## ""range-end"": ""192.0.2.20"", ## ""public_vlan"": { ## ""tag"": 10, ## ""ip"": ""198.51.100.2/24"", ## ""start"": ""198.51.100.3"", ## ""finish"": ""198.51.100.10"" ## } ## }, ## ""undercloud"": { ## ""range-start"": ""192.0.2.21"", ## ""range-end"": ""192.0.2.40"", ## ""public_vlan"": { ## ""tag"": 10, ## ""start"": ""198.51.100.11"", ## ""finish"": ""198.51.100.20"" ## }","## ## ""baremetal-network"": {",54,7
openstack%2Ftaskflow~master~I4851a08ff1ab4101dbec4a6656177908095c3c52,openstack/taskflow,master,I4851a08ff1ab4101dbec4a6656177908095c3c52,Ensure that the zookeeper backend creates missing atoms,MERGED,2014-11-21 21:11:49.000000000,2014-12-05 19:23:53.000000000,2014-12-05 19:23:52.000000000,"[{'_account_id': 3}, {'_account_id': 1297}, {'_account_id': 8092}, {'_account_id': 11024}]","[{'number': 1, 'created': '2014-11-21 21:11:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/75492596a35f722300a3939d23f12f72ef97dd44', 'message': ""Ensure that the zookeeper creates missing atom paths\n\nWhen 'create_missing' is true the atom should be created\ninstead of raising an exception; this is used when a flow\ndetail is updated with a new detail and then saved.\n\nThis also adds test cases that verify this happens so that\nwe verify this on an ongoing basis.\n\nChange-Id: I4851a08ff1ab4101dbec4a6656177908095c3c52\n""}, {'number': 2, 'created': '2014-11-21 21:12:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/taskflow/commit/97ecb5a226394f03a1ebd069f00639686dfe7c64', 'message': ""Ensure that the zookeeper backend creates missing atoms\n\nWhen 'create_missing' is true the atom should be created\ninstead of raising an exception; this is used when a flow\ndetail is updated with a new detail and then saved.\n\nThis also adds test cases that verify this happens so that\nwe verify this on an ongoing basis.\n\nChange-Id: I4851a08ff1ab4101dbec4a6656177908095c3c52\n""}, {'number': 3, 'created': '2014-11-24 18:06:21.000000000', 'files': ['taskflow/persistence/backends/impl_zookeeper.py', 'taskflow/tests/unit/persistence/base.py'], 'web_link': 'https://opendev.org/openstack/taskflow/commit/2832d6e677413d08bd1645bb8bd8067b3ab665d6', 'message': ""Ensure that the zookeeper backend creates missing atoms\n\nWhen 'create_missing' is true the atom should be created\ninstead of raising an exception; this is used when a flow\ndetail is updated with a new detail and then saved.\n\nThis also adds test cases that verify this happens so that\nwe verify this on an ongoing basis.\n\nFixes bug 1395812\n\nChange-Id: I4851a08ff1ab4101dbec4a6656177908095c3c52\n""}]",0,136491,2832d6e677413d08bd1645bb8bd8067b3ab665d6,17,4,3,1297,,,0,"Ensure that the zookeeper backend creates missing atoms

When 'create_missing' is true the atom should be created
instead of raising an exception; this is used when a flow
detail is updated with a new detail and then saved.

This also adds test cases that verify this happens so that
we verify this on an ongoing basis.

Fixes bug 1395812

Change-Id: I4851a08ff1ab4101dbec4a6656177908095c3c52
",git fetch https://review.opendev.org/openstack/taskflow refs/changes/91/136491/1 && git format-patch -1 --stdout FETCH_HEAD,"['taskflow/persistence/backends/impl_zookeeper.py', 'taskflow/tests/unit/persistence/base.py']",2,75492596a35f722300a3939d23f12f72ef97dd44,bug/1395812," raise NotImplementedError('_get_connection() implementation required') def test_task_detail_update_not_existing(self): lb_id = uuidutils.generate_uuid() lb_name = 'lb-%s' % (lb_id) lb = logbook.LogBook(name=lb_name, uuid=lb_id) fd = logbook.FlowDetail('test', uuid=uuidutils.generate_uuid()) lb.add(fd) td = logbook.TaskDetail(""detail-1"", uuid=uuidutils.generate_uuid()) fd.add(td) with contextlib.closing(self._get_connection()) as conn: conn.save_logbook(lb) td2 = logbook.TaskDetail(""detail-1"", uuid=uuidutils.generate_uuid()) fd.add(td2) with contextlib.closing(self._get_connection()) as conn: conn.update_flow_details(fd) with contextlib.closing(self._get_connection()) as conn: lb2 = conn.get_logbook(lb.uuid) fd2 = lb2.find(fd.uuid) self.assertIsNotNone(fd2.find(td.uuid)) self.assertIsNotNone(fd2.find(td2.uuid)) def test_flow_detail_update_not_existing(self): lb_id = uuidutils.generate_uuid() lb_name = 'lb-%s' % (lb_id) lb = logbook.LogBook(name=lb_name, uuid=lb_id) fd = logbook.FlowDetail('test', uuid=uuidutils.generate_uuid()) lb.add(fd) with contextlib.closing(self._get_connection()) as conn: conn.save_logbook(lb) fd2 = logbook.FlowDetail('test-2', uuid=uuidutils.generate_uuid()) lb.add(fd2) with contextlib.closing(self._get_connection()) as conn: conn.save_logbook(lb) with contextlib.closing(self._get_connection()) as conn: lb2 = conn.get_logbook(lb.uuid) self.assertIsNotNone(lb2.find(fd.uuid)) self.assertIsNotNone(lb2.find(fd2.uuid))", raise NotImplementedError(),47,2
openstack%2Fopenstack-planet~master~Idf37dd0875f27cb87dd901ba3067020921b20f47,openstack/openstack-planet,master,Idf37dd0875f27cb87dd901ba3067020921b20f47,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:43:04.000000000,2014-12-05 19:17:44.000000000,2014-12-05 08:56:49.000000000,"[{'_account_id': 3}, {'_account_id': 287}, {'_account_id': 308}]","[{'number': 1, 'created': '2014-12-05 03:43:04.000000000', 'files': ['classic_fancy/index.html.tmpl'], 'web_link': 'https://opendev.org/openstack/openstack-planet/commit/ce8f827918f7768774fb58c58223471e893cf1d0', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Idf37dd0875f27cb87dd901ba3067020921b20f47\n'}]",0,139345,ce8f827918f7768774fb58c58223471e893cf1d0,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Idf37dd0875f27cb87dd901ba3067020921b20f47
",git fetch https://review.opendev.org/openstack/openstack-planet refs/changes/45/139345/1 && git format-patch -1 --stdout FETCH_HEAD,['classic_fancy/index.html.tmpl'],1,ce8f827918f7768774fb58c58223471e893cf1d0,infra-manual," <li><a href=""http://docs.openstack.org/infra/manual/developers.html"">Contribute</a></li>"," <li><a href=""http://wiki.openstack.org/HowToContribute"">Contribute</a></li>",1,1
openstack%2Fkeystone~master~I63bdb6d8f03a0510e34b7e10cec25263e7c6c63c,openstack/keystone,master,I63bdb6d8f03a0510e34b7e10cec25263e7c6c63c,Provide useful info when parsing policy file,MERGED,2014-10-28 21:24:00.000000000,2014-12-05 19:14:24.000000000,2014-12-05 19:14:24.000000000,"[{'_account_id': 3}, {'_account_id': 2218}, {'_account_id': 5046}, {'_account_id': 6482}, {'_account_id': 9751}, {'_account_id': 11022}]","[{'number': 1, 'created': '2014-10-28 21:24:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/efbf4230fc51de121f6ed495b94e64be644edfb2', 'message': 'Provide useful info when parsing policy file\n\nWhen parsing the policy.json file we should provide a message that helps\nthe user or deployer understand there was an issue with the policy file,\nand that they should make sure the policy.json file is valid JSON.\n\nChange-Id: I63bdb6d8f03a0510e34b7e10cec25263e7c6c63c\nCloses-Bug: #1177623\n'}, {'number': 2, 'created': '2014-10-29 20:39:16.000000000', 'files': ['keystone/tests/test_policy.py', 'keystone/exception.py', 'keystone/policy/backends/rules.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/383422f2434414d5d736ebdd9a8b4dcc4e15a288', 'message': 'Provide useful info when parsing policy file\n\nWhen parsing the policy.json file we should provide a message that helps\nthe user or deployer understand there was an issue with the policy file,\nand that they should make sure the policy.json file is valid JSON.\n\nChange-Id: I63bdb6d8f03a0510e34b7e10cec25263e7c6c63c\nCloses-Bug: #1177623\n'}]",4,131574,383422f2434414d5d736ebdd9a8b4dcc4e15a288,13,6,2,5046,,,0,"Provide useful info when parsing policy file

When parsing the policy.json file we should provide a message that helps
the user or deployer understand there was an issue with the policy file,
and that they should make sure the policy.json file is valid JSON.

Change-Id: I63bdb6d8f03a0510e34b7e10cec25263e7c6c63c
Closes-Bug: #1177623
",git fetch https://review.opendev.org/openstack/keystone refs/changes/74/131574/2 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/tests/test_policy.py', 'keystone/exception.py', 'keystone/policy/backends/rules.py']",3,efbf4230fc51de121f6ed495b94e64be644edfb2,bug/1177623," try: _ENFORCER.set_rules(common_policy.Rules.load_json( data, default_rule)) except ValueError: raise exception.PolicyError(policy_file=_POLICY_PATH)"," _ENFORCER.set_rules(common_policy.Rules.load_json( data, default_rule))",19,2
openstack%2Fnova~master~I913077d3b0fdee78e423c35b3a48137a17946a7b,openstack/nova,master,I913077d3b0fdee78e423c35b3a48137a17946a7b,Replacement `_` on `_LW` in all LOG.warning part 4,MERGED,2014-11-20 08:53:11.000000000,2014-12-05 19:11:20.000000000,2014-12-05 19:11:17.000000000,"[{'_account_id': 3}, {'_account_id': 642}, {'_account_id': 1779}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-20 08:53:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3882350a067c34c666c2398838feacb744cfe579', 'message': ""Replacement `_` on `_LW` in all LOG.warning part 4\n\noslo.i18n uses different marker functions to separate the\ntranslatable messages into different catalogs, which the translation\nteams can prioritize translating. For details, please refer to:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack\n\nThere were not marker fuctions some places in directory network.\nThis commit makes changes:\n* Add missing marker functions\n* Use ',' instead of '%' while adding variables to log messages\n\nChange-Id: I913077d3b0fdee78e423c35b3a48137a17946a7b\n""}, {'number': 2, 'created': '2014-11-20 09:19:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/0976bf2c46a6a6bf6f91c84daa870f41c5723d5c', 'message': ""Replacement `_` on `_LW` in all LOG.warning part 4\n\noslo.i18n uses different marker functions to separate the\ntranslatable messages into different catalogs, which the translation\nteams can prioritize translating. For details, please refer to:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack\n\nThere were not marker fuctions some places in directory network.\nThis commit makes changes:\n* Add missing marker functions\n* Use ',' instead of '%' while adding variables to log messages\n\nChange-Id: I913077d3b0fdee78e423c35b3a48137a17946a7b\n""}, {'number': 3, 'created': '2014-11-20 13:31:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8f93565e37896be551995c9fdea41ba1ea74a61c', 'message': ""Replacement `_` on `_LW` in all LOG.warning part 4\n\noslo.i18n uses different marker functions to separate the\ntranslatable messages into different catalogs, which the translation\nteams can prioritize translating. For details, please refer to:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack\n\nThere were not marker fuctions some places in directory network.\nThis commit makes changes:\n* Add missing marker functions\n* Use ',' instead of '%' while adding variables to log messages\n\nChange-Id: I913077d3b0fdee78e423c35b3a48137a17946a7b\n""}, {'number': 4, 'created': '2014-11-23 07:57:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9a60073024659da4e8065754d5fb51d98423f694', 'message': ""Replacement `_` on `_LW` in all LOG.warning part 4\n\noslo.i18n uses different marker functions to separate the\ntranslatable messages into different catalogs, which the translation\nteams can prioritize translating. For details, please refer to:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack\n\nThere were not marker fuctions some places in directory network.\nThis commit makes changes:\n* Add missing marker functions\n* Use ',' instead of '%' while adding variables to log messages\n\nChange-Id: I913077d3b0fdee78e423c35b3a48137a17946a7b\n""}, {'number': 5, 'created': '2014-11-26 08:25:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/34481ad8f65afa386047e2e0af0d4a4570fb4e0f', 'message': ""Replacement `_` on `_LW` in all LOG.warning part 4\n\noslo.i18n uses different marker functions to separate the\ntranslatable messages into different catalogs, which the translation\nteams can prioritize translating. For details, please refer to:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack\n\nThere were not marker fuctions some places in directory network.\nThis commit makes changes:\n* Add missing marker functions\n* Use ',' instead of '%' while adding variables to log messages\n\nChange-Id: I913077d3b0fdee78e423c35b3a48137a17946a7b\n""}, {'number': 6, 'created': '2014-12-04 12:15:06.000000000', 'files': ['nova/virt/ironic/client_wrapper.py', 'nova/virt/xenapi/vm_utils.py', 'nova/virt/hyperv/vmutils.py', 'nova/virt/disk/mount/api.py', 'nova/virt/disk/mount/nbd.py', 'nova/virt/vmwareapi/imagecache.py', 'nova/virt/configdrive.py', 'nova/virt/ironic/driver.py', 'nova/virt/xenapi/volumeops.py', 'nova/virt/xenapi/vmops.py', 'nova/virt/xenapi/volume_utils.py', 'nova/virt/vmwareapi/vm_util.py', 'nova/virt/xenapi/host.py', 'nova/virt/fake.py', 'nova/virt/vmwareapi/vif.py', 'nova/virt/storage_users.py', 'nova/virt/hyperv/snapshotops.py', 'nova/virt/hyperv/volumeops.py', 'nova/virt/disk/vfs/guestfs.py', 'nova/virt/disk/api.py', 'nova/virt/vmwareapi/vim_util.py', 'nova/virt/vmwareapi/vmops.py', 'nova/virt/xenapi/agent.py', 'nova/virt/xenapi/driver.py', 'nova/virt/xenapi/client/session.py', 'nova/hacking/checks.py', 'nova/virt/vmwareapi/driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/76953c00c3bd6185a6b08ee7dcea524bada25f96', 'message': ""Replacement `_` on `_LW` in all LOG.warning part 4\n\noslo.i18n uses different marker functions to separate the\ntranslatable messages into different catalogs, which the translation\nteams can prioritize translating. For details, please refer to:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack\n\nThere were not marker fuctions some places in directory network.\nThis commit makes changes:\n* Add missing marker functions\n* Use ',' instead of '%' while adding variables to log messages\n\nChange-Id: I913077d3b0fdee78e423c35b3a48137a17946a7b\n""}]",1,135887,76953c00c3bd6185a6b08ee7dcea524bada25f96,39,8,6,8412,,,0,"Replacement `_` on `_LW` in all LOG.warning part 4

oslo.i18n uses different marker functions to separate the
translatable messages into different catalogs, which the translation
teams can prioritize translating. For details, please refer to:
http://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack

There were not marker fuctions some places in directory network.
This commit makes changes:
* Add missing marker functions
* Use ',' instead of '%' while adding variables to log messages

Change-Id: I913077d3b0fdee78e423c35b3a48137a17946a7b
",git fetch https://review.opendev.org/openstack/nova refs/changes/87/135887/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/ironic/client_wrapper.py', 'nova/virt/xenapi/vm_utils.py', 'nova/virt/hyperv/vmutils.py', 'nova/virt/disk/mount/api.py', 'nova/virt/disk/mount/nbd.py', 'nova/virt/vmwareapi/imagecache.py', 'nova/virt/configdrive.py', 'nova/virt/ironic/driver.py', 'nova/virt/xenapi/volumeops.py', 'nova/virt/xenapi/vmops.py', 'nova/virt/xenapi/volume_utils.py', 'nova/virt/vmwareapi/vm_util.py', 'nova/virt/xenapi/host.py', 'nova/virt/fake.py', 'nova/virt/vmwareapi/vif.py', 'nova/virt/storage_users.py', 'nova/virt/hyperv/snapshotops.py', 'nova/virt/hyperv/volumeops.py', 'nova/virt/disk/vfs/guestfs.py', 'nova/virt/disk/api.py', 'nova/virt/vmwareapi/vim_util.py', 'nova/virt/vmwareapi/vmops.py', 'nova/virt/xenapi/agent.py', 'nova/virt/xenapi/driver.py', 'nova/virt/xenapi/client/session.py', 'nova/hacking/checks.py', 'nova/virt/vmwareapi/driver.py']",27,3882350a067c34c666c2398838feacb744cfe579,Replacement_on_LW_in_all_LOG.warning," LOG.warning(_LW(""The following clusters could not be found in the "" ""vCenter %s""), list(missing_clusters)) LOG.warning(_LW('Instance cannot be found in host, or in an ' 'unknown state.'), instance=instance)"," LOG.warn(_LW(""The following clusters could not be found in the "" ""vCenter %s"") % list(missing_clusters)) LOG.warn(_LW('Instance cannot be found in host, or in an unknown' 'state.'), instance=instance)",148,144
openstack%2Fnova~master~I14bb1fbf7044769697024b03c965ed9d70ab69c1,openstack/nova,master,I14bb1fbf7044769697024b03c965ed9d70ab69c1,Replacement `_` on `_LW` in all LOG.warning part 3,MERGED,2014-11-20 08:53:11.000000000,2014-12-05 19:10:53.000000000,2014-12-05 19:10:51.000000000,"[{'_account_id': 3}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 8412}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 9708}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-20 08:53:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9017da9fb524d37fc5e26b43bd06bbc4b73dd1a6', 'message': ""Replacement `_` on `_LW` in all LOG.warning part 3\n\noslo.i18n uses different marker functions to separate the\ntranslatable messages into different catalogs, which the translation\nteams can prioritize translating. For details, please refer to:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack\n\nThere were not marker fuctions some places in directory network.\nThis commit makes changes:\n* Add missing marker functions\n* Use ',' instead of '%' while adding variables to log messages\n\nChange-Id: I14bb1fbf7044769697024b03c965ed9d70ab69c1\n""}, {'number': 2, 'created': '2014-11-20 09:19:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ac1f456a28a9d341038b1e59952bb5057a6c4bce', 'message': ""Replacement `_` on `_LW` in all LOG.warning part 3\n\noslo.i18n uses different marker functions to separate the\ntranslatable messages into different catalogs, which the translation\nteams can prioritize translating. For details, please refer to:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack\n\nThere were not marker fuctions some places in directory network.\nThis commit makes changes:\n* Add missing marker functions\n* Use ',' instead of '%' while adding variables to log messages\n\nChange-Id: I14bb1fbf7044769697024b03c965ed9d70ab69c1\n""}, {'number': 3, 'created': '2014-11-20 13:31:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3f3e4b251257f4dd0c8048603f6370c78e3d50e2', 'message': ""Replacement `_` on `_LW` in all LOG.warning part 3\n\noslo.i18n uses different marker functions to separate the\ntranslatable messages into different catalogs, which the translation\nteams can prioritize translating. For details, please refer to:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack\n\nThere were not marker fuctions some places in directory network.\nThis commit makes changes:\n* Add missing marker functions\n* Use ',' instead of '%' while adding variables to log messages\n\nChange-Id: I14bb1fbf7044769697024b03c965ed9d70ab69c1\n""}, {'number': 4, 'created': '2014-11-23 07:57:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3b1a03b477770dc75da8dde1400170122dc48991', 'message': ""Replacement `_` on `_LW` in all LOG.warning part 3\n\noslo.i18n uses different marker functions to separate the\ntranslatable messages into different catalogs, which the translation\nteams can prioritize translating. For details, please refer to:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack\n\nThere were not marker fuctions some places in directory network.\nThis commit makes changes:\n* Add missing marker functions\n* Use ',' instead of '%' while adding variables to log messages\n\nChange-Id: I14bb1fbf7044769697024b03c965ed9d70ab69c1\n""}, {'number': 5, 'created': '2014-11-26 08:25:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c24f2d6601b3f870e717fa197e53d25a5b6f85c7', 'message': ""Replacement `_` on `_LW` in all LOG.warning part 3\n\noslo.i18n uses different marker functions to separate the\ntranslatable messages into different catalogs, which the translation\nteams can prioritize translating. For details, please refer to:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack\n\nThere were not marker fuctions some places in directory network.\nThis commit makes changes:\n* Add missing marker functions\n* Use ',' instead of '%' while adding variables to log messages\n\nChange-Id: I14bb1fbf7044769697024b03c965ed9d70ab69c1\n""}, {'number': 6, 'created': '2014-12-04 12:15:06.000000000', 'files': ['nova/compute/resources/__init__.py', 'nova/hacking/checks.py', 'nova/tests/unit/compute/test_resource_tracker.py', 'nova/compute/monitors/__init__.py', 'nova/compute/manager.py', 'nova/compute/api.py', 'nova/compute/resource_tracker.py', 'nova/compute/utils.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/1e8df2f00b2b93ee6c31627036df4befff760680', 'message': ""Replacement `_` on `_LW` in all LOG.warning part 3\n\noslo.i18n uses different marker functions to separate the\ntranslatable messages into different catalogs, which the translation\nteams can prioritize translating. For details, please refer to:\nhttp://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack\n\nThere were not marker fuctions some places in directory network.\nThis commit makes changes:\n* Add missing marker functions\n* Use ',' instead of '%' while adding variables to log messages\n\nChange-Id: I14bb1fbf7044769697024b03c965ed9d70ab69c1\n""}]",1,135886,1e8df2f00b2b93ee6c31627036df4befff760680,43,10,6,8412,,,0,"Replacement `_` on `_LW` in all LOG.warning part 3

oslo.i18n uses different marker functions to separate the
translatable messages into different catalogs, which the translation
teams can prioritize translating. For details, please refer to:
http://docs.openstack.org/developer/oslo.i18n/guidelines.html#guidelines-for-use-in-openstack

There were not marker fuctions some places in directory network.
This commit makes changes:
* Add missing marker functions
* Use ',' instead of '%' while adding variables to log messages

Change-Id: I14bb1fbf7044769697024b03c965ed9d70ab69c1
",git fetch https://review.opendev.org/openstack/nova refs/changes/86/135886/4 && git format-patch -1 --stdout FETCH_HEAD,"['nova/compute/resources/__init__.py', 'nova/hacking/checks.py', 'nova/compute/monitors/__init__.py', 'nova/compute/manager.py', 'nova/compute/api.py', 'nova/compute/resource_tracker.py', 'nova/compute/utils.py']",7,9017da9fb524d37fc5e26b43bd06bbc4b73dd1a6,Replacement_on_LW_in_all_LOG.warning," LOG.warning(_LW(""No host name specified for the notification of "" ""HostAPI.%s and it will be ignored""), event_suffix)"," LOG.warn(_LW(""No host name specified for the notification of "" ""HostAPI.%s and it will be ignored""), event_suffix)",162,154
openstack%2Ftripleo-ci~master~I5ed1a7bb473d9f7183b3a036702a1c71bfc5870d,openstack/tripleo-ci,master,I5ed1a7bb473d9f7183b3a036702a1c71bfc5870d,Set TripleO CI control variables based on job type,MERGED,2014-10-16 15:54:34.000000000,2014-12-05 19:04:07.000000000,2014-12-05 19:04:07.000000000,"[{'_account_id': 3}, {'_account_id': 360}, {'_account_id': 1726}, {'_account_id': 1926}, {'_account_id': 4220}, {'_account_id': 6133}, {'_account_id': 8688}, {'_account_id': 9453}]","[{'number': 1, 'created': '2014-10-16 15:54:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/9bd184f51d83d8cf845bd53d5c156f300c0b915b', 'message': ""Set tripleo ci control variables based on job type\n\nCurrently if we change any of these variables we wont know if we did the\nright thing until is merged into openstack-infra/config. Moving these\ninto tripleo-ci will give us the ability to make changes and have CI\nvalidate we did the correct thing. It will also ensure all jobs run with\nthe same defaults (e.g. currently not all the jobs run with USE_CIRROS=1\nwhich I don't think was ever intended).\n\nThis commit should be merged before its counterpart in the infra/config\nrepository I58462d3ce3e63ea190c95e2a210c087c978c7d16\n\nChange-Id: I5ed1a7bb473d9f7183b3a036702a1c71bfc5870d\n""}, {'number': 2, 'created': '2014-10-23 11:03:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/d02b38412561db3a984d287cffe2bc547b39a17d', 'message': ""Set tripleo ci control variables based on job type\n\nCurrently if we change any of these variables we wont know if we did the\nright thing until is merged into openstack-infra/config. Moving these\ninto tripleo-ci will give us the ability to make changes and have CI\nvalidate we did the correct thing. It will also ensure all jobs run with\nthe same defaults (e.g. currently not all the jobs run with USE_CIRROS=1\nwhich I don't think was ever intended).\n\nThis commit should be merged before its counterpart in the infra/config\nrepository I58462d3ce3e63ea190c95e2a210c087c978c7d16\n\nChange-Id: I5ed1a7bb473d9f7183b3a036702a1c71bfc5870d\n""}, {'number': 3, 'created': '2014-10-30 09:23:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/9900440ff05c9fca93e5405bd2694054532ceec8', 'message': ""Set tripleo ci control variables based on job type\n\nCurrently if we change any of these variables we wont know if we did the\nright thing until is merged into openstack-infra/config. Moving these\ninto tripleo-ci will give us the ability to make changes and have CI\nvalidate we did the correct thing. It will also ensure all jobs run with\nthe same defaults (e.g. currently not all the jobs run with USE_CIRROS=1\nwhich I don't think was ever intended).\n\nThis commit should be merged before its counterpart in the infra/config\nrepository I58462d3ce3e63ea190c95e2a210c087c978c7d16\n\nChange-Id: I5ed1a7bb473d9f7183b3a036702a1c71bfc5870d\n""}, {'number': 4, 'created': '2014-11-12 16:37:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/8a4aaf615ff658baaa93314a2f63457b6ab1f408', 'message': ""Set TripleO CI control variables based on job type\n\nCurrently if we change any of these variables we won't know if we did the right\nthing until it's merged into openstack-infra/config. Moving these into\ntripleo-ci will give us the ability to make changes and have CI validate we did\nthe correct thing. It will also ensure all jobs run with the same defaults\n(e.g. currently not all the jobs run with USE_CIRROS=1 which I don't think was\never intended).\n\nThis commit should be merged before its counterpart in the infra/config\nrepository I58462d3ce3e63ea190c95e2a210c087c978c7d16\n\nChange-Id: I5ed1a7bb473d9f7183b3a036702a1c71bfc5870d\n""}, {'number': 5, 'created': '2014-11-12 16:38:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/48e496b0423de810d0f9d57b23215b469ae13045', 'message': ""Set TripleO CI control variables based on job type\n\nCurrently if we change any of these variables we won't know if we did\nthe right thing until it's merged into openstack-infra/config. Moving\nthese into tripleo-ci will give us the ability to make changes and have\nCI validate we did the correct thing. It will also ensure all jobs run\nwith the same defaults (e.g. currently not all the jobs run with\nUSE_CIRROS=1 which I don't think was ever intended).\n\nThis commit should be merged before its counterpart in the infra/config\nrepository I58462d3ce3e63ea190c95e2a210c087c978c7d16\n\nChange-Id: I5ed1a7bb473d9f7183b3a036702a1c71bfc5870d\n""}, {'number': 6, 'created': '2014-12-04 13:18:04.000000000', 'files': ['toci_gate_test.sh'], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/b8b2035405898797d4a11d01daac759f22e0a784', 'message': ""Set TripleO CI control variables based on job type\n\nCurrently if we change any of these variables we won't know if we did\nthe right thing until it's merged into openstack-infra/config. Moving\nthese into tripleo-ci will give us the ability to make changes and have\nCI validate we did the correct thing. It will also ensure all jobs run\nwith the same defaults (e.g. currently not all the jobs run with\nUSE_CIRROS=1 which I don't think was ever intended).\n\nThis commit should be merged before its counterpart in the infra/config\nrepository I58462d3ce3e63ea190c95e2a210c087c978c7d16\n\nChange-Id: I5ed1a7bb473d9f7183b3a036702a1c71bfc5870d\n""}]",12,128972,b8b2035405898797d4a11d01daac759f22e0a784,42,8,6,1926,,,0,"Set TripleO CI control variables based on job type

Currently if we change any of these variables we won't know if we did
the right thing until it's merged into openstack-infra/config. Moving
these into tripleo-ci will give us the ability to make changes and have
CI validate we did the correct thing. It will also ensure all jobs run
with the same defaults (e.g. currently not all the jobs run with
USE_CIRROS=1 which I don't think was ever intended).

This commit should be merged before its counterpart in the infra/config
repository I58462d3ce3e63ea190c95e2a210c087c978c7d16

Change-Id: I5ed1a7bb473d9f7183b3a036702a1c71bfc5870d
",git fetch https://review.opendev.org/openstack/tripleo-ci refs/changes/72/128972/6 && git format-patch -1 --stdout FETCH_HEAD,['toci_gate_test.sh'],1,9bd184f51d83d8cf845bd53d5c156f300c0b915b,move-control-vars,"# and the geard server on 192.168.1.1export DIB_COMMON_ELEMENTS=${DIB_COMMON_ELEMENTS:-""common-venv stackuser pypi-openstack""} export TRIPLEO_TEST=${TRIPLEO_TEST:-""overcloud""} export USE_CIRROS=${USE_CIRROS:-""1""} export OVERCLOUD_CONTROLSCALE=${OVERCLOUD_CONTROLSCALE:-""1""} export TRIPLEO_DEBUG=${TRIPLEO_DEBUG:-""0""} # Switch defaults based on the job name for JOB_TYPE_PART in $(sed 's/-/ /g' <<< ""${TOCI_JOBTYPE:-}"") ; do case $JOB_TYPE_PART in undercloud) export TRIPLEO_TEST=undercloud ;; ha) export OVERCLOUD_CONTROLSCALE=3 export TRIPLEO_DEBUG=1 ;; vlan) export TRIPLEO_TEST=vlan ;; esac doneenv | grep -E ""(DIB_COMMON_ELEMENTS|OVERCLOUD_CONTROLSCALE|TRIPLEO_TEST|USE_CIRROS|TRIPLEO_DEBUG)=""","# This change may eventually belong in openstack-infra/config but we # can test and use it here for nowexport DIB_COMMON_ELEMENTS=""common-venv stackuser pypi-openstack"" export USE_CIRROS=1env | grep -E ""(DIB_COMMON_ELEMENTS|OVERCLOUD_CONTROLSCALE|TRIPLEO_TEST|USE_IRONIC|USE_CIRROS)=""",23,5
openstack%2Ftripleo-ci~master~Idde47b728f9b472cd48420f9e212cdc8a6a1c8bf,openstack/tripleo-ci,master,Idde47b728f9b472cd48420f9e212cdc8a6a1c8bf,"Revert ""TR: Switch Cinder to use oslo.concurrency""",MERGED,2014-11-24 13:08:24.000000000,2014-12-05 19:01:37.000000000,2014-12-05 19:01:37.000000000,"[{'_account_id': 3}, {'_account_id': 360}, {'_account_id': 1926}, {'_account_id': 6133}, {'_account_id': 8399}, {'_account_id': 9369}]","[{'number': 1, 'created': '2014-11-24 13:08:24.000000000', 'files': ['toci_devtest.sh'], 'web_link': 'https://opendev.org/openstack/tripleo-ci/commit/5f80f711d3ce6083361319cbd7b248988706cbcd', 'message': 'Revert ""TR: Switch Cinder to use oslo.concurrency""\n\nThis reverts commit 8a1f2ecafceaa8ce81b670d7f06a23da575b4403.\nThe revert is no longer needed, we\'ve added the appropriate\nconfig variable into the cinder element.\nSee Idcabf808d5f7255d0cc382b7857a4c13b9b7a2dc\n\nChange-Id: Idde47b728f9b472cd48420f9e212cdc8a6a1c8bf\n'}]",0,136750,5f80f711d3ce6083361319cbd7b248988706cbcd,13,6,1,1926,,,0,"Revert ""TR: Switch Cinder to use oslo.concurrency""

This reverts commit 8a1f2ecafceaa8ce81b670d7f06a23da575b4403.
The revert is no longer needed, we've added the appropriate
config variable into the cinder element.
See Idcabf808d5f7255d0cc382b7857a4c13b9b7a2dc

Change-Id: Idde47b728f9b472cd48420f9e212cdc8a6a1c8bf
",git fetch https://review.opendev.org/openstack/tripleo-ci refs/changes/50/136750/1 && git format-patch -1 --stdout FETCH_HEAD,['toci_devtest.sh'],1,5f80f711d3ce6083361319cbd7b248988706cbcd,,,# https://review.openstack.org/#/c/133478/10 temprevert cinder 372df8a75b33d23a42a8e636090c91668a56a586 1394963,0,2
openstack%2Frally~master~I6e441e0c4e68af067648ae39b96d74d82e8929c5,openstack/rally,master,I6e441e0c4e68af067648ae39b96d74d82e8929c5,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:58:22.000000000,2014-12-05 18:51:37.000000000,2014-12-05 18:51:36.000000000,"[{'_account_id': 3}, {'_account_id': 6172}, {'_account_id': 9545}]","[{'number': 1, 'created': '2014-12-05 03:58:22.000000000', 'files': ['doc/source/improve_rally.rst', 'doc/source/installation.rst', 'tests/unit/deploy/engines/test_devstack.py', 'rally/deploy/engines/devstack.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/520ee876e61f3e2d9daaa952a45ac18cc3688b1e', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I6e441e0c4e68af067648ae39b96d74d82e8929c5\n'}]",0,139498,520ee876e61f3e2d9daaa952a45ac18cc3688b1e,8,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I6e441e0c4e68af067648ae39b96d74d82e8929c5
",git fetch https://review.opendev.org/openstack/rally refs/changes/98/139498/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/improve_rally.rst', 'doc/source/installation.rst', 'tests/unit/deploy/engines/test_devstack.py', 'rally/deploy/engines/devstack.py']",4,520ee876e61f3e2d9daaa952a45ac18cc3688b1e,infra-manual,DEVSTACK_REPO = 'https://git.openstack.org/cgit/openstack-dev/devstack.git',DEVSTACK_REPO = 'https://github.com/openstack-dev/devstack.git',5,5
openstack%2Ftripleo-heat-templates~master~I96f91fb0d67e7fe203d3767c8ab89ce82adbe331,openstack/tripleo-heat-templates,master,I96f91fb0d67e7fe203d3767c8ab89ce82adbe331,Remove LiveUpdate params,MERGED,2014-12-01 15:15:10.000000000,2014-12-05 18:51:25.000000000,2014-12-05 18:51:25.000000000,"[{'_account_id': 3}, {'_account_id': 360}, {'_account_id': 6488}, {'_account_id': 8399}, {'_account_id': 9397}, {'_account_id': 11655}]","[{'number': 1, 'created': '2014-12-01 15:15:10.000000000', 'files': ['overcloud-without-mergepy.yaml', 'nova-compute-config.yaml', 'nova-compute-instance.yaml', 'compute.yaml', 'compute-config.yaml', 'overcloud-source.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/d1b7e15806d728703a8ab11c911fd7592f76b1a5', 'message': ""Remove LiveUpdate params\n\nThe params were added in I2997d23c584055c40034827e9beb58e6542ea11c\nas a means to pass undercloud image data to overcloud instances\nso they could perform an update via takeovernode). We've\nnever actually made use of them via takeovernode... furthermore\nthese params are a bit stale in that they haven't been applied\nto other instance types (storage, etc.).\n\nI propose we remove them entirely and start with a fresh plan for\nhow these would get used (perhaps a blueprint).  As is these don't\nappear to have ever been fully wired up to do anything removing\nthem should have no effect on end users.\n\nChange-Id: I96f91fb0d67e7fe203d3767c8ab89ce82adbe331\n""}]",0,138085,d1b7e15806d728703a8ab11c911fd7592f76b1a5,23,6,1,360,,,0,"Remove LiveUpdate params

The params were added in I2997d23c584055c40034827e9beb58e6542ea11c
as a means to pass undercloud image data to overcloud instances
so they could perform an update via takeovernode). We've
never actually made use of them via takeovernode... furthermore
these params are a bit stale in that they haven't been applied
to other instance types (storage, etc.).

I propose we remove them entirely and start with a fresh plan for
how these would get used (perhaps a blueprint).  As is these don't
appear to have ever been fully wired up to do anything removing
them should have no effect on end users.

Change-Id: I96f91fb0d67e7fe203d3767c8ab89ce82adbe331
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/85/138085/1 && git format-patch -1 --stdout FETCH_HEAD,"['overcloud-without-mergepy.yaml', 'nova-compute-config.yaml', 'nova-compute-instance.yaml', 'compute.yaml', 'compute-config.yaml', 'overcloud-source.yaml']",6,d1b7e15806d728703a8ab11c911fd7592f76b1a5,remove_live_update_params,, LiveUpdateComputeImage: type: string description: The image ID for live-updates to the overcloud compute nodes. default: '' LiveUpdateHost: type: string description: The IP address for the undercloud Glance API. default: '' LiveUpdatePassword: type: string default: '' description: The live-update password for the undercloud Glance API. hidden: true LiveUpdateTenantName: type: string description: The live-update tenant name for the undercloud Glance API. default: '' LiveUpdateUserName: type: string description: The live-update username for the undercloud Glance API. default: '',0,115
openstack%2Frally~master~Ibac269afdfcc9a0b2b13827cf16dab2c6041f2d4,openstack/rally,master,Ibac269afdfcc9a0b2b13827cf16dab2c6041f2d4,Adds trove python client to rally.osclients,MERGED,2014-12-04 13:31:12.000000000,2014-12-05 18:51:16.000000000,2014-12-05 18:51:15.000000000,"[{'_account_id': 3}, {'_account_id': 6172}, {'_account_id': 9545}, {'_account_id': 12395}, {'_account_id': 14135}]","[{'number': 1, 'created': '2014-12-04 13:31:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/rally/commit/ec01792ae3d8981f0d48fe19f4da1fe6d8a3ec75', 'message': 'Adds trove python client to rally.osclients\n\nChange-Id: Ibac269afdfcc9a0b2b13827cf16dab2c6041f2d4\n'}, {'number': 2, 'created': '2014-12-05 12:31:27.000000000', 'files': ['requirements.txt', 'tests/unit/test_osclients.py', 'tests/unit/fakes.py', 'rally/osclients.py'], 'web_link': 'https://opendev.org/openstack/rally/commit/662af3dda5626f878b0c0919fc0ed35e31f3a587', 'message': 'Adds trove python client to rally.osclients\n\nChange-Id: Ibac269afdfcc9a0b2b13827cf16dab2c6041f2d4\n'}]",0,139047,662af3dda5626f878b0c0919fc0ed35e31f3a587,15,5,2,1795,,,0,"Adds trove python client to rally.osclients

Change-Id: Ibac269afdfcc9a0b2b13827cf16dab2c6041f2d4
",git fetch https://review.opendev.org/openstack/rally refs/changes/47/139047/2 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'rally/osclients.py']",2,ec01792ae3d8981f0d48fe19f4da1fe6d8a3ec75,add-trove-to-osclients,"from troveclient import client as trove def trove(self, version='1.0'): """"""Returns trove client."""""" client = trove.Client(version, self.endpoint.username, self.endpoint.password, project_id=self.endpoint.tenant_name, auth_url=self.endpoint.auth_url, region_name=self.endpoint.region_name, timeout=CONF.openstack_client_http_timeout, insecure=CONF.https_insecure, cacert=CONF.https_cacert) return client @cached",,16,0
openstack%2Fpython-congressclient~master~I17ce9e942ab89501b7cc05105791d9f0aca53be8,openstack/python-congressclient,master,I17ce9e942ab89501b7cc05105791d9f0aca53be8,Fix version issue,MERGED,2014-12-05 06:22:57.000000000,2014-12-05 18:38:24.000000000,2014-12-05 18:38:23.000000000,"[{'_account_id': 3}, {'_account_id': 6482}, {'_account_id': 8215}, {'_account_id': 9253}]","[{'number': 1, 'created': '2014-12-05 06:22:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-congressclient/commit/7d3349016453682b3d38bd1762a728108f658894', 'message': 'Fix version issue\n\nThis patch fixes a bug that causes pbr to throw an error and also\nupdates the license header to match that of python-openstackclient.\n\nChange-Id: I17ce9e942ab89501b7cc05105791d9f0aca53be8\nCloses-bug: 1399538\n'}, {'number': 2, 'created': '2014-12-05 08:12:52.000000000', 'files': ['congressclient/__init__.py'], 'web_link': 'https://opendev.org/openstack/python-congressclient/commit/1e31e9de1025693d763a95eb99d5fe498bd6612b', 'message': 'Fix version issue\n\nThis patch fixes a bug that causes pbr to throw an error.\n\nCloses-bug: 1399538\nChange-Id: I17ce9e942ab89501b7cc05105791d9f0aca53be8\n'}]",0,139534,1e31e9de1025693d763a95eb99d5fe498bd6612b,9,4,2,4395,,,0,"Fix version issue

This patch fixes a bug that causes pbr to throw an error.

Closes-bug: 1399538
Change-Id: I17ce9e942ab89501b7cc05105791d9f0aca53be8
",git fetch https://review.opendev.org/openstack/python-congressclient refs/changes/34/139534/1 && git format-patch -1 --stdout FETCH_HEAD,['congressclient/__init__.py'],1,7d3349016453682b3d38bd1762a728108f658894,bug/1399538,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # __all__ = ['__version__']version_info = pbr.version.VersionInfo('python-congressclient') try: __version__ = version_info.version_string() except AttributeError: __version__ = None","# -*- coding: utf-8 -*- # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. __version__ = pbr.version.VersionInfo( 'congressclient').version_string()",18,15
openstack%2Fhorizon~master~I3d7d57bd8c8de40fe63979b892db7b7e8d6121c6,openstack/horizon,master,I3d7d57bd8c8de40fe63979b892db7b7e8d6121c6,Updated from global requirements,MERGED,2014-12-05 16:33:52.000000000,2014-12-05 18:37:34.000000000,2014-12-05 18:37:33.000000000,"[{'_account_id': 3}, {'_account_id': 5623}, {'_account_id': 6914}]","[{'number': 1, 'created': '2014-12-05 16:33:52.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/horizon/commit/a66afe996e29e075f852c51962e33ee38a6f6c5c', 'message': 'Updated from global requirements\n\nChange-Id: I3d7d57bd8c8de40fe63979b892db7b7e8d6121c6\n'}]",0,139680,a66afe996e29e075f852c51962e33ee38a6f6c5c,7,3,1,11131,,,0,"Updated from global requirements

Change-Id: I3d7d57bd8c8de40fe63979b892db7b7e8d6121c6
",git fetch https://review.opendev.org/openstack/horizon refs/changes/80/139680/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,a66afe996e29e075f852c51962e33ee38a6f6c5c,openstack/requirements,"XStatic>=1.0.0 # MIT License XStatic-Angular>=1.2.1.1 # MIT License XStatic-Angular-Cookies>=1.2.1.1 # MIT License XStatic-Angular-Mock>=1.2.1.1 # MIT License XStatic-Bootstrap-Datepicker>=1.3.1.0 # Apache 2.0 License XStatic-Bootstrap-SCSS>=3 # Apache 2.0 License XStatic-D3>=3.1.6.2 # BSD License (3 clause) XStatic-Hogan>=2.0.0.2 # Apache 2.0 License XStatic-Font-Awesome>=4.1.0 # SIL OFL 1.1 License, MIT License XStatic-Jasmine>=1.3.1.1 # MIT License XStatic-jQuery>=1.7.2 # MIT License XStatic-JQuery-Migrate>=1.2.1.1 # MIT License XStatic-JQuery.quicksearch>=2.0.3.1 # MIT License XStatic-JQuery.TableSorter>=2.0.5b.0 # MIT License XStatic-jquery-ui>=1.10.1 # MIT License XStatic-JSEncrypt>=2.0.0.2 # MIT License XStatic-QUnit>=1.14.0.2 # MIT License XStatic-Rickshaw>=1.5.0 # BSD License (prior) XStatic-Spin>=1.2.5.2 # MIT License","xstatic>=1.0.0 # MIT License xstatic-angular>=1.2.1.1 # MIT License xstatic-angular-cookies>=1.2.1.1 # MIT License xstatic-angular-mock>=1.2.1.1 # MIT License xstatic-bootstrap-datepicker>=1.3.1.0 # Apache 2.0 License xstatic-bootstrap-scss>=3 # Apache 2.0 License xstatic-d3>=3.1.6.2 # BSD License (3 clause) xstatic-hogan>=2.0.0.2 # Apache 2.0 License xstatic-font-awesome>=4.1.0 # SIL OFL 1.1 License, MIT License xstatic-jasmine>=1.3.1.1 # MIT License xstatic-jquery>=1.7.2 # MIT License xstatic-jquery-migrate>=1.2.1.1 # MIT License xstatic-jquery.quicksearch>=2.0.3.1 # MIT License xstatic-jquery.tablesorter>=2.0.5b.0 # MIT License xstatic-jquery-ui>=1.10.1 # MIT License xstatic-jsencrypt>=2.0.0.2 # MIT License xstatic-qunit>=1.14.0.2 # MIT License xstatic-rickshaw>=1.5.0 # BSD License (prior) xstatic-spin>=1.2.5.2 # MIT License",19,19
openstack%2Fironic~master~I233df03368dbb4b7805d6dce28de00fa037353c7,openstack/ironic,master,I233df03368dbb4b7805d6dce28de00fa037353c7,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:41:46.000000000,2014-12-05 18:30:22.000000000,2014-12-05 18:30:20.000000000,"[{'_account_id': 3}, {'_account_id': 2889}, {'_account_id': 3099}, {'_account_id': 10239}, {'_account_id': 12081}, {'_account_id': 13362}]","[{'number': 1, 'created': '2014-12-05 03:41:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cf5c164712ae742561c185e6288e43996c345f5f', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I233df03368dbb4b7805d6dce28de00fa037353c7\n'}, {'number': 2, 'created': '2014-12-05 14:28:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/871b5d8ba5bc8b765b11cc85878862cc5594e5e6', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nDropped sections that duplicate each other after the change.\n\nCo-Authored-By: Dmitry Tantsur <dtantsur@redhat.com>\nChange-Id: I233df03368dbb4b7805d6dce28de00fa037353c7\n'}, {'number': 3, 'created': '2014-12-05 15:55:45.000000000', 'files': ['CONTRIBUTING.rst', 'doc/source/dev/dev-quickstart.rst', 'doc/source/dev/contributing.rst', 'README.rst'], 'web_link': 'https://opendev.org/openstack/ironic/commit/eb150c51df0cce82d2e43dd1227609c058c4ba22', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nDropped sections that duplicate each other after the change.\n\nCo-Authored-By: Dmitry Tantsur <dtantsur@redhat.com>\nChange-Id: I233df03368dbb4b7805d6dce28de00fa037353c7\n'}]",4,139329,eb150c51df0cce82d2e43dd1227609c058c4ba22,25,6,3,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Dropped sections that duplicate each other after the change.

Co-Authored-By: Dmitry Tantsur <dtantsur@redhat.com>
Change-Id: I233df03368dbb4b7805d6dce28de00fa037353c7
",git fetch https://review.opendev.org/openstack/ironic refs/changes/29/139329/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'doc/source/dev/dev-quickstart.rst', 'doc/source/dev/contributing.rst', 'README.rst']",4,cf5c164712ae742561c185e6288e43996c345f5f,infra-manual, http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow, https://wiki.openstack.org/wiki/HowToContribute https://wiki.openstack.org/wiki/GerritWorkflow,8,8
openstack%2Fironic-specs~master~I6c4844884921b9d19cadafca1d4be2700659bc6d,openstack/ironic-specs,master,I6c4844884921b9d19cadafca1d4be2700659bc6d,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:41:53.000000000,2014-12-05 18:16:00.000000000,2014-12-05 18:16:00.000000000,"[{'_account_id': 3}, {'_account_id': 2889}, {'_account_id': 10239}]","[{'number': 1, 'created': '2014-12-05 03:41:53.000000000', 'files': ['README.rst'], 'web_link': 'https://opendev.org/openstack/ironic-specs/commit/5aeb76ecbde3539aa12ff07fca3b7db12a94f60f', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I6c4844884921b9d19cadafca1d4be2700659bc6d\n'}]",0,139331,5aeb76ecbde3539aa12ff07fca3b7db12a94f60f,7,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I6c4844884921b9d19cadafca1d4be2700659bc6d
",git fetch https://review.opendev.org/openstack/ironic-specs refs/changes/31/139331/1 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,5aeb76ecbde3539aa12ff07fca3b7db12a94f60f,infra-manual, http://docs.openstack.org/infra/manual/developers.html#development-workflow, https://wiki.openstack.org/wiki/Gerrit_Workflow,1,1
openstack%2Fsolum~master~Id853e4b6914db36b9a5b241e37b13f7feac29a21,openstack/solum,master,Id853e4b6914db36b9a5b241e37b13f7feac29a21,Fix value type in parameter,MERGED,2014-12-03 22:08:19.000000000,2014-12-05 18:05:59.000000000,2014-12-05 18:05:59.000000000,"[{'_account_id': 3}, {'_account_id': 1375}, {'_account_id': 2506}, {'_account_id': 6662}, {'_account_id': 9095}, {'_account_id': 11813}]","[{'number': 1, 'created': '2014-12-03 22:08:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/solum/commit/59a2511fbd77d69e17b225eedaaccad1698eb92f', 'message': 'Fix value type in parameter\n\nAdded support for bool, integer and float type of values.\n\nChange-Id: Id853e4b6914db36b9a5b241e37b13f7feac29a21\n'}, {'number': 2, 'created': '2014-12-03 22:11:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/solum/commit/04ccb8ea37f688e02206d6b5d38e36c0ae9dd1f7', 'message': 'Fix value type in parameter\n\nAdded support for bool, integer and float types of values.\n\nChange-Id: Id853e4b6914db36b9a5b241e37b13f7feac29a21\n'}, {'number': 3, 'created': '2014-12-04 18:19:15.000000000', 'files': ['solum/api/controllers/v1/datamodel/plan.py', 'solum/worker/handlers/shell.py', 'solum/api/handlers/plan_handler.py'], 'web_link': 'https://opendev.org/openstack/solum/commit/5d9ffa440e4d4b20c93891d9150c494417fa9a9b', 'message': 'Fix value type in parameter\n\nAdded support for bool, integer and float types of values.\n\nFix a minor bug in plan_handler related to parameter\n\nChange-Id: Id853e4b6914db36b9a5b241e37b13f7feac29a21\n'}]",2,138877,5d9ffa440e4d4b20c93891d9150c494417fa9a9b,17,6,3,6662,,,0,"Fix value type in parameter

Added support for bool, integer and float types of values.

Fix a minor bug in plan_handler related to parameter

Change-Id: Id853e4b6914db36b9a5b241e37b13f7feac29a21
",git fetch https://review.opendev.org/openstack/solum refs/changes/77/138877/2 && git format-patch -1 --stdout FETCH_HEAD,"['solum/api/controllers/v1/datamodel/plan.py', 'solum/worker/handlers/shell.py']",2,59a2511fbd77d69e17b225eedaaccad1698eb92f,drone," LOG.debug(""v type is %s, and value is %s"" % (type(s), s)) if type(s) in [str, unicode]: # Handles the case of exporting a var with a multi-line string return ''.join(['""', s.strip('\n').replace('""', '\\""'), '""']) else: return str(s) param_env[k] = _sanitize_param(v) else: f.write(""export %s=%s\n"" % (k, _sanitize_param(v)))"," # Handles the case of exporting a var with a multi-line string return ''.join(['""', s.strip('\n').replace('""', '\\""'), '""']) param_env[k] = v f.write(""export %s=%s\n"" % (k, _sanitize_param(v)))",13,5
openstack%2Fironic-inspector~master~I9d7107322530c5ac6e407a374acb52c4effe4563,openstack/ironic-inspector,master,I9d7107322530c5ac6e407a374acb52c4effe4563,Extend node_cache.pop_node() result to be a structure,MERGED,2014-12-04 15:48:28.000000000,2014-12-05 17:50:06.000000000,2014-12-05 17:50:05.000000000,"[{'_account_id': 3}, {'_account_id': 6773}, {'_account_id': 10239}]","[{'number': 1, 'created': '2014-12-04 15:48:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/1d3cf544d723fffa2f3769e7c925e749714e3b82', 'message': 'Extend node_cache.pop_node() result to be a structure\n\nIn the next patch we will need `started_at` to calculate timeout for\nwaiting for power off state. We will probably need other fields that\nmight be stored in the cache.\n\nChange-Id: I9d7107322530c5ac6e407a374acb52c4effe4563\nImplements: blueprint returning-to-ramdisk\n'}, {'number': 2, 'created': '2014-12-05 17:32:20.000000000', 'files': ['ironic_discoverd/test.py', 'ironic_discoverd/discoverd.py', 'ironic_discoverd/node_cache.py'], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/a29c3999faa1a28ba77c133b70ea4a46f05a63d5', 'message': 'Extend node_cache.pop_node() result to be a structure\n\nIn the next patch we will need `started_at` to calculate timeout for\nwaiting for power off state. We will probably need other fields that\nmight be stored in the cache.\n\nChange-Id: I9d7107322530c5ac6e407a374acb52c4effe4563\nImplements: blueprint returning-to-ramdisk\n'}]",1,139096,a29c3999faa1a28ba77c133b70ea4a46f05a63d5,12,3,2,10239,,,0,"Extend node_cache.pop_node() result to be a structure

In the next patch we will need `started_at` to calculate timeout for
waiting for power off state. We will probably need other fields that
might be stored in the cache.

Change-Id: I9d7107322530c5ac6e407a374acb52c4effe4563
Implements: blueprint returning-to-ramdisk
",git fetch https://review.opendev.org/openstack/ironic-inspector refs/changes/96/139096/2 && git format-patch -1 --stdout FETCH_HEAD,"['ironic_discoverd/test.py', 'ironic_discoverd/discoverd.py', 'ironic_discoverd/node_cache.py']",3,1d3cf544d723fffa2f3769e7c925e749714e3b82,bp/returning-to-ramdisk,"import collectionsNodeInfo = collections.namedtuple('NodeInfo', ('uuid', 'started_at')) """"""Record about a node in the cache."""""" :returns: structure NodeInfo with attributes ``uuid`` and ``created_at`` try: row = (db.execute('select started_at from nodes where uuid=?', (uuid,)) .fetchone()) if not row: LOG.error('Inconsistent database: %s is in attributes table, ' 'but not in nodes table', uuid) raise utils.DiscoveryFailed('Could not find a node', code=404) return NodeInfo(uuid=uuid, started_at=row[0]) finally: drop_node(uuid)", :returns: UUID drop_node(uuid) return uuid,41,14
openstack%2Fsahara~master~I5f84247f2afe056bff159baeccdcbbd12ad42545,openstack/sahara,master,I5f84247f2afe056bff159baeccdcbbd12ad42545,Update conf sample after oslo.messaging release,MERGED,2014-12-04 13:27:58.000000000,2014-12-05 17:48:14.000000000,2014-12-05 16:09:16.000000000,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7125}, {'_account_id': 7213}, {'_account_id': 7555}, {'_account_id': 7710}, {'_account_id': 8090}, {'_account_id': 8411}]","[{'number': 1, 'created': '2014-12-04 13:27:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/2aabaf150148fb9905a31744241c7757aa8df6bb', 'message': 'Update sahara.conf.sample after oslo.msg release\n\nChange-Id: I5f84247f2afe056bff159baeccdcbbd12ad42545\n'}, {'number': 2, 'created': '2014-12-04 20:31:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/sahara/commit/2faf1914da7f97c291c329fcfb08fa25759b715f', 'message': 'Update sahara.conf.sample after oslo.msg release\n\nCo-Authored-By: Andrew Lazarev <alazarev@mirantis.com>\n\nChange-Id: I5f84247f2afe056bff159baeccdcbbd12ad42545\n'}, {'number': 3, 'created': '2014-12-05 12:37:48.000000000', 'files': ['etc/sahara/sahara.conf.sample'], 'web_link': 'https://opendev.org/openstack/sahara/commit/df275db74867b8fb67ac19b04eba51a2ec76d32c', 'message': 'Update conf sample after oslo.messaging release\n\nChange-Id: I5f84247f2afe056bff159baeccdcbbd12ad42545\n'}]",0,139046,df275db74867b8fb67ac19b04eba51a2ec76d32c,19,8,3,6786,,,0,"Update conf sample after oslo.messaging release

Change-Id: I5f84247f2afe056bff159baeccdcbbd12ad42545
",git fetch https://review.opendev.org/openstack/sahara refs/changes/46/139046/1 && git format-patch -1 --stdout FETCH_HEAD,['etc/sahara/sahara.conf.sample'],1,2aabaf150148fb9905a31744241c7757aa8df6bb,,# Auto-delete queues in AMQP. (boolean value)# Use durable queues in AMQP. (boolean value)# TLSv1 and SSLv23. SSLv2 and SSLv3 may be available on some# The RabbitMQ login method. (string value) [oslo_messaging_amqp] # # From oslo.messaging # # Accept clients using either SSL or plain TCP (boolean value) # Deprecated group/name - [amqp1]/allow_insecure_clients #allow_insecure_clients = false # address prefix used when broadcasting to all servers (string value) # Deprecated group/name - [amqp1]/broadcast_prefix #broadcast_prefix = broadcast # Name for the AMQP container (string value) # Deprecated group/name - [amqp1]/container_name #container_name = <None> # address prefix when sending to any server in group (string value) # Deprecated group/name - [amqp1]/group_request_prefix #group_request_prefix = unicast # Timeout for inactive connections (in seconds) (integer value) # Deprecated group/name - [amqp1]/idle_timeout #idle_timeout = 0 # address prefix used when sending to a specific server (string value) # Deprecated group/name - [amqp1]/server_request_prefix #server_request_prefix = exclusive # CA certificate PEM file for verifing server certificate (string # value) # Deprecated group/name - [amqp1]/ssl_ca_file #ssl_ca_file = # Identifying certificate PEM file to present to clients (string # value) # Deprecated group/name - [amqp1]/ssl_cert_file #ssl_cert_file = # Private key PEM file used to sign cert_file certificate (string # value) # Deprecated group/name - [amqp1]/ssl_key_file #ssl_key_file = # Password for decrypting ssl_key_file (if encrypted) (string value) # Deprecated group/name - [amqp1]/ssl_key_password #ssl_key_password = <None> # Debug: dump AMQP frames to stdout (boolean value) # Deprecated group/name - [amqp1]/trace #trace = false,"# Auto-delete queues in amqp. (boolean value)# Use durable queues in amqp. (boolean value)# If passed, use a fake RabbitMQ provider. (boolean value) #fake_rabbit = false # TLSv1, SSLv23 and SSLv3. SSLv2 may be available on some# the RabbitMQ login method (string value)",58,7
openstack%2Foslo-incubator~master~Ia1820495a989f4f84530ab83f2d87d53a9f761df,openstack/oslo-incubator,master,Ia1820495a989f4f84530ab83f2d87d53a9f761df,Reports: Use sig handler traceback for curr thread,MERGED,2014-03-06 19:53:27.000000000,2014-12-05 17:37:53.000000000,2014-12-05 17:37:52.000000000,"[{'_account_id': 3}, {'_account_id': 597}, {'_account_id': 2472}, {'_account_id': 5638}, {'_account_id': 6928}, {'_account_id': 7536}, {'_account_id': 7677}, {'_account_id': 7725}]","[{'number': 1, 'created': '2014-03-06 19:53:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/2f40fce86ea702121ee9c772c93e44433b3da3af', 'message': 'Reports: Use sig handler traceback for curr thread\n\nPreviously, when an application dumped a GMR, the traceback for\nthe thread in which the GMR code executed showed the GMR code\nitself.  This is not particularly useful.\n\nThankfully, signal handlers are passed a Python frame object\nwhich contains the traceback from just before the signal handler\nis triggered.  We take this traceback and use it to replace\nthe traceback for the current thread in the threading generator.\n\nWhile the GMR signal handler passes this on to the GMR class\nand threading generator, the traceback parameter is optional\nin both places.  When not supplied with a traceback, the GMR\nand generator revert the the behavior from before this patch.\n\nChange-Id: Ia1820495a989f4f84530ab83f2d87d53a9f761df\n'}, {'number': 2, 'created': '2014-03-06 21:56:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/7584676562e30f8b7099fb70a361b42307aeb3fd', 'message': 'Reports: Use sig handler traceback for curr thread\n\nPreviously, when an application dumped a GMR, the traceback for\nthe thread in which the GMR code executed showed the GMR code\nitself.  This is not particularly useful.\n\nThankfully, signal handlers are passed a Python frame object\nwhich contains the traceback from just before the signal handler\nis triggered.  We take this traceback and use it to replace\nthe traceback for the current thread in the threading generator.\n\nWhile the GMR signal handler passes this on to the GMR class\nand threading generator, the traceback parameter is optional\nin both places.  When not supplied with a traceback, the GMR\nand generator revert the the behavior from before this patch.\n\nChange-Id: Ia1820495a989f4f84530ab83f2d87d53a9f761df\n'}, {'number': 3, 'created': '2014-05-01 20:29:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/2662dd8ddd3f0f912d6b5a9a3295ad77b50e84d4', 'message': 'Reports: Use sig handler traceback for curr thread\n\nPreviously, when an application dumped a GMR, the traceback for\nthe thread in which the GMR code executed showed the GMR code\nitself.  This is not particularly useful.\n\nThankfully, signal handlers are passed a Python frame object\nwhich contains the traceback from just before the signal handler\nis triggered.  We take this traceback and use it to replace\nthe traceback for the current thread in the threading generator.\n\nWhile the GMR signal handler passes this on to the GMR class\nand threading generator, the traceback parameter is optional\nin both places.  When not supplied with a traceback, the GMR\nand generator revert the the behavior from before this patch.\n\nChange-Id: Ia1820495a989f4f84530ab83f2d87d53a9f761df\n'}, {'number': 4, 'created': '2014-05-01 20:46:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/9b5d8892a65d7567f7797982408e68dd814a5759', 'message': 'Reports: Use sig handler traceback for curr thread\n\nPreviously, when an application dumped a GMR, the traceback for\nthe thread in which the GMR code executed showed the GMR code\nitself.  This is not particularly useful.\n\nThankfully, signal handlers are passed a Python frame object\nwhich contains the traceback from just before the signal handler\nis triggered.  We take this traceback and use it to replace\nthe traceback for the current thread in the threading generator.\n\nWhile the GMR signal handler passes this on to the GMR class\nand threading generator, the traceback parameter is optional\nin both places.  When not supplied with a traceback, the GMR\nand generator revert the the behavior from before this patch.\n\nChange-Id: Ia1820495a989f4f84530ab83f2d87d53a9f761df\n'}, {'number': 5, 'created': '2014-11-19 19:54:19.000000000', 'files': ['tests/unit/reports/test_openstack_generators.py', 'openstack/common/report/guru_meditation_report.py', 'openstack/common/report/generators/threading.py'], 'web_link': 'https://opendev.org/openstack/oslo-incubator/commit/8018ae2e3af5c1bf9cb51ffd2a6f3109dd2484dc', 'message': 'Reports: Use sig handler traceback for curr thread\n\nPreviously, when an application dumped a GMR, the traceback for\nthe thread in which the GMR code executed showed the GMR code\nitself.  This is not particularly useful.\n\nThankfully, signal handlers are passed a Python frame object\nwhich contains the traceback from just before the signal handler\nis triggered.  We take this traceback and use it to replace\nthe traceback for the current thread in the threading generator.\n\nWhile the GMR signal handler passes this on to the GMR class\nand threading generator, the traceback parameter is optional\nin both places.  When not supplied with a traceback, the GMR\nand generator revert the the behavior from before this patch.\n\nChange-Id: Ia1820495a989f4f84530ab83f2d87d53a9f761df\n'}]",6,78735,8018ae2e3af5c1bf9cb51ffd2a6f3109dd2484dc,43,8,5,7677,,,0,"Reports: Use sig handler traceback for curr thread

Previously, when an application dumped a GMR, the traceback for
the thread in which the GMR code executed showed the GMR code
itself.  This is not particularly useful.

Thankfully, signal handlers are passed a Python frame object
which contains the traceback from just before the signal handler
is triggered.  We take this traceback and use it to replace
the traceback for the current thread in the threading generator.

While the GMR signal handler passes this on to the GMR class
and threading generator, the traceback parameter is optional
in both places.  When not supplied with a traceback, the GMR
and generator revert the the behavior from before this patch.

Change-Id: Ia1820495a989f4f84530ab83f2d87d53a9f761df
",git fetch https://review.opendev.org/openstack/oslo-incubator refs/changes/35/78735/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack/common/report/guru_meditation_report.py', 'openstack/common/report/generators/threading.py']",2,2f40fce86ea702121ee9c772c93e44433b3da3af,report/use-signal-handler-traceback,"from __future__ import absolute_import import threading :func:`sys._current_frames()` . Its constuctor may optionally be passed a frame object. This frame object will be interpreted as the actual stack trace for the current thread, and, come generation time, will be used to replace the stack trace of the thread in which this code is running. def __init__(self, curr_thread_traceback=None): self.traceback = curr_thread_traceback def __call__(self): threadModels = dict( (thread_id, tm.ThreadModel(thread_id, stack)) for thread_id, stack in sys._current_frames().items() ) if self.traceback is not None: curr_thread_id = threading.current_thread().ident threadModels[curr_thread_id] = tm.ThreadModel(curr_thread_id, self.traceback) return mwdv.ModelWithDefaultViews(threadModels,"," :func:`sys._current_frames()` . def __call__(self): threadModels = [ tm.ThreadModel(thread_id, stack) for thread_id, stack in sys._current_frames().items() ] thread_pairs = dict(zip(range(len(threadModels)), threadModels)) return mwdv.ModelWithDefaultViews(thread_pairs,",32,15
openstack%2Ffuel-library~master~I3254f333b9caf3d417ec595896c6695fad617f1e,openstack/fuel-library,master,I3254f333b9caf3d417ec595896c6695fad617f1e,Fix type in pkill function,MERGED,2014-12-05 16:22:18.000000000,2014-12-05 17:33:17.000000000,2014-12-05 17:33:16.000000000,"[{'_account_id': 3}, {'_account_id': 6926}, {'_account_id': 8786}, {'_account_id': 8971}, {'_account_id': 9037}, {'_account_id': 11090}]","[{'number': 1, 'created': '2014-12-05 16:22:18.000000000', 'files': ['deployment/puppet/galera/files/ocf/mysql-wss'], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/0524973e9a4470a8269542c8327dfaec750831b3', 'message': 'Fix type in pkill function\n\nIn Different --pgroup behaves differently. -g behaves exactly the same\n\nChange-Id: I3254f333b9caf3d417ec595896c6695fad617f1e\nCloses-Bug: 1388779\n'}]",0,139674,0524973e9a4470a8269542c8327dfaec750831b3,13,6,1,11090,,,0,"Fix type in pkill function

In Different --pgroup behaves differently. -g behaves exactly the same

Change-Id: I3254f333b9caf3d417ec595896c6695fad617f1e
Closes-Bug: 1388779
",git fetch https://review.opendev.org/openstack/fuel-library refs/changes/74/139674/1 && git format-patch -1 --stdout FETCH_HEAD,['deployment/puppet/galera/files/ocf/mysql-wss'],1,0524973e9a4470a8269542c8327dfaec750831b3,bug/1388779, /usr/bin/pkill -9 -g ${pgrp} > /dev/null /usr/bin/pkill -9 -g ${pgrp} > /dev/null, /usr/bin/pkill -9 --pgroup ${pgrp} > /dev/null /usr/bin/pkill -9 --pgroup ${pgrp} > /dev/null,2,2
openstack%2Fsolum~master~I22a6ac59c1d25a8c872686815567d4bd26f5a41a,openstack/solum,master,I22a6ac59c1d25a8c872686815567d4bd26f5a41a,Not use Drone for unittest by default,MERGED,2014-12-03 18:11:38.000000000,2014-12-05 17:33:07.000000000,2014-12-05 17:33:06.000000000,"[{'_account_id': 3}, {'_account_id': 1375}, {'_account_id': 2506}, {'_account_id': 7784}, {'_account_id': 9095}]","[{'number': 1, 'created': '2014-12-03 18:11:38.000000000', 'files': ['solum/worker/handlers/shell.py', 'contrib/lp-cedarish/docker/unittest-app', 'solum/tests/worker/handlers/test_shell.py', 'solum/worker/handlers/shell_nobuild.py', 'contrib/lp-chef/docker/unittest-app'], 'web_link': 'https://opendev.org/openstack/solum/commit/d35ca3d284f72e053e422f585e08f40edc64e354', 'message': ""Not use Drone for unittest by default\n\nUser can choose to use Drone by defining '_SYSTEM_USE_DRONE: 1'\nin the parameter file.\n\nChange-Id: I22a6ac59c1d25a8c872686815567d4bd26f5a41a\n""}]",0,138810,d35ca3d284f72e053e422f585e08f40edc64e354,9,5,1,6662,,,0,"Not use Drone for unittest by default

User can choose to use Drone by defining '_SYSTEM_USE_DRONE: 1'
in the parameter file.

Change-Id: I22a6ac59c1d25a8c872686815567d4bd26f5a41a
",git fetch https://review.opendev.org/openstack/solum refs/changes/10/138810/1 && git format-patch -1 --stdout FETCH_HEAD,"['solum/worker/handlers/shell.py', 'contrib/lp-cedarish/docker/unittest-app', 'solum/tests/worker/handlers/test_shell.py', 'solum/worker/handlers/shell_nobuild.py', 'contrib/lp-chef/docker/unittest-app']",5,d35ca3d284f72e053e422f585e08f40edc64e354,drone,USE_DRONE=${_SYSTEM_USE_DRONE:-null}if [[ $USE_DRONE != null && $(which drone) ]]; then,if [[ $(which drone) ]]; then,22,19
openstack%2Fkeystone~master~Ib3928fbe55427ff49af722f47f402292b931cd24,openstack/keystone,master,Ib3928fbe55427ff49af722f47f402292b931cd24,Adds correct checks in LDAP backend tests,MERGED,2014-12-02 21:40:10.000000000,2014-12-05 17:31:00.000000000,2014-12-05 17:30:59.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 792}, {'_account_id': 1916}, {'_account_id': 2218}, {'_account_id': 2903}, {'_account_id': 5046}, {'_account_id': 5707}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 7191}, {'_account_id': 7725}, {'_account_id': 8871}, {'_account_id': 10046}]","[{'number': 1, 'created': '2014-12-02 21:40:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/7001b1c9cd31869cdf2026641839b27d64f2fa8d', 'message': 'Adds correct checks in LDAP backend tests\n\nCo-Authored-By: Rodrigo Duarte Sousa <rodrigods@lsd.ufcg.edu.br>\n\nChange-Id: Ib3928fbe55427ff49af722f47f402292b931cd24\nImplements: bp hierarchical-multitenancy\n'}, {'number': 2, 'created': '2014-12-02 21:45:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/6d9dec3906062d9546bcafb66c28f41805089dc2', 'message': 'Adds correct checks in LDAP backend tests\n\nCo-Authored-By: Rodrigo Duarte Sousa <rodrigods@lsd.ufcg.edu.br>\n\nChange-Id: Ib3928fbe55427ff49af722f47f402292b931cd24\nImplements: bp hierarchical-multitenancy\n'}, {'number': 3, 'created': '2014-12-03 14:45:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/f57a8417cefd5931b6fc77e6d2908c9d7e742ab5', 'message': 'Adds correct checks in LDAP backend tests\n\nCo-Authored-By: Rodrigo Duarte Sousa <rodrigods@lsd.ufcg.edu.br>\n\nChange-Id: Ib3928fbe55427ff49af722f47f402292b931cd24\nImplements: bp hierarchical-multitenancy\n'}, {'number': 4, 'created': '2014-12-03 19:00:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/5f54443b230d662f343fafd939c035fd20f39889', 'message': 'Adds correct checks in LDAP backend tests\n\nCo-Authored-By: Rodrigo Duarte Sousa <rodrigods@lsd.ufcg.edu.br>\n\nChange-Id: Ib3928fbe55427ff49af722f47f402292b931cd24\nImplements: bp hierarchical-multitenancy\n'}, {'number': 5, 'created': '2014-12-03 19:48:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/345721dd0e17ff600a4e883c31cfa1b3c417c1f9', 'message': 'Adds correct checks in LDAP backend tests\n\nCo-Authored-By: Rodrigo Duarte Sousa <rodrigods@lsd.ufcg.edu.br>\n\nChange-Id: Ib3928fbe55427ff49af722f47f402292b931cd24\nImplements: bp hierarchical-multitenancy\n'}, {'number': 6, 'created': '2014-12-04 21:25:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/847800a2d089cb3e75707c673a27965566beb3a8', 'message': 'Adds correct checks in LDAP backend tests\n\nCo-Authored-By: Rodrigo Duarte Sousa <rodrigods@lsd.ufcg.edu.br>\n\nChange-Id: Ib3928fbe55427ff49af722f47f402292b931cd24\nImplements: bp hierarchical-multitenancy\n'}, {'number': 7, 'created': '2014-12-05 15:06:25.000000000', 'files': ['keystone/exception.py', 'keystone/tests/test_backend_ldap.py', 'keystone/assignment/backends/ldap.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/8c630288b8f106a5097e81c165834aa9405cec53', 'message': 'Adds correct checks in LDAP backend tests\n\nCo-Authored-By: Henrique Truta <henrique@lsd.ufcg.edu.br>\n\nChange-Id: Ib3928fbe55427ff49af722f47f402292b931cd24\nImplements: bp hierarchical-multitenancy\n'}]",0,138551,8c630288b8f106a5097e81c165834aa9405cec53,21,14,7,11022,,,0,"Adds correct checks in LDAP backend tests

Co-Authored-By: Henrique Truta <henrique@lsd.ufcg.edu.br>

Change-Id: Ib3928fbe55427ff49af722f47f402292b931cd24
Implements: bp hierarchical-multitenancy
",git fetch https://review.opendev.org/openstack/keystone refs/changes/51/138551/3 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/exception.py', 'keystone/tests/test_backend_ldap.py', 'keystone/assignment/backends/ldap.py']",3,7001b1c9cd31869cdf2026641839b27d64f2fa8d,bp/hierarchical-multitenancy," def _validate_parent_project_is_none(self, ref): """"""If a parent_id different from None was given, raises InvalidProjectException. """""" parent_id = ref.get('parent_id') if parent_id is not None: raise exception.InvalidParentProject(parent_id) self._validate_parent_project_is_none(tenant)",,73,15
openstack%2Fmagnum~master~Idf88d6553bca5c0db0ceff65ea19813844ee01de,openstack/magnum,master,Idf88d6553bca5c0db0ceff65ea19813844ee01de,Add bay uuid to pod model objects,MERGED,2014-12-04 23:08:36.000000000,2014-12-05 17:30:52.000000000,2014-12-05 17:30:51.000000000,"[{'_account_id': 3}, {'_account_id': 2834}, {'_account_id': 5638}, {'_account_id': 6924}]","[{'number': 1, 'created': '2014-12-04 23:08:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/a6244c5c397f4254ee794d2c8ff820a75ab6c03c', 'message': 'Add bay uuid to pod model objects\n\nChange-Id: Idf88d6553bca5c0db0ceff65ea19813844ee01de\n'}, {'number': 2, 'created': '2014-12-05 17:23:53.000000000', 'files': ['magnum/db/sqlalchemy/api.py', 'magnum/objects/pod.py', 'magnum/tests/test_functional.py', 'magnum/api/controllers/v1/pod.py', 'magnum/db/sqlalchemy/models.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/c6ad7207e487ff7285fa40d2bc1c376b99c746a9', 'message': 'Add bay uuid to pod model objects\n\nChange-Id: Idf88d6553bca5c0db0ceff65ea19813844ee01de\n'}]",0,139256,c6ad7207e487ff7285fa40d2bc1c376b99c746a9,12,4,2,6924,,,0,"Add bay uuid to pod model objects

Change-Id: Idf88d6553bca5c0db0ceff65ea19813844ee01de
",git fetch https://review.opendev.org/openstack/magnum refs/changes/56/139256/1 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/db/sqlalchemy/api.py', 'magnum/objects/pod.py', 'magnum/api/controllers/v1/pod.py', 'magnum/db/sqlalchemy/models.py']",4,a6244c5c397f4254ee794d2c8ff820a75ab6c03c,, bay_uuid = Column(String(36)),,26,2
openstack%2Fhorizon~master~If9aed4b547f843d16ba729f84cb2c02acb2a0a4d,openstack/horizon,master,If9aed4b547f843d16ba729f84cb2c02acb2a0a4d,Fix dropdown menu does not open with firefox 34,MERGED,2014-12-04 22:54:58.000000000,2014-12-05 17:28:07.000000000,2014-12-05 17:28:05.000000000,"[{'_account_id': 3}, {'_account_id': 4978}, {'_account_id': 5623}, {'_account_id': 7509}, {'_account_id': 8577}, {'_account_id': 8648}, {'_account_id': 9317}, {'_account_id': 13325}]","[{'number': 1, 'created': '2014-12-04 22:54:58.000000000', 'files': ['openstack_dashboard/test/integration_tests/regions/menus.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/34e959e006283f8a37c294ab39fe70666293fe06', 'message': 'Fix dropdown menu does not open with firefox 34\n\nFirefox update changed behaviour of the selenium tests, it is not\nenough to click on the div representing dropdown menu region,\none must click on the first child (which is now button, but i suppose\nit can change over time to input or anything else) in order to open the\ndropdown menu.\n\nCloses-Bug: #1399268\nPartially implements blueprint: selenium-integration-testing\n\nChange-Id: If9aed4b547f843d16ba729f84cb2c02acb2a0a4d\n'}]",0,139246,34e959e006283f8a37c294ab39fe70666293fe06,13,8,1,12954,,,0,"Fix dropdown menu does not open with firefox 34

Firefox update changed behaviour of the selenium tests, it is not
enough to click on the div representing dropdown menu region,
one must click on the first child (which is now button, but i suppose
it can change over time to input or anything else) in order to open the
dropdown menu.

Closes-Bug: #1399268
Partially implements blueprint: selenium-integration-testing

Change-Id: If9aed4b547f843d16ba729f84cb2c02acb2a0a4d
",git fetch https://review.opendev.org/openstack/horizon refs/changes/46/139246/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack_dashboard/test/integration_tests/regions/menus.py'],1,34e959e006283f8a37c294ab39fe70666293fe06,bp/selenium-integration-testing," _menu_first_child_locator = (by.By.CSS_SELECTOR, '*') """"""Opens menu by clicking on the first child of the source element."""""" if self.is_open() is False: self._get_element(*self._menu_first_child_locator).click()"," """"""Opens menu."""""" if self.is_open() is False: self.src_elem.click()",3,2
openstack%2Fneutron~master~I5103e0d7c111880d9a02dd93de56c567066ed14c,openstack/neutron,master,I5103e0d7c111880d9a02dd93de56c567066ed14c,tox.ini: Prevent casual addition of bash dependency,MERGED,2014-12-03 06:58:18.000000000,2014-12-05 17:27:53.000000000,2014-12-05 17:27:51.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 748}, {'_account_id': 4656}, {'_account_id': 5170}, {'_account_id': 6854}, {'_account_id': 7787}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10980}, {'_account_id': 12040}, {'_account_id': 13051}]","[{'number': 1, 'created': '2014-12-03 06:58:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/bff5676dd9c6638baead8e2afefe586abc854af0', 'message': 'tox.ini: Prevent casual addition of bash dependency\n\nWhile bash is not available on every environments, there seems to be\na tendency to introduce more bash dependency very casually.  This check\nis intended to be a reminder to give people a chance to consider\nalternatives.\n\nRelated-Bug: #1398266\nChange-Id: I5103e0d7c111880d9a02dd93de56c567066ed14c\n'}, {'number': 2, 'created': '2014-12-05 03:22:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e23df5479ee43df1cefebb33b3a3bef9f700b05f', 'message': 'tox.ini: Prevent casual addition of bash dependency\n\nWhile bash is not available on every environments, there seems to be\na tendency to introduce more bash dependency very casually.  This check\nis intended to be a reminder to give people a chance to consider\nalternatives.\n\nRelated-Bug: #1398266\nChange-Id: I5103e0d7c111880d9a02dd93de56c567066ed14c\n'}, {'number': 3, 'created': '2014-12-05 03:25:48.000000000', 'files': ['tools/check_bash.sh', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/neutron/commit/846ae08d877ea4537dcc706ecf8d1dbf3cc9bfc7', 'message': 'tox.ini: Prevent casual addition of bash dependency\n\nWhile bash is not available on every environments, there seems to be\na tendency to introduce more bash dependency very casually.  This check\nis intended to be a reminder to give people a chance to consider\nalternatives.\n\nRelated-Bug: #1398266\nChange-Id: I5103e0d7c111880d9a02dd93de56c567066ed14c\n'}]",5,138650,846ae08d877ea4537dcc706ecf8d1dbf3cc9bfc7,58,23,3,6854,,,0,"tox.ini: Prevent casual addition of bash dependency

While bash is not available on every environments, there seems to be
a tendency to introduce more bash dependency very casually.  This check
is intended to be a reminder to give people a chance to consider
alternatives.

Related-Bug: #1398266
Change-Id: I5103e0d7c111880d9a02dd93de56c567066ed14c
",git fetch https://review.opendev.org/openstack/neutron refs/changes/50/138650/3 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,bff5676dd9c6638baead8e2afefe586abc854af0,bug/1398266," sh -c ""test $(grep bash tox.ini tools/*.sh | wc -l) -eq 3""",,1,0
openstack%2Frally~master~I156ffee8e1365e8a99dfd9c2ae0addd649ccbc84,openstack/rally,master,I156ffee8e1365e8a99dfd9c2ae0addd649ccbc84,"Added hacking rule for assertEqual(A in/not in B, True/False)",MERGED,2014-12-04 11:40:53.000000000,2014-12-05 17:20:21.000000000,2014-12-05 17:20:20.000000000,"[{'_account_id': 3}, {'_account_id': 8507}, {'_account_id': 9545}, {'_account_id': 12395}, {'_account_id': 14135}]","[{'number': 1, 'created': '2014-12-04 11:40:53.000000000', 'files': ['tests/hacking/checks.py', 'tests/unit/test_hacking.py', 'tests/hacking/README.rst'], 'web_link': 'https://opendev.org/openstack/rally/commit/e5ab898c0ae8582b2ac3ecab476db2a2a8f365f6', 'message': 'Added hacking rule for assertEqual(A in/not in B, True/False)\n\nAdded hacking rule to ensure that follow asserts are not used:\n- assertEqual(a in b, True/False)\n- assertEqual(a not in b, True/False)\n- assertEqual(True/False, a in b)\n- assertEqual(True/False, a not in b)\n\nChange-Id: I156ffee8e1365e8a99dfd9c2ae0addd649ccbc84\n'}]",0,139022,e5ab898c0ae8582b2ac3ecab476db2a2a8f365f6,9,5,1,9569,,,0,"Added hacking rule for assertEqual(A in/not in B, True/False)

Added hacking rule to ensure that follow asserts are not used:
- assertEqual(a in b, True/False)
- assertEqual(a not in b, True/False)
- assertEqual(True/False, a in b)
- assertEqual(True/False, a not in b)

Change-Id: I156ffee8e1365e8a99dfd9c2ae0addd649ccbc84
",git fetch https://review.opendev.org/openstack/rally refs/changes/22/139022/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/hacking/checks.py', 'tests/unit/test_hacking.py', 'tests/hacking/README.rst']",3,e5ab898c0ae8582b2ac3ecab476db2a2a8f365f6,," * [N324] - Ensure that ``assertEqual(A in/not in B, True/False)`` and ``assertEqual(True/False, A in/not in B)`` are not used with collection contents",,84,0
openstack%2Fneutron~master~I66ba3248a7ff502fa92e1c46f40e280c503524a2,openstack/neutron,master,I66ba3248a7ff502fa92e1c46f40e280c503524a2,Cleanup recent generalization in post mortem debugger,MERGED,2014-11-28 17:03:17.000000000,2014-12-05 17:16:46.000000000,2014-12-05 17:16:45.000000000,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 704}, {'_account_id': 748}, {'_account_id': 2035}, {'_account_id': 2874}, {'_account_id': 4656}, {'_account_id': 5170}, {'_account_id': 8645}, {'_account_id': 8788}, {'_account_id': 8873}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9845}, {'_account_id': 9846}, {'_account_id': 9925}, {'_account_id': 10116}, {'_account_id': 10117}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10184}, {'_account_id': 10294}, {'_account_id': 12040}, {'_account_id': 13051}]","[{'number': 1, 'created': '2014-11-28 17:03:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9b4f9ed866b8694bb737af55669da922785ec476', 'message': 'Cleanup recent generalization in post mortem debugger\n\nRemove unnecessary default arguments and constants in\nthe post mortem debugger code.\n\nAlso remove exception internationalization in testing code.\n\nChange-Id: I66ba3248a7ff502fa92e1c46f40e280c503524a2\n'}, {'number': 2, 'created': '2014-11-28 21:34:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/07acab76cb45b381ac61071fa79b174ec836e2ca', 'message': 'Cleanup recent generalization in post mortem debugger\n\nRemove unnecessary default arguments and constants in\nthe post mortem debugger code.\n\nAlso remove exception internationalization in testing code.\n\nChange-Id: I66ba3248a7ff502fa92e1c46f40e280c503524a2\n'}, {'number': 3, 'created': '2014-12-04 14:28:44.000000000', 'files': ['neutron/tests/unit/test_post_mortem_debug.py', 'neutron/tests/post_mortem_debug.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/13120bbd27bf0f6353b64e8c202525914b8dce3b', 'message': 'Cleanup recent generalization in post mortem debugger\n\nRemove unnecessary default arguments and constants in\nthe post mortem debugger code.\n\nAlso remove exception internationalization in testing code.\n\nChange-Id: I66ba3248a7ff502fa92e1c46f40e280c503524a2\n'}]",2,137833,13120bbd27bf0f6353b64e8c202525914b8dce3b,73,27,3,8788,,,0,"Cleanup recent generalization in post mortem debugger

Remove unnecessary default arguments and constants in
the post mortem debugger code.

Also remove exception internationalization in testing code.

Change-Id: I66ba3248a7ff502fa92e1c46f40e280c503524a2
",git fetch https://review.opendev.org/openstack/neutron refs/changes/33/137833/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/post_mortem_debug.py'],1,9b4f9ed866b8694bb737af55669da922785ec476,post-mortem-debugger," def get_exception_handler(debugger_name): debugger = _get_debugger(debugger_name) raise ValueError(""can't import %s module as a post mortem debugger"" % debugger_name) raise ValueError(""%s is not a supported post mortem debugger"" % debugger_name)","DEFAULT_DEBUGGER = 'pdb' def get_exception_handler(debugger_name=None): debugger = _get_debugger(debugger_name or DEFAULT_DEBUGGER) raise ValueError( _(""can't import %s module as a post mortem debugger"") % debugger_name) raise ValueError( _(""%s is not a supported post mortem debugger"") % debugger_name)",6,9
openstack%2Fmonasca-agent~master~I4915b2b548dd9d0197ae577e1f54fec71a970d38,openstack/monasca-agent,master,I4915b2b548dd9d0197ae577e1f54fec71a970d38,Fix http_status check,ABANDONED,2014-12-03 22:58:58.000000000,2014-12-05 17:08:35.000000000,,"[{'_account_id': 3}, {'_account_id': 10046}, {'_account_id': 11809}]","[{'number': 1, 'created': '2014-12-03 22:58:58.000000000', 'files': ['monagent/collector/checks_d/http_check.py'], 'web_link': 'https://opendev.org/openstack/monasca-agent/commit/542cfa313693c836da8553f3d525f56d33b023b7', 'message': 'Fix http_status check\n\nA tuple mismatch between _load_conf() and _check() was preventing the\nhttp_status check from functioning.\n\nChange-Id: I4915b2b548dd9d0197ae577e1f54fec71a970d38\n'}]",0,138888,542cfa313693c836da8553f3d525f56d33b023b7,11,3,1,12443,,,0,"Fix http_status check

A tuple mismatch between _load_conf() and _check() was preventing the
http_status check from functioning.

Change-Id: I4915b2b548dd9d0197ae577e1f54fec71a970d38
",git fetch https://review.opendev.org/openstack/monasca-agent refs/changes/88/138888/1 && git format-patch -1 --stdout FETCH_HEAD,['monagent/collector/checks_d/http_check.py'],1,542cfa313693c836da8553f3d525f56d33b023b7,feature/http_status-fix," return url, username, password, timeout, headers, response_time, dimensions, ssl, pattern, use_keystone addr, username, password, timeout, headers, response_time, dimensions, disable_ssl_validation, pattern, use_keystone = self._load_conf(instance)"," return url, username, password, timeout, headers, response_time, dimensions, ssl, pattern, use_keystone addr, username, password, timeout, headers, response_time, dimensions, disable_ssl_validation, pattern, use_keystone, token = self._load_conf( instance)",4,3
openstack%2Freviewday~master~I13da6fae21fe694344ced32cb6ef7e21ed85b215,openstack/reviewday,master,I13da6fae21fe694344ced32cb6ef7e21ed85b215,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:54:35.000000000,2014-12-05 17:06:10.000000000,2014-12-05 17:06:09.000000000,"[{'_account_id': 3}, {'_account_id': 6133}, {'_account_id': 6609}, {'_account_id': 7069}]","[{'number': 1, 'created': '2014-12-05 03:54:35.000000000', 'files': ['README.md'], 'web_link': 'https://opendev.org/openstack/reviewday/commit/afb28318ed23c8a2f13314a8942a0f11ca9494fd', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I13da6fae21fe694344ced32cb6ef7e21ed85b215\n'}]",0,139430,afb28318ed23c8a2f13314a8942a0f11ca9494fd,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I13da6fae21fe694344ced32cb6ef7e21ed85b215
",git fetch https://review.opendev.org/openstack/reviewday refs/changes/30/139430/1 && git format-patch -1 --stdout FETCH_HEAD,['README.md'],1,afb28318ed23c8a2f13314a8942a0f11ca9494fd,infra-manual,3. Setup your ssh credentials to work w/ Gerrit. See http://docs.openstack.org/infra/manual/developers.html#development-workflow for details.,3. Setup your ssh credentials to work w/ Gerrit. See http://wiki.openstack.org/GerritWorkflow for details.,1,1
openstack%2Ffuel-library~master~I2c228fbb9cb2f786bd9a0372979a489a78c1e7f3,openstack/fuel-library,master,I2c228fbb9cb2f786bd9a0372979a489a78c1e7f3,Create micro flavor after keystone,MERGED,2014-12-05 15:53:38.000000000,2014-12-05 17:05:06.000000000,2014-12-05 17:05:06.000000000,"[{'_account_id': 3}, {'_account_id': 6926}, {'_account_id': 7195}, {'_account_id': 7428}, {'_account_id': 8786}, {'_account_id': 8971}, {'_account_id': 11090}]","[{'number': 1, 'created': '2014-12-05 15:53:38.000000000', 'files': ['deployment/puppet/osnailyfacter/manifests/cluster_simple.pp'], 'web_link': 'https://opendev.org/openstack/fuel-library/commit/64fb306a99344678955d1472dee33d851ba8149e', 'message': 'Create micro flavor after keystone\n\nCreate micro flavor after keystone class.\nCloses-bug: #1399654\n\nChange-Id: I2c228fbb9cb2f786bd9a0372979a489a78c1e7f3\n'}]",0,139665,64fb306a99344678955d1472dee33d851ba8149e,13,7,1,9387,,,0,"Create micro flavor after keystone

Create micro flavor after keystone class.
Closes-bug: #1399654

Change-Id: I2c228fbb9cb2f786bd9a0372979a489a78c1e7f3
",git fetch https://review.opendev.org/openstack/fuel-library refs/changes/65/139665/1 && git format-patch -1 --stdout FETCH_HEAD,['deployment/puppet/osnailyfacter/manifests/cluster_simple.pp'],1,64fb306a99344678955d1472dee33d851ba8149e,bug/1399654," require => [Class['nova'],Class['openstack::auth_file'],Class['keystone']],"," require => [Class['nova'],Class['openstack::auth_file']],",1,1
openstack%2Fmagnum~master~Icda9a3086b69d5bf9071ef52c00edeac3c069b3c,openstack/magnum,master,Icda9a3086b69d5bf9071ef52c00edeac3c069b3c,Add service object to ReST API,ABANDONED,2014-12-04 00:33:08.000000000,2014-12-05 16:59:00.000000000,,[{'_account_id': 3}],"[{'number': 1, 'created': '2014-12-04 00:33:08.000000000', 'files': ['magnum/tests/test_functional.py', 'magnum/api/controllers/v1/__init__.py', 'magnum/db/sqlalchemy/alembic/versions/2581ebaf0cb2_initial_migration.py', 'magnum/db/sqlalchemy/models.py', 'magnum/api/controllers/v1/service.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/c423c5d274dbed5b72239ebce80488455ec5a53e', 'message': 'Add service object to ReST API\n\nThis adds a service object to the ReST API matching the versioned\nobject codebase.\n\nChange-Id: Icda9a3086b69d5bf9071ef52c00edeac3c069b3c\n'}]",0,138912,c423c5d274dbed5b72239ebce80488455ec5a53e,3,1,1,2834,,,0,"Add service object to ReST API

This adds a service object to the ReST API matching the versioned
object codebase.

Change-Id: Icda9a3086b69d5bf9071ef52c00edeac3c069b3c
",git fetch https://review.opendev.org/openstack/magnum refs/changes/12/138912/1 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/tests/test_functional.py', 'magnum/api/controllers/v1/__init__.py', 'magnum/db/sqlalchemy/alembic/versions/2581ebaf0cb2_initial_migration.py', 'magnum/db/sqlalchemy/models.py', 'magnum/api/controllers/v1/service.py']",5,c423c5d274dbed5b72239ebce80488455ec5a53e,,"# Copyright 2013 UnitedStack Inc. # All Rights Reserved.# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import datetimeimport wsmeext.pecan as wsme_pecan from magnum.api.controllers import base from magnum.api.controllers import link from magnum.api.controllers.v1 import collection from magnum.api.controllers.v1 import types from magnum.api.controllers.v1 import utils as api_utilsfrom magnum import objects class ServicePatchType(types.JsonPatchType): @staticmethod def mandatory_attrs(): return ['/service_uuid'] class Service(base.APIBase): """"""API representation of a service. This class enforces type checking and value constraints, and converts between the internal object model and the API representation of a service. """""" _service_uuid = None def _get_service_uuid(self): return self._service_uuid def _set_service_uuid(self, value): if value and self._service_uuid != value: try: # FIXME(comstud): One should only allow UUID here, but # there seems to be a bug in that tests are passing an # ID. See bug #1301046 for more details. service = objects.Node.get(pecan.request.context, value) self._service_uuid = service.uuid # NOTE(lucasagomes): Create the service_id attribute on-the-fly # to satisfy the api -> rpc object # conversion. self.service_id = service.id except exception.NodeNotFound as e: # Change error code because 404 (NotFound) is inappropriate # response for a POST request to create a Service e.code = 400 # BadRequest raise e elif value == wtypes.Unset: self._service_uuid = wtypes.Unset uuid = types.uuid """"""Unique UUID for this service"""""" name = wtypes.text """"""Name of this service"""""" links = wsme.wsattr([link.Link], readonly=True) """"""A list containing a self link and associated service links"""""" self.fields = [] fields = list(objects.Service.fields) # NOTE(lucasagomes): service_uuid is not part of objects.Service.fields # because it's an API-only attribute fields.append('service_uuid') for field in fields: # Skip fields we do not expose. if not hasattr(self, field): continue self.fields.append(field) setattr(self, field, kwargs.get(field, wtypes.Unset)) # NOTE(lucasagomes): service_id is an attribute created on-the-fly # by _set_service_uuid(), it needs to be present in the fields so # that as_dict() will contain service_id field when converting it # before saving it in the database. self.fields.append('service_id') setattr(self, 'service_uuid', kwargs.get('service_id', wtypes.Unset)) @staticmethod def _convert_with_links(service, url, expand=True): if not expand: service.unset_fields_except(['uuid', 'name']) # never expose the service_id attribute service.service_id = wtypes.Unset service.links = [link.Link.make_link('self', url, 'services', service.uuid), link.Link.make_link('bookmark', url, 'services', service.uuid, bookmark=True) ] return service @classmethod def convert_with_links(cls, rpc_service, expand=True): service = Service(**rpc_service.as_dict()) return cls._convert_with_links(service, pecan.request.host_url, expand) @classmethod def sample(cls, expand=True): sample = cls(uuid='27e3153e-d5bf-4b7e-b517-fb518e17f34c', name='example', type='virt', created_at=datetime.datetime.utcnow(), updated_at=datetime.datetime.utcnow()) # NOTE(lucasagomes): service_uuid getter() method look at the # _service_uuid variable sample._service_uuid = '7ae81bb3-dec3-4289-8d6c-da80bd8001ae' return cls._convert_with_links(sample, 'http://localhost:9511', expand) class ServiceCollection(collection.Collection): """"""API representation of a collection of services."""""" services = [Service] """"""A list containing services objects"""""" def __init__(self, **kwargs): self._type = 'services' @staticmethod def convert_with_links(rpc_services, limit, url=None, expand=False, **kwargs): collection = ServiceCollection() collection.services = [Service.convert_with_links(p, expand) for p in rpc_services] collection.next = collection.get_next(limit, url=url, **kwargs) return collection sample = cls() sample.services = [Service.sample(expand=False)] return sample class ServicesController(rest.RestController): """"""REST controller for Services."""""" from_services = False """"""A flag to indicate if the requests to this controller are coming from the top-level resource Nodes."""""" _custom_actions = { 'detail': ['GET'], } def _get_services_collection(self, marker, limit, sort_key, sort_dir, expand=False, resource_url=None): limit = api_utils.validate_limit(limit) sort_dir = api_utils.validate_sort_dir(sort_dir) marker_obj = None if marker: marker_obj = objects.Service.get_by_uuid(pecan.request.context, marker) services = objects.Service.list(pecan.request.context, limit, marker_obj, sort_key=sort_key, sort_dir=sort_dir) return ServiceCollection.convert_with_links(services, limit, url=resource_url, expand=expand, sort_key=sort_key, sort_dir=sort_dir) @wsme_pecan.wsexpose(ServiceCollection, types.uuid, types.uuid, int, wtypes.text, wtypes.text) def get_all(self, service_uuid=None, marker=None, limit=None, sort_key='id', sort_dir='asc'): """"""Retrieve a list of services. :param marker: pagination marker for large data sets. :param limit: maximum number of resources to return in a single result. :param sort_key: column to sort results by. Default: id. :param sort_dir: direction to sort. ""asc"" or ""desc"". Default: asc. """""" return self._get_services_collection(marker, limit, sort_key, sort_dir) @wsme_pecan.wsexpose(ServiceCollection, types.uuid, types.uuid, int, wtypes.text, wtypes.text) def detail(self, service_uuid=None, marker=None, limit=None, sort_key='id', sort_dir='asc'): """"""Retrieve a list of services with detail. :param service_uuid: UUID of a service, to get only services for that service. :param marker: pagination marker for large data sets. :param limit: maximum number of resources to return in a single result. :param sort_key: column to sort results by. Default: id. :param sort_dir: direction to sort. ""asc"" or ""desc"". Default: asc. """""" # NOTE(lucasagomes): /detail should only work agaist collections parent = pecan.request.path.split('/')[:-1][-1] if parent != ""services"": raise exception.HTTPNotFound expand = True resource_url = '/'.join(['services', 'detail']) return self._get_services_collection(marker, limit, sort_key, sort_dir, expand, resource_url) @wsme_pecan.wsexpose(Service, types.uuid) def get_one(self, service_uuid): """"""Retrieve information about the given service. :param service_uuid: UUID of a service. """""" if self.from_services: raise exception.OperationNotPermitted rpc_service = objects.Service.get_by_uuid(pecan.request.context, service_uuid) return Service.convert_with_links(rpc_service) @wsme_pecan.wsexpose(Service, body=Service, status_code=201) def post(self, service): """"""Create a new service. :param service: a service within the request body. """""" if self.from_services: raise exception.OperationNotPermitted new_service = objects.Service(pecan.request.context, **service.as_dict()) new_service.create() # Set the HTTP Location Header pecan.response.location = link.build_url('services', new_service.uuid) return Service.convert_with_links(new_service) @wsme.validate(types.uuid, [ServicePatchType]) @wsme_pecan.wsexpose(Service, types.uuid, body=[ServicePatchType]) def patch(self, service_uuid, patch): """"""Update an existing service. :param service_uuid: UUID of a service. :param patch: a json PATCH document to apply to this service. """""" if self.from_services: raise exception.OperationNotPermitted rpc_service = objects.Service.get_by_uuid(pecan.request.context, service_uuid) try: service_dict = rpc_service.as_dict() # NOTE(lucasagomes): # 1) Remove service_id because it's an internal value and # not present in the API object # 2) Add service_uuid service_dict['service_uuid'] = service_dict.pop('service_id', None) service = Service(**api_utils.apply_jsonpatch(service_dict, patch)) except api_utils.JSONPATCH_EXCEPTIONS as e: raise exception.PatchError(patch=patch, reason=e) # Update only the fields that have changed for field in objects.Service.fields: try: patch_val = getattr(service, field) except AttributeError: # Ignore fields that aren't exposed in the API continue if patch_val == wtypes.Unset: patch_val = None if rpc_service[field] != patch_val: rpc_service[field] = patch_val rpc_service = objects.Node.get_by_id(pecan.request.context, rpc_service.service_id) topic = pecan.request.rpcapi.get_topic_for(rpc_service) new_service = pecan.request.rpcapi.update_service( pecan.request.context, rpc_service, topic) return Service.convert_with_links(new_service) @wsme_pecan.wsexpose(None, types.uuid, status_code=204) def delete(self, service_uuid): """"""Delete a service. :param service_uuid: UUID of a service. """""" if self.from_services: raise exception.OperationNotPermitted rpc_service = objects.Service.get_by_uuid(pecan.request.context, service_uuid) rpc_service.destroy()","# Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at# http://www.apache.org/licenses/LICENSE-2.0# distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import uuid from magnum.api.controllers.v1.base import Basefrom magnum.common import yamlutils # NOTE(dims): We don't depend on oslo*i18n yet _ = _LI = _LW = _LE = _LC = lambda x: x class ServiceController(rest.RestController): @exception.wrap_pecan_controller_exception @pecan.expose(content_type='application/x-yaml') def get(self): """"""Retrieve a service by UUID."""""" res_yaml = yamlutils.dump({'dummy_data'}) pecan.response.status = 200 return res_yaml @exception.wrap_pecan_controller_exception @pecan.expose(content_type='application/x-yaml') def put(self): """"""Create a new service."""""" res_yaml = yamlutils.dump({'dummy_data'}) pecan.response.status = 200 return res_yaml @exception.wrap_pecan_controller_exception @pecan.expose(content_type='application/x-yaml') def delete(self): """"""Delete an existing service."""""" res_yaml = yamlutils.dump({'dummy_data'}) pecan.response.status = 200 return res_yaml class Service(Base): id = wtypes.text """""" The ID of the services."""""" name = wsme.wsattr(wtypes.text, mandatory=True) """""" The name of the service."""""" desc = wsme.wsattr(wtypes.text, mandatory=True) super(Service, self).__init__(**kwargs) return cls(id=str(uuid.uuid1(), name=""Docker"", desc='Docker Services'))",337,93
openstack%2Fmagnum~master~Ic241be46f8dca46081cff5672496d504a6084502,openstack/magnum,master,Ic241be46f8dca46081cff5672496d504a6084502,Load objects on start,ABANDONED,2014-12-02 02:36:36.000000000,2014-12-05 16:56:55.000000000,,"[{'_account_id': 3}, {'_account_id': 2834}, {'_account_id': 5638}]","[{'number': 1, 'created': '2014-12-02 02:36:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/e06767f3a7be0b5a05b0dde22bd567e7ca52744a', 'message': 'Load objects on start\n\nOn start of services, load the database objects into the registry.\n\nChange-Id: Ic241be46f8dca46081cff5672496d504a6084502\n'}, {'number': 2, 'created': '2014-12-02 02:55:02.000000000', 'files': ['magnum/common/rpc/service.py', 'magnum/common/service.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/ef98f823ab1d817bd36aa87ed7c598148ff02c64', 'message': 'Load objects on start\n\nOn start of services, load the database objects into the registry.\n\nChange-Id: Ic241be46f8dca46081cff5672496d504a6084502\n'}]",0,138229,ef98f823ab1d817bd36aa87ed7c598148ff02c64,7,3,2,2834,,,0,"Load objects on start

On start of services, load the database objects into the registry.

Change-Id: Ic241be46f8dca46081cff5672496d504a6084502
",git fetch https://review.opendev.org/openstack/magnum refs/changes/29/138229/2 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/common/rpc/service.py', 'magnum/common/service.py']",2,e06767f3a7be0b5a05b0dde22bd567e7ca52744a,,from magnum import objects objects.load(),,4,0
openstack%2Fdesignate~stable%2Fjuno~I7729a4dd25622aa101f1850baea227b0d0a25066,openstack/designate,stable/juno,I7729a4dd25622aa101f1850baea227b0d0a25066,Updated from global requirements,MERGED,2014-12-05 00:13:44.000000000,2014-12-05 16:50:48.000000000,2014-12-05 16:50:47.000000000,"[{'_account_id': 3}, {'_account_id': 741}, {'_account_id': 8094}]","[{'number': 1, 'created': '2014-12-05 00:13:44.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/designate/commit/2a2ede84eb91453cf8f4fa87f91aa6fa386e7005', 'message': 'Updated from global requirements\n\nChange-Id: I7729a4dd25622aa101f1850baea227b0d0a25066\n'}]",0,139275,2a2ede84eb91453cf8f4fa87f91aa6fa386e7005,7,3,1,11131,,,0,"Updated from global requirements

Change-Id: I7729a4dd25622aa101f1850baea227b0d0a25066
",git fetch https://review.opendev.org/openstack/designate refs/changes/75/139275/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,2a2ede84eb91453cf8f4fa87f91aa6fa386e7005,openstack/requirements,"oslo.messaging>=1.4.0,!=1.5.0",oslo.messaging>=1.4.0,1,1
openstack%2Fneutron~stable%2Ficehouse~Ic11815d29551b9232d2506fa69af1627cf863246,openstack/neutron,stable/icehouse,Ic11815d29551b9232d2506fa69af1627cf863246,(Don't Merge) Apic backport on icehouse stable,ABANDONED,2014-09-23 22:31:31.000000000,2014-12-05 16:50:33.000000000,,"[{'_account_id': 3}, {'_account_id': 5170}, {'_account_id': 7987}, {'_account_id': 9656}, {'_account_id': 9681}, {'_account_id': 9682}, {'_account_id': 9695}, {'_account_id': 9732}, {'_account_id': 9787}, {'_account_id': 9846}, {'_account_id': 10119}, {'_account_id': 10121}, {'_account_id': 10153}, {'_account_id': 10192}, {'_account_id': 10294}, {'_account_id': 10310}, {'_account_id': 10387}, {'_account_id': 10503}, {'_account_id': 10692}, {'_account_id': 12040}, {'_account_id': 13308}]","[{'number': 1, 'created': '2014-09-23 22:31:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/f4976aa18a698ddb96a3f8c769687e67e65cc8ec', 'message': ""(Don't Merge) Apic backport on icehouse stable\n\nChange-Id: Ic11815d29551b9232d2506fa69af1627cf863246\n""}, {'number': 2, 'created': '2014-09-24 18:22:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/1bed5c521c52b66bf78f32aef96a514b7cc02d01', 'message': ""(Don't Merge) Apic backport on icehouse stable\n\nChange-Id: Ic11815d29551b9232d2506fa69af1627cf863246\n""}, {'number': 3, 'created': '2014-09-24 18:28:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/cce0656a980277dc0784dbb49eafbed864a471ae', 'message': ""(Don't Merge) Apic backport on icehouse stable\n\nChange-Id: Ic11815d29551b9232d2506fa69af1627cf863246\n""}, {'number': 4, 'created': '2014-10-06 21:56:40.000000000', 'files': ['neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_sync.py', 'etc/neutron/rootwrap.d/cisco-apic.filters', 'neutron/tests/unit/services/l3_router/__init__.py', 'neutron/plugins/ml2/drivers/cisco/apic/apic_topology.py', 'neutron/db/migration/alembic_migrations/versions/HEAD', 'neutron/plugins/ml2/drivers/cisco/apic/apic_sync.py', 'neutron/plugins/ml2/drivers/cisco/apic/mechanism_apic.py', 'neutron/tests/unit/services/l3_router/test_l3_apic_plugin.py', 'neutron/services/l3_router/l3_apic.py', 'neutron/db/migration/alembic_migrations/versions/32f3915891fd_cisco_apic_driver_update.py', 'neutron/plugins/ml2/drivers/cisco/apic/apic_model.py', 'neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_topology_agent.py', 'neutron/db/migration/alembic_migrations/versions/1b837a7125a9_cisco_apic_driver.py', 'etc/neutron/plugins/ml2/ml2_conf_cisco.ini', 'neutron/tests/unit/ml2/drivers/cisco/apic/__init__.py', 'neutron/service.py', 'neutron/manager.py', 'neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_common.py', 'neutron/plugins/ml2/drivers/cisco/apic/__init__.py', 'setup.cfg', 'neutron/plugins/ml2/drivers/cisco/apic/config.py', 'neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_mechanism_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/f935389da278be0bd2e2e64ccf711efad21030b8', 'message': ""(Don't Merge) Apic backport on icehouse stable\n\nChange-Id: Ic11815d29551b9232d2506fa69af1627cf863246\n""}]",0,123596,f935389da278be0bd2e2e64ccf711efad21030b8,73,21,4,7987,,,0,"(Don't Merge) Apic backport on icehouse stable

Change-Id: Ic11815d29551b9232d2506fa69af1627cf863246
",git fetch https://review.opendev.org/openstack/neutron refs/changes/96/123596/4 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_sync.py', 'etc/neutron/rootwrap.d/cisco-apic.filters', 'neutron/tests/unit/services/l3_router/__init__.py', 'neutron/plugins/ml2/drivers/cisco/apic/apic_topology.py', 'neutron/plugins/ml2/drivers/cisco/apic/apic_sync.py', 'neutron/plugins/ml2/drivers/cisco/apic/mechanism_apic.py', 'neutron/tests/unit/services/l3_router/test_l3_apic_plugin.py', 'neutron/services/l3_router/l3_apic.py', 'neutron/db/migration/alembic_migrations/versions/32f3915891fd_cisco_apic_driver_update.py', 'neutron/plugins/ml2/drivers/cisco/apic/apic_model.py', 'neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_topology_agent.py', 'neutron/db/migration/alembic_migrations/versions/1b837a7125a9_cisco_apic_driver.py', 'etc/neutron/plugins/ml2/ml2_conf_cisco.ini', 'neutron/tests/unit/ml2/drivers/cisco/apic/__init__.py', 'neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_common.py', 'neutron/plugins/ml2/drivers/cisco/apic/__init__.py', 'neutron/plugins/ml2/drivers/cisco/apic/config.py', 'neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_mechanism_driver.py']",18,f4976aa18a698ddb96a3f8c769687e67e65cc8ec,apic-driver-backport,"# Copyright (c) 2014 Cisco Systems # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # # @author: Henry Gessau, Cisco Systems import sys import mock sys.modules[""apicapi""] = mock.Mock() from neutron.common import constants as n_constants from neutron.extensions import portbindings from neutron.plugins.ml2.drivers.cisco.apic import mechanism_apic as md from neutron.plugins.ml2.drivers import type_vlan # noqa from neutron.tests import base from neutron.tests.unit.ml2.drivers.cisco.apic import ( test_cisco_apic_common as mocked) HOST_ID1 = 'ubuntu' HOST_ID2 = 'rhel' ENCAP = '101' SUBNET_GATEWAY = '10.3.2.1' SUBNET_CIDR = '10.3.1.0/24' SUBNET_NETMASK = '24' TEST_SEGMENT1 = 'test-segment1' TEST_SEGMENT2 = 'test-segment2' class TestCiscoApicMechDriver(base.BaseTestCase, mocked.ControllerMixin, mocked.ConfigMixin): def setUp(self): super(TestCiscoApicMechDriver, self).setUp() mocked.ControllerMixin.set_up_mocks(self) mocked.ConfigMixin.set_up_mocks(self) self.mock_apic_manager_login_responses() self.driver = md.APICMechanismDriver() self.driver.synchronizer = None md.APICMechanismDriver.get_base_synchronizer = mock.Mock() self.driver.vif_type = 'test-vif_type' self.driver.cap_port_filter = 'test-cap_port_filter' self.driver.name_mapper = mock.Mock() self.driver.name_mapper.tenant.return_value = mocked.APIC_TENANT self.driver.name_mapper.network.return_value = mocked.APIC_NETWORK self.driver.name_mapper.subnet.return_value = mocked.APIC_SUBNET self.driver.name_mapper.port.return_value = mocked.APIC_PORT self.driver.name_mapper.router.return_value = mocked.APIC_ROUTER self.driver.name_mapper.app_profile.return_value = mocked.APIC_AP self.driver.apic_manager = mock.Mock( name_mapper=mock.Mock(), ext_net_dict=self.external_network_dict) self.driver.apic_manager.apic.transaction = self.fake_transaction def test_initialize(self): mgr = self.driver.apic_manager self.driver.initialize() mgr.ensure_infra_created_on_apic.assert_called_once() mgr.ensure_bgp_pod_policy_created_on_apic.assert_called_once() def test_update_port_postcommit(self): net_ctx = self._get_network_context(mocked.APIC_TENANT, mocked.APIC_NETWORK, TEST_SEGMENT1) port_ctx = self._get_port_context(mocked.APIC_TENANT, mocked.APIC_NETWORK, 'vm1', net_ctx, HOST_ID1) mgr = self.driver.apic_manager self.driver.update_port_postcommit(port_ctx) mgr.ensure_path_created_for_port.assert_called_once_with( mocked.APIC_TENANT, mocked.APIC_NETWORK, HOST_ID1, ENCAP, transaction='transaction') def test_update_gw_port_postcommit(self): net_ctx = self._get_network_context(mocked.APIC_TENANT, mocked.APIC_NETWORK, TEST_SEGMENT1, external=True) port_ctx = self._get_port_context(mocked.APIC_TENANT, mocked.APIC_NETWORK, 'vm1', net_ctx, HOST_ID1, gw=True) mgr = self.driver.apic_manager mgr.get_router_contract.return_value = mocked.FakeDbContract( mocked.APIC_CONTRACT) self.driver.update_port_postcommit(port_ctx) mgr.get_router_contract.assert_called_once_with( port_ctx.current['device_id']) mgr.ensure_context_enforced.assert_called_once() mgr.ensure_external_routed_network_created.assert_called_once_with( mocked.APIC_NETWORK, transaction='transaction') mgr.ensure_logical_node_profile_created.assert_called_once_with( mocked.APIC_NETWORK, mocked.APIC_EXT_SWITCH, mocked.APIC_EXT_MODULE, mocked.APIC_EXT_PORT, mocked.APIC_EXT_ENCAP, mocked.APIC_EXT_CIDR_EXPOSED, transaction='transaction') mgr.ensure_static_route_created.assert_called_once_with( mocked.APIC_NETWORK, mocked.APIC_EXT_SWITCH, mocked.APIC_EXT_GATEWAY_IP, transaction='transaction') mgr.ensure_external_epg_created.assert_called_once_with( mocked.APIC_NETWORK, transaction='transaction') mgr.ensure_external_epg_consumed_contract.assert_called_once_with( mocked.APIC_NETWORK, mgr.get_router_contract.return_value, transaction='transaction') mgr.ensure_external_epg_provided_contract.assert_called_once_with( mocked.APIC_NETWORK, mgr.get_router_contract.return_value, transaction='transaction') def test_update_gw_port_postcommit_fail_contract_create(self): net_ctx = self._get_network_context(mocked.APIC_TENANT, mocked.APIC_NETWORK, TEST_SEGMENT1, external=True) port_ctx = self._get_port_context(mocked.APIC_TENANT, mocked.APIC_NETWORK, 'vm1', net_ctx, HOST_ID1, gw=True) mgr = self.driver.apic_manager self.driver.update_port_postcommit(port_ctx) mgr.ensure_external_routed_network_deleted.assert_called_once() def test_create_network_postcommit(self): ctx = self._get_network_context(mocked.APIC_TENANT, mocked.APIC_NETWORK, TEST_SEGMENT1) mgr = self.driver.apic_manager self.driver.create_network_postcommit(ctx) mgr.ensure_bd_created_on_apic.assert_called_once_with( mocked.APIC_TENANT, mocked.APIC_NETWORK, transaction='transaction') mgr.ensure_epg_created.assert_called_once_with( mocked.APIC_TENANT, mocked.APIC_NETWORK, transaction='transaction') def test_create_external_network_postcommit(self): ctx = self._get_network_context(mocked.APIC_TENANT, mocked.APIC_NETWORK, TEST_SEGMENT1, external=True) mgr = self.driver.apic_manager self.driver.create_network_postcommit(ctx) self.assertFalse(mgr.ensure_bd_created_on_apic.called) self.assertFalse(mgr.ensure_epg_created.called) def test_delete_network_postcommit(self): ctx = self._get_network_context(mocked.APIC_TENANT, mocked.APIC_NETWORK, TEST_SEGMENT1) mgr = self.driver.apic_manager self.driver.delete_network_postcommit(ctx) mgr.delete_bd_on_apic.assert_called_once_with( mocked.APIC_TENANT, mocked.APIC_NETWORK, transaction='transaction') mgr.delete_epg_for_network.assert_called_once_with( mocked.APIC_TENANT, mocked.APIC_NETWORK, transaction='transaction') def test_delete_external_network_postcommit(self): ctx = self._get_network_context(mocked.APIC_TENANT, mocked.APIC_NETWORK, TEST_SEGMENT1, external=True) mgr = self.driver.apic_manager self.driver.delete_network_postcommit(ctx) mgr.delete_external_routed_network.assert_called_once_with( mocked.APIC_NETWORK) def test_create_subnet_postcommit(self): net_ctx = self._get_network_context(mocked.APIC_TENANT, mocked.APIC_NETWORK, TEST_SEGMENT1) subnet_ctx = self._get_subnet_context(SUBNET_GATEWAY, SUBNET_CIDR, net_ctx) mgr = self.driver.apic_manager self.driver.create_subnet_postcommit(subnet_ctx) mgr.ensure_subnet_created_on_apic.assert_called_once_with( mocked.APIC_TENANT, mocked.APIC_NETWORK, '%s/%s' % (SUBNET_GATEWAY, SUBNET_NETMASK)) def _get_network_context(self, tenant_id, net_id, seg_id=None, seg_type='vlan', external=False): network = {'id': net_id, 'name': net_id + '-name', 'tenant_id': tenant_id, 'provider:segmentation_id': seg_id} if external: network['router:external'] = True if seg_id: network_segments = [{'id': seg_id, 'segmentation_id': ENCAP, 'network_type': seg_type, 'physical_network': 'physnet1'}] else: network_segments = [] return FakeNetworkContext(network, network_segments) def _get_subnet_context(self, gateway_ip, cidr, network): subnet = {'tenant_id': network.current['tenant_id'], 'network_id': network.current['id'], 'id': '[%s/%s]' % (gateway_ip, cidr), 'gateway_ip': gateway_ip, 'cidr': cidr} return FakeSubnetContext(subnet, network) def _get_port_context(self, tenant_id, net_id, vm_id, network, host, gw=False): port = {'device_id': vm_id, 'device_owner': 'compute', 'binding:host_id': host, 'tenant_id': tenant_id, 'id': mocked.APIC_PORT, 'name': mocked.APIC_PORT, 'network_id': net_id} if gw: port['device_owner'] = n_constants.DEVICE_OWNER_ROUTER_GW port['device_id'] = mocked.APIC_ROUTER return FakePortContext(port, network) class FakeNetworkContext(object): """"""To generate network context for testing purposes only."""""" def __init__(self, network, segments): self._network = network self._segments = segments @property def current(self): return self._network @property def network_segments(self): return self._segments class FakeSubnetContext(object): """"""To generate subnet context for testing purposes only."""""" def __init__(self, subnet, network): self._subnet = subnet self._network = network self._plugin = mock.Mock() self._plugin_context = mock.Mock() self._plugin.get_network.return_value = {} @property def current(self): return self._subnet @property def network(self): return self._network class FakePortContext(object): """"""To generate port context for testing purposes only."""""" def __init__(self, port, network): self._port = port self._network = network self._plugin = mock.Mock() self._plugin_context = mock.Mock() self._plugin.get_ports.return_value = [] if network.network_segments: self._bound_segment = network.network_segments[0] else: self._bound_segment = None @property def current(self): return self._port @property def network(self): return self._network @property def bound_segment(self): return self._bound_segment def set_binding(self, segment_id, vif_type, cap_port_filter): pass @property def host(self): return self._port.get(portbindings.HOST_ID) @property def original_host(self): return self._original_port.get(portbindings.HOST_ID) ",,2503,0
openstack%2Fmagnum~master~I28f477f596a9a464b3ffbb33ef295b375fa3fd65,openstack/magnum,master,I28f477f596a9a464b3ffbb33ef295b375fa3fd65,Add stubs for the container actions,MERGED,2014-12-05 02:56:00.000000000,2014-12-05 16:48:35.000000000,2014-12-05 16:48:34.000000000,"[{'_account_id': 3}, {'_account_id': 2834}]","[{'number': 1, 'created': '2014-12-05 02:56:00.000000000', 'files': ['magnum/api/controllers/v1/container.py', 'magnum/tests/test_functional.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/b2c57bbd85ab12f2e9f9f6efa6a1148c9d5af21e', 'message': 'Add stubs for the container actions\n\nChange-Id: I28f477f596a9a464b3ffbb33ef295b375fa3fd65\n'}]",0,139304,b2c57bbd85ab12f2e9f9f6efa6a1148c9d5af21e,6,2,1,5638,,,0,"Add stubs for the container actions

Change-Id: I28f477f596a9a464b3ffbb33ef295b375fa3fd65
",git fetch https://review.opendev.org/openstack/magnum refs/changes/04/139304/1 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/api/controllers/v1/container.py', 'magnum/tests/test_functional.py']",2,b2c57bbd85ab12f2e9f9f6efa6a1148c9d5af21e,," # Execute some actions actions = ['start', 'stop', 'pause', 'unpause', 'reboot', 'logs', 'execute'] for action in actions: response = self.app.put('/v1/containers/%s/%s' % (c.get('uuid'), action)) self.assertEqual(response.status_int, 200) ",,58,0
openstack%2Fdocs-specs~master~I4ee5218e9c77661a30ef1550797c49689b9f95a6,openstack/docs-specs,master,I4ee5218e9c77661a30ef1550797c49689b9f95a6,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:41:09.000000000,2014-12-05 16:47:13.000000000,2014-12-05 16:47:13.000000000,"[{'_account_id': 3}, {'_account_id': 964}, {'_account_id': 6547}, {'_account_id': 6772}]","[{'number': 1, 'created': '2014-12-05 03:41:09.000000000', 'files': ['README.rst'], 'web_link': 'https://opendev.org/openstack/docs-specs/commit/4f30d71dc0a7ef5a8d44847a35f559aa38aaa6ca', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I4ee5218e9c77661a30ef1550797c49689b9f95a6\n'}]",0,139320,4f30d71dc0a7ef5a8d44847a35f559aa38aaa6ca,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I4ee5218e9c77661a30ef1550797c49689b9f95a6
",git fetch https://review.opendev.org/openstack/docs-specs refs/changes/20/139320/1 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,4f30d71dc0a7ef5a8d44847a35f559aa38aaa6ca,infra-manual,http://docs.openstack.org/infra/manual/developers.html#development-workflow.,https://wiki.openstack.org/wiki/Gerrit_Workflow.,1,1
openstack%2Fmagnum~master~I7d238bb23597d42e9ec0648e56b3741ad9e2791b,openstack/magnum,master,I7d238bb23597d42e9ec0648e56b3741ad9e2791b,removed unused file for root controller,MERGED,2014-12-05 02:25:10.000000000,2014-12-05 16:46:56.000000000,2014-12-05 16:46:55.000000000,"[{'_account_id': 3}, {'_account_id': 2834}]","[{'number': 1, 'created': '2014-12-05 02:25:10.000000000', 'files': ['magnum/api/controllers/v1/root.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/1d671849442c6e6c9315968e88d8ae5ac908dcbb', 'message': 'removed unused file for root controller\n\nChange-Id: I7d238bb23597d42e9ec0648e56b3741ad9e2791b\n'}]",0,139302,1d671849442c6e6c9315968e88d8ae5ac908dcbb,6,2,1,5638,,,0,"removed unused file for root controller

Change-Id: I7d238bb23597d42e9ec0648e56b3741ad9e2791b
",git fetch https://review.opendev.org/openstack/magnum refs/changes/02/139302/1 && git format-patch -1 --stdout FETCH_HEAD,['magnum/api/controllers/v1/root.py'],1,1d671849442c6e6c9315968e88d8ae5ac908dcbb,,,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import pecan import wsmeext.pecan as wsme_pecan from magnum.api.controllers import common_types from magnum.api.controllers.v1 import bay from magnum.api.controllers.v1 import container from magnum.api.controllers.v1.datamodel import types as api_types from magnum.api.controllers.v1 import pod from magnum.api.controllers.v1 import service from magnum.common import exception from magnum import version class Platform(api_types.Base): bays_uri = common_types.Uri ""URI to Bays"" pods_uri = common_types.Uri ""URI to Pods"" services_uri = common_types.Uri ""URI to Services"" containers_uri = common_types.Uri ""URI to Services"" @classmethod def sample(cls): return cls(uri='http://example.com/v1', name='magnum', type='platform', description='magnum native implementation', bays_uri='http://example.com:9511/v1/bays', pods_uri='http://example.com:9511/v1/pods', services_uri='http://example.com:9511/v1/services', containers_uri='http://example.com:9511/v1/containers') class Controller(object): """"""Version 1 API Controller Root."""""" bays = bay.BayController() pods = pod.PodController() services = service.ServiceController() containers = container.ContainerController() @exception.wrap_wsme_controller_exception @wsme_pecan.wsexpose(Platform) def index(self): host_url = '%s/%s' % (pecan.request.host_url, 'v1') return Platform(uri=host_url, name='magnum', type='platform', description='magnum native implementation', implementation_version=version.version_string(), bays_uri='%s/bays' % host_url, pods_uri='%s/pods' % host_url, services_uri='%s/services' % host_url, containers_uri='%s/containers' % host_url) ",0,72
openstack%2Fpython-novaclient~master~I75618aaf9e07324985a83ca74d35bcab59e186c1,openstack/python-novaclient,master,I75618aaf9e07324985a83ca74d35bcab59e186c1,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:51:38.000000000,2014-12-05 16:44:32.000000000,2014-12-05 16:44:31.000000000,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 1849}, {'_account_id': 8959}]","[{'number': 1, 'created': '2014-12-05 03:51:38.000000000', 'files': ['CONTRIBUTING.rst', 'doc/source/index.rst', 'README.rst'], 'web_link': 'https://opendev.org/openstack/python-novaclient/commit/c4233cfd2aaf864028571491fb1c9dd249dd3337', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I75618aaf9e07324985a83ca74d35bcab59e186c1\n'}]",0,139381,c4233cfd2aaf864028571491fb1c9dd249dd3337,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I75618aaf9e07324985a83ca74d35bcab59e186c1
",git fetch https://review.opendev.org/openstack/python-novaclient refs/changes/81/139381/1 && git format-patch -1 --stdout FETCH_HEAD,"['CONTRIBUTING.rst', 'doc/source/index.rst', 'README.rst']",3,c4233cfd2aaf864028571491fb1c9dd249dd3337,infra-manual,.. _Gerrit: http://docs.openstack.org/infra/manual/developers.html#development-workflow,.. _Gerrit: http://wiki.openstack.org/GerritWorkflow,4,4
openstack%2Foslo.messaging~master~I2c1977c3bfc1923bcb03744e909f2e70c7fdb14c,openstack/oslo.messaging,master,I2c1977c3bfc1923bcb03744e909f2e70c7fdb14c,Remove the use of PROTOCOL_SSLv3,MERGED,2014-11-21 09:43:03.000000000,2014-12-05 16:36:35.000000000,2014-11-28 08:09:13.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 2813}, {'_account_id': 5638}, {'_account_id': 6476}, {'_account_id': 6486}, {'_account_id': 8415}, {'_account_id': 9656}, {'_account_id': 12000}]","[{'number': 1, 'created': '2014-11-21 09:43:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.messaging/commit/4bce0d0d63c636e8f0fd5395090db10c18cdc7e8', 'message': ""Remove the use of PROTOCOL_SSLv3\n\nThe PROTOCOL_SSLv3 should not be used, as it can be exploited with\na protocol downgrade attack. Also, its support has been removed in\nDebian, so it simply doesn't work at all now in Sid.\n\nThis patch removes PROTOCOL_SSLv3 from one of the possible protocols\nused by oslo.messaging.\n\nChange-Id: I2c1977c3bfc1923bcb03744e909f2e70c7fdb14c\n""}, {'number': 2, 'created': '2014-11-25 16:47:31.000000000', 'files': ['oslo/messaging/_drivers/impl_rabbit.py'], 'web_link': 'https://opendev.org/openstack/oslo.messaging/commit/42f55a1dda96d4ceecf8cca5fba9cd723673f6e3', 'message': ""Remove the use of PROTOCOL_SSLv3\n\nThe PROTOCOL_SSLv3 should not be used, as it can be exploited with\na protocol downgrade attack. Also, its support has been removed in\nDebian, so it simply doesn't work at all now in Sid.\n\nThis patch removes PROTOCOL_SSLv3 from one of the possible protocols\nused by oslo.messaging.\n\nCloses-Bug: #1395095\nChange-Id: I2c1977c3bfc1923bcb03744e909f2e70c7fdb14c\n""}]",9,136278,42f55a1dda96d4ceecf8cca5fba9cd723673f6e3,35,9,2,6476,,,0,"Remove the use of PROTOCOL_SSLv3

The PROTOCOL_SSLv3 should not be used, as it can be exploited with
a protocol downgrade attack. Also, its support has been removed in
Debian, so it simply doesn't work at all now in Sid.

This patch removes PROTOCOL_SSLv3 from one of the possible protocols
used by oslo.messaging.

Closes-Bug: #1395095
Change-Id: I2c1977c3bfc1923bcb03744e909f2e70c7fdb14c
",git fetch https://review.opendev.org/openstack/oslo.messaging refs/changes/78/136278/1 && git format-patch -1 --stdout FETCH_HEAD,['oslo/messaging/_drivers/impl_rabbit.py'],1,4bce0d0d63c636e8f0fd5395090db10c18cdc7e8,136278," ""sslv23"": ssl.PROTOCOL_SSLv23"," ""sslv23"": ssl.PROTOCOL_SSLv23, ""sslv3"": ssl.PROTOCOL_SSLv3",1,2
openstack%2Frequirements~master~I560b1d7a5885909b4d683e2ea0b38e75b536c38d,openstack/requirements,master,I560b1d7a5885909b4d683e2ea0b38e75b536c38d,Fix casing of xstatic-*,MERGED,2014-10-22 17:38:47.000000000,2014-12-05 16:32:05.000000000,2014-12-05 16:32:03.000000000,"[{'_account_id': 3}, {'_account_id': 2592}, {'_account_id': 2750}, {'_account_id': 4146}, {'_account_id': 6476}, {'_account_id': 9317}]","[{'number': 1, 'created': '2014-10-22 17:38:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/3ff9d322d92326e41550caa4951c65936b40ba47', 'message': 'Fix casing of xstatic-*\n\nAlthough pip can technically handle package names case insensitively,\nthis is known to cause problems when using things like multiple packages\nindexes.\n\nChange-Id: I560b1d7a5885909b4d683e2ea0b38e75b536c38d\n'}, {'number': 2, 'created': '2014-10-22 17:45:58.000000000', 'files': ['global-requirements.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/abc95c2eee857e1fe88377e1e1ca29aa3b44dc4a', 'message': 'Fix casing of xstatic-*\n\nAlthough pip can technically handle package names case insensitively,\nthis is known to cause problems when using things like multiple packages\nindexes.\n\nChange-Id: I560b1d7a5885909b4d683e2ea0b38e75b536c38d\n'}]",0,130287,abc95c2eee857e1fe88377e1e1ca29aa3b44dc4a,12,6,2,10035,,,0,"Fix casing of xstatic-*

Although pip can technically handle package names case insensitively,
this is known to cause problems when using things like multiple packages
indexes.

Change-Id: I560b1d7a5885909b4d683e2ea0b38e75b536c38d
",git fetch https://review.opendev.org/openstack/requirements refs/changes/87/130287/2 && git format-patch -1 --stdout FETCH_HEAD,['global-requirements.txt'],1,3ff9d322d92326e41550caa4951c65936b40ba47,fix/xstatic-casing,"XStatic>=1.0.0 # MIT License XStatic-angular>=1.2.1.1 # MIT License XStatic-angular-cookies>=1.2.1.1 # MIT License XStatic-angular-mock>=1.2.1.1 # MIT License XStatic-bootstrap-datepicker>=1.3.1.0 # Apache 2.0 License XStatic-bootstrap-scss>=3 # Apache 2.0 License XStatic-d3>=3.1.6.2 # BSD License (3 clause) XStatic-font-awesome>=4.1.0 # SIL OFL 1.1 License, MIT License XStatic-hogan>=2.0.0.2 # Apache 2.0 License XStatic-jasmine>=1.3.1.1 # MIT License XStatic-jquery>=1.7.2 # MIT License XStatic-jquery.bootstrap.wizard>=1.0.0.1 # MIT License XStatic-jquery-migrate>=1.2.1.1 # MIT License XStatic-jquery.quicksearch>=2.0.3.1 # MIT License XStatic-jquery.tablesorter>=2.0.5b.0 # MIT License XStatic-jquery-ui>=1.10.1 # MIT License XStatic-jsencrypt>=2.0.0.2 # MIT License XStatic-qunit>=1.14.0.2 # MIT License XStatic-rickshaw>=1.5.0 # BSD License (prior) XStatic-spin>=1.2.5.2 # MIT License","xstatic>=1.0.0 # MIT License xstatic-angular>=1.2.1.1 # MIT License xstatic-angular-cookies>=1.2.1.1 # MIT License xstatic-angular-mock>=1.2.1.1 # MIT License xstatic-bootstrap-datepicker>=1.3.1.0 # Apache 2.0 License xstatic-bootstrap-scss>=3 # Apache 2.0 License xstatic-d3>=3.1.6.2 # BSD License (3 clause) xstatic-font-awesome>=4.1.0 # SIL OFL 1.1 License, MIT License xstatic-hogan>=2.0.0.2 # Apache 2.0 License xstatic-jasmine>=1.3.1.1 # MIT License xstatic-jquery>=1.7.2 # MIT License xstatic-jquery.bootstrap.wizard>=1.0.0.1 # MIT License xstatic-jquery-migrate>=1.2.1.1 # MIT License xstatic-jquery.quicksearch>=2.0.3.1 # MIT License xstatic-jquery.tablesorter>=2.0.5b.0 # MIT License xstatic-jquery-ui>=1.10.1 # MIT License xstatic-jsencrypt>=2.0.0.2 # MIT License xstatic-qunit>=1.14.0.2 # MIT License xstatic-rickshaw>=1.5.0 # BSD License (prior) xstatic-spin>=1.2.5.2 # MIT License",20,20
openstack%2Fopenstack-ansible~master~I335743fe253bd501401208baf6aa8e772ba641f0,openstack/openstack-ansible,master,I335743fe253bd501401208baf6aa8e772ba641f0,Run horizon-manage.py script as {{ system_user }},MERGED,2014-12-04 22:51:42.000000000,2014-12-05 16:28:23.000000000,2014-12-05 16:28:23.000000000,"[{'_account_id': 3}, {'_account_id': 7217}, {'_account_id': 7307}, {'_account_id': 7353}, {'_account_id': 9884}]","[{'number': 1, 'created': '2014-12-04 22:51:42.000000000', 'files': ['rpc_deployment/inventory/group_vars/horizon.yml', 'rpc_deployment/roles/horizon_common/tasks/main.yml', 'rpc_deployment/roles/horizon_setup/tasks/main.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/da560992ba208c79e642c8920f0b638220d49222', 'message': 'Run horizon-manage.py script as {{ system_user }}\n\nCurrently, we seem to always have 1/3 horizon nodes where\n/var/lib/horizon/.secret_key_store is owned by root which breaks\nhorizon logins.  This appears to be a result of running\nhorizon-manage.py syncdb as root (which only happens on 1/3 nodes).\nThis change moves all horizon-manage.py commands to run as\n{{ system_user }}, which requires us to also pre-create some dirs\nowned by {{ system_user }} by adding them to container_directories so\nthat the horizon-manage.py commands will all complete successfully.\n\nLastly, we remove the creation of /etc/horizon and the chown of files\nin {{ install_lib_dir }}/static in heat_common role since those have\nbeen added to container_directories.\n\nChange-Id: I335743fe253bd501401208baf6aa8e772ba641f0\n'}]",0,139243,da560992ba208c79e642c8920f0b638220d49222,8,5,1,4,,,0,"Run horizon-manage.py script as {{ system_user }}

Currently, we seem to always have 1/3 horizon nodes where
/var/lib/horizon/.secret_key_store is owned by root which breaks
horizon logins.  This appears to be a result of running
horizon-manage.py syncdb as root (which only happens on 1/3 nodes).
This change moves all horizon-manage.py commands to run as
{{ system_user }}, which requires us to also pre-create some dirs
owned by {{ system_user }} by adding them to container_directories so
that the horizon-manage.py commands will all complete successfully.

Lastly, we remove the creation of /etc/horizon and the chown of files
in {{ install_lib_dir }}/static in heat_common role since those have
been added to container_directories.

Change-Id: I335743fe253bd501401208baf6aa8e772ba641f0
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/43/139243/1 && git format-patch -1 --stdout FETCH_HEAD,"['rpc_deployment/inventory/group_vars/horizon.yml', 'rpc_deployment/roles/horizon_common/tasks/main.yml', 'rpc_deployment/roles/horizon_setup/tasks/main.yml']",3,da560992ba208c79e642c8920f0b638220d49222,,"# This will create /var/lib/horizon/.secret_key_store, which needs to be owned # by {{ system_user }}, otherwise horizon logins will fail sudo: yes sudo_user: ""{{ system_user }}""","# Unlike the 'db sync' command run in other projects, we do not run this under # horizon's {{ system_user }} as horizon is run through Apache and logs are # written to as root",8,21
openstack%2Fopenstack-ansible~stable%2Ficehouse~Ic081b96e525359f9cb7cc7fe4b5df605c097b1d1,openstack/openstack-ansible,stable/icehouse,Ic081b96e525359f9cb7cc7fe4b5df605c097b1d1,Added nova console variable,MERGED,2014-12-04 22:58:15.000000000,2014-12-05 16:26:27.000000000,2014-12-05 16:26:26.000000000,"[{'_account_id': 3}, {'_account_id': 7217}, {'_account_id': 7307}, {'_account_id': 7353}, {'_account_id': 7414}, {'_account_id': 9884}]","[{'number': 1, 'created': '2014-12-04 22:58:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/e3650ba11813bc714b39a31f1ed95fb7c9d87862', 'message': 'Added nova console variable\n\nAdded specific console variable to the config and provided and example of how to use the new variable.\nThe variable is ""nova_console_endpoint"" variable\n\nResolves Issue #409\n\nChange-Id: Ic081b96e525359f9cb7cc7fe4b5df605c097b1d1\n'}, {'number': 2, 'created': '2014-12-05 02:10:11.000000000', 'files': ['rpc_deployment/vars/openstack_service_vars/nova_spice_console_endpoint.yml', 'etc/rpc_deploy/user_variables.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/5f39d4ec5260aee1aacb6efbac46012c81207854', 'message': 'Added nova console variable\n\nAdded specific console variable to the config and provided and example of how to use the new variable.\nThe variable is ""nova_console_endpoint"" variable\n\nCloses-Bug: 1399420\nChange-Id: Ic081b96e525359f9cb7cc7fe4b5df605c097b1d1\n'}]",1,139252,5f39d4ec5260aee1aacb6efbac46012c81207854,12,6,2,4,,,0,"Added nova console variable

Added specific console variable to the config and provided and example of how to use the new variable.
The variable is ""nova_console_endpoint"" variable

Closes-Bug: 1399420
Change-Id: Ic081b96e525359f9cb7cc7fe4b5df605c097b1d1
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/52/139252/2 && git format-patch -1 --stdout FETCH_HEAD,"['rpc_deployment/vars/openstack_service_vars/nova_spice_console_endpoint.yml', 'etc/rpc_deploy/user_variables.yml']",2,e3650ba11813bc714b39a31f1ed95fb7c9d87862,bug/1399420,"# Uncomment ""nova_console_endpoint"" to define a specific nova console URI or # IP address this will construct the specific proxy endpoint for the console. # nova_console_endpoint: console.company_domain.name ",,5,1
openstack%2Fheat~master~Ida30e51d526a3f5d2a4d0a4102b1036f1c518302,openstack/heat,master,Ida30e51d526a3f5d2a4d0a4102b1036f1c518302,Fix add_dependencies for floating_ip resource,MERGED,2014-10-22 09:44:12.000000000,2014-12-05 16:26:23.000000000,2014-10-23 13:23:40.000000000,"[{'_account_id': 3}, {'_account_id': 4257}, {'_account_id': 4328}, {'_account_id': 4571}, {'_account_id': 4715}, {'_account_id': 8289}]","[{'number': 1, 'created': '2014-10-22 09:44:12.000000000', 'files': ['heat/engine/resources/neutron/floatingip.py', 'heat/tests/test_neutron.py'], 'web_link': 'https://opendev.org/openstack/heat/commit/29dbe697516e3e5dd38a926b76f623661cbce020', 'message': 'Fix add_dependencies for floating_ip resource\n\nThe cause of problem is related with tricky method of Dependencies\nclass. So required_by method returns dictionary with resources which\nhave current resource in dependency. In the same time port resource\ndoes not require floating ip, contrariwise floating ip require port.\nAs result required_by will not return depended port. There is one\nway to solve issue: use dictionary of requires resources.\n\nAlso this patch fixes missed code error with incorrect using properties\nattribute for value fixed_ip.\n\nNotice that both these issues were not detected, because:\n - method port_on_subnet is not executed, because required_by was always\n   empty dictionary.\n - current test has shadow mistake. Dependencies for router_interface\n   always contain FloatingIP resource, because router gateway also was\n   presented in using templates and during creation graph FloatingIp\n   automatically was added to result dictionary.\n\nChange-Id: Ida30e51d526a3f5d2a4d0a4102b1036f1c518302\nCloses-Bug: #1384114\n'}]",1,130176,29dbe697516e3e5dd38a926b76f623661cbce020,10,6,1,6577,,,0,"Fix add_dependencies for floating_ip resource

The cause of problem is related with tricky method of Dependencies
class. So required_by method returns dictionary with resources which
have current resource in dependency. In the same time port resource
does not require floating ip, contrariwise floating ip require port.
As result required_by will not return depended port. There is one
way to solve issue: use dictionary of requires resources.

Also this patch fixes missed code error with incorrect using properties
attribute for value fixed_ip.

Notice that both these issues were not detected, because:
 - method port_on_subnet is not executed, because required_by was always
   empty dictionary.
 - current test has shadow mistake. Dependencies for router_interface
   always contain FloatingIP resource, because router gateway also was
   presented in using templates and during creation graph FloatingIp
   automatically was added to result dictionary.

Change-Id: Ida30e51d526a3f5d2a4d0a4102b1036f1c518302
Closes-Bug: #1384114
",git fetch https://review.opendev.org/openstack/heat refs/changes/76/130176/1 && git format-patch -1 --stdout FETCH_HEAD,"['heat/engine/resources/neutron/floatingip.py', 'heat/tests/test_neutron.py']",2,29dbe697516e3e5dd38a926b76f623661cbce020,fip_dependencies," def test_floating_ip_router_interface(self): t = template_format.parse(neutron_floating_template) del t['Resources']['gateway'] self._test_floating_ip(t) def test_floating_ip_router_gateway(self): t = template_format.parse(neutron_floating_template) del t['Resources']['router_interface'] self._test_floating_ip(t, r_iface=False) def test_floating_ip_deprecated_router_interface(self): t = template_format.parse(neutron_floating_template_deprecated) del t['Resources']['gateway'] self._test_floating_ip(t, resolve_neutron=False) def test_floating_ip_deprecated_router_gateway(self): t = template_format.parse(neutron_floating_template_deprecated) del t['Resources']['router_interface'] self._test_floating_ip(t, resolve_neutron=False, r_iface=False) def _test_floating_ip(self, tmpl, resolve_neutron=True, r_iface=True): stack = utils.parse_stack(tmpl) if r_iface: deps = stack.dependencies[stack['router_interface']] self.assertIn(stack['floating_ip'], deps) else: deps = stack.dependencies[stack['gateway']] self.assertIn(stack['floating_ip'], deps)"," def test_floating_ip(self): self._test_floating_ip() def test_floating_ip_deprecated(self): self._test_floating_ip(resolve_neutron=False) def _test_floating_ip(self, resolve_neutron=True): t = template_format.parse(neutron_floating_template) else: t = template_format.parse(neutron_floating_template_deprecated) stack = utils.parse_stack(t) deps = stack.dependencies[stack['gateway']] self.assertIn(stack['floating_ip'], deps) deps = stack.dependencies[stack['router_interface']] self.assertIn(stack['floating_ip'], deps)",28,17
openstack%2Fopenstack-ansible~master~I56951c5423090500f2f69c1dbd9689674bb2c90d,openstack/openstack-ansible,master,I56951c5423090500f2f69c1dbd9689674bb2c90d,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:57:35.000000000,2014-12-05 16:26:21.000000000,2014-12-05 16:26:20.000000000,"[{'_account_id': 3}, {'_account_id': 7217}, {'_account_id': 7307}, {'_account_id': 9884}]","[{'number': 1, 'created': '2014-12-05 03:57:35.000000000', 'files': ['CONTRIBUTING.md'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/7fdd8a04d4438c0ea26e13e8556e66cdef7a9ea4', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I56951c5423090500f2f69c1dbd9689674bb2c90d\n'}]",0,139483,7fdd8a04d4438c0ea26e13e8556e66cdef7a9ea4,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I56951c5423090500f2f69c1dbd9689674bb2c90d
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/83/139483/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.md'],1,7fdd8a04d4438c0ea26e13e8556e66cdef7a9ea4,infra-manual, http://docs.openstack.org/infra/manual/developers.html#development-workflow, https://wiki.openstack.org/GerritWorkflow,1,1
openstack%2Fnova~master~If89007f45d90ca5321336e3623139f9538ac06b8,openstack/nova,master,If89007f45d90ca5321336e3623139f9538ac06b8,Fixes multi-line strings with missing spaces,MERGED,2014-12-05 13:17:23.000000000,2014-12-05 16:20:04.000000000,2014-12-05 16:20:00.000000000,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 6873}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-12-05 13:17:23.000000000', 'files': ['nova/tests/unit/test_iptables_network.py', 'nova/tests/unit/virt/xenapi/test_xenapi.py', 'nova/api/openstack/compute/contrib/attach_interfaces.py', 'nova/compute/manager.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/1e72d1ea549aff20c273d68bd5325851e5ac4c00', 'message': 'Fixes multi-line strings with missing spaces\n\nMulti-line concatenated strings are easily given bad spacing:\n\n  ""This looks like""\n  ""it will be OK""    => ""This looks likeit will be OK""\n\nThis patch fixes those. I don\'t think it\'s possible to create\na hacking rule for this as any automated check will generate\ntoo many false positives.\n\nChange-Id: If89007f45d90ca5321336e3623139f9538ac06b8\n'}]",0,139621,1e72d1ea549aff20c273d68bd5325851e5ac4c00,11,7,1,9420,,,0,"Fixes multi-line strings with missing spaces

Multi-line concatenated strings are easily given bad spacing:

  ""This looks like""
  ""it will be OK""    => ""This looks likeit will be OK""

This patch fixes those. I don't think it's possible to create
a hacking rule for this as any automated check will generate
too many false positives.

Change-Id: If89007f45d90ca5321336e3623139f9538ac06b8
",git fetch https://review.opendev.org/openstack/nova refs/changes/21/139621/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/test_iptables_network.py', 'nova/tests/unit/virt/xenapi/test_xenapi.py', 'nova/api/openstack/compute/contrib/attach_interfaces.py', 'nova/compute/manager.py']",4,1e72d1ea549aff20c273d68bd5325851e5ac4c00,," ""before going into ERROR status. "" "" Valid options are 'noop', 'log', 'shutdown', or 'reap'. """," ""before going into ERROR status."" ""Valid options are 'noop', 'log', 'shutdown', or 'reap'. """,5,5
openstack%2Fmagnum~master~Ief4decc4aa2b4410df333a2aae4bf88709b28684,openstack/magnum,master,Ief4decc4aa2b4410df333a2aae4bf88709b28684,Add object.service,MERGED,2014-12-02 13:01:21.000000000,2014-12-05 16:17:26.000000000,2014-12-03 22:56:21.000000000,"[{'_account_id': 3}, {'_account_id': 5638}, {'_account_id': 8580}]","[{'number': 1, 'created': '2014-12-02 13:01:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/711d2364d8589b226d2f8f1740f476b52319c859', 'message': 'Add object.service\n\nObject.service is used for reading/writing the sql database.  They\nare remoteable, so they can be called over RPC.\n\nChange-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684\n'}, {'number': 2, 'created': '2014-12-02 18:11:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/e5223da834bc985eca9c6984b9a93ac5aee19400', 'message': 'Add object.service\n\nObject.service is used for reading/writing the sql database.  They\nare remoteable, so they can be called over RPC.\n\nChange-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684\n'}, {'number': 3, 'created': '2014-12-02 21:23:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/b10d2e984504aad2f27c5431d6ae73eee8805cb3', 'message': 'Add object.service\n\nObject.service is used for reading/writing the sql database.  They\nare remoteable, so they can be called over RPC.\n\nChange-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684\n'}, {'number': 4, 'created': '2014-12-02 21:31:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/037d0a61cb023dc666c5895ddfc3c96996e080b2', 'message': 'Add object.service\n\nObject.service is used for reading/writing the sql database.  They\nare remoteable, so they can be called over RPC.\n\nChange-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684\n'}, {'number': 5, 'created': '2014-12-02 21:44:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/b2944844319669363adc3208493674fe76593a9e', 'message': 'Add object.service\n\nObject.service is used for reading/writing the sql database.  They\nare remoteable, so they can be called over RPC.\n\nChange-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684\n'}, {'number': 6, 'created': '2014-12-02 22:04:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/f4816baa06cc8f23ee5bdae962708d4ac44ee98a', 'message': 'Add object.service\n\nObject.service is used for reading/writing the sql database.  They\nare remoteable, so they can be called over RPC.\n\nChange-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684\n'}, {'number': 7, 'created': '2014-12-02 22:30:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/fc595c3d0bc44e024529da9ff881a78ee13c4608', 'message': 'Add object.service\n\nObject.service is used for reading/writing the sql database.  They\nare remoteable, so they can be called over RPC.\n\nChange-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684\n'}, {'number': 8, 'created': '2014-12-02 23:10:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/d5b785cbb9d89cb6035cb14af1278cd02f76805b', 'message': 'Add object.service\n\nObject.service is used for reading/writing the sql database.  They\nare remoteable, so they can be called over RPC.\n\nChange-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684\n'}, {'number': 9, 'created': '2014-12-02 23:18:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/61fbcab44755f19e9a23eaa3624762920ad0590f', 'message': 'Add object.service\n\nObject.service is used for reading/writing the sql database.  They\nare remoteable, so they can be called over RPC.\n\nChange-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684\n'}, {'number': 10, 'created': '2014-12-02 23:25:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/e3da854f3a508a3d0eb21f9649cb9f1988011c8f', 'message': 'Add object.service\n\nObject.service is used for reading/writing the sql database.  They\nare remoteable, so they can be called over RPC.\n\nChange-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684\n'}, {'number': 11, 'created': '2014-12-03 01:25:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/09f1ce5e41b3016993cc662c33dccec7cef82907', 'message': 'Add object.service\n\nObject.service is used for reading/writing the sql database.  They\nare remoteable, so they can be called over RPC.\n\nChange-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684\n'}, {'number': 12, 'created': '2014-12-03 01:37:15.000000000', 'files': ['magnum/objects/service.py', 'magnum/objects/__init__.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/3b5d9b1f40e75c128a0d6619feeb5d1d93d0c82f', 'message': 'Add object.service\n\nObject.service is used for reading/writing the sql database.  They\nare remoteable, so they can be called over RPC.\n\nChange-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684\n'}]",0,138352,3b5d9b1f40e75c128a0d6619feeb5d1d93d0c82f,28,3,12,2834,,,0,"Add object.service

Object.service is used for reading/writing the sql database.  They
are remoteable, so they can be called over RPC.

Change-Id: Ief4decc4aa2b4410df333a2aae4bf88709b28684
",git fetch https://review.opendev.org/openstack/magnum refs/changes/52/138352/2 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/objects/service.py', 'magnum/objects/__init__.py']",2,711d2364d8589b226d2f8f1740f476b52319c859,,"from magnum.objects import serviceService = service.Service Pod, Service)", Pod),162,2
openstack%2Fironic-inspector~master~I5536b2697416569aa3f7a5b969cfc5ef539db602,openstack/ironic-inspector,master,I5536b2697416569aa3f7a5b969cfc5ef539db602,Return serialized node to the ramdisk,MERGED,2014-11-26 15:13:46.000000000,2014-12-05 16:17:21.000000000,2014-12-05 16:17:20.000000000,"[{'_account_id': 3}, {'_account_id': 6773}, {'_account_id': 10239}]","[{'number': 1, 'created': '2014-11-26 15:13:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/312a1d8cc09a8257aa054631ab99f34228028374', 'message': 'Return serialized node to the ramdisk\n\nChange-Id: I5536b2697416569aa3f7a5b969cfc5ef539db602\nImplements: blueprint returning-to-ramdisk\n'}, {'number': 2, 'created': '2014-11-28 10:13:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/283d0527c6aebe0e11542e315253d3a26bb20f2c', 'message': 'Return serialized node to the ramdisk\n\nAlso add option `power_off_after_discovery` to control whether\nto shut down node after the discovery.\n\nChange-Id: I5536b2697416569aa3f7a5b969cfc5ef539db602\nImplements: blueprint returning-to-ramdisk\n'}, {'number': 3, 'created': '2014-11-28 10:53:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/227106f367bc18bbb2b46799f7c3151218ea6e7d', 'message': 'Return serialized node to the ramdisk\n\nAlso add option `power_off_after_discovery` to control whether\nto shut down node after the discovery.\n\nChange-Id: I5536b2697416569aa3f7a5b969cfc5ef539db602\nImplements: blueprint returning-to-ramdisk\n'}, {'number': 4, 'created': '2014-11-28 10:59:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/b873b4692e1ef2ac524368e42cdfe60890c1e33c', 'message': 'Return serialized node to the ramdisk\n\nAlso add option `power_off_after_discovery` to control whether\nto shut down node after the discovery.\n\nChange-Id: I5536b2697416569aa3f7a5b969cfc5ef539db602\nImplements: blueprint returning-to-ramdisk\n'}, {'number': 5, 'created': '2014-12-01 17:38:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/2cbe3a27a9e34c111e662d2fe043aff86d4eb7db', 'message': 'Return serialized node to the ramdisk\n\nAlso add option `power_off_after_discovery` to control whether\nto shut down node after the discovery.\n\nChange-Id: I5536b2697416569aa3f7a5b969cfc5ef539db602\nImplements: blueprint returning-to-ramdisk\n'}, {'number': 6, 'created': '2014-12-02 10:30:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/7fb86186972c45862e6631cdb58d2536adcfa355', 'message': 'Return serialized node to the ramdisk\n\nAlso add option `power_off_after_discovery` to control whether\nto shut down node after the discovery.\n\nChange-Id: I5536b2697416569aa3f7a5b969cfc5ef539db602\nImplements: blueprint returning-to-ramdisk\n'}, {'number': 7, 'created': '2014-12-04 13:50:23.000000000', 'files': ['ironic_discoverd/test.py', 'ironic_discoverd/discoverd.py', 'README.rst'], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/f43fecf3940f1b5bc942fd7d19c7dc9ce6a615ab', 'message': 'Return serialized node to the ramdisk\n\nImplements: blueprint returning-to-ramdisk\nChange-Id: I5536b2697416569aa3f7a5b969cfc5ef539db602\n'}]",2,137374,f43fecf3940f1b5bc942fd7d19c7dc9ce6a615ab,21,3,7,10239,,,0,"Return serialized node to the ramdisk

Implements: blueprint returning-to-ramdisk
Change-Id: I5536b2697416569aa3f7a5b969cfc5ef539db602
",git fetch https://review.opendev.org/openstack/ironic-inspector refs/changes/74/137374/5 && git format-patch -1 --stdout FETCH_HEAD,"['ironic_discoverd/test.py', 'ironic_discoverd/discoverd.py', 'README.rst']",3,312a1d8cc09a8257aa054631ab99f34228028374,bp/returning-to-ramdisk, Successful response body is a JSON dictionary with keys: * ``node`` node as returned by Ironic ,,8,1
openstack%2Fnova~master~Ia223797798679fecfcf90983ca81a9f395e67c3b,openstack/nova,master,Ia223797798679fecfcf90983ca81a9f395e67c3b,Only filter once for trusted filters,MERGED,2014-10-10 16:06:09.000000000,2014-12-05 16:14:11.000000000,2014-12-05 16:14:08.000000000,"[{'_account_id': 3}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 6062}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-10-10 16:06:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/7b3d4af84707d6887ca7078bf1a2d6bf79c0ea2a', 'message': 'Only filter once for trusted filters\n\nIn spawn process the filters will be used only once if the\nrun_filter_once_per_request is set to True. This apply to the hosts\nthat will not be affected by previous instance resource consumption.\nTrusted filter should be in this kind of filters.\n\nChange-Id: Ia223797798679fecfcf90983ca81a9f395e67c3b\n'}, {'number': 2, 'created': '2014-11-19 03:11:24.000000000', 'files': ['nova/scheduler/filters/trusted_filter.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/d5f1a535936c5d907ad1dd5d008fc45db2b26020', 'message': 'Only filter once for trusted filters\n\nIn spawn process the filters will be used only once if the\nrun_filter_once_per_request is set to True. This apply to the hosts\nthat will not be affected by previous instance resource consumption.\nTrusted filter should be in this kind of filters.\n\nChange-Id: Ia223797798679fecfcf90983ca81a9f395e67c3b\n'}]",0,127562,d5f1a535936c5d907ad1dd5d008fc45db2b26020,22,9,2,6062,,,0,"Only filter once for trusted filters

In spawn process the filters will be used only once if the
run_filter_once_per_request is set to True. This apply to the hosts
that will not be affected by previous instance resource consumption.
Trusted filter should be in this kind of filters.

Change-Id: Ia223797798679fecfcf90983ca81a9f395e67c3b
",git fetch https://review.opendev.org/openstack/nova refs/changes/62/127562/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/scheduler/filters/trusted_filter.py'],1,7b3d4af84707d6887ca7078bf1a2d6bf79c0ea2a,scheduler_trusted_master_only_once, # The hosts the instances are running on doesn't change within a request run_filter_once_per_request = True ,,3,0
openstack%2Fnova~master~Ia297524fc1acf06ab708509c805d14e215b10b86,openstack/nova,master,Ia297524fc1acf06ab708509c805d14e215b10b86,make get_by_host use slave in periodic task,MERGED,2014-11-11 06:08:09.000000000,2014-12-05 16:09:29.000000000,2014-12-05 16:09:26.000000000,"[{'_account_id': 3}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-11 06:08:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9cd51f0bd7710c414bec6cfa8e064655a8a3f11b', 'message': 'make get_by_host use slave in periodic task\n\nIn periodic task functions nova prefer to use slave db\ninstead of primary db. This patch makes function\n_get_host_volume_bdms to use slave db.\n\nChange-Id: Ia297524fc1acf06ab708509c805d14e215b10b86\n'}, {'number': 2, 'created': '2014-11-11 07:26:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e97aaa1364c1dd3787a63e69dc8eceda53ee20c2', 'message': 'make get_by_host use slave in periodic task\n\nIn periodic task functions nova prefer to use slave db\ninstead of primary db. This patch makes function\n_get_host_volume_bdms to use slave db.\n\nChange-Id: Ia297524fc1acf06ab708509c805d14e215b10b86\n'}, {'number': 3, 'created': '2014-11-13 08:06:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/fdc6f0298db16a144659a7ffd3ec28c0b2801933', 'message': 'make get_by_host use slave in periodic task\n\nIn periodic task functions nova prefer to use slave db\ninstead of primary db. This patch makes function\n_get_host_volume_bdms to use slave db.\n\nChange-Id: Ia297524fc1acf06ab708509c805d14e215b10b86\n'}, {'number': 4, 'created': '2014-11-14 02:47:56.000000000', 'files': ['nova/tests/unit/compute/test_compute.py', 'nova/compute/manager.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/07480c805af581ed99eb696d7d1b4a39da0f61cc', 'message': 'make get_by_host use slave in periodic task\n\nIn periodic task functions nova prefer to use slave db\ninstead of primary db. This patch makes function\n_get_host_volume_bdms to use slave db\n\nChange-Id: Ia297524fc1acf06ab708509c805d14e215b10b86\n'}]",0,133617,07480c805af581ed99eb696d7d1b4a39da0f61cc,32,8,4,6062,,,0,"make get_by_host use slave in periodic task

In periodic task functions nova prefer to use slave db
instead of primary db. This patch makes function
_get_host_volume_bdms to use slave db

Change-Id: Ia297524fc1acf06ab708509c805d14e215b10b86
",git fetch https://review.opendev.org/openstack/nova refs/changes/17/133617/4 && git format-patch -1 --stdout FETCH_HEAD,['nova/compute/manager.py'],1,9cd51f0bd7710c414bec6cfa8e064655a8a3f11b,use_slave_for_periodic_task," instances = objects.InstanceList.get_by_host(context, self.host, use_slave=use_slave)"," instances = objects.InstanceList.get_by_host(context, self.host)",2,1
openstack%2Fnova~master~I05e12b56411f4475d1ef50e90e09abf75d7355b0,openstack/nova,master,I05e12b56411f4475d1ef50e90e09abf75d7355b0,Indicate whether service is down for mc driver,MERGED,2014-11-21 14:08:13.000000000,2014-12-05 16:05:48.000000000,2014-12-05 16:05:45.000000000,"[{'_account_id': 3}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-21 14:08:13.000000000', 'files': ['nova/servicegroup/drivers/mc.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/746fb8ea7488575d1043b5d66c4fe6d16b7f9be6', 'message': ""Indicate whether service is down for mc driver\n\nmemcache driver for service didn't log any clue\nwhether the service is down or not. This patch add a\nlog to indicate it at DEBUG level.\n\nCloses-Bug: #1382564\n\nChange-Id: I05e12b56411f4475d1ef50e90e09abf75d7355b0\n""}]",0,136351,746fb8ea7488575d1043b5d66c4fe6d16b7f9be6,12,8,1,6062,,,0,"Indicate whether service is down for mc driver

memcache driver for service didn't log any clue
whether the service is down or not. This patch add a
log to indicate it at DEBUG level.

Closes-Bug: #1382564

Change-Id: I05e12b56411f4475d1ef50e90e09abf75d7355b0
",git fetch https://review.opendev.org/openstack/nova refs/changes/51/136351/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/servicegroup/drivers/mc.py'],1,746fb8ea7488575d1043b5d66c4fe6d16b7f9be6,bug/1382564, is_up = self.mc.get(str(key)) is not None if not is_up: LOG.debug('Seems service %s is down' % key) return is_up, return self.mc.get(str(key)) is not None,5,1
openstack%2Fnova~master~I1c65041bd3a7152088f1660800d3a58f6fe12eea,openstack/nova,master,I1c65041bd3a7152088f1660800d3a58f6fe12eea,Network object: add missing translations,MERGED,2014-11-21 15:10:50.000000000,2014-12-05 16:04:33.000000000,2014-12-05 16:04:30.000000000,"[{'_account_id': 3}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-11-21 15:10:50.000000000', 'files': ['nova/objects/network.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/5303b3585900655f69f521b78d5dc5e7de2af9cc', 'message': 'Network object: add missing translations\n\nAn exception was raised and not translated in commit\ne134f81d85f42b87238f7786b0b50066129c968d.\n\nChange-Id: I1c65041bd3a7152088f1660800d3a58f6fe12eea\n'}]",0,136382,5303b3585900655f69f521b78d5dc5e7de2af9cc,12,8,1,1653,,,0,"Network object: add missing translations

An exception was raised and not translated in commit
e134f81d85f42b87238f7786b0b50066129c968d.

Change-Id: I1c65041bd3a7152088f1660800d3a58f6fe12eea
",git fetch https://review.opendev.org/openstack/nova refs/changes/82/136382/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/objects/network.py'],1,5303b3585900655f69f521b78d5dc5e7de2af9cc,missing-network-translation,"from nova.i18n import _ raise ValueError(_('IPv6 netmask ""%s"" must be a netmask ' 'or integral prefix') % netmask)"," raise ValueError('IPv6 netmask ""%s"" must be a netmask ' 'or integral prefix' % netmask)",3,2
openstack%2Fapi-site~master~Icbbfca07dc418ede61376a2b07e28979358cf497,openstack/api-site,master,Icbbfca07dc418ede61376a2b07e28979358cf497,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:40:22.000000000,2014-12-05 16:04:10.000000000,2014-12-05 16:04:09.000000000,"[{'_account_id': 3}, {'_account_id': 964}, {'_account_id': 6547}, {'_account_id': 6772}]","[{'number': 1, 'created': '2014-12-05 03:40:22.000000000', 'files': ['README.rst'], 'web_link': 'https://opendev.org/openstack/api-site/commit/f1c0ed537998891c01a69ccfa67e53238ceb0ffd', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: Icbbfca07dc418ede61376a2b07e28979358cf497\n'}]",0,139307,f1c0ed537998891c01a69ccfa67e53238ceb0ffd,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: Icbbfca07dc418ede61376a2b07e28979358cf497
",git fetch https://review.opendev.org/openstack/api-site refs/changes/07/139307/1 && git format-patch -1 --stdout FETCH_HEAD,['README.rst'],1,f1c0ed537998891c01a69ccfa67e53238ceb0ffd,infra-manual,"For more details about the Gerrit workflow, see `Gerrit Workflow <http://docs.openstack.org/infra/manual/developers.html#development-workflow>`_.#. To check in your changes, see `Gerrit Workflow <http://docs.openstack.org/infra/manual/developers.html#development-workflow>`_.","For more details about the Gerrit workflow, see `Gerrit Workflow <https://wiki.openstack.org/wiki/GerritWorkflow>`_.#. To check in your changes, see `Gerrit Workflow <https://wiki.openstack.org/wiki/GerritWorkflow>`_.",2,2
openstack%2Fkeystone~master~Ia8ae4c1d8d1e7e591a272928af0dfd17da4047c1,openstack/keystone,master,Ia8ae4c1d8d1e7e591a272928af0dfd17da4047c1,"Create, update and delete hierarchical projects",MERGED,2014-12-02 21:40:10.000000000,2014-12-05 16:02:37.000000000,2014-12-05 16:02:36.000000000,"[{'_account_id': 3}, {'_account_id': 4}, {'_account_id': 792}, {'_account_id': 1916}, {'_account_id': 2218}, {'_account_id': 2903}, {'_account_id': 5046}, {'_account_id': 5707}, {'_account_id': 6482}, {'_account_id': 6486}, {'_account_id': 7191}, {'_account_id': 7725}, {'_account_id': 8866}, {'_account_id': 8871}, {'_account_id': 10697}, {'_account_id': 11022}, {'_account_id': 14101}]","[{'number': 1, 'created': '2014-12-02 21:40:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/07a4f53139bb3632f51b0b4725a58093d19ecdb8', 'message': ""Create, update and delete hierarchical projects\n\nChanges at basic operations related to projects\nwere modified to handle projects hierarchy.\n\nThis changes includes:\n- Create and update projects with parent_project_id\n  field\n- Avoid deleting projects that are not leaf in the\n  hierarchy\n- Deny the update of a project's parent\n- Limit the max depth of a tree branch (in the\n  hierarchy)\n\nCo-Authored-By: Henrique Truta <henrique@lsd.ufcg.edu.br>\nCo-Authored-By: Rodrigo Duarte <rodrigods@lsd.ufcg.edu.br>\nCo-Authored-By: Raildo Mascena <raildo@lsd.ufcg.edu.br>\nCo-Authored-By: Samuel de Medeiros Queiroz <samuel@lsd.ufcg.edu.br>\nCo-Authored-By: Thiago Paiva Brito <thiagop@lsd.ufcg.edu.br>\n\nChange-Id: Ia8ae4c1d8d1e7e591a272928af0dfd17da4047c1\nImplements: blueprint hierarchical-multitenancy\n""}, {'number': 2, 'created': '2014-12-02 21:45:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/8c48c7974dbea66bc3f6c1ff7c7668f65c58fbf8', 'message': ""Create, update and delete hierarchical projects\n\nChanges at basic operations related to projects\nwere modified to handle projects hierarchy.\n\nThis changes includes:\n- Create and update projects with parent_project_id\n  field\n- Avoid deleting projects that are not leaf in the\n  hierarchy\n- Deny the update of a project's parent\n- Limit the max depth of a tree branch (in the\n  hierarchy)\n\nCo-Authored-By: Henrique Truta <henrique@lsd.ufcg.edu.br>\nCo-Authored-By: Rodrigo Duarte <rodrigods@lsd.ufcg.edu.br>\nCo-Authored-By: Raildo Mascena <raildo@lsd.ufcg.edu.br>\nCo-Authored-By: Samuel de Medeiros Queiroz <samuel@lsd.ufcg.edu.br>\nCo-Authored-By: Thiago Paiva Brito <thiagop@lsd.ufcg.edu.br>\n\nChange-Id: Ia8ae4c1d8d1e7e591a272928af0dfd17da4047c1\nImplements: blueprint hierarchical-multitenancy\n""}, {'number': 3, 'created': '2014-12-03 14:45:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/35f386b9285dcb329e6039b5ec19ddefe65e4da3', 'message': ""Create, update and delete hierarchical projects\n\nChanges at basic operations related to projects\nwere modified to handle projects hierarchy.\n\nThis changes includes:\n- Create and update projects with parent_project_id\n  field\n- Avoid deleting projects that are not leaf in the\n  hierarchy\n- Deny the update of a project's parent\n- Limit the max depth of a tree branch (in the\n  hierarchy)\n\nCo-Authored-By: Henrique Truta <henrique@lsd.ufcg.edu.br>\nCo-Authored-By: Rodrigo Duarte <rodrigods@lsd.ufcg.edu.br>\nCo-Authored-By: Raildo Mascena <raildo@lsd.ufcg.edu.br>\nCo-Authored-By: Samuel de Medeiros Queiroz <samuel@lsd.ufcg.edu.br>\nCo-Authored-By: Thiago Paiva Brito <thiagop@lsd.ufcg.edu.br>\n\nChange-Id: Ia8ae4c1d8d1e7e591a272928af0dfd17da4047c1\nImplements: blueprint hierarchical-multitenancy\n""}, {'number': 4, 'created': '2014-12-03 18:59:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/25754deb3c4bed2a44de7e769a573a5ed277f29b', 'message': ""Create, update and delete hierarchical projects\n\nChanges at basic operations related to projects\nwere modified to handle projects hierarchy.\n\nThis changes includes:\n- Create and update projects with project_id field\n- Avoid deleting projects that are not leaf in the\n  hierarchy\n- Deny the update of a project's parent\n- Limit the max depth of a tree branch (in the\n  hierarchy)\n\nCo-Authored-By: Henrique Truta <henrique@lsd.ufcg.edu.br>\nCo-Authored-By: Rodrigo Duarte <rodrigods@lsd.ufcg.edu.br>\nCo-Authored-By: Raildo Mascena <raildo@lsd.ufcg.edu.br>\nCo-Authored-By: Samuel de Medeiros Queiroz <samuel@lsd.ufcg.edu.br>\nCo-Authored-By: Thiago Paiva Brito <thiagop@lsd.ufcg.edu.br>\n\nChange-Id: Ia8ae4c1d8d1e7e591a272928af0dfd17da4047c1\nImplements: blueprint hierarchical-multitenancy\n""}, {'number': 5, 'created': '2014-12-03 18:59:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/3a21dba56fa1f67bad5951d5e833f3cb7d0d1ff3', 'message': ""Create, update and delete hierarchical projects\n\nChanges at basic operations related to projects\nwere modified to handle projects hierarchy.\n\nThis changes includes:\n- Create and update projects with parent_id field\n- Avoid deleting projects that are not leaf in the\n  hierarchy\n- Deny the update of a project's parent\n- Limit the max depth of a tree branch (in the\n  hierarchy)\n\nCo-Authored-By: Henrique Truta <henrique@lsd.ufcg.edu.br>\nCo-Authored-By: Rodrigo Duarte <rodrigods@lsd.ufcg.edu.br>\nCo-Authored-By: Raildo Mascena <raildo@lsd.ufcg.edu.br>\nCo-Authored-By: Samuel de Medeiros Queiroz <samuel@lsd.ufcg.edu.br>\nCo-Authored-By: Thiago Paiva Brito <thiagop@lsd.ufcg.edu.br>\n\nChange-Id: Ia8ae4c1d8d1e7e591a272928af0dfd17da4047c1\nImplements: blueprint hierarchical-multitenancy\n""}, {'number': 6, 'created': '2014-12-03 19:48:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/efd9733e7fa89a51de9278d3591d18aa33e5e885', 'message': ""Create, update and delete hierarchical projects\n\nChanges at basic operations related to projects\nwere modified to handle projects hierarchy.\n\nThis changes includes:\n- Create and update projects with parent_project_id\n  field\n- Avoid deleting projects that are not leaf in the\n  hierarchy\n- Deny the update of a project's parent\n- Limit the max depth of a tree branch (in the\n  hierarchy)\n\nCo-Authored-By: Henrique Truta <henrique@lsd.ufcg.edu.br>\nCo-Authored-By: Rodrigo Duarte <rodrigods@lsd.ufcg.edu.br>\nCo-Authored-By: Raildo Mascena <raildo@lsd.ufcg.edu.br>\nCo-Authored-By: Samuel de Medeiros Queiroz <samuel@lsd.ufcg.edu.br>\nCo-Authored-By: Thiago Paiva Brito <thiagop@lsd.ufcg.edu.br>\n\nChange-Id: Ia8ae4c1d8d1e7e591a272928af0dfd17da4047c1\nImplements: blueprint hierarchical-multitenancy\n""}, {'number': 7, 'created': '2014-12-04 21:25:57.000000000', 'files': ['etc/keystone.conf.sample', 'keystone/auth/controllers.py', 'keystone/assignment/controllers.py', 'keystone/common/config.py', 'keystone/common/controller.py', 'keystone/assignment/core.py', 'keystone/tests/test_v3_assignment.py', 'keystone/tests/test_backend_ldap.py', 'keystone/tests/test_backend.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/11cb0d303388b56b5974c74cd2349ebef3bf1d20', 'message': ""Create, update and delete hierarchical projects\n\nChanges at basic operations related to projects\nwere modified to handle projects hierarchy.\n\nThis changes includes:\n- Create and update projects with parent_project_id\n  field\n- Avoid deleting projects that are not leaf in the\n  hierarchy\n- Deny the update of a project's parent\n- Limit the max depth of a tree branch (in the\n  hierarchy)\n\nCo-Authored-By: Henrique Truta <henrique@lsd.ufcg.edu.br>\nCo-Authored-By: Rodrigo Duarte <rodrigods@lsd.ufcg.edu.br>\nCo-Authored-By: Raildo Mascena <raildo@lsd.ufcg.edu.br>\nCo-Authored-By: Samuel de Medeiros Queiroz <samuel@lsd.ufcg.edu.br>\nCo-Authored-By: Thiago Paiva Brito <thiagop@lsd.ufcg.edu.br>\n\nChange-Id: Ia8ae4c1d8d1e7e591a272928af0dfd17da4047c1\nImplements: blueprint hierarchical-multitenancy\n""}]",36,138550,11cb0d303388b56b5974c74cd2349ebef3bf1d20,40,17,7,11022,,,0,"Create, update and delete hierarchical projects

Changes at basic operations related to projects
were modified to handle projects hierarchy.

This changes includes:
- Create and update projects with parent_project_id
  field
- Avoid deleting projects that are not leaf in the
  hierarchy
- Deny the update of a project's parent
- Limit the max depth of a tree branch (in the
  hierarchy)

Co-Authored-By: Henrique Truta <henrique@lsd.ufcg.edu.br>
Co-Authored-By: Rodrigo Duarte <rodrigods@lsd.ufcg.edu.br>
Co-Authored-By: Raildo Mascena <raildo@lsd.ufcg.edu.br>
Co-Authored-By: Samuel de Medeiros Queiroz <samuel@lsd.ufcg.edu.br>
Co-Authored-By: Thiago Paiva Brito <thiagop@lsd.ufcg.edu.br>

Change-Id: Ia8ae4c1d8d1e7e591a272928af0dfd17da4047c1
Implements: blueprint hierarchical-multitenancy
",git fetch https://review.opendev.org/openstack/keystone refs/changes/50/138550/5 && git format-patch -1 --stdout FETCH_HEAD,"['etc/keystone.conf.sample', 'keystone/assignment/controllers.py', 'keystone/auth/controllers.py', 'keystone/common/config.py', 'keystone/common/controller.py', 'keystone/assignment/core.py', 'keystone/tests/test_v3_assignment.py', 'keystone/tests/test_backend_ldap.py', 'keystone/tests/test_backend.py']",9,07a4f53139bb3632f51b0b4725a58093d19ecdb8,bp/hierarchical-multitenancy," def _create_projects_hierarchy(self, hierarchy_size=1): """"""Creates a project hierarchy with specified size. :param hierarchy_size: the desired hierarchy size, default is 1 - a project with one child. :returns projects: a list of the projects in the created hierarchy. """""" project_id = uuid.uuid4().hex project = {'id': project_id, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'name': uuid.uuid4().hex, 'parent_id': None} self.assignment_api.create_project(project_id, project) projects = [project] for i in range(hierarchy_size): new_project = {'id': uuid.uuid4().hex, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'name': uuid.uuid4().hex, 'parent_id': project_id} self.assignment_api.create_project(new_project['id'], new_project) projects.append(new_project) project_id = new_project['id'] return projects def test_check_leaf_projects(self): projects_hierarchy = self._create_projects_hierarchy() root_project = projects_hierarchy[0] leaf_project = projects_hierarchy[1] projects_hierarchy = self._create_projects_hierarchy(2) project1 = projects_hierarchy[0] project2 = projects_hierarchy[1] project3 = projects_hierarchy[2] 'name': uuid.uuid4().hex, projects_hierarchy = self._create_projects_hierarchy(2) project1 = projects_hierarchy[0] project2 = projects_hierarchy[1] project3 = projects_hierarchy[2] 'name': uuid.uuid4().hex, def test_domain_delete_hierarchy(self): domain = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'enabled': True} self.assignment_api.create_domain(domain['id'], domain) # Creating a root and a leaf project inside the domain root_project = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'domain_id': domain['id'], 'parent_id': None} self.assignment_api.create_project(root_project['id'], root_project) leaf_project = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'domain_id': domain['id'], 'parent_id': root_project['id']} self.assignment_api.create_project(leaf_project['id'], leaf_project) # Disable the domain domain['enabled'] = False self.assignment_api.update_domain(domain['id'], domain) # Delete the domain self.assignment_api.delete_domain(domain['id']) # Make sure the domain no longer exists self.assertRaises(exception.DomainNotFound, self.assignment_api.get_domain, domain['id']) # Make sure the root project no longer exists self.assertRaises(exception.ProjectNotFound, self.assignment_api.get_project, root_project['id']) # Make sure the leaf project no longer exists self.assertRaises(exception.ProjectNotFound, self.assignment_api.get_project, leaf_project['id']) def test_hierarchical_projects_crud(self): root_project1 = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'parent_id': None} self.assignment_api.create_project(root_project1['id'], root_project1) root_project2 = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'parent_id': None} self.assignment_api.create_project(root_project2['id'], root_project2) leaf_project = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'parent_id': root_project1['id']} self.assignment_api.create_project(leaf_project['id'], leaf_project) leaf_project['description'] = 'new description' self.assignment_api.update_project(leaf_project['id'], leaf_project) proj_ref = self.assignment_api.get_project(leaf_project['id']) self.assertDictEqual(proj_ref, leaf_project) leaf_project['parent_id'] = root_project2['id'] self.assertRaises(exception.ForbiddenAction, self.assignment_api.update_project, leaf_project['id'], leaf_project) self.assignment_api.delete_project(leaf_project['id']) self.assertRaises(exception.ProjectNotFound, self.assignment_api.get_project, leaf_project['id']) self.assignment_api.delete_project(root_project1['id']) self.assertRaises(exception.ProjectNotFound, self.assignment_api.get_project, root_project1['id']) self.assignment_api.delete_project(root_project2['id']) self.assertRaises(exception.ProjectNotFound, self.assignment_api.get_project, root_project2['id']) def test_create_project_with_invalid_parent(self): project = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'parent_id': 'fake'} self.assertRaises(exception.ProjectNotFound, self.assignment_api.create_project, project['id'], project) def test_create_leaf_project_with_invalid_domain(self): root_project = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'parent_id': None} self.assignment_api.create_project(root_project['id'], root_project) leaf_project = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'description': '', 'domain_id': 'fake', 'enabled': True, 'parent_id': root_project['id']} self.assertRaises(exception.ForbiddenAction, self.assignment_api.create_project, leaf_project['id'], leaf_project) def test_delete_hierarchical_leaf_project(self): projects_hierarchy = self._create_projects_hierarchy() root_project = projects_hierarchy[0] leaf_project = projects_hierarchy[1] self.assignment_api.delete_project(leaf_project['id']) self.assertRaises(exception.ProjectNotFound, self.assignment_api.get_project, leaf_project['id']) self.assignment_api.delete_project(root_project['id']) self.assertRaises(exception.ProjectNotFound, self.assignment_api.get_project, root_project['id']) def test_delete_hierarchical_not_leaf_project(self): projects_hierarchy = self._create_projects_hierarchy() root_project = projects_hierarchy[0] self.assertRaises(exception.ForbiddenAction, self.assignment_api.delete_project, root_project['id']) def test_update_project_parent(self): projects_hierarchy = self._create_projects_hierarchy() leaf_project = projects_hierarchy[1] leaf_project['parent_id'] = 'fake' self.assertRaises(exception.ForbiddenAction, self.assignment_api.update_project, leaf_project['id'], leaf_project) def test_create_project_under_disabled_one(self): project1 = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': False, 'parent_id': None} self.assignment_api.create_project(project1['id'], project1) project2 = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'domain_id': DEFAULT_DOMAIN_ID, 'parent_id': project1['id']} # It's not possible to create a project under a disabled one in the # hierarchy self.assertRaises(exception.ForbiddenAction, self.assignment_api.create_project, project2['id'], project2) def test_disable_hierarchical_leaf_project(self): projects_hierarchy = self._create_projects_hierarchy() leaf_project = projects_hierarchy[1] leaf_project['enabled'] = False self.assignment_api.update_project(leaf_project['id'], leaf_project) project_ref = self.assignment_api.get_project(leaf_project['id']) self.assertEqual(project_ref['enabled'], leaf_project['enabled']) def test_disable_hierarchical_not_leaf_project(self): projects_hierarchy = self._create_projects_hierarchy() root_project = projects_hierarchy[0] root_project['enabled'] = False self.assertRaises(exception.ForbiddenAction, self.assignment_api.update_project, root_project['id'], root_project) def test_enable_project_with_disabled_parent(self): projects_hierarchy = self._create_projects_hierarchy() root_project = projects_hierarchy[0] leaf_project = projects_hierarchy[1] # Disable leaf and root leaf_project['enabled'] = False self.assignment_api.update_project(leaf_project['id'], leaf_project) root_project['enabled'] = False self.assignment_api.update_project(root_project['id'], root_project) # Try to enable the leaf project, it's not possible since it has # a disabled parent leaf_project['enabled'] = True self.assertRaises(exception.ForbiddenAction, self.assignment_api.update_project, leaf_project['id'], leaf_project) def _get_hierarchy_depth(self, project_id): return len(self.assignment_api.list_project_parents(project_id)) + 1 def test_check_hierarchy_depth(self): # First create a hierarchy with the max allowed depth projects_hierarchy = self._create_projects_hierarchy( CONF.max_project_tree_depth - 1) leaf_project = projects_hierarchy[CONF.max_project_tree_depth - 1] depth = self._get_hierarchy_depth(leaf_project['id']) self.assertEqual(CONF.max_project_tree_depth, depth) # Creating another project in the hierarchy shouldn't be allowed project_id = uuid.uuid4().hex project = { 'id': project_id, 'name': uuid.uuid4().hex, 'domain_id': DEFAULT_DOMAIN_ID, 'parent_id': leaf_project['id']} self.assertRaises(exception.ForbiddenAction, self.assignment_api.create_project, project_id, project) "," def test_check_leaf_projects(self): root_project = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'domain_id': DEFAULT_DOMAIN_ID, 'parent_id': None} self.assignment_api.create_project(root_project['id'], root_project) leaf_project = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'domain_id': DEFAULT_DOMAIN_ID, 'parent_id': root_project['id']} self.assignment_api.create_project(leaf_project['id'], leaf_project) project1 = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'parent_id': None} self.assignment_api.create_project(project1['id'], project1) project2 = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'parent_id': project1['id']} self.assignment_api.create_project(project2['id'], project2) project3 = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'parent_id': project2['id']} self.assignment_api.create_project(project3['id'], project3) 'name': uuid.uuid4().hex, project1 = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'parent_id': None} self.assignment_api.create_project(project1['id'], project1) project2 = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'parent_id': project1['id']} self.assignment_api.create_project(project2['id'], project2) project3 = {'id': uuid.uuid4().hex, 'name': uuid.uuid4().hex, 'description': '', 'domain_id': DEFAULT_DOMAIN_ID, 'enabled': True, 'parent_id': project2['id']} self.assignment_api.create_project(project3['id'], project3) 'name': uuid.uuid4().hex,",633,105
openstack%2Fnova~master~I56503c0736cb465a9991b50b71bc1a904347d4f7,openstack/nova,master,I56503c0736cb465a9991b50b71bc1a904347d4f7,xenapi: support the hotplug of a neutron port,ABANDONED,2013-11-09 13:06:15.000000000,2014-12-05 15:48:49.000000000,,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 688}, {'_account_id': 782}, {'_account_id': 1653}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}]","[{'number': 1, 'created': '2013-11-09 13:06:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/fe50b6f3b9886a5c224872fc26532496c97f8983', 'message': 'xenapi: add hotplug vif support\n\nChange-Id: I56503c0736cb465a9991b50b71bc1a904347d4f7\n'}, {'number': 2, 'created': '2013-11-16 12:00:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b15a1e80b26d0ec68e0e563866bd3c4bc8fa8667', 'message': 'xenapi: support the hotplug of a neutron port\n\nNova already has support for hotplugging neutorn ports in the libvirt\ndriver. This extends the support to the XenAPI driver.\n\nimplements blueprint xenapi-vif-hotplug\nChange-Id: I56503c0736cb465a9991b50b71bc1a904347d4f7\n'}, {'number': 4, 'created': '2013-11-16 12:08:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c9e10dd78d86faedcd1ac480ec81898b34127f4e', 'message': 'xenapi: support the hotplug of a neutron port\n\nNova already has support for hotplugging neutorn ports in the libvirt\ndriver. This extends the support to the XenAPI driver.\n\nimplements blueprint xenapi-vif-hotplug\nChange-Id: I56503c0736cb465a9991b50b71bc1a904347d4f7\n'}, {'number': 3, 'created': '2013-11-16 12:08:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/213ca73a8b13bbf3701a44b1a2378dce4419ca4c', 'message': 'xenapi: support the hotplug of a neutron port\n\nNova already has support for hotplugging neutorn ports in the libvirt\ndriver. This extends the support to the XenAPI driver.\n\nimplements blueprint xenapi-vif-hotplug\nChange-Id: I56503c0736cb465a9991b50b71bc1a904347d4f7\n'}, {'number': 5, 'created': '2013-12-16 17:58:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/4d1f183e362c8c49be456380ef2eebb1891675a6', 'message': 'xenapi: support the hotplug of a neutron port\n\nNova already has support for hotplugging neutorn ports in the libvirt\ndriver. This extends the support to the XenAPI driver.\n\nThe code path used for attaching VIFs during VM boot is reused, with the\naddition of code to find the next available device_id.\n\nblueprint xenapi-vif-hotplug\nChange-Id: I56503c0736cb465a9991b50b71bc1a904347d4f7\n'}, {'number': 6, 'created': '2014-02-27 15:09:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ca9c6a7b8e94531ba0a54003dddfb58723100deb', 'message': 'xenapi: support the hotplug of a neutron port\n\nNova already has support for hotplugging neutorn ports in the libvirt\ndriver. This extends the support to the XenAPI driver.\n\nThe code path used for attaching VIFs during VM boot is reused, with the\naddition of code to find the next available device_id.\n\nblueprint xenapi-vif-hotplug\nChange-Id: I56503c0736cb465a9991b50b71bc1a904347d4f7\n'}, {'number': 7, 'created': '2014-02-27 15:09:40.000000000', 'files': ['nova/virt/xenapi/driver.py', 'nova/tests/virt/xenapi/test_xenapi.py', 'nova/virt/xenapi/vmops.py', 'nova/virt/xenapi/fake.py', 'nova/tests/virt/xenapi/test_vmops.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/8c1e3cf03de8e71c6769e363ec58fe1a97e3759d', 'message': 'xenapi: support the hotplug of a neutron port\n\nNova already has support for hotplugging neutorn ports in the libvirt\ndriver. This extends the support to the XenAPI driver.\n\nThe code path used for attaching VIFs during VM boot is reused, with the\naddition of code to find the next available device_id.\n\nblueprint xenapi-vif-hotplug\nChange-Id: I56503c0736cb465a9991b50b71bc1a904347d4f7\n'}]",3,55720,8c1e3cf03de8e71c6769e363ec58fe1a97e3759d,84,10,7,782,,,0,"xenapi: support the hotplug of a neutron port

Nova already has support for hotplugging neutorn ports in the libvirt
driver. This extends the support to the XenAPI driver.

The code path used for attaching VIFs during VM boot is reused, with the
addition of code to find the next available device_id.

blueprint xenapi-vif-hotplug
Change-Id: I56503c0736cb465a9991b50b71bc1a904347d4f7
",git fetch https://review.opendev.org/openstack/nova refs/changes/20/55720/6 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/xenapi/driver.py', 'nova/virt/xenapi/vmops.py']",2,fe50b6f3b9886a5c224872fc26532496c97f8983,bp/xenapi-vif-hotplug," def plug_vif(self, instance, vif, device=None): @utils.synchronized('vif-' + instance['uuid']) def plug(): if device is None: device = self._find_next_vif_device(instance) plug() def _find_next_vif_device(instance): vm_ref = self._get_vm_opaque_ref(instance) vif_refs = self.session.call_plugin(""VM.get_VIFs"") max_device = -1 for vif_ref in vif_refs: device = self._session.call_xenapi(""VIF.get_device"") if device > max_device: max_device = device return max_device + 1 def unplug_vif(self, instance, vif): @utils.synchronized('vif-' + instance['uuid']) def unplug(): self.vif_driver.unplug(instance, vif) unplug()"," def plug_vifs(self, instance, network_info): """"""Set up VIF networking on the host."""""" for device, vif in enumerate(network_info): def unplug_vifs(self, instance, network_info): if network_info: for vif in network_info: self.vif_driver.unplug(instance, vif) @utils.synchronized('xenstore-' + instance['uuid'])",37,10
openstack%2Fnova~master~I00bacd46267a58fe05b627e9d5c87b9af87ffdd4,openstack/nova,master,I00bacd46267a58fe05b627e9d5c87b9af87ffdd4,Remove calls to call_xenapi from xenapi/vif.py,ABANDONED,2014-04-28 20:01:23.000000000,2014-12-05 15:48:48.000000000,,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 782}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 8163}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-04-28 20:01:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f313b5f81a90c571ace748000c4825f5131f43cb', 'message': 'Remove calls to call_xenapi from xenapi/vif.py\n\nAlso added PIF and VLAN that were required to for vif.py\n\nTODO - need to improve coverage here!\n\nChange-Id: I00bacd46267a58fe05b627e9d5c87b9af87ffdd4\n'}, {'number': 2, 'created': '2014-04-28 20:01:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/af8e78f0638c927d12eb52407beb862498f0d0a0', 'message': 'Remove calls to call_xenapi from xenapi/vif.py\n\nAlso added PIF and VLAN that were required to for vif.py\n\nTODO - need to improve coverage here!\n\nChange-Id: I00bacd46267a58fe05b627e9d5c87b9af87ffdd4\n'}, {'number': 3, 'created': '2014-04-28 20:01:23.000000000', 'files': ['nova/virt/xenapi/vif.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/a7daf90dec644b9ce128f8ff51a6843bd1f16be0', 'message': 'Remove calls to call_xenapi from xenapi/vif.py\n\nAlso added PIF and VLAN that were required to for vif.py\n\nTODO - need to improve coverage here!\n\nChange-Id: I00bacd46267a58fe05b627e9d5c87b9af87ffdd4\n'}]",0,72371,a7daf90dec644b9ce128f8ff51a6843bd1f16be0,26,9,3,782,,,0,"Remove calls to call_xenapi from xenapi/vif.py

Also added PIF and VLAN that were required to for vif.py

TODO - need to improve coverage here!

Change-Id: I00bacd46267a58fe05b627e9d5c87b9af87ffdd4
",git fetch https://review.opendev.org/openstack/nova refs/changes/71/72371/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/xenapi/client/objects.py', 'nova/virt/xenapi/vif.py', 'nova/tests/virt/xenapi/client/test_objects.py']",3,f313b5f81a90c571ace748000c4825f5131f43cb,xenapi-refactor," def test_apply_session_helpers_add_PIF(self): self.session.PIF.get_X(""ref"") self.session.call_xenapi.assert_called_once_with(""PIF.get_X"", ""ref"") def test_apply_session_helpers_add_VLAN(self): self.session.VLAN.get_X(""ref"") self.session.call_xenapi.assert_called_once_with(""VLAN.get_X"", ""ref"") ",,28,12
openstack%2Fnova~master~Iaea7d224ee71ccb6ea08a2157e998a94003a7cfb,openstack/nova,master,Iaea7d224ee71ccb6ea08a2157e998a94003a7cfb,Add virt type check before image_type usage,ABANDONED,2014-08-07 08:25:57.000000000,2014-12-05 15:48:47.000000000,,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 6062}, {'_account_id': 8412}, {'_account_id': 8802}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-07 08:25:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d06be561dac79f4fa22863651e6c89dd3d00c6d0', 'message': ""Add virt type check before image_type usage\n\nCurrently in virt image file, there is a check for libvirt\nspecified configuration which logically is not correct.\nAdd a type check and ensure it's only used for libvirt.\n\nChange-Id: Iaea7d224ee71ccb6ea08a2157e998a94003a7cfb\n""}, {'number': 2, 'created': '2014-08-11 07:14:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a85224ce1c46ffbc56ed5ae2e6a2c76bd574a5aa', 'message': ""Add virt type check before image_type usage\n\nCurrently in virt image file, there is a check for libvirt\nspecified configuration which logically is not correct.\nAdd a type check and ensure it's only used for libvirt.\n\nChange-Id: Iaea7d224ee71ccb6ea08a2157e998a94003a7cfb\n""}, {'number': 3, 'created': '2014-10-08 04:01:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/436df92540ff0122a5bb304f611bc180a29ae492', 'message': ""Add virt type check before image_type usage\n\nCurrently in virt image file, there is a check for libvirt\nspecified configuration which logically is not correct.\nAdd a type check and ensure it's only used for libvirt.\n\nChange-Id: Iaea7d224ee71ccb6ea08a2157e998a94003a7cfb\n""}, {'number': 4, 'created': '2014-10-08 07:28:23.000000000', 'files': ['nova/tests/virt/test_images.py', 'nova/virt/images.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/a913ccd504b8ca7acfbffdce9b550f943d9a0def', 'message': ""Add virt type check before image_type usage\n\nCurrently in virt image file, there is a check for libvirt\nspecified configuration which logically is not correct.\nAdd a type check and ensure it's only used for libvirt.\n\nChange-Id: Iaea7d224ee71ccb6ea08a2157e998a94003a7cfb\n""}]",2,112510,a913ccd504b8ca7acfbffdce9b550f943d9a0def,32,11,4,6062,,,0,"Add virt type check before image_type usage

Currently in virt image file, there is a check for libvirt
specified configuration which logically is not correct.
Add a type check and ensure it's only used for libvirt.

Change-Id: Iaea7d224ee71ccb6ea08a2157e998a94003a7cfb
",git fetch https://review.opendev.org/openstack/nova refs/changes/10/112510/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/test_images.py', 'nova/virt/images.py']",2,d06be561dac79f4fa22863651e6c89dd3d00c6d0,add_libvirt_check_for_qemu,from nova.virt import driverdef _is_libvirt_image_type_rbd(): if (driver.compute_driver_matches('ibvirt.LibvirtDriver') and CONF.libvirt.images_type == 'rbd'): return True return False if not os.path.exists(path) and not _is_libvirt_image_type_rbd():, # TODO(mikal): this code should not be referring to a libvirt specific # flag. if not os.path.exists(path) and CONF.libvirt.images_type != 'rbd':,30,3
openstack%2Fnova~master~I36598e3ab9165960449d315421f7407075d160ae,openstack/nova,master,I36598e3ab9165960449d315421f7407075d160ae,disk: add support to extend ntfs file system,ABANDONED,2014-08-19 15:59:14.000000000,2014-12-05 15:48:46.000000000,,"[{'_account_id': 3}, {'_account_id': 1779}, {'_account_id': 1812}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 7730}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-19 15:59:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/ed18f2fc35c7802a51dedfa49fdd2856a74574ff', 'message': 'disk: add support to extend ntfs file system\n\nIntroduces ntfsresize to resize NTFS file system after\nan extend of the disk image.\n\nCloses-Bug: #1358316\nChange-Id: I36598e3ab9165960449d315421f7407075d160ae\n'}, {'number': 2, 'created': '2014-08-20 08:36:51.000000000', 'files': ['nova/tests/virt/disk/test_api.py', 'nova/virt/disk/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/51f71290d2ba64a0953f4f32781a853e007079f6', 'message': 'disk: add support to extend ntfs file system\n\nIntroduces ntfsresize to resize NTFS file system after\nan extend of the disk image.\n\nCloses-Bug: #1358316\nChange-Id: I36598e3ab9165960449d315421f7407075d160ae\n'}]",0,115329,51f71290d2ba64a0953f4f32781a853e007079f6,21,9,2,7730,,,0,"disk: add support to extend ntfs file system

Introduces ntfsresize to resize NTFS file system after
an extend of the disk image.

Closes-Bug: #1358316
Change-Id: I36598e3ab9165960449d315421f7407075d160ae
",git fetch https://review.opendev.org/openstack/nova refs/changes/29/115329/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/virt/disk/test_api.py', 'nova/virt/disk/api.py']",2,ed18f2fc35c7802a51dedfa49fdd2856a74574ff,bug/1358316," raisedef ntfsresize(image, check_exit_code=False, run_as_root=False): try: utils.execute('ntfsresize', '-c', image, check_exit_code=[0], run_as_root=run_as_root) except processutils.ProcessExecutionError as exc: LOG.debug(""Checking the file system has failed, "" ""the resize will be aborted. (%s)"", exc) raise else: utils.execute('ntfsresize', '-x', image, check_exit_code=check_exit_code, run_as_root=run_as_root) def safe_resize(dev, run_as_root=False, finally_call=lambda: None): try: for f in (resize2fs, ntfsresize): try: f(dev, run_as_root=run_as_root, check_exit_code=[0]) break except processutils.ProcessExecutionError as exc: LOG.debug(""Resizing the file system with %(name)s "" ""has failed with error: %(error)s"", { ""name"": f, ""error"": exc }) safe_resize(mounter.device, run_as_root=True, finally_call=mounter.unget_dev) else: safe_resize(image)"," def safe_resize2fs(dev, run_as_root=False, finally_call=lambda: None): try: resize2fs(dev, run_as_root=run_as_root, check_exit_code=[0]) except processutils.ProcessExecutionError as exc: LOG.debug(""Resizing the file system with resize2fs "" ""has failed with error: %s"", exc) safe_resize2fs(mounter.device, run_as_root=True, finally_call=mounter.unget_dev) else: safe_resize2fs(image)",88,9
openstack%2Fnova~master~Ica40858cea02f6f467fa0bfd551a5131ad54d50c,openstack/nova,master,Ica40858cea02f6f467fa0bfd551a5131ad54d50c,Remove Nova v3 API endpoint,ABANDONED,2014-10-29 03:50:41.000000000,2014-12-05 15:48:43.000000000,,"[{'_account_id': 3}, {'_account_id': 679}, {'_account_id': 2750}, {'_account_id': 6167}, {'_account_id': 9420}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-10-29 03:50:41.000000000', 'files': ['etc/nova/api-paste.ini'], 'web_link': 'https://opendev.org/openstack/nova/commit/16c699d033ee9756afd78a75d46b1f2f8e4cad37', 'message': ""Remove Nova v3 API endpoint\n\nNova v3 API has disappeared in Juno cycle, and we don't use the API\nnow on the gate because I85f87b37558a15d1eaaa781b02fec5b02bd2ab44\nhas removed the endpoint from DevStack.\n\nThis patch removes the endpoint of Nova v3 API from Nova.\n\nPartially implements blueprint v2-on-v3-api\n\nChange-Id: Ica40858cea02f6f467fa0bfd551a5131ad54d50c\n""}]",0,131627,16c699d033ee9756afd78a75d46b1f2f8e4cad37,6,6,1,6167,,,0,"Remove Nova v3 API endpoint

Nova v3 API has disappeared in Juno cycle, and we don't use the API
now on the gate because I85f87b37558a15d1eaaa781b02fec5b02bd2ab44
has removed the endpoint from DevStack.

This patch removes the endpoint of Nova v3 API from Nova.

Partially implements blueprint v2-on-v3-api

Change-Id: Ica40858cea02f6f467fa0bfd551a5131ad54d50c
",git fetch https://review.opendev.org/openstack/nova refs/changes/27/131627/1 && git format-patch -1 --stdout FETCH_HEAD,['etc/nova/api-paste.ini'],1,16c699d033ee9756afd78a75d46b1f2f8e4cad37,bp/v2-on-v3-api,,/v3: openstack_compute_api_v3[composite:openstack_compute_api_v3] use = call:nova.api.auth:pipeline_factory_v21 noauth = request_id faultwrap sizelimit noauth_v3 osapi_compute_app_v3 keystone = request_id faultwrap sizelimit authtoken keystonecontext osapi_compute_app_v3 [app:osapi_compute_app_v3] paste.app_factory = nova.api.openstack.compute:APIRouterV3.factory ,0,9
openstack%2Fnova~master~I3f1159ffb988ffcb94d00ade76da83193a61e73f,openstack/nova,master,I3f1159ffb988ffcb94d00ade76da83193a61e73f,Raise exception when deleting a not existing floating ip bulk,ABANDONED,2014-09-26 07:06:01.000000000,2014-12-05 15:48:42.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 5292}, {'_account_id': 6167}, {'_account_id': 6348}, {'_account_id': 9008}, {'_account_id': 9578}]","[{'number': 1, 'created': '2014-09-26 07:06:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/039e486770581f75b222b7763932bbdb8fc9e878', 'message': 'Raise exception when deleting a not existing floating ip bulk\n\nCurrently when deleting a floating ip bulk which does not exist,\n200 is returned to the client without any warning.\nThis patch adds a new exception FloatingIpBulkNotFound, and\nreturn 404 to the client.\n\nChange-Id: I3f1159ffb988ffcb94d00ade76da83193a61e73f\nCloses-Bug: #1374305\n'}, {'number': 2, 'created': '2014-09-29 01:50:49.000000000', 'files': ['nova/exception.py', 'nova/api/openstack/compute/contrib/floating_ips_bulk.py', 'nova/tests/integrated/test_api_samples.py', 'nova/tests/db/test_db_api.py', 'nova/tests/api/openstack/compute/contrib/test_floating_ips_bulk.py', 'nova/db/sqlalchemy/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/8eea82ae17ed05aff3d8e577d239c0399c1e802b', 'message': 'Raise exception when deleting a not existing floating ip bulk\n\nCurrently when deleting a floating ip bulk which does not exist,\n200 is returned to the client without any warning.\nThis patch adds a new exception FloatingIpBulkNotFound, and\nreturn 404 to the client.\n\nChange-Id: I3f1159ffb988ffcb94d00ade76da83193a61e73f\nCloses-Bug: #1374305\n'}]",3,124311,8eea82ae17ed05aff3d8e577d239c0399c1e802b,18,8,2,6348,,,0,"Raise exception when deleting a not existing floating ip bulk

Currently when deleting a floating ip bulk which does not exist,
200 is returned to the client without any warning.
This patch adds a new exception FloatingIpBulkNotFound, and
return 404 to the client.

Change-Id: I3f1159ffb988ffcb94d00ade76da83193a61e73f
Closes-Bug: #1374305
",git fetch https://review.opendev.org/openstack/nova refs/changes/11/124311/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/exception.py', 'nova/api/openstack/compute/contrib/floating_ips_bulk.py', 'nova/tests/api/openstack/compute/contrib/test_floating_ips_bulk.py', 'nova/db/sqlalchemy/api.py']",4,039e486770581f75b222b7763932bbdb8fc9e878,delete_floatingip_bulk_not_exist," if len(rows) == 0: raise exception.FloatingIpBulkNotFound() else: for row in rows: # The count is negative since we release quota by # reserving negative quota. project_id_to_quota_count[row['project_id']] -= 1 # Delete the floating IPs. model_query(context, models.FloatingIp).\ filter(models.FloatingIp.address.in_(ip_block)).\ soft_delete(synchronize_session='fetch')"," for row in rows: # The count is negative since we release quota by # reserving negative quota. project_id_to_quota_count[row['project_id']] -= 1 # Delete the floating IPs. model_query(context, models.FloatingIp).\ filter(models.FloatingIp.address.in_(ip_block)).\ soft_delete(synchronize_session='fetch')",31,9
openstack%2Fnova~master~I8ba66a272cba9bd7ecbcd4d56eab1a28508610b1,openstack/nova,master,I8ba66a272cba9bd7ecbcd4d56eab1a28508610b1,Allow deleting instances while uuid lock is held,ABANDONED,2014-04-16 19:28:13.000000000,2014-12-05 15:48:40.000000000,,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 782}, {'_account_id': 1501}, {'_account_id': 1653}, {'_account_id': 1779}, {'_account_id': 1849}, {'_account_id': 2750}, {'_account_id': 3185}, {'_account_id': 4393}, {'_account_id': 4690}, {'_account_id': 5170}, {'_account_id': 5638}, {'_account_id': 6062}, {'_account_id': 8163}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 9732}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-04-16 19:28:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/6c600684b66ea9db9b33b2f0186eff925fd359f7', 'message': 'Allow deleting instances while uuid lock is held\n\nAvoid issues where locked up operations on the instance prevent it\nfrom being deleted by the user. Reasons for the lock up are a separate\nissue and will be handled elsewhere, but terminating the instance\nshould not affect any tasks anyway. Any modification should already\ngracefully handle the instance going away.\n\nFor more discussion about the issue see:\nhttp://lists.openstack.org/pipermail/openstack-dev/2013-October/017454.html\n\nThis patch was merged once before and had to be reverted in\nI72fb1f9de7767b7206c5fa39b87216fa3f1b5c68 as it exposed a race condition in\nthe Hyper-V driver.\n\nThis version of the patch makes the use of a delete specific lock dependent\non the capabilities exposed by the virt driver, allowing it to be implemented\non a per driver basis.\n\nDrivers which do not support this are:\n\n- Hyper-V because of the aforementioned race condition\n\n- Bare metal because ""delete"" of a node is really just a de-allocate, and it\nmay not be safe to re-use a node that is for example still imaging its disk\nin another thread.\n\nChange-Id: I8ba66a272cba9bd7ecbcd4d56eab1a28508610b1\nCloses-Bug: #1276772\n'}, {'number': 2, 'created': '2014-04-22 11:22:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e4e34106ddd3bf2892004f0dc7cfeb597b2c78df', 'message': 'Allow deleting instances while uuid lock is held\n\nAvoid issues where locked up operations on the instance prevent it\nfrom being deleted by the user. Reasons for the lock up are a separate\nissue and will be handled elsewhere, but terminating the instance\nshould not affect any tasks anyway. Any modification should already\ngracefully handle the instance going away.\n\nFor more discussion about the issue see:\nhttp://lists.openstack.org/pipermail/openstack-dev/2013-October/017454.html\n\nThis patch was merged once before and had to be reverted in\nI72fb1f9de7767b7206c5fa39b87216fa3f1b5c68 as it exposed a race condition in\nthe Hyper-V driver.\n\nThis version of the patch makes the use of a delete specific lock dependent\non the capabilities exposed by the virt driver, allowing it to be implemented\non a per driver basis.\n\nDrivers which do not support this are:\n\n- Hyper-V because of the aforementioned race condition\n\n- Bare metal because ""delete"" of a node is really just a de-allocate, and it\nmay not be safe to re-use a node that is for example still imaging its disk\nin another thread.\n\nChange-Id: I8ba66a272cba9bd7ecbcd4d56eab1a28508610b1\nCloses-Bug: #1276772\n'}, {'number': 3, 'created': '2014-04-22 12:35:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/7d2a0dce7c7845bad6ce1cb45f00369a30e85630', 'message': 'Allow deleting instances while uuid lock is held\n\nAvoid issues where locked up operations on the instance prevent it\nfrom being deleted by the user. Reasons for the lock up are a separate\nissue and will be handled elsewhere, but terminating the instance\nshould not affect any tasks anyway. Any modification should already\ngracefully handle the instance going away.\n\nFor more discussion about the issue see:\nhttp://lists.openstack.org/pipermail/openstack-dev/2013-October/017454.html\n\nThis patch was merged once before and had to be reverted in\nI72fb1f9de7767b7206c5fa39b87216fa3f1b5c68 as it exposed a race condition in\nthe Hyper-V driver.\n\nThis version of the patch makes the use of a delete specific lock dependent\non the capabilities exposed by the virt driver, allowing it to be implemented\non a per driver basis.\n\nDrivers which do not support this are:\n\n- Hyper-V because of the aforementioned race condition\n\n- Bare metal because ""delete"" of a node is really just a de-allocate, and it\nmay not be safe to re-use a node that is for example still imaging its disk\nin another thread.\n\nChange-Id: I8ba66a272cba9bd7ecbcd4d56eab1a28508610b1\nCloses-Bug: #1276772\n'}, {'number': 4, 'created': '2014-04-22 13:29:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1b6fe64b4a6de7551742a214ddfe928cfe51a200', 'message': 'Allow deleting instances while uuid lock is held\n\nAvoid issues where locked up operations on the instance prevent it\nfrom being deleted by the user. Reasons for the lock up are a separate\nissue and will be handled elsewhere, but terminating the instance\nshould not affect any tasks anyway. Any modification should already\ngracefully handle the instance going away.\n\nFor more discussion about the issue see:\nhttp://lists.openstack.org/pipermail/openstack-dev/2013-October/017454.html\n\nThis patch was merged once before and had to be reverted in\nI72fb1f9de7767b7206c5fa39b87216fa3f1b5c68 as it exposed a race condition in\nthe Hyper-V driver.\n\nThis version of the patch makes the use of a delete specific lock dependent\non the capabilities exposed by the virt driver, allowing it to be implemented\non a per driver basis.\n\nThe default value is to declare the driver as not supporting thread safe\ndelete, which matches the current behaviour.\n\nDrivers which are known to not support thread safe delete are:\n\n- Hyper-V because of the aforementioned race condition\n\n- Bare metal because ""delete"" of a node is really just a de-allocate, and it\nmay not be safe to re-use a node that is for example still imaging its disk\nin another thread.\n\nChange-Id: I8ba66a272cba9bd7ecbcd4d56eab1a28508610b1\nCloses-Bug: #1276772\n'}, {'number': 5, 'created': '2014-06-02 16:41:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/5ea063f5a016f26f1f5209260a7cebcebaa5052f', 'message': 'Allow deleting instances while uuid lock is held\n\nAvoid issues where locked up operations on the instance prevent it\nfrom being deleted by the user. Reasons for the lock up are a separate\nissue and will be handled elsewhere, but terminating the instance\nshould not affect any tasks anyway. Any modification should already\ngracefully handle the instance going away.\n\nFor more discussion about the issue see:\nhttp://lists.openstack.org/pipermail/openstack-dev/2013-October/017454.html\n\nThis patch was merged once before and had to be reverted in\nI72fb1f9de7767b7206c5fa39b87216fa3f1b5c68 as it exposed a race condition in\nthe Hyper-V driver.\n\nThis version of the patch makes the use of a delete specific lock dependent\non the capabilities exposed by the virt driver, allowing it to be implemented\non a per driver basis.\n\nThe default value is to declare the driver as not supporting thread safe\ndelete, which matches the current behaviour.\n\nDrivers which are known to not support thread safe delete are:\n\n- Hyper-V because of the aforementioned race condition\n\n- Bare metal because ""delete"" of a node is really just a de-allocate, and it\nmay not be safe to re-use a node that is for example still imaging its disk\nin another thread.\n\nChange-Id: I8ba66a272cba9bd7ecbcd4d56eab1a28508610b1\nCloses-Bug: #1276772\n'}, {'number': 6, 'created': '2014-06-24 15:26:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/754fea980e1d80c25d273be749d0eab4f3771c28', 'message': 'Allow deleting instances while uuid lock is held\n\nAvoid issues where locked up operations on the instance prevent it\nfrom being deleted by the user. Reasons for the lock up are a separate\nissue and will be handled elsewhere, but terminating the instance\nshould not affect any tasks anyway. Any modification should already\ngracefully handle the instance going away.\n\nFor more discussion about the issue see:\nhttp://lists.openstack.org/pipermail/openstack-dev/2013-October/017454.html\n\nThis patch was merged once before and had to be reverted in\nI72fb1f9de7767b7206c5fa39b87216fa3f1b5c68 as it exposed a race condition in\nthe Hyper-V driver.\n\nThis version of the patch makes the use of a delete specific lock dependent\non the capabilities exposed by the virt driver, allowing it to be implemented\non a per driver basis.\n\nThe default value is to declare the driver as not supporting thread safe\ndelete, which matches the current behaviour.\n\nDrivers which are known to not support thread safe delete are:\n\n- Hyper-V because of the aforementioned race condition\n\n- Bare metal because ""delete"" of a node is really just a de-allocate, and it\nmay not be safe to re-use a node that is for example still imaging its disk\nin another thread.\n\nThe patch also provides a more comprehensive fix for the change to bug 1326998\nthan that mergered in I3b7c5afce7f3bacce1b444c245b035334962ff7c by covering\nthe case where a domain disapears during cleanup operation, rather than just\nchecking if the domain exists.\n\nChange-Id: I8ba66a272cba9bd7ecbcd4d56eab1a28508610b1\nCloses-Bug: #1276772\n'}, {'number': 7, 'created': '2014-07-04 15:03:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/d9170ccc1c8bce81341e83a5b38344f6518d0dfc', 'message': 'Allow deleting instances while uuid lock is held\n\nAvoid issues where locked up operations on the instance prevent it\nfrom being deleted by the user. Reasons for the lock up are a separate\nissue and will be handled elsewhere, but terminating the instance\nshould not affect any tasks anyway. Any modification should already\ngracefully handle the instance going away.\n\nFor more discussion about the issue see:\nhttp://lists.openstack.org/pipermail/openstack-dev/2013-October/017454.html\n\nThis patch was merged once before and had to be reverted in\nI72fb1f9de7767b7206c5fa39b87216fa3f1b5c68 as it exposed a race condition in\nthe Hyper-V driver.\n\nThis version of the patch makes the use of a delete specific lock dependent\non the capabilities exposed by the virt driver, allowing it to be implemented\non a per driver basis.\n\nThe default value is to declare the driver as not supporting thread safe\ndelete, which matches the current behaviour.\n\nDrivers which are known to not support thread safe delete are:\n\n- Hyper-V because of the aforementioned race condition\n\n- Bare metal because ""delete"" of a node is really just a de-allocate, and it\nmay not be safe to re-use a node that is for example still imaging its disk\nin another thread.\n\nThe patch also provides a more comprehensive fix for the change to bug 1326998\nthan that mergered in I3b7c5afce7f3bacce1b444c245b035334962ff7c by covering\nthe case where a domain disapears during cleanup operation, rather than just\nchecking if the domain exists.\n\nBeacuse instances can now be deleted during a build the patch also avoids\nrescheduling a build failure where the vm or task state is changed (delete in\nprogress) or the instance is deleted.\n\nChange-Id: I8ba66a272cba9bd7ecbcd4d56eab1a28508610b1\nCloses-Bug: #1276772\n'}, {'number': 8, 'created': '2014-07-25 22:50:27.000000000', 'files': ['nova/virt/hyperv/driver.py', 'nova/virt/libvirt/driver.py', 'nova/virt/fake.py', 'nova/tests/compute/test_compute_mgr.py', 'nova/virt/vmwareapi/driver.py', 'nova/network/manager.py', 'nova/virt/driver.py', 'nova/compute/manager.py', 'nova/tests/compute/test_compute.py', 'nova/virt/baremetal/driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/5ce4b5292ade05ee48ed687b0d1644a8d40a5a30', 'message': 'Allow deleting instances while uuid lock is held\n\nAvoid issues where locked up operations on the instance prevent it\nfrom being deleted by the user. Reasons for the lock up are a separate\nissue and will be handled elsewhere, but terminating the instance\nshould not affect any tasks anyway. Any modification should already\ngracefully handle the instance going away.\n\nFor more discussion about the issue see:\nhttp://lists.openstack.org/pipermail/openstack-dev/2013-October/017454.html\n\nThis patch was merged once before and had to be reverted in\nI72fb1f9de7767b7206c5fa39b87216fa3f1b5c68 as it exposed a race condition in\nthe Hyper-V driver.\n\nThis version of the patch makes the use of a delete specific lock dependent\non the capabilities exposed by the virt driver, allowing it to be implemented\non a per driver basis.\n\nThe default value is to declare the driver as not supporting thread safe\ndelete, which matches the current behaviour.\n\nDrivers which are known to not support thread safe delete are:\n\n- Hyper-V because of the aforementioned race condition\n\n- Bare metal because ""delete"" of a node is really just a de-allocate, and it\nmay not be safe to re-use a node that is for example still imaging its disk\nin another thread.\n\nThe patch also provides a more comprehensive fix for the change to bug 1326998\nthan that mergered in I3b7c5afce7f3bacce1b444c245b035334962ff7c by covering\nthe case where a domain disapears during cleanup operation, rather than just\nchecking if the domain exists.\n\nBeacuse instances can now be deleted during a build the patch also avoids\nrescheduling a build failure where the vm or task state is changed (delete in\nprogress) or the instance is deleted.\n\nChange-Id: I8ba66a272cba9bd7ecbcd4d56eab1a28508610b1\nCloses-Bug: #1276772\n'}]",52,88067,5ce4b5292ade05ee48ed687b0d1644a8d40a5a30,202,22,8,1501,,,0,"Allow deleting instances while uuid lock is held

Avoid issues where locked up operations on the instance prevent it
from being deleted by the user. Reasons for the lock up are a separate
issue and will be handled elsewhere, but terminating the instance
should not affect any tasks anyway. Any modification should already
gracefully handle the instance going away.

For more discussion about the issue see:
http://lists.openstack.org/pipermail/openstack-dev/2013-October/017454.html

This patch was merged once before and had to be reverted in
I72fb1f9de7767b7206c5fa39b87216fa3f1b5c68 as it exposed a race condition in
the Hyper-V driver.

This version of the patch makes the use of a delete specific lock dependent
on the capabilities exposed by the virt driver, allowing it to be implemented
on a per driver basis.

The default value is to declare the driver as not supporting thread safe
delete, which matches the current behaviour.

Drivers which are known to not support thread safe delete are:

- Hyper-V because of the aforementioned race condition

- Bare metal because ""delete"" of a node is really just a de-allocate, and it
may not be safe to re-use a node that is for example still imaging its disk
in another thread.

The patch also provides a more comprehensive fix for the change to bug 1326998
than that mergered in I3b7c5afce7f3bacce1b444c245b035334962ff7c by covering
the case where a domain disapears during cleanup operation, rather than just
checking if the domain exists.

Beacuse instances can now be deleted during a build the patch also avoids
rescheduling a build failure where the vm or task state is changed (delete in
progress) or the instance is deleted.

Change-Id: I8ba66a272cba9bd7ecbcd4d56eab1a28508610b1
Closes-Bug: #1276772
",git fetch https://review.opendev.org/openstack/nova refs/changes/67/88067/8 && git format-patch -1 --stdout FETCH_HEAD,"['nova/virt/hyperv/driver.py', 'nova/virt/libvirt/driver.py', 'nova/network/manager.py', 'nova/virt/driver.py', 'nova/compute/manager.py', 'nova/virt/baremetal/driver.py']",6,6c600684b66ea9db9b33b2f0186eff925fd359f7,bug/1326998," ""thread_safe_delete"": False,",,62,18
openstack%2Fnova~master~I51b51da1f01de803f681034dacfcebab45f8e1ac,openstack/nova,master,I51b51da1f01de803f681034dacfcebab45f8e1ac,VMware: Add docstrings for functions in vmops,ABANDONED,2014-09-30 08:12:50.000000000,2014-12-05 15:48:39.000000000,,"[{'_account_id': 3}, {'_account_id': 1653}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 8412}, {'_account_id': 9008}, {'_account_id': 9555}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-30 08:12:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/26ca1005c7f59992025a043b39693fd759db111b', 'message': ""VMware: Add docstrings for functions in vmops\n\nAlso add a couple of explanatory comments which don't live in\ndocstrings.\n\nChange-Id: I51b51da1f01de803f681034dacfcebab45f8e1ac\n""}, {'number': 2, 'created': '2014-09-30 13:41:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c7b74dbd9f159c9b6d23413c693a2d71cd6fcbb8', 'message': ""VMware: Add docstrings for functions in vmops\n\nAlso add a couple of explanatory comments which don't live in\ndocstrings.\n\nChange-Id: I51b51da1f01de803f681034dacfcebab45f8e1ac\n""}, {'number': 3, 'created': '2014-10-02 15:56:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/40733f7e72bfcd8a93e18f98688265681d26baf9', 'message': ""VMware: Add docstrings for functions in vmops\n\nAlso add a couple of explanatory comments which don't live in\ndocstrings.\n\nChange-Id: I51b51da1f01de803f681034dacfcebab45f8e1ac\n""}, {'number': 4, 'created': '2014-10-08 08:34:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/70347571573081ee908155a89ca7919e8cfec119', 'message': ""VMware: Add docstrings for functions in vmops\n\nAlso add a couple of explanatory comments which don't live in\ndocstrings.\n\nChange-Id: I51b51da1f01de803f681034dacfcebab45f8e1ac\n""}, {'number': 5, 'created': '2014-10-08 09:33:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/56b05f408e853924334246c6293c62cd9ab2fb18', 'message': ""VMware: Add docstrings for functions in vmops\n\nAlso add a couple of explanatory comments which don't live in\ndocstrings.\n\nChange-Id: I51b51da1f01de803f681034dacfcebab45f8e1ac\n""}, {'number': 6, 'created': '2014-10-15 15:08:11.000000000', 'files': ['nova/virt/vmwareapi/vmops.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/7d0fec83a58f12e56ff4c5a56d82d4ce18e78633', 'message': ""VMware: Add docstrings for functions in vmops\n\nAlso add a couple of explanatory comments which don't live in\ndocstrings.\n\nChange-Id: I51b51da1f01de803f681034dacfcebab45f8e1ac\n""}]",18,124972,7d0fec83a58f12e56ff4c5a56d82d4ce18e78633,45,10,6,9555,,,0,"VMware: Add docstrings for functions in vmops

Also add a couple of explanatory comments which don't live in
docstrings.

Change-Id: I51b51da1f01de803f681034dacfcebab45f8e1ac
",git fetch https://review.opendev.org/openstack/nova refs/changes/72/124972/3 && git format-patch -1 --stdout FETCH_HEAD,['nova/virt/vmwareapi/vmops.py'],1,26ca1005c7f59992025a043b39693fd759db111b,vmware_refactor_cleanup," """"""The DatastorePath of the root disk for the created instance."""""" """"""Retry a VIM call which failed with TaskInProgress. Some VIM calls will fail if there is already a long running task executing against a target object. These will fail immediately without creating a new Task object, so must be handled separately. This call traps TaskInProgress and retries to schedule the new Task at increasing intervals. """""" """"""Increase the size of a virtual disk."""""" """"""Delete a file on a datastore. Logs a message, but does not fail if the file cannot be deleted. """""" """"""Create and attach a config drive to an instance."""""" """"""Create a new virtual machine for an instance. Configures networking, but doesn't add any disks to the VM. """""" """"""Return the download location for a sparse image."""""" # For sparse image, we download the raw data, then copy to create the # VMDK afterwards """"""Return the download location for a flat image."""""" # For flat image we create the VMDK first and overwrite the -flat.vmdk # file """"""Return the download location for an iso image."""""" # We don't create a VMDK for an iso image """"""Move a downloaded disk from its temp directory to the cache."""""" """"""Convert and downloaded sparse image to a preallocated VMDK."""""" """"""Return the prepare and process methods for this image type."""""" """"""Get the DatastorePath of a cached copy of the image in vi. Cache the image first if required."""""" """"""Spawn a new instance. Arguments ultimately as passed directly to the driver."""""" """"""Return the path to a newly created config drive."""""" """"""Create a snapshot of an instance in vSphere."""""" """"""Delete a snapshot in vSphere."""""" """"""Delete a VM in vSphere."""""" """"""Pause an instance (not supported)."""""" """"""Unpause an instance (not supported)."""""" """"""Power on an instance."""""" """"""Create a VM machine id string."""""" """"""Return a Datastore browser for the given datastore. Returns a cached object if it's available.""""""", # Destroy a VM instance,51,1
openstack%2Fnova~master~If3ee816a4322204e198f46d0c36d3450ed7ad775,openstack/nova,master,If3ee816a4322204e198f46d0c36d3450ed7ad775,Fixing Libvirt's test_create_domain,ABANDONED,2014-08-26 14:55:17.000000000,2014-12-05 15:48:37.000000000,,"[{'_account_id': 3}, {'_account_id': 1779}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 5387}, {'_account_id': 8412}, {'_account_id': 8574}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10385}, {'_account_id': 10618}, {'_account_id': 11069}]","[{'number': 1, 'created': '2014-08-26 14:55:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a5fe222d8326c476daa9e782d50937f7ec386239', 'message': ""Fixing Libvirt's test_create_domain\n\nChange-Id: If3ee816a4322204e198f46d0c36d3450ed7ad775\n""}, {'number': 2, 'created': '2014-08-26 17:38:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9b9d83fb4827275d22ab8a4b7b4b77d8e5cbc778', 'message': ""Fixing Libvirt's test_create_domain\n\nChange-Id: If3ee816a4322204e198f46d0c36d3450ed7ad775\n""}, {'number': 3, 'created': '2014-08-27 12:41:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/4434fd915ee2ebc9e58210c490848a044db87c3e', 'message': ""Fixing Libvirt's test_create_domain\n\nChange-Id: If3ee816a4322204e198f46d0c36d3450ed7ad775\n""}, {'number': 4, 'created': '2014-09-30 21:03:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/6c7b988a1111fcdb4cf6a68ec4c78624ddc3ffc9', 'message': ""Fixing Libvirt's test_create_domain\n\nThe assertHasCalls call should have been assert_called_once_with\notherwise, it's not actually asserting anyhting. And, adding some\nassertions that functions that shouldn't have been called, were\nindeed not called.\n\nChange-Id: If3ee816a4322204e198f46d0c36d3450ed7ad775\n""}, {'number': 5, 'created': '2014-10-24 18:39:36.000000000', 'files': ['nova/tests/virt/libvirt/test_driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/a4a03db313c48549630d9b56d97cd79ddb050b73', 'message': ""Fixing Libvirt's test_create_domain\n\nThe assertHasCalls call should have been assert_called_once_with\notherwise, it's not actually asserting anything. And, adding some\nassertions that functions that shouldn't have been called, were\nindeed not called.\n\nChange-Id: If3ee816a4322204e198f46d0c36d3450ed7ad775\n""}]",9,116914,a4a03db313c48549630d9b56d97cd79ddb050b73,47,13,5,5387,,,0,"Fixing Libvirt's test_create_domain

The assertHasCalls call should have been assert_called_once_with
otherwise, it's not actually asserting anything. And, adding some
assertions that functions that shouldn't have been called, were
indeed not called.

Change-Id: If3ee816a4322204e198f46d0c36d3450ed7ad775
",git fetch https://review.opendev.org/openstack/nova refs/changes/14/116914/5 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/virt/libvirt/test_driver.py'],1,a5fe222d8326c476daa9e782d50937f7ec386239,fix_test_create_domain, def test_create_domain(self): mock_domain.createWithFlags.assert_called_once_with(0)," @mock.patch.object(fake_libvirt_utils, 'get_instance_path') def test_create_domain(self, mock_get_inst_path): mock_get_inst_path.return_value = '/tmp/' mock_get_inst_path.assertHasCalls([mock.call(mock_instance)]) mock_domain.createWithFlags.assertHasCalls([mock.call(0)])",2,5
openstack%2Fnova~master~I3d5ad11acdad0154c19ee09c1bb5dcb3bdb1f45d,openstack/nova,master,I3d5ad11acdad0154c19ee09c1bb5dcb3bdb1f45d,cells: adds CONF.cells.host_reserve_percent,ABANDONED,2014-10-02 12:07:43.000000000,2014-12-05 15:48:34.000000000,,"[{'_account_id': 3}, {'_account_id': 782}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 7166}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-10-02 12:07:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/8dc1af40051b00bd542de4ccd7945ef7b04cb58b', 'message': 'cells: adds CONF.cells.host_reserve_percent\n\nIts useful to keep some hosts free to allow for easier maintenance\nwithin a cell.\n\nThis only affects the amount of space the cell reports, the child cell\nscheduler is responsible for keeping those hosts empty, but using things\nsuch as fill first.\n\nThe reserve_percent can still be used to keep some free space that will\nallow for resizes.\n\nDocImpact\n\nChange-Id: I3d5ad11acdad0154c19ee09c1bb5dcb3bdb1f45d\n'}, {'number': 2, 'created': '2014-10-06 12:31:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1d939775bc4923b14e75af78ae102a081a600ae5', 'message': 'cells: adds CONF.cells.host_reserve_percent\n\nIts useful to keep some hosts free to allow for easier maintenance\nwithin a cell.\n\nThis only affects the amount of space the cell reports, the child cell\nscheduler is responsible for keeping those hosts empty, but using things\nsuch as fill first.\n\nThe reserve_percent can still be used to keep some free space that will\nallow for resizes.\n\nDocImpact\n\nChange-Id: I3d5ad11acdad0154c19ee09c1bb5dcb3bdb1f45d\n'}, {'number': 3, 'created': '2014-10-28 12:00:58.000000000', 'files': ['nova/cells/opts.py', 'nova/cells/state.py', 'nova/tests/cells/test_cells_state_manager.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/9e602981c52e8948f4705b80bc2539251e1c4bf9', 'message': 'cells: adds CONF.cells.host_reserve_percent\n\nIts useful to keep some hosts free to allow for easier maintenance\nwithin a cell.\n\nThis only affects the amount of space the cell reports, the child cell\nscheduler is responsible for keeping those hosts empty, but using things\nsuch as fill first.\n\nThe reserve_percent can still be used to keep some free space that will\nallow for resizes.\n\nDocImpact\n\nimplements blueprint cells-host-reserve-scheduling\n\nChange-Id: I3d5ad11acdad0154c19ee09c1bb5dcb3bdb1f45d\n'}]",1,125607,9e602981c52e8948f4705b80bc2539251e1c4bf9,30,9,3,782,,,0,"cells: adds CONF.cells.host_reserve_percent

Its useful to keep some hosts free to allow for easier maintenance
within a cell.

This only affects the amount of space the cell reports, the child cell
scheduler is responsible for keeping those hosts empty, but using things
such as fill first.

The reserve_percent can still be used to keep some free space that will
allow for resizes.

DocImpact

implements blueprint cells-host-reserve-scheduling

Change-Id: I3d5ad11acdad0154c19ee09c1bb5dcb3bdb1f45d
",git fetch https://review.opendev.org/openstack/nova refs/changes/07/125607/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/cells/opts.py', 'nova/cells/state.py']",2,8dc1af40051b00bd542de4ccd7945ef7b04cb58b,bp/cells-host-reserve-scheduling,"import operatorCONF.import_opt('host_reserve_percent', 'nova.cells.opts', group='cells') self._remove_reserved_hosts(compute_hosts) def _remove_reserved_hosts(self, compute_hosts): if CONF.cells.host_reserve_percent <= 0.0: return memory = [(host, stats['free_ram_mb']) for host, stats in compute_hosts.iteritems()] ordered = sorted(memory, key=operator.itemgetter(0)) ordered = sorted(ordered, key=operator.itemgetter(1), reverse=True) hosts_to_remove = int(len(ordered) / CONF.cells.host_reserve_percent) count = 0 while count < hosts_to_remove: host, free_mem = ordered[count] del compute_hosts[host] count += 1 LOG.debug(""Reserving host: %s"" % host) ",,25,0
openstack%2Fnova~master~Ia8f93d364cdedea88d239dde2513a92648cb36ed,openstack/nova,master,Ia8f93d364cdedea88d239dde2513a92648cb36ed,Copy instance type info into instance metadata,ABANDONED,2014-09-07 11:05:38.000000000,2014-12-05 15:48:33.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 4393}, {'_account_id': 5170}, {'_account_id': 5292}, {'_account_id': 7567}, {'_account_id': 9008}, {'_account_id': 9578}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-07 11:05:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e0d4f3822ceb49fd09bdd1c470c9c974959cac14', 'message': 'Do not delete flavor access when delete a flavor\n\nThis commit make flavor access mapping keep still after the flavor is\ndeleted, so that instances pointing to deleted private flavor will be\nable to retrieve their configuration.\n\nChange-Id: Ia8f93d364cdedea88d239dde2513a92648cb36ed\nCloses-Bug: #1366168\n'}, {'number': 2, 'created': '2014-10-28 20:57:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/361195cb2e2d1fba0919edbebf328228630c7f39', 'message': 'Copy instance type info into instance metadata\n\nThis commit copy instance type info into instance metadata while\nbooting and successful resizing, so that user can retrieve instance\ntype data even the flavor is deleted or not accessable.\n\nChange-Id: Ia8f93d364cdedea88d239dde2513a92648cb36ed\nCloses-Bug: #1366168\n'}, {'number': 3, 'created': '2014-10-28 22:30:33.000000000', 'files': ['nova/compute/manager.py', 'nova/tests/compute/test_compute.py', 'nova/compute/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/f010ba009f6656721aac2ab1a2b5ee010d392a73', 'message': 'Copy instance type info into instance metadata\n\nThis commit copy instance type info into instance metadata while\nbooting and successful resizing, so that user can retrieve instance\ntype data even the flavor is deleted or not accessable.\n\nChange-Id: Ia8f93d364cdedea88d239dde2513a92648cb36ed\nCloses-Bug: #1366168\n'}]",4,119608,f010ba009f6656721aac2ab1a2b5ee010d392a73,21,9,3,7567,,,0,"Copy instance type info into instance metadata

This commit copy instance type info into instance metadata while
booting and successful resizing, so that user can retrieve instance
type data even the flavor is deleted or not accessable.

Change-Id: Ia8f93d364cdedea88d239dde2513a92648cb36ed
Closes-Bug: #1366168
",git fetch https://review.opendev.org/openstack/nova refs/changes/08/119608/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/db/test_db_api.py', 'nova/db/sqlalchemy/api.py']",2,e0d4f3822ceb49fd09bdd1c470c9c974959cac14,bug/1366168,," model_query(context, models.InstanceTypeProjects, session=session, read_deleted=""no"").\ filter_by(instance_type_id=ref['id']).\ soft_delete()",29,4
openstack%2Fdevstack~master~I53d34aabc75ae99e6b750f2891ea04ff2c883e40,openstack/devstack,master,I53d34aabc75ae99e6b750f2891ea04ff2c883e40,use_unicode=0 for mysql connections,ABANDONED,2014-08-25 08:22:01.000000000,2014-12-05 15:47:47.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 5803}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-25 08:22:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/bed15e512bb142c770ead96c54bf4303cb48405a', 'message': 'use_unicode=0 for mysql connections\n\nAccording to sqlalchemy documentation and the\nuse_unicode=0 is recommended for the MYSQLdb driver.\nhttp://docs.sqlalchemy.org/en/rel_0_9/dialects/mysql.html#unicode\n\nChange-Id: I53d34aabc75ae99e6b750f2891ea04ff2c883e40\n'}, {'number': 2, 'created': '2014-08-25 08:54:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/28fe845c12b15685f92ae354c69e62ba5a3e6154', 'message': 'use_unicode=0 for mysql connections\n\nAccording to sqlalchemy documentation and the\nuse_unicode=0 is recommended for the MYSQLdb driver.\nhttp://docs.sqlalchemy.org/en/rel_0_9/dialects/mysql.html#unicode\n\nWith a simple table (1 integer id, 3 String(64) attribute) 15% improvement seen in full table queries.\n\nChange-Id: I53d34aabc75ae99e6b750f2891ea04ff2c883e40'}, {'number': 3, 'created': '2014-08-25 08:54:28.000000000', 'files': ['lib/databases/mysql'], 'web_link': 'https://opendev.org/openstack/devstack/commit/da63dd6bccb3455fd955afce6d7626cef686213b', 'message': 'use_unicode=0 for mysql connections\n\nAccording to sqlalchemy documentation and the\nuse_unicode=0 is recommended for the MYSQLdb driver.\nhttp://docs.sqlalchemy.org/en/rel_0_9/dialects/mysql.html#unicode\n\nWith a simple table (1 integer id, 3 String(64) attribute) 15%\n improvement seen in full table queries.\n\nChange-Id: I53d34aabc75ae99e6b750f2891ea04ff2c883e40'}]",0,116570,da63dd6bccb3455fd955afce6d7626cef686213b,13,4,3,5803,,,0,"use_unicode=0 for mysql connections

According to sqlalchemy documentation and the
use_unicode=0 is recommended for the MYSQLdb driver.
http://docs.sqlalchemy.org/en/rel_0_9/dialects/mysql.html#unicode

With a simple table (1 integer id, 3 String(64) attribute) 15%
 improvement seen in full table queries.

Change-Id: I53d34aabc75ae99e6b750f2891ea04ff2c883e40",git fetch https://review.opendev.org/openstack/devstack refs/changes/70/116570/1 && git format-patch -1 --stdout FETCH_HEAD,['lib/databases/mysql'],1,bed15e512bb142c770ead96c54bf4303cb48405a,use_unicode=0," echo ""$BASE_SQL_CONN/$db?charset=utf8&use_unicode=0"""," echo ""$BASE_SQL_CONN/$db?charset=utf8""",1,1
openstack%2Fopenstack-ansible~stable%2Fjuno~I33068df8b85962e35cfde2038e60af2b3ca6ca12,openstack/openstack-ansible,stable/juno,I33068df8b85962e35cfde2038e60af2b3ca6ca12,Mass merge of master into stable/juno,MERGED,2014-12-05 01:13:30.000000000,2014-12-05 15:38:01.000000000,2014-12-05 15:38:01.000000000,"[{'_account_id': 3}, {'_account_id': 6714}, {'_account_id': 6816}, {'_account_id': 7217}, {'_account_id': 7353}, {'_account_id': 9884}, {'_account_id': 12000}]","[{'number': 1, 'created': '2014-12-05 01:13:30.000000000', 'files': ['rpc_deployment/roles/utility_create_configs/templates/glance_registry_spec.rb.j2', 'rpc_deployment/roles/container_destroy/tasks/main.yml', 'rpc_deployment/inventory/dynamic_inventory.py', 'rpc_deployment/roles/swift_account/templates/account-server-replicator.conf.j2', 'rpc_deployment/vars/repo_packages/utility.yml', 'rpc_deployment/roles/swift_object/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/heat_engine_spec.rb.j2', 'rpc_deployment/playbooks/monitoring/raxmon_cli.yml', 'rpc_deployment/vars/repo_packages/tempest.yml', 'rpc_deployment/vars/openstack_service_vars/swift_account.yml', 'rpc_deployment/roles/swift_account/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/nova_conductor_spec.rb.j2', 'rpc_deployment/roles/openstack_common/tasks/install_git_source.yml', 'rpc_deployment/vars/repo_packages/raxmon_agent.yml', 'rpc_deployment/playbooks/monitoring/raxmon-all.yml', 'rpc_deployment/roles/swift_container/templates/container-server-replicator.conf.j2', 'rpc_deployment/roles/glance_common/templates/glance-api.conf', 'rpc_deployment/roles/raxmon_agent_install/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/keystone_spec.rb.j2', 'rpc_deployment/roles/utility_create_configs/templates/horizon_spec.rb.j2', 'rpc_deployment/roles/swift_ring_builder/tasks/main.yml', 'rpc_deployment/roles/horizon_apache/tasks/main.yml', 'rpc_deployment/roles/swift_common/templates/swift-rsyslog.conf.j2', 'rpc_deployment/roles/rabbit_join_cluster/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/nova_all_spec.rb.j2', 'scripts/f5-config.py', 'CONTRIBUTING.md', 'rpc_deployment/playbooks/monitoring/swift_maas.yml', 'rpc_deployment/roles/nfs_client/tasks/main.yml', 'etc/rpc_deploy/user_variables.yml', 'scripts/os-ansible-aio-check.sh', 'rpc_deployment/roles/utility_create_configs/templates/nova_scheduler_spec.rb.j2', 'rpc_deployment/roles/swift_dispersion_user/tasks/main.yml', 'rpc_deployment/roles/swift_object/templates/object-server-replicator.conf.j2', 'etc/rpc_deploy/rpc_user_config.yml.example', 'rpc_deployment/playbooks/setup/containers-setup.yml', 'rpc_deployment/roles/container_common/tasks/container_os_setup.yml', 'rpc_deployment/roles/keystone_common/templates/keystone.conf.j2', 'scripts/rpc-aio-heat-template.yml', 'rpc_deployment/playbooks/monitoring/maas_local.yml', 'rpc_deployment/roles/swift_storage_setup/tasks/swift_recon_cron.yml', 'rpc_deployment/roles/tempest/files/rpc_tempest_gate.sh', 'scripts/maas_remove.sh', 'rpc_deployment/vars/openstack_service_vars/swift_proxy_endpoint.yml', 'rpc_deployment/roles/raxmon_cli/tasks/main.yml', 'scripts/cloudserver-aio.sh', 'scripts/uklabsetup.sh', 'rpc_deployment/roles/utility_create_configs/templates/nova_api_ec2_spec.rb.j2', 'rpc_deployment/roles/utility_create_configs/templates/all_spec.rb.j2', 'rpc_deployment/roles/container_extra_setup/tasks/container_setup.yml', 'rpc_deployment/roles/utility_create_configs/templates/cinder_volume_spec.rb.j2', 'rpc_deployment/roles/swift_ring_distribute/tasks/main.yml', 'rpc_deployment/roles/neutron_common/tasks/main.yml', 'rpc_deployment/inventory/group_vars/swift_all.yml', 'rpc_deployment/vars/repo_packages/nova.yml', 'rpc_deployment/roles/utility_create_configs/templates/cinder_scheduler_spec.rb.j2', 'rpc_deployment/roles/swiftoperator_role_create/tasks/main.yml', 'rpc_deployment/playbooks/openstack/swift-proxy.yml', 'rpc_deployment/roles/utility_create_configs/templates/neutron_server_spec.rb.j2', 'rpc_deployment/roles/swift_object/templates/object-server.conf.j2', 'rpc_deployment/roles/nova_common/templates/nova.conf', 'scripts/rpc-wheel-builder.sh', 'scripts/rpc_maas_tool.py', 'rpc_deployment/playbooks/setup/archive-container.yml', 'rpc_deployment/vars/repo_packages/horizon.yml', 'rpc_deployment/roles/utility_create_configs/templates/neutron_all_spec.rb.j2', 'rpc_deployment/roles/tempest/templates/tempest.conf.j2', 'rpc_deployment/playbooks/openstack/swift-storage.yml', 'rpc_deployment/roles/galera_client_cnf/templates/client_my.cnf', 'rpc_deployment/vars/repo_packages/heat.yml', 'rpc_deployment/vars/openstack_service_vars/nova_spice_console_endpoint.yml', 'rpc_deployment/playbooks/setup/restart-containers.yml', 'rpc_deployment/roles/utility_create_configs/templates/nova_api_metadata_spec.rb.j2', 'rpc_deployment/roles/container_restart/tasks/container_restart.yml', 'rpc_deployment/roles/swift_storage_setup/tasks/main.yml', 'rpc_deployment/roles/cinder_common/templates/cinder.conf', 'rpc_deployment/roles/swift_account/templates/account-server.conf.j2', 'rpc_deployment/roles/utility_create_configs/templates/neutron_metadata_agent_spec.rb.j2', 'rpc_deployment/playbooks/openstack/swift-all.yml', 'rpc_deployment/roles/utility_create_configs/templates/heat_api_cfn_spec.rb.j2', 'rpc_deployment/vars/repo_packages/cinder.yml', 'rpc_deployment/roles/nfs_client/templates/nfs_shares.j2', 'rpc_deployment/inventory/group_vars/galera.yml', 'rpc_deployment/roles/swift_common/templates/swift-dispersion.conf.j2', 'rpc_deployment/roles/utility_create_configs/templates/nova_compute_spec.rb.j2', 'rpc_deployment/vars/openstack_service_vars/swift_container.yml', 'rpc_deployment/playbooks/openstack/swift-account.yml', 'rpc_deployment/vars/repo_packages/swift.yml', 'rpc_deployment/playbooks/setup/build-containers.yml', 'rpc_deployment/roles/container_clone/tasks/main.yml', 'rpc_deployment/roles/keystone_add_service/tasks/main.yml', 'rpc_deployment/roles/tempest_resources/tasks/main.yml', '.gitreview', 'rpc_deployment/playbooks/openstack/utility-serverspec.yml', 'etc/rpc_deploy/rpc_user_config.yml', 'rpc_deployment/roles/galera_config/templates/my.cnf', 'rpc_deployment/roles/host_common/tasks/authorized_keys.yml', 'rpc_deployment/roles/swift_common/handlers/main.yml', 'rpc_deployment/library/neutron', 'rpc_deployment/roles/neutron_add_network_interfaces/tasks/main.yml', 'rpc_deployment/roles/utility_keystone_checks/tasks/main.yml', 'rpc_deployment/playbooks/openstack/swift-container.yml', 'rpc_deployment/roles/utility_create_configs/tasks/main.yml', 'rpc_deployment/roles/swift_ring_builder/tasks/check_ring.yml', 'rpc_deployment/roles/swift_storage_setup/handlers/main.yml', 'rpc_deployment/roles/rabbit_user/tasks/main.yml', 'rpc_deployment/roles/galera_config/templates/mysql_init.sh', 'rpc_deployment/roles/swift_init_scripts/tasks/main.yml', 'scripts/rpc-aio-rax-heat-template.yml', 'rpc_deployment/playbooks/openstack/swift-object.yml', 'rpc_deployment/playbooks/openstack/utility-all.yml', 'rpc_deployment/roles/neutron_common/templates/dnsmasq-neutron.conf', 'rpc_deployment/roles/glance_common/templates/glance-registry.conf', 'rpc_deployment/playbooks/openstack/cinder-volume.yml', 'rpc_deployment/roles/swift_storage_setup/tasks/rsync_setup.yml', 'rpc_deployment/vars/repo_packages/keystone.yml', 'rpc_deployment/roles/keystone_add_user/tasks/main.yml', 'rpc_deployment/roles/container_create/tasks/container_create.yml', 'rpc_deployment/roles/heat_common/templates/heat.conf', 'rpc_deployment/roles/swift_storage_setup/templates/rsyncd.conf.j2', 'rpc_deployment/vars/config_vars/haproxy_config.yml', 'rpc_deployment/roles/logstash/tasks/main.yml', 'rpc_deployment/roles/swift_ring_builder/templates/ring.contents.j2', 'rpc_deployment/roles/utility_create_configs/templates/keystone_all_spec.rb.j2', 'rpc_deployment/roles/swift_common/tasks/log_setup.yml', 'rpc_deployment/roles/galera_config/templates/mysql_defaults', 'rpc_deployment/roles/swift_init_scripts/handlers/main.yml', 'rpc_deployment/roles/swift_proxy/tasks/main.yml', 'rpc_deployment/roles/rabbit_common/tasks/restart_rabbit.yml', 'rpc_deployment/roles/swift_common/tasks/main.yml', 'rpc_deployment/roles/neutron_setup/tasks/main.yml', 'rpc_deployment/playbooks/setup/deploy-archived-container.yml', 'rpc_deployment/roles/rpc_support_common/tasks/motd.yml', 'rpc_deployment/roles/lxc_common/files/lxc-openstack', 'rpc_deployment/inventory/group_vars/all.yml', 'rpc_deployment/roles/utility_create_configs/templates/heat_api_cloudwatch_spec.rb.j2', 'rpc_deployment/roles/container_common/tasks/container_user_create.yml', 'rpc_deployment/roles/openstack_common/tasks/main.yml', 'rpc_deployment/vars/openstack_service_vars/swift_object.yml', 'rpc_deployment/vars/repo_packages/raxmon_cli.yml', 'rpc_deployment/roles/galera_setup/tasks/main.yml', 'rpc_deployment/vars/repo_packages/glance.yml', 'rpc_deployment/playbooks/openstack/swift-common.yml', 'rpc_deployment/roles/utility_create_configs/templates/glance_api_spec.rb.j2', 'rpc_deployment/roles/container_create/tasks/main.yml', 'rpc_deployment/vars/repo_packages/os-ansible-deployment.yml', 'rpc_deployment/roles/galera_config/tasks/main.yml', 'rpc_deployment/playbooks/openstack/swift-build-rings.yml', 'rpc_deployment/roles/swift_init_scripts/templates/init-config', 'rpc_deployment/roles/container_common/tasks/add_interfaces.yml', 'rpc_deployment/roles/swift_proxy/templates/proxy-server.conf.j2', 'rpc_deployment/roles/swift_ring_builder/templates/swift_rings.py', 'scripts/pw-token-gen.py', 'rpc_deployment/roles/utility_create_configs/templates/nova_api_os_compute_spec.rb.j2', 'rpc_deployment/roles/container_archive/tasks/swift_upload.yml', 'rpc_deployment/roles/neutron_common/templates/neutron.conf', 'rpc_deployment/roles/utility_create_configs/templates/cinder_all_spec.rb.j2', 'rpc_deployment/inventory/group_vars/neutron_all.yml', 'rpc_deployment/roles/swift_common/templates/swift.conf.j2', 'rpc_deployment/roles/utility_create_configs/templates/neutron_dhcp_agent_spec.rb.j2', 'rpc_deployment/roles/utility_create_configs/templates/glance_all_spec.rb.j2', 'rpc_deployment/roles/container_setup/tasks/container_setup.yml', 'rpc_deployment/roles/container_extra_setup/tasks/sysctl.yml', 'etc/rpc_deploy/conf.d/swift.yml', 'rpc_deployment/roles/rsyslog_stop/tasks/main.yml', 'rpc_deployment/roles/cinder_backend_types/tasks/main.yml', 'rpc_deployment/roles/galera_config/templates/debian.cnf', 'rpc_deployment/roles/swift_ring_md5sum/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/cinder_api_spec.rb.j2', 'rpc_deployment/library/memcached', 'rpc_deployment/roles/nova_libvirt/tasks/main.yml', 'rpc_deployment/roles/utility_common/tasks/main.yml', 'rpc_deployment/handlers/swift_services.yml', 'rpc_deployment/roles/swift_common/tasks/swift_dispersion.yml', 'scripts/f5-monitor.sh', 'rpc_deployment/roles/swift_container/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/heat_all_spec.rb.j2', 'rpc_deployment/roles/utility_keystone_checks/templates/keystone_checks_spec.rb.j2', 'rpc_deployment/roles/utility_create_configs/templates/heat_api_spec.rb.j2', 'rpc_deployment/roles/swift_container/templates/container-server.conf.j2', 'etc/rpc_deploy/rpc_environment.yml', 'rpc_deployment/vars/repo_packages/neutron.yml', 'rpc_deployment/playbooks/monitoring/raxmon_agent.yml', 'rpc_deployment/vars/openstack_service_vars/swift_proxy.yml', 'rpc_deployment/inventory/group_vars/hosts.yml', 'rpc_deployment/inventory/group_vars/nova_all.yml', 'rpc_deployment/roles/kibana/tasks/main.yml', 'rpc_deployment/roles/horizon_common/templates/local_settings.py', 'rpc_deployment/roles/container_setup/tasks/main.yml', 'rpc_deployment/roles/container_restart/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/neutron_linuxbridge_agent_spec.rb.j2'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/30d08febe51314cec705fbd4b94fb85d627d696f', 'message': 'Mass merge of master into stable/juno\n\nThis merges all of master into stable/juno for use in the 10.1.0\nrelease cycle. This merge was taken directly out of master and\napplied as a patch. Due to permission issues and general merge\nconflicts this was the easiest and most reliable way to\naccomplish the mass merge.\n\nChange-Id: I33068df8b85962e35cfde2038e60af2b3ca6ca12\n'}]",0,139295,30d08febe51314cec705fbd4b94fb85d627d696f,11,7,1,7353,,,0,"Mass merge of master into stable/juno

This merges all of master into stable/juno for use in the 10.1.0
release cycle. This merge was taken directly out of master and
applied as a patch. Due to permission issues and general merge
conflicts this was the easiest and most reliable way to
accomplish the mass merge.

Change-Id: I33068df8b85962e35cfde2038e60af2b3ca6ca12
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/95/139295/1 && git format-patch -1 --stdout FETCH_HEAD,"['rpc_deployment/roles/utility_create_configs/templates/glance_registry_spec.rb.j2', 'rpc_deployment/roles/container_destroy/tasks/main.yml', 'rpc_deployment/inventory/dynamic_inventory.py', 'rpc_deployment/roles/swift_account/templates/account-server-replicator.conf.j2', 'rpc_deployment/vars/repo_packages/utility.yml', 'rpc_deployment/roles/swift_object/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/heat_engine_spec.rb.j2', 'rpc_deployment/playbooks/monitoring/raxmon_cli.yml', 'rpc_deployment/vars/repo_packages/tempest.yml', 'rpc_deployment/vars/openstack_service_vars/swift_account.yml', 'rpc_deployment/roles/swift_account/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/nova_conductor_spec.rb.j2', 'rpc_deployment/roles/openstack_common/tasks/install_git_source.yml', 'rpc_deployment/vars/repo_packages/raxmon_agent.yml', 'rpc_deployment/playbooks/monitoring/raxmon-all.yml', 'rpc_deployment/roles/swift_container/templates/container-server-replicator.conf.j2', 'rpc_deployment/roles/glance_common/templates/glance-api.conf', 'rpc_deployment/roles/raxmon_agent_install/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/keystone_spec.rb.j2', 'rpc_deployment/roles/utility_create_configs/templates/horizon_spec.rb.j2', 'rpc_deployment/roles/swift_ring_builder/tasks/main.yml', 'rpc_deployment/roles/horizon_apache/tasks/main.yml', 'rpc_deployment/roles/swift_common/templates/swift-rsyslog.conf.j2', 'rpc_deployment/roles/rabbit_join_cluster/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/nova_all_spec.rb.j2', 'scripts/f5-config.py', 'CONTRIBUTING.md', 'rpc_deployment/playbooks/monitoring/swift_maas.yml', 'rpc_deployment/roles/nfs_client/tasks/main.yml', 'etc/rpc_deploy/user_variables.yml', 'scripts/os-ansible-aio-check.sh', 'rpc_deployment/roles/utility_create_configs/templates/nova_scheduler_spec.rb.j2', 'rpc_deployment/roles/swift_dispersion_user/tasks/main.yml', 'rpc_deployment/roles/swift_object/templates/object-server-replicator.conf.j2', 'etc/rpc_deploy/rpc_user_config.yml.example', 'rpc_deployment/playbooks/setup/containers-setup.yml', 'rpc_deployment/roles/container_common/tasks/container_os_setup.yml', 'rpc_deployment/roles/keystone_common/templates/keystone.conf.j2', 'scripts/rpc-aio-heat-template.yml', 'rpc_deployment/playbooks/monitoring/maas_local.yml', 'rpc_deployment/roles/swift_storage_setup/tasks/swift_recon_cron.yml', 'rpc_deployment/roles/tempest/files/rpc_tempest_gate.sh', 'scripts/maas_remove.sh', 'rpc_deployment/vars/openstack_service_vars/swift_proxy_endpoint.yml', 'rpc_deployment/roles/raxmon_cli/tasks/main.yml', 'scripts/cloudserver-aio.sh', 'scripts/uklabsetup.sh', 'rpc_deployment/roles/utility_create_configs/templates/nova_api_ec2_spec.rb.j2', 'rpc_deployment/roles/utility_create_configs/templates/all_spec.rb.j2', 'rpc_deployment/roles/container_extra_setup/tasks/container_setup.yml', 'rpc_deployment/roles/utility_create_configs/templates/cinder_volume_spec.rb.j2', 'rpc_deployment/roles/swift_ring_distribute/tasks/main.yml', 'rpc_deployment/roles/neutron_common/tasks/main.yml', 'rpc_deployment/inventory/group_vars/swift_all.yml', 'rpc_deployment/vars/repo_packages/nova.yml', 'rpc_deployment/roles/utility_create_configs/templates/cinder_scheduler_spec.rb.j2', 'rpc_deployment/roles/swiftoperator_role_create/tasks/main.yml', 'rpc_deployment/playbooks/openstack/swift-proxy.yml', 'rpc_deployment/roles/utility_create_configs/templates/neutron_server_spec.rb.j2', 'rpc_deployment/roles/swift_object/templates/object-server.conf.j2', 'rpc_deployment/roles/nova_common/templates/nova.conf', 'scripts/rpc-wheel-builder.sh', 'scripts/rpc_maas_tool.py', 'rpc_deployment/playbooks/setup/archive-container.yml', 'rpc_deployment/vars/repo_packages/horizon.yml', 'rpc_deployment/roles/utility_create_configs/templates/neutron_all_spec.rb.j2', 'rpc_deployment/roles/tempest/templates/tempest.conf.j2', 'rpc_deployment/playbooks/openstack/swift-storage.yml', 'rpc_deployment/roles/galera_client_cnf/templates/client_my.cnf', 'rpc_deployment/vars/repo_packages/heat.yml', 'rpc_deployment/vars/openstack_service_vars/nova_spice_console_endpoint.yml', 'rpc_deployment/playbooks/setup/restart-containers.yml', 'rpc_deployment/roles/utility_create_configs/templates/nova_api_metadata_spec.rb.j2', 'rpc_deployment/roles/container_restart/tasks/container_restart.yml', 'rpc_deployment/roles/swift_storage_setup/tasks/main.yml', 'rpc_deployment/roles/cinder_common/templates/cinder.conf', 'rpc_deployment/roles/swift_account/templates/account-server.conf.j2', 'rpc_deployment/roles/utility_create_configs/templates/neutron_metadata_agent_spec.rb.j2', 'rpc_deployment/playbooks/openstack/swift-all.yml', 'rpc_deployment/roles/utility_create_configs/templates/heat_api_cfn_spec.rb.j2', 'rpc_deployment/vars/repo_packages/cinder.yml', 'rpc_deployment/roles/nfs_client/templates/nfs_shares.j2', 'rpc_deployment/inventory/group_vars/galera.yml', 'rpc_deployment/roles/swift_common/templates/swift-dispersion.conf.j2', 'rpc_deployment/roles/utility_create_configs/templates/nova_compute_spec.rb.j2', 'rpc_deployment/vars/openstack_service_vars/swift_container.yml', 'rpc_deployment/playbooks/openstack/swift-account.yml', 'rpc_deployment/vars/repo_packages/swift.yml', 'rpc_deployment/playbooks/setup/build-containers.yml', 'rpc_deployment/roles/container_clone/tasks/main.yml', 'rpc_deployment/roles/keystone_add_service/tasks/main.yml', 'rpc_deployment/roles/tempest_resources/tasks/main.yml', '.gitreview', 'rpc_deployment/playbooks/openstack/utility-serverspec.yml', 'etc/rpc_deploy/rpc_user_config.yml', 'rpc_deployment/roles/galera_config/templates/my.cnf', 'rpc_deployment/roles/host_common/tasks/authorized_keys.yml', 'rpc_deployment/roles/swift_common/handlers/main.yml', 'rpc_deployment/library/neutron', 'rpc_deployment/roles/neutron_add_network_interfaces/tasks/main.yml', 'rpc_deployment/roles/utility_keystone_checks/tasks/main.yml', 'rpc_deployment/playbooks/openstack/swift-container.yml', 'rpc_deployment/roles/utility_create_configs/tasks/main.yml', 'rpc_deployment/roles/swift_ring_builder/tasks/check_ring.yml', 'rpc_deployment/roles/swift_storage_setup/handlers/main.yml', 'rpc_deployment/roles/rabbit_user/tasks/main.yml', 'rpc_deployment/roles/galera_config/templates/mysql_init.sh', 'rpc_deployment/roles/swift_init_scripts/tasks/main.yml', 'scripts/rpc-aio-rax-heat-template.yml', 'rpc_deployment/playbooks/openstack/swift-object.yml', 'rpc_deployment/playbooks/openstack/utility-all.yml', 'rpc_deployment/roles/neutron_common/templates/dnsmasq-neutron.conf', 'rpc_deployment/roles/glance_common/templates/glance-registry.conf', 'rpc_deployment/playbooks/openstack/cinder-volume.yml', 'rpc_deployment/roles/swift_storage_setup/tasks/rsync_setup.yml', 'rpc_deployment/vars/repo_packages/keystone.yml', 'rpc_deployment/roles/keystone_add_user/tasks/main.yml', 'rpc_deployment/roles/container_create/tasks/container_create.yml', 'rpc_deployment/roles/heat_common/templates/heat.conf', 'rpc_deployment/roles/swift_storage_setup/templates/rsyncd.conf.j2', 'rpc_deployment/vars/config_vars/haproxy_config.yml', 'rpc_deployment/roles/logstash/tasks/main.yml', 'rpc_deployment/roles/swift_ring_builder/templates/ring.contents.j2', 'rpc_deployment/roles/utility_create_configs/templates/keystone_all_spec.rb.j2', 'rpc_deployment/roles/swift_common/tasks/log_setup.yml', 'rpc_deployment/roles/galera_config/templates/mysql_defaults', 'rpc_deployment/roles/swift_init_scripts/handlers/main.yml', 'rpc_deployment/roles/swift_proxy/tasks/main.yml', 'rpc_deployment/roles/rabbit_common/tasks/restart_rabbit.yml', 'rpc_deployment/roles/swift_common/tasks/main.yml', 'rpc_deployment/roles/neutron_setup/tasks/main.yml', 'rpc_deployment/playbooks/setup/deploy-archived-container.yml', 'rpc_deployment/roles/rpc_support_common/tasks/motd.yml', 'rpc_deployment/roles/lxc_common/files/lxc-openstack', 'rpc_deployment/inventory/group_vars/all.yml', 'rpc_deployment/roles/utility_create_configs/templates/heat_api_cloudwatch_spec.rb.j2', 'rpc_deployment/roles/container_common/tasks/container_user_create.yml', 'rpc_deployment/roles/openstack_common/tasks/main.yml', 'rpc_deployment/vars/openstack_service_vars/swift_object.yml', 'rpc_deployment/vars/repo_packages/raxmon_cli.yml', 'rpc_deployment/roles/galera_setup/tasks/main.yml', 'rpc_deployment/vars/repo_packages/glance.yml', 'rpc_deployment/playbooks/openstack/swift-common.yml', 'rpc_deployment/roles/utility_create_configs/templates/glance_api_spec.rb.j2', 'rpc_deployment/roles/container_create/tasks/main.yml', 'rpc_deployment/vars/repo_packages/os-ansible-deployment.yml', 'rpc_deployment/roles/galera_config/tasks/main.yml', 'rpc_deployment/playbooks/openstack/swift-build-rings.yml', 'rpc_deployment/roles/swift_init_scripts/templates/init-config', 'rpc_deployment/roles/container_common/tasks/add_interfaces.yml', 'rpc_deployment/roles/swift_proxy/templates/proxy-server.conf.j2', 'rpc_deployment/roles/swift_ring_builder/templates/swift_rings.py', 'scripts/pw-token-gen.py', 'rpc_deployment/roles/utility_create_configs/templates/nova_api_os_compute_spec.rb.j2', 'rpc_deployment/roles/container_archive/tasks/swift_upload.yml', 'rpc_deployment/roles/neutron_common/templates/neutron.conf', 'rpc_deployment/roles/utility_create_configs/templates/cinder_all_spec.rb.j2', 'rpc_deployment/inventory/group_vars/neutron_all.yml', 'rpc_deployment/roles/swift_common/templates/swift.conf.j2', 'rpc_deployment/roles/utility_create_configs/templates/neutron_dhcp_agent_spec.rb.j2', 'rpc_deployment/roles/utility_create_configs/templates/glance_all_spec.rb.j2', 'rpc_deployment/roles/container_setup/tasks/container_setup.yml', 'rpc_deployment/roles/container_extra_setup/tasks/sysctl.yml', 'etc/rpc_deploy/conf.d/swift.yml', 'rpc_deployment/roles/rsyslog_stop/tasks/main.yml', 'rpc_deployment/roles/cinder_backend_types/tasks/main.yml', 'rpc_deployment/roles/galera_config/templates/debian.cnf', 'rpc_deployment/roles/swift_ring_md5sum/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/cinder_api_spec.rb.j2', 'rpc_deployment/library/memcached', 'rpc_deployment/roles/nova_libvirt/tasks/main.yml', 'rpc_deployment/roles/utility_common/tasks/main.yml', 'rpc_deployment/handlers/swift_services.yml', 'rpc_deployment/roles/swift_common/tasks/swift_dispersion.yml', 'scripts/f5-monitor.sh', 'rpc_deployment/roles/swift_container/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/heat_all_spec.rb.j2', 'rpc_deployment/roles/utility_keystone_checks/templates/keystone_checks_spec.rb.j2', 'rpc_deployment/roles/utility_create_configs/templates/heat_api_spec.rb.j2', 'rpc_deployment/roles/swift_container/templates/container-server.conf.j2', 'etc/rpc_deploy/rpc_environment.yml', 'rpc_deployment/vars/repo_packages/neutron.yml', 'rpc_deployment/playbooks/monitoring/raxmon_agent.yml', 'rpc_deployment/vars/openstack_service_vars/swift_proxy.yml', 'rpc_deployment/inventory/group_vars/hosts.yml', 'rpc_deployment/inventory/group_vars/nova_all.yml', 'rpc_deployment/roles/kibana/tasks/main.yml', 'rpc_deployment/roles/horizon_common/templates/local_settings.py', 'rpc_deployment/roles/container_setup/tasks/main.yml', 'rpc_deployment/roles/container_restart/tasks/main.yml', 'rpc_deployment/roles/utility_create_configs/templates/neutron_linuxbridge_agent_spec.rb.j2']",191,30d08febe51314cec705fbd4b94fb85d627d696f,stable/juno-mass-merge,,require 'spec_helper' describe service('neutron-linuxbridge-agent') do it { should be_enabled } it { should be_running } end describe process('neutron-linuxbridge-agent') do it { should be_running } end ,4936,1559
openstack%2Fceilometer~master~I2a0df1190f04e14c03cb2bc9c1a8fe7a669821dc,openstack/ceilometer,master,I2a0df1190f04e14c03cb2bc9c1a8fe7a669821dc,Updated from global requirements,MERGED,2014-12-04 22:35:30.000000000,2014-12-05 15:37:40.000000000,2014-12-05 15:37:39.000000000,"[{'_account_id': 3}, {'_account_id': 4491}, {'_account_id': 6537}, {'_account_id': 7729}]","[{'number': 1, 'created': '2014-12-04 22:35:30.000000000', 'files': ['requirements.txt', 'requirements-py3.txt'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/3dba1b1bda683d9aeecdf877712300997c0b0c1b', 'message': 'Updated from global requirements\n\nChange-Id: I2a0df1190f04e14c03cb2bc9c1a8fe7a669821dc\n'}]",0,139225,3dba1b1bda683d9aeecdf877712300997c0b0c1b,8,4,1,11131,,,0,"Updated from global requirements

Change-Id: I2a0df1190f04e14c03cb2bc9c1a8fe7a669821dc
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/25/139225/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'requirements-py3.txt']",2,3dba1b1bda683d9aeecdf877712300997c0b0c1b,openstack/requirements,"oslo.messaging>=1.4.0,!=1.5.0",oslo.messaging>=1.4.0,2,2
openstack%2Foslo.messaging~master~I1ba19377ed279ccb9f8d698b15678c528cbeade3,openstack/oslo.messaging,master,I1ba19377ed279ccb9f8d698b15678c528cbeade3,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:43:50.000000000,2014-12-05 15:35:24.000000000,2014-12-05 15:35:22.000000000,"[{'_account_id': 3}, {'_account_id': 2472}, {'_account_id': 2813}]","[{'number': 1, 'created': '2014-12-05 03:43:50.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/oslo.messaging/commit/737afde2e136de28f6e96ba99ecd5ec9ddcd7595', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I1ba19377ed279ccb9f8d698b15678c528cbeade3\n'}]",0,139358,737afde2e136de28f6e96ba99ecd5ec9ddcd7595,11,3,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I1ba19377ed279ccb9f8d698b15678c528cbeade3
",git fetch https://review.opendev.org/openstack/oslo.messaging refs/changes/58/139358/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,737afde2e136de28f6e96ba99ecd5ec9ddcd7595,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Fopenstack-manuals~master~Ifaf92418552396c992764cb55696da30dd654853,openstack/openstack-manuals,master,Ifaf92418552396c992764cb55696da30dd654853,Linked Neutron overview content in the Installation Guide,MERGED,2014-12-05 06:31:30.000000000,2014-12-05 15:34:13.000000000,2014-12-05 15:34:12.000000000,"[{'_account_id': 3}, {'_account_id': 167}, {'_account_id': 6547}, {'_account_id': 6772}, {'_account_id': 6843}, {'_account_id': 9515}]","[{'number': 1, 'created': '2014-12-05 06:31:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/52e2ec5684e3256dce9dbaa7c96e9c0ddad50b41', 'message': ""Linked Neutron overview content in the Installation Guide\n\n'xi:include parse' the  neutron overview content in the OpenStack Networking section\n\nbackport: Juno\nCloses_Bug: #1340524\n\nChange-Id: Ifaf92418552396c992764cb55696da30dd654853\n""}, {'number': 2, 'created': '2014-12-05 13:05:10.000000000', 'files': ['doc/install-guide/ch_networking.xml'], 'web_link': 'https://opendev.org/openstack/openstack-manuals/commit/25c96dd7e552de26b80343066e5cc32a9f5fce50', 'message': ""Linked Neutron overview content in the Installation Guide\n\n'xi:include parse' the  neutron overview content in the OpenStack Networking section\n\nbackport: Juno\nCloses-Bug: #1340524\n\nChange-Id: Ifaf92418552396c992764cb55696da30dd654853\n""}]",0,139538,25c96dd7e552de26b80343066e5cc32a9f5fce50,11,6,2,10705,,,0,"Linked Neutron overview content in the Installation Guide

'xi:include parse' the  neutron overview content in the OpenStack Networking section

backport: Juno
Closes-Bug: #1340524

Change-Id: Ifaf92418552396c992764cb55696da30dd654853
",git fetch https://review.opendev.org/openstack/openstack-manuals refs/changes/38/139538/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/install-guide/ch_networking.xml'],1,52e2ec5684e3256dce9dbaa7c96e9c0ddad50b41,neutron_installguide/darren," <xi:include parse=""xml"" href=""../common/section_getstart_networking.xml""/>",,1,0
openstack%2Fnova~master~I98313b600977f32edd4ec89486b1770ae54ac179,openstack/nova,master,I98313b600977f32edd4ec89486b1770ae54ac179,Fix incorrectly formatted log message,MERGED,2014-12-05 08:43:50.000000000,2014-12-05 15:22:11.000000000,2014-12-05 15:22:08.000000000,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 2750}, {'_account_id': 5170}, {'_account_id': 8688}, {'_account_id': 9420}, {'_account_id': 9578}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-12-05 08:43:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/be29dcfdd452e4d2b93b3756fa675b73a9eace61', 'message': 'Fix typo in locale string\n\nWhilst troubleshooting a nova migration issue, I kept seeing this\nstring being logged.  This change fixes the typo, and also attempts to\ncorrect the French translation of the message.  Please note that I do\nnot speak French, so this translation may need adjusting.\n\nChange-Id: I98313b600977f32edd4ec89486b1770ae54ac179\n'}, {'number': 2, 'created': '2014-12-05 12:39:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f216899d100c60f1f5d4bc6c65475affca3ddd38', 'message': 'Fix typo in locale string\n\nWhilst troubleshooting a nova migration issue, I kept seeing this\nstring being logged.  This minor change fixes the log string in\nnova/virt/disk/vfs/api.py to address this.\n\nChange-Id: I98313b600977f32edd4ec89486b1770ae54ac179\n'}, {'number': 3, 'created': '2014-12-05 12:41:44.000000000', 'files': ['nova/virt/disk/vfs/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/42b5bd686bfa9ba453b2d46c01ad1bf7f4f71fa2', 'message': 'Fix incorrectly formatted log message\n\nWhilst troubleshooting a nova migration issue, I kept seeing this\nstring being logged.  This minor change fixes the log string in\nnova/virt/disk/vfs/api.py to address the lack of comma and spacing.\n\nChange-Id: I98313b600977f32edd4ec89486b1770ae54ac179\n'}]",0,139572,42b5bd686bfa9ba453b2d46c01ad1bf7f4f71fa2,23,9,3,7307,,,0,"Fix incorrectly formatted log message

Whilst troubleshooting a nova migration issue, I kept seeing this
string being logged.  This minor change fixes the log string in
nova/virt/disk/vfs/api.py to address the lack of comma and spacing.

Change-Id: I98313b600977f32edd4ec89486b1770ae54ac179
",git fetch https://review.opendev.org/openstack/nova refs/changes/72/139572/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/locale/en_GB/LC_MESSAGES/nova-log-info.po', 'nova/locale/es/LC_MESSAGES/nova-log-info.po', 'nova/locale/ja/LC_MESSAGES/nova-log-info.po', 'nova/locale/zh_TW/LC_MESSAGES/nova-log-info.po', 'nova/locale/zh_CN/LC_MESSAGES/nova-log-info.po', 'nova/locale/fr/LC_MESSAGES/nova-log-info.po', 'nova/locale/ko_KR/LC_MESSAGES/nova-log-info.po', 'nova/locale/pt_BR/LC_MESSAGES/nova-log-info.po', 'nova/locale/it/LC_MESSAGES/nova-log-info.po', 'nova/locale/nova-log-info.pot']",10,be29dcfdd452e4d2b93b3756fa675b73a9eace61,fix_nova_log_locale_bugs,"msgid ""Unable to import guestfs, falling back to VFSLocalFS""","msgid ""Unable to import guestfsfalling back to VFSLocalFS""",11,11
openstack%2Fneutron-specs~master~I2252ea358bfab32ec19087a93995d7810af7c72e,openstack/neutron-specs,master,I2252ea358bfab32ec19087a93995d7810af7c72e,Layer 3 Service Plugin for Cisco Nexus Switches,ABANDONED,2014-05-27 21:09:12.000000000,2014-12-05 15:22:04.000000000,,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 107}, {'_account_id': 162}, {'_account_id': 261}, {'_account_id': 490}, {'_account_id': 704}, {'_account_id': 1689}, {'_account_id': 2592}, {'_account_id': 3217}, {'_account_id': 6524}, {'_account_id': 6558}, {'_account_id': 6659}, {'_account_id': 6694}, {'_account_id': 8645}]","[{'number': 1, 'created': '2014-05-27 21:09:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/5a13556f2e198f1c5a094f2ac3a02cb9c385d15e', 'message': 'Layer 3 Service Plugin for Cisco Nexus Switches\n\nThis spec describes the work being done by the blueprint\nhttps://blueprints.launchpad.net/neutron/+spec/ml2-cisco-nexus-mechdriver-svi\nwhich implements Layer 3 features for the Cisco 3K, 5K and 7K Nexus switches.\n\nChange-Id: I2252ea358bfab32ec19087a93995d7810af7c72e\n'}, {'number': 2, 'created': '2014-06-04 21:20:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/4acc5a23e7ee8936eddc76147c5bb09e6a60bd88', 'message': 'Layer 3 Service Plugin for Cisco Nexus Switches\n\nThis spec describes the work being done by the blueprint\nhttps://blueprints.launchpad.net/neutron/+spec/ml2-cisco-nexus-mechdriver-svi\nwhich implements Layer 3 features for the Cisco 3K, 5K and 7K Nexus switches.\n\nChange-Id: I2252ea358bfab32ec19087a93995d7810af7c72e\n'}, {'number': 3, 'created': '2014-06-19 17:02:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/8c483436ef8965ec170997a358e9ffdde00d36c9', 'message': 'Layer 3 Service Plugin for Cisco Nexus Switches\n\nThis spec describes the work being done by the blueprint\nhttps://blueprints.launchpad.net/neutron/+spec/ml2-cisco-nexus-mechdriver-svi\nwhich implements Layer 3 features for the Cisco 3K, 5K and 7K Nexus switches.\n\nChange-Id: I2252ea358bfab32ec19087a93995d7810af7c72e\n'}, {'number': 4, 'created': '2014-06-19 17:38:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/e199c82fed7439c2e6c2caec99836c3e1a186723', 'message': 'Layer 3 Service Plugin for Cisco Nexus Switches\n\nThis spec describes the work being done by the blueprint\nhttps://blueprints.launchpad.net/neutron/+spec/ml2-cisco-nexus-mechdriver-svi\nwhich implements Layer 3 features for the Cisco 3K, 5K and 7K Nexus switches.\n\nChange-Id: I2252ea358bfab32ec19087a93995d7810af7c72e\n'}, {'number': 5, 'created': '2014-06-19 17:39:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/ef59791061ab5dc7097808de07f69b7e7b1315dc', 'message': 'Layer 3 Service Plugin for Cisco Nexus Switches\n\nThis spec describes the work being done by the blueprint\nhttps://blueprints.launchpad.net/neutron/+spec/ml2-cisco-nexus-mechdriver-svi\nwhich implements Layer 3 features for the Cisco 3K, 5K and 7K Nexus switches.\n\nChange-Id: I2252ea358bfab32ec19087a93995d7810af7c72e\n'}, {'number': 6, 'created': '2014-07-15 14:38:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/fa3b377c2afecffa7b3237f1178c010835297790', 'message': 'Layer 3 Service Plugin for Cisco Nexus Switches\n\nThis spec describes the work being done by the blueprint\nhttps://blueprints.launchpad.net/neutron/+spec/ml2-cisco-nexus-mechdriver-svi\nwhich implements Layer 3 features for the Cisco 3K, 5K and 7K Nexus switches.\n\nChange-Id: I2252ea358bfab32ec19087a93995d7810af7c72e\n'}, {'number': 7, 'created': '2014-07-24 13:44:07.000000000', 'files': ['specs/juno/cisco-nexus-l3-service-plugin.rst'], 'web_link': 'https://opendev.org/openstack/neutron-specs/commit/6277d01333ac61e6d749738803e99520edd18129', 'message': 'Layer 3 Service Plugin for Cisco Nexus Switches\n\nThis spec describes the work being done by the blueprint\nhttps://blueprints.launchpad.net/neutron/+spec/ml2-cisco-nexus-mechdriver-svi\nwhich implements Layer 3 features for the Cisco 3K, 5K and 7K Nexus switches.\n\nChange-Id: I2252ea358bfab32ec19087a93995d7810af7c72e\n'}]",15,95910,6277d01333ac61e6d749738803e99520edd18129,71,15,7,6694,,,0,"Layer 3 Service Plugin for Cisco Nexus Switches

This spec describes the work being done by the blueprint
https://blueprints.launchpad.net/neutron/+spec/ml2-cisco-nexus-mechdriver-svi
which implements Layer 3 features for the Cisco 3K, 5K and 7K Nexus switches.

Change-Id: I2252ea358bfab32ec19087a93995d7810af7c72e
",git fetch https://review.opendev.org/openstack/neutron-specs refs/changes/10/95910/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/juno/cisco-nexus-l3-service-plugin.rst'],1,5a13556f2e198f1c5a094f2ac3a02cb9c385d15e,bp/https,"=============================================== Layer 3 Service Plugin for Cisco Nexus Switches =============================================== https://blueprints.launchpad.net/neutron/+spec/ml2-cisco-nexus-mechdriver-svi This blueprint is to implement Layer 3 features for the Cisco 3K, 5K and 7K Nexus switches. The core cisco plugin supports this feature. Much of the work required is a port of this code. Problem description =================== The core Cisco plugin supports both L2 and L3 features. This core plugin is being deprecated in favor of the ML2 core plugin. ML2 supports only L2 features. To support L3 features this service plugin is being created. Switched Virtual Interface (SVI) support will be the first feature to use this service plugin. Proposed change =============== This proposal is to introduce a new Layer 3 service plugin that communicates with Cisco 3K, 5K and 7K Nexus switches. Initially the driver implements the following neutron events: * Add a new router interface * Delete a router interface The plugin will use the existing ML2 cisco_nexus mechanism driver code base to program the Cisco switches. New cisco_nexus ML2 MD APIs will be introduced to support the L3 events. It is required that the ML2 cisco_nexus MD be running along with this Cisco Nexus service plugin. (i.e. cisco_nexus is set in the mechanism_drivers ML2 configuration variable.) Alternatives ------------ An additional effort of supporting L3 features inside of Cisco is Bob Melander's, ""Framework for OpenStack Neutron L3+ services in Cisco devices"". This design has not been pushed upstream. When this design (or something similar) is available then this service plugin may be able to be deprecated. Data model impact ----------------- n.a. REST API impact --------------- n.a Security impact --------------- n.a Notifications impact -------------------- n.a. Other end user impact --------------------- n.a. Performance Impact ------------------ The service plugin is triggered instead of polled, there are no changes to any existing code patterns. The potential bottleneck for this plugin would be the link between neutron and the cisco nexus switches (netconf). Other deployer impact --------------------- There are no config options specific to the Layer 3 plugin, it relies on the configuration options of the ML2 cisco_nexus mechanism driver. Developer impact ---------------- n.a. Implementation ============== Assignee(s) ----------- Rich Curran <rcurran> Work Items ---------- Single work item for the L3 Cisco Nexus service plugin. Dependencies ============ Depends on the Cisco Nexus ML2 blueprint: https://blueprints.launchpad.net/neutron/+spec/ml2-md-cisco-nexus Testing ======= Complete unit test coverage of the code is included. For tempest test coverage, third party testing is provided. The Cisco CI reports on all changes affecting this driver. The testing is run in a setup with an OpenStack deployment (devstack) connected to a Cisco Nexus 3K physical switch. Documentation Impact ==================== Deployment documentation on how to configure and deploy this service plugin will be documented in the Openstack wiki. References ========== ML2 Cisco Nexus WIKI: https://wiki.openstack.org/wiki/Neutron/ML2/MechCiscoNexus Similar design being proposed to support Cisco APIC: specs/juno/apic-l3-service-plugin.rst Bug review used to commit SVI support for the core Cisco plugin: https://review.openstack.org/#/c/30153/ ",,124,0
openstack%2Fpython-saharaclient~master~I8793b04fac2da9da34467342f6031163d8cc67b1,openstack/python-saharaclient,master,I8793b04fac2da9da34467342f6031163d8cc67b1,Remove node group tmpl update function,ABANDONED,2014-11-24 10:01:46.000000000,2014-12-05 15:19:00.000000000,,"[{'_account_id': 3}, {'_account_id': 6786}, {'_account_id': 7213}, {'_account_id': 8090}, {'_account_id': 8411}, {'_account_id': 12039}]","[{'number': 1, 'created': '2014-11-24 10:01:46.000000000', 'files': ['saharaclient/api/node_group_templates.py'], 'web_link': 'https://opendev.org/openstack/python-saharaclient/commit/bd5c246f7d74164e279bf9d86ae6b68ef75c9da9', 'message': ""Remove node group tmpl update function\n\nSahara doesn't support this feature on server side, it was merged\nin time when server side were really near to be done:\nIc217885efe8d4098b12bccb01995d77d2f91b5a9\n\nIt should be added back only when the server side feature will be\nmerged to the sahara.\n\nChange-Id: I8793b04fac2da9da34467342f6031163d8cc67b1\n""}]",0,136726,bd5c246f7d74164e279bf9d86ae6b68ef75c9da9,16,6,1,6786,,,0,"Remove node group tmpl update function

Sahara doesn't support this feature on server side, it was merged
in time when server side were really near to be done:
Ic217885efe8d4098b12bccb01995d77d2f91b5a9

It should be added back only when the server side feature will be
merged to the sahara.

Change-Id: I8793b04fac2da9da34467342f6031163d8cc67b1
",git fetch https://review.opendev.org/openstack/python-saharaclient refs/changes/26/136726/1 && git format-patch -1 --stdout FETCH_HEAD,['saharaclient/api/node_group_templates.py'],1,bd5c246f7d74164e279bf9d86ae6b68ef75c9da9,ng-tmpl-upd,," def update(self, ng_template_id, name, plugin_name, hadoop_version, flavor_id, description=None, volumes_per_node=None, volumes_size=None, node_processes=None, node_configs=None, floating_ip_pool=None, security_groups=None, auto_security_group=None, availability_zone=None, volumes_availability_zone=None, volume_type=None): data = self._assign_field(name, plugin_name, hadoop_version, flavor_id, description, volumes_per_node, volumes_size, node_processes, node_configs, floating_ip_pool, security_groups, auto_security_group, availability_zone, volumes_availability_zone, volume_type) return self._update('/node-group-templates/%s' % ng_template_id, data, 'node_group_template') ",0,17
openstack%2Freviewstats~master~I308b906ae36af756233087a5f941b79d72766d45,openstack/reviewstats,master,I308b906ae36af756233087a5f941b79d72766d45,Workflow documentation is now in infra-manual,MERGED,2014-12-05 03:54:39.000000000,2014-12-05 15:14:56.000000000,2014-12-05 15:14:56.000000000,"[{'_account_id': 3}, {'_account_id': 1561}, {'_account_id': 6133}, {'_account_id': 7069}]","[{'number': 1, 'created': '2014-12-05 03:54:39.000000000', 'files': ['CONTRIBUTING.rst'], 'web_link': 'https://opendev.org/openstack/reviewstats/commit/d953c5b4580e0171eb31ccf800679375ba3a0517', 'message': 'Workflow documentation is now in infra-manual\n\nReplace URLs for workflow documentation to appropriate parts of the\nOpenStack Project Infrastructure Manual.\n\nChange-Id: I308b906ae36af756233087a5f941b79d72766d45\n'}]",0,139431,d953c5b4580e0171eb31ccf800679375ba3a0517,8,4,1,5263,,,0,"Workflow documentation is now in infra-manual

Replace URLs for workflow documentation to appropriate parts of the
OpenStack Project Infrastructure Manual.

Change-Id: I308b906ae36af756233087a5f941b79d72766d45
",git fetch https://review.opendev.org/openstack/reviewstats refs/changes/31/139431/1 && git format-patch -1 --stdout FETCH_HEAD,['CONTRIBUTING.rst'],1,d953c5b4580e0171eb31ccf800679375ba3a0517,infra-manual,you must follow the steps in this page: http://docs.openstack.org/infra/manual/developers.html http://docs.openstack.org/infra/manual/developers.html#development-workflow,"you must follow the steps in the ""If you're a developer, start here"" section of this page: http://wiki.openstack.org/HowToContribute http://wiki.openstack.org/GerritWorkflow",3,4
openstack%2Freviewstats~master~I31a97fd615195dfbe4ae1c7cbd7bbeebace2e6ae,openstack/reviewstats,master,I31a97fd615195dfbe4ae1c7cbd7bbeebace2e6ae,markmc is not nova-core,MERGED,2014-12-05 14:26:16.000000000,2014-12-05 15:13:59.000000000,2014-12-05 15:13:59.000000000,"[{'_account_id': 3}, {'_account_id': 1561}, {'_account_id': 1779}, {'_account_id': 6873}, {'_account_id': 7730}]","[{'number': 1, 'created': '2014-12-05 14:26:16.000000000', 'files': ['projects/nova.json'], 'web_link': 'https://opendev.org/openstack/reviewstats/commit/79f80a055631aacf66e246912b08d6474b8babf1', 'message': 'markmc is not nova-core\n\nRemove markmc from nova-core group so the stats are accurate.\n\nChange-Id: I31a97fd615195dfbe4ae1c7cbd7bbeebace2e6ae\n'}]",0,139637,79f80a055631aacf66e246912b08d6474b8babf1,9,5,1,6486,,,0,"markmc is not nova-core

Remove markmc from nova-core group so the stats are accurate.

Change-Id: I31a97fd615195dfbe4ae1c7cbd7bbeebace2e6ae
",git fetch https://review.opendev.org/openstack/reviewstats refs/changes/37/139637/1 && git format-patch -1 --stdout FETCH_HEAD,['projects/nova.json'],1,79f80a055631aacf66e246912b08d6474b8babf1,nova-core-2014-12-05,," ""markmc"",",0,1
openstack%2Freviewstats~master~I7151d2e437652a8fc8ce6f6177f9e647360a14db,openstack/reviewstats,master,I7151d2e437652a8fc8ce6f6177f9e647360a14db,Add new keystone subprojects,MERGED,2014-12-05 14:34:23.000000000,2014-12-05 15:13:11.000000000,2014-12-05 15:13:10.000000000,"[{'_account_id': 3}, {'_account_id': 1561}, {'_account_id': 2903}]","[{'number': 1, 'created': '2014-12-05 14:34:23.000000000', 'files': ['projects/keystone.json'], 'web_link': 'https://opendev.org/openstack/reviewstats/commit/7d7be7c5746c6bc0e71a71137b7199841f544f54', 'message': 'Add new keystone subprojects\n\nSeveral new keystone subprojects have been added. Update the list\nso that we get accurate stats.\n\nChange-Id: I7151d2e437652a8fc8ce6f6177f9e647360a14db\n'}]",0,139642,7d7be7c5746c6bc0e71a71137b7199841f544f54,6,3,1,6486,,,0,"Add new keystone subprojects

Several new keystone subprojects have been added. Update the list
so that we get accurate stats.

Change-Id: I7151d2e437652a8fc8ce6f6177f9e647360a14db
",git fetch https://review.opendev.org/openstack/reviewstats refs/changes/42/139642/1 && git format-patch -1 --stdout FETCH_HEAD,['projects/keystone.json'],1,7d7be7c5746c6bc0e71a71137b7199841f544f54,nova-core-2014-12-05," ""openstack/keystone-specs"", ""openstack/keystonemiddleware"", ""openstack/python-keystoneclient-federation"", ""openstack/python-keystoneclient-kerberos"",",,4,0
openstack%2Freviewstats~master~Id8a3f33f38e329341d0f4702af03924022b16c1b,openstack/reviewstats,master,Id8a3f33f38e329341d0f4702af03924022b16c1b,Alphabetize keystone cores,MERGED,2014-12-05 14:34:23.000000000,2014-12-05 15:08:44.000000000,2014-12-05 15:08:44.000000000,"[{'_account_id': 3}, {'_account_id': 1561}]","[{'number': 1, 'created': '2014-12-05 14:34:23.000000000', 'files': ['projects/keystone.json'], 'web_link': 'https://opendev.org/openstack/reviewstats/commit/a08119e1cdfd303b0ace47c8b4c69088b20262ab', 'message': ""Alphabetize keystone cores\n\nIt's easier for a human to find a name in an alphabetized list.\n\nChange-Id: Id8a3f33f38e329341d0f4702af03924022b16c1b\n""}]",0,139641,a08119e1cdfd303b0ace47c8b4c69088b20262ab,6,2,1,6486,,,0,"Alphabetize keystone cores

It's easier for a human to find a name in an alphabetized list.

Change-Id: Id8a3f33f38e329341d0f4702af03924022b16c1b
",git fetch https://review.opendev.org/openstack/reviewstats refs/changes/41/139641/1 && git format-patch -1 --stdout FETCH_HEAD,['projects/keystone.json'],1,a08119e1cdfd303b0ace47c8b4c69088b20262ab,nova-core-2014-12-05," ""dstanek"", ""ldbragst"", ""mdrnstm"", ""stevemar"""," ""mdrnstm"", ""stevemar"", ""dstanek"", ""ldbragst""",4,4
openstack%2Freviewstats~master~I3dfc06603196800b3178370d0d2ba749d8816cc1,openstack/reviewstats,master,I3dfc06603196800b3178370d0d2ba749d8816cc1,Format keystone.json,MERGED,2014-12-05 14:34:23.000000000,2014-12-05 15:08:05.000000000,2014-12-05 15:08:05.000000000,"[{'_account_id': 3}, {'_account_id': 1561}]","[{'number': 1, 'created': '2014-12-05 14:34:23.000000000', 'files': ['projects/keystone.json'], 'web_link': 'https://opendev.org/openstack/reviewstats/commit/3a043b0c1d178f4b11afad7ab8ed59617577b825', 'message': 'Format keystone.json\n\nReformat keystone.json so a human can read it.\n\nChange-Id: I3dfc06603196800b3178370d0d2ba749d8816cc1\n'}]",0,139640,3a043b0c1d178f4b11afad7ab8ed59617577b825,6,2,1,6486,,,0,"Format keystone.json

Reformat keystone.json so a human can read it.

Change-Id: I3dfc06603196800b3178370d0d2ba749d8816cc1
",git fetch https://review.opendev.org/openstack/reviewstats refs/changes/40/139640/1 && git format-patch -1 --stdout FETCH_HEAD,['projects/keystone.json'],1,3a043b0c1d178f4b11afad7ab8ed59617577b825,nova-core-2014-12-05,"{ ""name"": ""keystone"", ""subprojects"": [ ""openstack/keystone"", ""openstack/python-keystoneclient"", ""openstack/identity-api"" ], ""core-team"": [ ""ayoung"", ""blk-u"", ""dolph"", ""guang-yee"", ""henry-nash"", ""mdrnstm"", ""stevemar"", ""jamielennox"", ""dstanek"", ""ldbragst"" ] } ","{""name"": ""keystone"", ""subprojects"": [""openstack/keystone"", ""openstack/python-keystoneclient"", ""openstack/identity-api""], ""core-team"": [""ayoung"", ""blk-u"", ""dolph"", ""guang-yee"", ""henry-nash"", ""mdrnstm"", ""stevemar"", ""jamielennox"", ""dstanek"", ""ldbragst""]}",21,1
openstack%2Ftempest~master~Ic52efc55399c4ab2fc9d8776e52763b1bd56a1fb,openstack/tempest,master,Ic52efc55399c4ab2fc9d8776e52763b1bd56a1fb,Adding test for security group in trove.,ABANDONED,2014-05-13 19:51:57.000000000,2014-12-05 15:06:18.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 7428}, {'_account_id': 8824}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-05-13 19:51:57.000000000', 'files': ['tempest/api/database/security_groups/__init__.py', 'etc/tempest.conf.sample', 'tempest/api/database/security_groups/test_security_groups.py', 'tempest/clients.py', 'tempest/config.py', 'tempest/api/database/base.py', 'tempest/services/database/json/security_groups_client.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/1db0467f19426fbb0ecf02cad960e30f8e3818d4', 'message': 'Adding test for security group in trove.\n\nChange-Id: Ic52efc55399c4ab2fc9d8776e52763b1bd56a1fb\n'}]",7,93494,1db0467f19426fbb0ecf02cad960e30f8e3818d4,10,7,1,1835,,,0,"Adding test for security group in trove.

Change-Id: Ic52efc55399c4ab2fc9d8776e52763b1bd56a1fb
",git fetch https://review.opendev.org/openstack/tempest refs/changes/94/93494/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/database/security_groups/__init__.py', 'etc/tempest.conf.sample', 'tempest/api/database/security_groups/test_security_groups.py', 'tempest/clients.py', 'tempest/config.py', 'tempest/api/database/base.py', 'tempest/services/database/json/security_groups_client.py']",7,1db0467f19426fbb0ecf02cad960e30f8e3818d4,trove/security_groups,"# Copyright 2014 OpenStack Foundation # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import urllib from tempest.common import rest_client from tempest import config CONF = config.CONF class DatabaseSecurityGroupsClientJSON(rest_client.RestClient): def __init__(self, auth_provider): super(DatabaseSecurityGroupsClientJSON, self).__init__(auth_provider) self.service = CONF.database.catalog_type def list_db_security_groups(self, params=None): url = 'security-groups' if params: url += '?%s' % urllib.urlencode(params) resp, body = self.get(url) return resp, self._parse_resp(body) def get_db_security_group_details(self, db_security_groups_id): resp, body = self.get(""security-groups/%s"" % str(db_security_groups_id)) return resp, self._parse_resp(body) ",,94,0
openstack%2Ftempest~master~I8ebfca61975c4b8d0545bdc8057dec8894380879,openstack/tempest,master,I8ebfca61975c4b8d0545bdc8057dec8894380879,Added negative testcase to check error code with snap-id list,ABANDONED,2014-05-15 06:50:26.000000000,2014-12-05 15:06:17.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 8824}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-05-15 06:50:26.000000000', 'files': ['tempest/api/volume/test_volumes_negative.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/88821df59408721ee0b9139eb0a362d200e7515d', 'message': 'Added negative testcase to check error code with snap-id list\n\nI have added testcase which create volume with dependent\nsnapshots list then calling delete volume which display\nsnapshot list in error message\n\nChange-Id: I8ebfca61975c4b8d0545bdc8057dec8894380879\nCloses-Bug: #1276101\n'}]",1,93666,88821df59408721ee0b9139eb0a362d200e7515d,7,4,1,10473,,,0,"Added negative testcase to check error code with snap-id list

I have added testcase which create volume with dependent
snapshots list then calling delete volume which display
snapshot list in error message

Change-Id: I8ebfca61975c4b8d0545bdc8057dec8894380879
Closes-Bug: #1276101
",git fetch https://review.opendev.org/openstack/tempest refs/changes/66/93666/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/volume/test_volumes_negative.py'],1,88821df59408721ee0b9139eb0a362d200e7515d,bug1276101," @attr(type=['negative', 'gate']) def test_delete_volume_with_dependant_snapshot(self): # Should not be able to delete volume with dependant snapshot volume = {} # create one volume v_name = rand_name('Volume-') resp, volume = self.volumes_client.create_volume(size=1, display_name=v_name) self.assertEqual(200, resp.status) self.volumes_client.wait_for_volume_status(volume['id'], 'available') #create two snapshots using created volume for i in range(0,2): s_name = rand_name('snap') snapshot = self.create_snapshot(volume['id'], display_name=s_name) #Try to delete volume self.assertRaises(exceptions.BadRequest, self.client.delete_volume, volume['id']) ",,19,0
openstack%2Ftempest~master~Ic621b9564bcae6bef6f3862d283e52342a5ebdda,openstack/tempest,master,Ic621b9564bcae6bef6f3862d283e52342a5ebdda,New test added for trove db-configuration     1) List all configuration     2) Get configuration details,ABANDONED,2014-05-15 10:33:28.000000000,2014-12-05 15:06:16.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 7139}, {'_account_id': 7428}, {'_account_id': 8824}, {'_account_id': 10068}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-05-15 10:33:28.000000000', 'files': ['tempest/api/database/configurations/test_configuration.py', 'tempest/clients.py', 'tempest/api/database/base.py', 'tempest/api/database/configurations/__init.py__', 'tempest/services/database/json/configuration_client.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/e1708cf462c88fe02c64a83ec950fbc33febde30', 'message': 'New test added for trove db-configuration\n    1) List all configuration\n    2) Get configuration details\n\nChange-Id: Ic621b9564bcae6bef6f3862d283e52342a5ebdda\n'}]",17,93694,e1708cf462c88fe02c64a83ec950fbc33febde30,10,7,1,1843,,,0,"New test added for trove db-configuration
    1) List all configuration
    2) Get configuration details

Change-Id: Ic621b9564bcae6bef6f3862d283e52342a5ebdda
",git fetch https://review.opendev.org/openstack/tempest refs/changes/94/93694/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/database/configurations/test_configuration.py', 'tempest/clients.py', 'tempest/api/database/base.py', 'tempest/api/database/configurations/__init.py__', 'tempest/services/database/json/configuration_client.py']",5,e1708cf462c88fe02c64a83ec950fbc33febde30,trove-db-configuation,"# Copyright 2014 OpenStack Foundation # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import urllib from tempest.common import rest_client from tempest import config CONF = config.CONF class DatabaseConfigurationClientJSON(rest_client.RestClient): def __init__(self, auth_provider): super(DatabaseConfigurationClientJSON, self).__init__(auth_provider) self.service = CONF.database.catalog_type def list_db_configuration(self, params=None): url = 'configurations' if params: url += '?%s' % urllib.urlencode(params) resp, body = self.get(url) return resp, self._parse_resp(body) def get_db_configuration_details(self, db_configuration_id): resp, body = self.get(""configurations/%s"" % str(db_configuration_id)) return resp, self._parse_resp(body) ",,94,16
openstack%2Ftempest~master~I3f5a520d4799df1cf9fe1331ca3c862470fdc15a,openstack/tempest,master,I3f5a520d4799df1cf9fe1331ca3c862470fdc15a,Fix PEP8 E129 issues,ABANDONED,2014-05-18 15:29:08.000000000,2014-12-05 15:06:15.000000000,,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 2750}, {'_account_id': 5803}, {'_account_id': 6039}, {'_account_id': 7139}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9828}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-05-18 15:29:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/750e0d63a0d14dba334d450a57b2208df849d6d6', 'message': 'Fix PEP8 E129 issues\n\nE129 helpes to distinguish the multi-line expressions, from the next\nlogical line.\n\nChange-Id: I3f5a520d4799df1cf9fe1331ca3c862470fdc15a\n'}, {'number': 2, 'created': '2014-05-19 15:26:51.000000000', 'files': ['tempest/api/network/test_extra_dhcp_options.py', 'tempest/stress/run_stress.py', 'tempest/api/compute/base.py', 'tempest/common/isolated_creds.py', 'tempest/thirdparty/boto/test.py', 'tempest/stress/actions/unit_test.py', 'tempest/api/network/base.py', 'tempest/test.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/76f8d1c1b22425e2584b52bfe89dcb0900d3767d', 'message': 'Fix PEP8 E129 issues\n\nE129 helps to distinguish the multi-line expressions, from the next\nlogical line.\n\nChange-Id: I3f5a520d4799df1cf9fe1331ca3c862470fdc15a\n'}]",1,94095,76f8d1c1b22425e2584b52bfe89dcb0900d3767d,26,10,2,5803,,,0,"Fix PEP8 E129 issues

E129 helps to distinguish the multi-line expressions, from the next
logical line.

Change-Id: I3f5a520d4799df1cf9fe1331ca3c862470fdc15a
",git fetch https://review.opendev.org/openstack/tempest refs/changes/95/94095/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/network/test_extra_dhcp_options.py', 'tempest/stress/run_stress.py', 'tempest/api/compute/base.py', 'tempest/common/isolated_creds.py', 'tempest/thirdparty/boto/test.py', 'tempest/stress/actions/unit_test.py', 'tempest/api/network/base.py', 'tempest/test.py']",8,750e0d63a0d14dba334d450a57b2208df849d6d6,E129," os.environ.get('OS_LOG_CAPTURE') != '0'): ""default_result_code"" in description):"," os.environ.get('OS_LOG_CAPTURE') != '0'): ""default_result_code"" in description):",20,19
openstack%2Ftempest~master~I6c1a3a83f67e8c66aa5013fd6d1251f2fca5acf2,openstack/tempest,master,I6c1a3a83f67e8c66aa5013fd6d1251f2fca5acf2,Add Trove (database) Instance API Tests,ABANDONED,2014-05-16 11:45:16.000000000,2014-12-05 15:06:14.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10068}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-05-16 11:45:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/e1baecc7c229c5897efd3f851d9251b4463e58ce', 'message': 'Add Trove (database) Instance API Tests\n\nThis patchset adds teh JSON clients and tests for the following\nTrove Instances APIs:\n\n1. list-instances\n\nChange-Id: I6c1a3a83f67e8c66aa5013fd6d1251f2fca5acf2\n'}, {'number': 2, 'created': '2014-05-21 09:30:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/feb328461717819f7a8990cbd385ebcd21286bbf', 'message': 'Add Trove (database) Instance API Tests\n\nThis patchset adds teh JSON clients and tests for the following\nTrove Instances APIs:\n\n1. list-instances\n\nChange-Id: I6c1a3a83f67e8c66aa5013fd6d1251f2fca5acf2\n'}, {'number': 3, 'created': '2014-05-22 10:35:31.000000000', 'files': ['tempest/api/database/instances/test_instances.py', 'tempest/api/database/instances/__init__.py', 'etc/tempest.conf.sample', 'tempest/clients.py', 'tempest/config.py', 'tempest/api/database/base.py', 'tempest/services/database/json/instances_client.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/60a642a9a1d79ffccd7543cd9a8b1eebae3073e4', 'message': 'Add Trove (database) Instance API Tests\n\nThis patchset adds teh JSON clients and tests for the following\nTrove Instances APIs:\n\n1. list-instances\n\nChange-Id: I6c1a3a83f67e8c66aa5013fd6d1251f2fca5acf2\n'}]",0,93895,60a642a9a1d79ffccd7543cd9a8b1eebae3073e4,19,6,3,11319,,,0,"Add Trove (database) Instance API Tests

This patchset adds teh JSON clients and tests for the following
Trove Instances APIs:

1. list-instances

Change-Id: I6c1a3a83f67e8c66aa5013fd6d1251f2fca5acf2
",git fetch https://review.opendev.org/openstack/tempest refs/changes/95/93895/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/database/instances/test_instances.py', 'tempest/api/database/instances/__init__.py', 'tempest/clients.py', 'tempest/config.py', 'tempest/api/database/base.py', 'tempest/services/database/json/instances_client.py']",6,e1baecc7c229c5897efd3f851d9251b4463e58ce,master,"# Copyright 2014 OpenStack Foundation # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import urllib from tempest.common import rest_client from tempest import config CONF = config.CONF class DatabaseInstancesClientJSON(rest_client.RestClient): def __init__(self, auth_provider): super(DatabaseInstancesClientJSON, self).__init__(auth_provider) self.service = CONF.database.catalog_type def list_db_instances(self, params=None): url = 'instances' if params: url += '?%s' % urllib.urlencode(params) resp, body = self.get(url) return resp, self._parse_resp(body) ",,90,0
openstack%2Ftempest~master~I300b99bc3f9045133dfb189ff3b679d27ce2b69c,openstack/tempest,master,I300b99bc3f9045133dfb189ff3b679d27ce2b69c,Add Trove Backups test cases,ABANDONED,2014-05-22 10:25:54.000000000,2014-12-05 15:06:13.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10068}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-05-22 10:25:54.000000000', 'files': ['tempest/services/database/json/backups_client.py', 'etc/tempest.conf.sample', 'tempest/clients.py', 'tempest/api/database/backups/__init__.py', 'tempest/api/database/backups/test_backups.py', 'tempest/config.py', 'tempest/api/database/base.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/3906fb387e59c589f13498120ce7518ff8b2d321', 'message': 'Add Trove Backups test cases\n\nThis patch adds the JSON clients and tests for the following\nTrove-Backups APIs:\n\n1. list-backups\n\nChange-Id: I300b99bc3f9045133dfb189ff3b679d27ce2b69c\n'}]",0,94816,3906fb387e59c589f13498120ce7518ff8b2d321,10,7,1,11249,,,0,"Add Trove Backups test cases

This patch adds the JSON clients and tests for the following
Trove-Backups APIs:

1. list-backups

Change-Id: I300b99bc3f9045133dfb189ff3b679d27ce2b69c
",git fetch https://review.opendev.org/openstack/tempest refs/changes/16/94816/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/services/database/json/backups_client.py', 'etc/tempest.conf.sample', 'tempest/clients.py', 'tempest/api/database/backups/__init__.py', 'tempest/api/database/backups/test_backups.py', 'tempest/config.py', 'tempest/api/database/base.py']",7,3906fb387e59c589f13498120ce7518ff8b2d321,master, cls.db_backups_ref = CONF.database.db_backups_ref cls.databases_backups_client = cls.os.databases_backups_client,,84,0
openstack%2Ftempest~master~I3a1bf7e4de8b5fe7d1b4463968b70587991f20f2,openstack/tempest,master,I3a1bf7e4de8b5fe7d1b4463968b70587991f20f2,Implements: Added a test_security_group_create_without_parameters like name & desciprtion. Closes-Bug: #1254722,ABANDONED,2014-04-30 09:12:38.000000000,2014-12-05 15:06:12.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 7428}, {'_account_id': 8247}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 10471}, {'_account_id': 10966}]","[{'number': 1, 'created': '2014-04-30 09:12:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/587a0de99d508a90782e22973e05588346188da7', 'message': 'Implements: Added a test_case for testing a test_security_group_create_without_parameters like name & desciprtion.\nCloses-Bug: #1254722\n\nChange-Id: I3a1bf7e4de8b5fe7d1b4463968b70587991f20f2\n'}, {'number': 2, 'created': '2014-05-23 05:30:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/4ce8afc83c78a62bdcc6dffbce03c631477516fb', 'message': 'Implements: Added a test_case for testing a test_security_group_create_without_parameters like name & desciprtion.\nCloses-Bug: #1254722\n\nChange-Id: I3a1bf7e4de8b5fe7d1b4463968b70587991f20f2\n'}, {'number': 3, 'created': '2014-05-23 12:48:28.000000000', 'files': ['tempest/api/compute/security_groups/test_security_groups.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/0b1a1c80d5067a686bd50039638fe33a667a1b8a', 'message': 'Implements: Added a test_security_group_create_without_parameters like name & desciprtion.\nCloses-Bug: #1254722\n\nChange-Id: I3a1bf7e4de8b5fe7d1b4463968b70587991f20f2\n'}]",4,91308,0b1a1c80d5067a686bd50039638fe33a667a1b8a,22,8,3,10471,,,0,"Implements: Added a test_security_group_create_without_parameters like name & desciprtion.
Closes-Bug: #1254722

Change-Id: I3a1bf7e4de8b5fe7d1b4463968b70587991f20f2
",git fetch https://review.opendev.org/openstack/tempest refs/changes/08/91308/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/compute/security_groups/test_security_groups.py'],1,587a0de99d508a90782e22973e05588346188da7,bug/1254722," @test.addr(type='smoke') def test_security_group_create_without_params(self): # Assign empty string to security group name and description # Create a security group s_name = """" s_description = """" resp, security_group =\ self.create_security_group(s_name, s_description) self.assertEqual(200, resp.status)",,9,0
openstack%2Ftempest~master~I6b220908f6bfde7e87dfd2eb00420cacbb2e36e5,openstack/tempest,master,I6b220908f6bfde7e87dfd2eb00420cacbb2e36e5,Fix nova unit tests to support multiple networks,ABANDONED,2014-05-28 10:37:11.000000000,2014-12-05 15:06:11.000000000,,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 3153}, {'_account_id': 5196}, {'_account_id': 6968}, {'_account_id': 9107}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-05-28 10:37:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/872b1394b6afc3b880603b6f61e6fcc4baa4b279', 'message': 'Fix nova unit tests to support multiple networks\n\nIf the tenant used to run Tempest tests has access to more than one\nnetwork, creating a server fails with the message:\n""Multiple possible networks found, use a Network ID to be more\nspecific"".\n\nTempest has already an option ""fixed_network_name"" to specify which\nnetwork should be used, but Nova tests don\'t use it right now. This\npatch changes the base test case of nova to handle this option.\n\nNova API of Tempest doesn\'t expose the service to list networks, so I\nchose to reuse the official client just for this call.\n\nThe change is only a partial change, it doesn\'t fix all tests. It fixes\nfor example test_add_remove_fixed_ip() of\ntempest.api.compute.servers.test_attach_interfaces. I\'m working on other\nchanges to fix other tests.\n\nChange-Id: I6b220908f6bfde7e87dfd2eb00420cacbb2e36e5\nPartial-bug: 1250866\n'}, {'number': 2, 'created': '2014-05-28 10:44:25.000000000', 'files': ['tempest/api/compute/base.py', 'tempest/services/compute/xml/servers_client.py', 'tempest/scenario/manager.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/03fa4f653f410f2253c1fa494c8d5fb34962f22f', 'message': 'Fix nova unit tests to support multiple networks\n\nIf the tenant used to run Tempest tests has access to more than one\nnetwork, creating a server fails with the message:\n""Multiple possible networks found, use a Network ID to be more\nspecific"".\n\nTempest has already an option ""fixed_network_name"" to specify which\nnetwork should be used, but Nova tests don\'t use it right now. This\npatch changes the base test case of nova to handle this option.\n\nNova API of Tempest doesn\'t expose the service to list networks, so I\nchose to reuse the official client just for this call.\n\nThe change is only a partial change, it doesn\'t fix all tests. It fixes\nfor example test_add_remove_fixed_ip() of\ntempest.api.compute.servers.test_attach_interfaces. I\'m working on other\nchanges to fix other tests.\n\nChange-Id: I6b220908f6bfde7e87dfd2eb00420cacbb2e36e5\nPartial-bug: 1250866\n'}]",8,96131,03fa4f653f410f2253c1fa494c8d5fb34962f22f,13,9,2,9107,,,0,"Fix nova unit tests to support multiple networks

If the tenant used to run Tempest tests has access to more than one
network, creating a server fails with the message:
""Multiple possible networks found, use a Network ID to be more
specific"".

Tempest has already an option ""fixed_network_name"" to specify which
network should be used, but Nova tests don't use it right now. This
patch changes the base test case of nova to handle this option.

Nova API of Tempest doesn't expose the service to list networks, so I
chose to reuse the official client just for this call.

The change is only a partial change, it doesn't fix all tests. It fixes
for example test_add_remove_fixed_ip() of
tempest.api.compute.servers.test_attach_interfaces. I'm working on other
changes to fix other tests.

Change-Id: I6b220908f6bfde7e87dfd2eb00420cacbb2e36e5
Partial-bug: 1250866
",git fetch https://review.opendev.org/openstack/tempest refs/changes/31/96131/2 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/compute/base.py', 'tempest/scenario/manager.py']",2,872b1394b6afc3b880603b6f61e6fcc4baa4b279,bug/1250866,"from tempest.api.compute import base as compute_base compute_base.server_set_network_parameter(client, create_kwargs)"," fixed_network_name = CONF.compute.fixed_network_name if 'nics' not in create_kwargs and fixed_network_name: networks = client.networks.list() # If several networks found, set the NetID on which to connect the # server to avoid the following error ""Multiple possible networks # found, use a Network ID to be more specific."" # See Tempest #1250866 if len(networks) > 1: for network in networks: if network.label == fixed_network_name: create_kwargs['nics'] = [{'net-id': network.id}] break # If we didn't find the network we were looking for : else: msg = (""The network on which the NIC of the server must "" ""be connected can not be found : "" ""fixed_network_name=%s. Starting instance without "" ""specifying a network."") % fixed_network_name LOG.info(msg)",47,19
openstack%2Ftempest~master~I12a9d6aff342e32e735ffbc915b785d0e0ffd4d5,openstack/tempest,master,I12a9d6aff342e32e735ffbc915b785d0e0ffd4d5,Fix nova unit tests attaching an interface,ABANDONED,2014-05-28 10:53:57.000000000,2014-12-05 15:06:10.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 3153}, {'_account_id': 6968}, {'_account_id': 8871}, {'_account_id': 9275}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-05-28 10:53:57.000000000', 'files': ['tempest/api/compute/servers/test_attach_interfaces.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/2a5c84a7acc40cc76403634a0efc09ae9809698a', 'message': ""Fix nova unit tests attaching an interface\n\nIf the tenant used to run Tempest tests has access to more than one\nnetwork, attaching a new interface without specifying a network raises\nan exception InterfaceAttachFailed('Failed to attach network adapter\ndevice to ...').\n\nThis patch is a partial fix for the bug #1250866, it fixes at least the\ntests of: tempest.api.compute.servers.test_attach_interfaces.\n\nPartial-bug: 1250866\nChange-Id: I12a9d6aff342e32e735ffbc915b785d0e0ffd4d5\n""}]",0,96135,2a5c84a7acc40cc76403634a0efc09ae9809698a,12,8,1,9107,,,0,"Fix nova unit tests attaching an interface

If the tenant used to run Tempest tests has access to more than one
network, attaching a new interface without specifying a network raises
an exception InterfaceAttachFailed('Failed to attach network adapter
device to ...').

This patch is a partial fix for the bug #1250866, it fixes at least the
tests of: tempest.api.compute.servers.test_attach_interfaces.

Partial-bug: 1250866
Change-Id: I12a9d6aff342e32e735ffbc915b785d0e0ffd4d5
",git fetch https://review.opendev.org/openstack/tempest refs/changes/35/96135/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/compute/servers/test_attach_interfaces.py'],1,2a5c84a7acc40cc76403634a0efc09ae9809698a,bug/1250866, compute_client = self.get_compute_client() networks = compute_client.networks.list() if len(networks) <= 1: # Attaching an interface without specifying the network only works # if the tenant has access to one network iface = self._test_create_interface(server) ifs.append(iface), iface = self._test_create_interface(server) ifs.append(iface),7,2
openstack%2Ftempest~master~Ib672c411800ccea7f75ea0f6e1802da2121fe355,openstack/tempest,master,Ib672c411800ccea7f75ea0f6e1802da2121fe355,Test cases for database limits,ABANDONED,2014-04-04 12:10:01.000000000,2014-12-05 15:06:09.000000000,,"[{'_account_id': 3}, {'_account_id': 1687}, {'_account_id': 1795}, {'_account_id': 2222}, {'_account_id': 2238}, {'_account_id': 2750}, {'_account_id': 6455}, {'_account_id': 7139}, {'_account_id': 8556}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10215}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-04-04 12:10:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0c9f86562a913e645619e0d8d6d7b5ce2c1f1c5d', 'message': 'Test cases for database limits\n\nThis submission adds a new test script ""test_limits.py"", so as\nto verify ""limit-list"" of Database API. Required supporting\nfunctions are added in new client file ""limits_client.py""\nunder JSON interface.\nModified base.py and clients.py files\n\nChange-Id: Ib672c411800ccea7f75ea0f6e1802da2121fe355\n'}, {'number': 2, 'created': '2014-04-04 17:26:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/1d60adf55f21ce88fcddb9ba810dcea942d91298', 'message': 'Test cases for database limits\n\nThis submission adds a new test script ""test_limits.py"", so as\nto verify ""limit-list"" of Database API. Required supporting\nfunctions are added in new client file ""limits_client.py""\nunder JSON interface.\nModified base.py and clients.py files\n\nChange-Id: Ib672c411800ccea7f75ea0f6e1802da2121fe355\n'}, {'number': 3, 'created': '2014-05-02 05:57:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d7f0fe8cfb9ed09c848a6e69c559d757aec82d42', 'message': 'Test cases for database limits\n\nThis submission adds a new test script ""test_limits.py"", so as\nto verify ""limit-list"" of Database API. Required supporting\nfunctions are added in new client file ""limits_client.py""\nunder JSON interface.\nModified base.py and clients.py files\n\nChange-Id: Ib672c411800ccea7f75ea0f6e1802da2121fe355\n'}, {'number': 4, 'created': '2014-05-07 10:01:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/76fa4c740d38d209a8d97b736f8bcfb58ef4737f', 'message': 'Test cases for database limits\n\nThis submission adds a new test script ""test_limits.py"", so as\nto verify ""limit-list"" of Database API. Required supporting\nfunctions are added in new client file ""limits_client.py""\nunder JSON interface.\nModified base.py and clients.py files\n\nChange-Id: Ib672c411800ccea7f75ea0f6e1802da2121fe355\n'}, {'number': 5, 'created': '2014-05-08 14:50:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/6c0580ede0b972a2810202c854be322765ad9c96', 'message': 'Test cases for database limits\n\nThis submission adds a new test script ""test_limits.py"", so as\nto verify ""limit-list"" of Database API. Required supporting\nfunctions are added in new client file ""limits_client.py""\nunder JSON interface.\nModified base.py and clients.py files\n\nChange-Id: Ib672c411800ccea7f75ea0f6e1802da2121fe355\n'}, {'number': 6, 'created': '2014-05-16 09:16:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5bc67436b247dcfd6131b7c8aac0429b7f4ad69a', 'message': 'Test cases for database limits\n\nThis submission adds a new test script ""test_limits.py"", so as\nto verify ""limit-list"" of Database API. Required supporting\nfunctions are added in new client file ""limits_client.py""\nunder JSON interface.\nModified base.py and clients.py files\n\nChange-Id: Ib672c411800ccea7f75ea0f6e1802da2121fe355\n'}, {'number': 7, 'created': '2014-06-03 13:15:34.000000000', 'files': ['tempest/api/database/limits/test_limits.py', 'tempest/services/database/json/limits_client.py', 'tempest/clients.py', 'tempest/api/database/base.py', 'tempest/api/database/limits/__init__.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/060f5a2052a72f76443cda2a4abca67ca3a673b8', 'message': 'Test cases for database limits\n\nThis submission adds a new test script ""test_limits.py"", so as\nto verify ""limit-list"" of Database API. Required supporting\nfunctions are added in new client file ""limits_client.py""\nunder JSON interface.\nModified base.py and clients.py files\n\nChange-Id: Ib672c411800ccea7f75ea0f6e1802da2121fe355\n'}]",29,85320,060f5a2052a72f76443cda2a4abca67ca3a673b8,114,13,7,1795,,,0,"Test cases for database limits

This submission adds a new test script ""test_limits.py"", so as
to verify ""limit-list"" of Database API. Required supporting
functions are added in new client file ""limits_client.py""
under JSON interface.
Modified base.py and clients.py files

Change-Id: Ib672c411800ccea7f75ea0f6e1802da2121fe355
",git fetch https://review.opendev.org/openstack/tempest refs/changes/20/85320/3 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/database/limits/test_limits.py', 'tempest/services/database/json/limits_client.py', 'tempest/clients.py', 'tempest/api/database/base.py', 'tempest/api/database/limits/__init__.py']",5,0c9f86562a913e645619e0d8d6d7b5ce2c1f1c5d,trove/api-limits-test,,,107,0
openstack%2Ftempest~master~Ic806507d5606f1061e5cf01ce07d0776e97fd06b,openstack/tempest,master,Ic806507d5606f1061e5cf01ce07d0776e97fd06b,WIP: Nova V2.1 API test using testscenarios,ABANDONED,2014-06-05 06:05:53.000000000,2014-12-05 15:06:08.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 8556}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-06-05 06:05:53.000000000', 'files': ['tempest/api/compute/base.py', 'tempest/api/compute/servers/test_servers.py', 'tempest/test.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/1a130728e5a52b46f4ec220230a7320d43bd5bb3', 'message': 'WIP: Nova V2.1 API test using testscenarios\n\nThis patch adds the basic changes required to test the Nova\nV2.1 APIs with exsiting V2 test cases using testscenarios.\n\nNOTE - This is just to give idea of how changes will looks\nlike if we go with the approach of testscenarios.\n\nThere may be some more changes we need to do to in term of\nmoving the class variable/function to instance one.\n\nPartially implements blueprint nova-v2.1-api-tests\n\nChange-Id: Ic806507d5606f1061e5cf01ce07d0776e97fd06b\n'}]",0,98039,1a130728e5a52b46f4ec220230a7320d43bd5bb3,9,6,1,8556,,,0,"WIP: Nova V2.1 API test using testscenarios

This patch adds the basic changes required to test the Nova
V2.1 APIs with exsiting V2 test cases using testscenarios.

NOTE - This is just to give idea of how changes will looks
like if we go with the approach of testscenarios.

There may be some more changes we need to do to in term of
moving the class variable/function to instance one.

Partially implements blueprint nova-v2.1-api-tests

Change-Id: Ic806507d5606f1061e5cf01ce07d0776e97fd06b
",git fetch https://review.opendev.org/openstack/tempest refs/changes/39/98039/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/compute/base.py', 'tempest/api/compute/servers/test_servers.py', 'tempest/test.py']",3,1a130728e5a52b46f4ec220230a7320d43bd5bb3,bp/nova-v2,"# @classmethod def get_client_manager(self, interface=None): _isolated_creds = isolated_creds.IsolatedCreds( self.__class__.__name__, network_resources=self.__class__.network_resources) # force_tenant_isolation = getattr(self, 'force_tenant_isolation', None) if CONF.compute.allow_tenant_isolation: creds = _isolated_creds.get_primary_creds() if getattr(self, '_interface', None): interface=self._interface, service=self.__class__._service) service=self.__class__._service) service=self.__class__._service) else: if getattr(self, '_interface', None): os = clients.Manager(interface=self._interface, service=self.__class__._service) elif interface: os = clients.Manager(interface=interface, service=self.__class__._service) else: os = clients.Manager(service=self.__class__._service)"," @classmethod def get_client_manager(cls, interface=None): cls.isolated_creds = isolated_creds.IsolatedCreds( cls.__name__, network_resources=cls.network_resources) force_tenant_isolation = getattr(cls, 'force_tenant_isolation', None) if CONF.compute.allow_tenant_isolation or force_tenant_isolation: creds = cls.isolated_creds.get_primary_creds() if getattr(cls, '_interface', None): interface=cls._interface, service=cls._service) service=cls._service) service=cls._service) else: if getattr(cls, '_interface', None): os = clients.Manager(interface=cls._interface, service=cls._service) elif interface: os = clients.Manager(interface=interface, service=cls._service) else: os = clients.Manager(service=cls._service)",159,89
openstack%2Ftempest~master~Ibdf59cf63bf615db6109932d4d401ad00f50b2de,openstack/tempest,master,Ibdf59cf63bf615db6109932d4d401ad00f50b2de,Validate responses to cinder list,ABANDONED,2014-05-29 12:28:23.000000000,2014-12-05 15:06:07.000000000,,"[{'_account_id': 3}, {'_account_id': 1207}, {'_account_id': 1773}, {'_account_id': 2750}, {'_account_id': 7428}, {'_account_id': 8556}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-05-29 12:28:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/118ab0c1c085bebe12cd48ec579af00a3db7f098', 'message': 'Validate responses to cinder list\n\nAdd response validation for cinder list volumes and cinder list volumes\ndetail requests.\n\nChange-Id: Ibdf59cf63bf615db6109932d4d401ad00f50b2de\n'}, {'number': 2, 'created': '2014-05-30 10:32:26.000000000', 'files': ['tempest/api_schema/volume/volumes.py', 'tempest/api_schema/volume/v2/__init__.py', 'tempest/services/volume/json/volumes_client.py', 'tempest/api_schema/volume/parameter_types.py', 'tempest/services/volume/v2/json/volumes_client.py', 'tempest/api_schema/volume/v1/__init__.py', 'tempest/api_schema/volume/v1/volumes.py', 'tempest/api_schema/volume/v2/volumes.py', 'tempest/api_schema/volume/__init__.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/c9ae39fe0f5c72d46d4ab438436434ce5b9bab66', 'message': 'Validate responses to cinder list\n\nAdd response validation for cinder list volumes and cinder list volumes\ndetail requests.\n\nChange-Id: Ibdf59cf63bf615db6109932d4d401ad00f50b2de\n'}]",1,96440,c9ae39fe0f5c72d46d4ab438436434ce5b9bab66,26,9,2,1773,,,0,"Validate responses to cinder list

Add response validation for cinder list volumes and cinder list volumes
detail requests.

Change-Id: Ibdf59cf63bf615db6109932d4d401ad00f50b2de
",git fetch https://review.opendev.org/openstack/tempest refs/changes/40/96440/2 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api_schema/volume/volumes.py', 'tempest/api_schema/volume/v2/__init__.py', 'tempest/api_schema/volume/parameter_types.py', 'tempest/services/volume/json/volumes_client.py', 'tempest/services/volume/v2/json/volumes_client.py', 'tempest/api_schema/volume/v1/__init__.py', 'tempest/api_schema/volume/v1/volumes.py', 'tempest/api_schema/volume/v2/volumes.py', 'tempest/api_schema/volume/__init__.py']",9,118ab0c1c085bebe12cd48ec579af00a3db7f098,volume_list_response_validation,,,221,0
openstack%2Ftempest~master~Ie9e70309245f2737857f429a0017dd6f1599b95c,openstack/tempest,master,Ie9e70309245f2737857f429a0017dd6f1599b95c,Clarify usage of tags in tests,ABANDONED,2014-06-11 09:22:47.000000000,2014-12-05 15:06:06.000000000,,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 5196}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-06-11 09:22:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/959549e4a72f6290f53f52db6ac7c802f480692f', 'message': 'Clarify usage of tags in tests\n\nAdd a section in HACKING.rst to clarify how to use (or not use)\nsmoke gate and slow attributes.\n\nChange-Id: Ie9e70309245f2737857f429a0017dd6f1599b95c\n'}, {'number': 2, 'created': '2014-06-11 11:34:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/dedcd03a471280c1d836b816221a57e0dfea1b3e', 'message': 'Clarify usage of tags in tests\n\nAdd a section in HACKING.rst to clarify how to use (or not use)\nsmoke gate and slow attributes.\n\nChange-Id: Ie9e70309245f2737857f429a0017dd6f1599b95c\n'}, {'number': 3, 'created': '2014-06-11 13:00:37.000000000', 'files': ['HACKING.rst'], 'web_link': 'https://opendev.org/openstack/tempest/commit/54f19ed3e2a90794ec8ca55e2cf58d983d33500b', 'message': 'Clarify usage of tags in tests\n\nAdd a section in HACKING.rst to clarify how to use (or not use)\nsmoke gate and slow attributes.\n\nChange-Id: Ie9e70309245f2737857f429a0017dd6f1599b95c\n'}]",4,99302,54f19ed3e2a90794ec8ca55e2cf58d983d33500b,20,6,3,1921,,,0,"Clarify usage of tags in tests

Add a section in HACKING.rst to clarify how to use (or not use)
smoke gate and slow attributes.

Change-Id: Ie9e70309245f2737857f429a0017dd6f1599b95c
",git fetch https://review.opendev.org/openstack/tempest refs/changes/02/99302/2 && git format-patch -1 --stdout FETCH_HEAD,['HACKING.rst'],1,959549e4a72f6290f53f52db6ac7c802f480692f,gate_tag_doc," @test.attr(type=['negative'])Other test tagging ------------------ Apart from service tagging an negative tests, there are three more tags in use in tempests tests: smoke, gate and slow. These three tags can be applied using the syntax:: @test.attr(type=['gate', 'slow']) def test_example(self): - smoke and gate are legacy tags, and they should not be applied to any new test written in tempest. They were introduced to identify a minimum set of essential tests and tests to be executed in the gate respectively. Nowadays all tests mathing the configuration are executed in the gate. The only purpose left for smoke and gate is to select tests executed when running `tox -e smoke`, which is still used by few projects for which the full run is not stable. - slow is used to tag slow tests, which are executed in a dedicated job to avoid impacting the overall duraction of check and gate. At the moment of writing, only heat slow test exists, and they can be be triggered running `tox -e heat-slow`. "," @test.attr(type=['negative', 'gate'])",23,1
openstack%2Ftempest~master~Ic21d3bda6bf776377bfa92b43f5d705de26851ee,openstack/tempest,master,Ic21d3bda6bf776377bfa92b43f5d705de26851ee,NEUTRON tests optimization,ABANDONED,2014-05-19 11:42:23.000000000,2014-12-05 15:06:05.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 7249}, {'_account_id': 7293}, {'_account_id': 8556}, {'_account_id': 8576}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9828}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 10966}]","[{'number': 1, 'created': '2014-05-19 11:42:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c600f7a1d525a592aaedc583ed9ca84ff9d08720', 'message': 'NEUTRON tests optimization\n\nThis patch optimizes the Neutron tests and removes the\nredundant codes and unused variables.\n\nChange-Id: Ic21d3bda6bf776377bfa92b43f5d705de26851ee\n'}, {'number': 2, 'created': '2014-05-22 07:33:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/343c599984f3920eb871f23c0f808d21ff536acb', 'message': 'NEUTRON tests optimization\n\nThis patch optimizes the Neutron tests and removes the\nredundant codes and unused variables.\n\nChange-Id: Ic21d3bda6bf776377bfa92b43f5d705de26851ee\n'}, {'number': 3, 'created': '2014-05-22 07:51:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/25aae4e955b596ee7c1c40538e3d7f62056ed1bb', 'message': 'NEUTRON tests optimization\n\nThis patch optimizes the Neutron tests and removes the\nredundant codes and unused variables.\n\nChange-Id: Ic21d3bda6bf776377bfa92b43f5d705de26851ee\n'}, {'number': 4, 'created': '2014-05-22 11:43:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/becfe63e4b7fb7d370d8690e7a3c594fbb7f46d6', 'message': 'NEUTRON tests optimization\n\nThis patch optimizes the Neutron tests and removes the\nredundant codes and unused variables.\nThis patch also adds a help method in tempest.test to check\nfor the expected response for multiple keys.\nand other help methods to validate the availability and\nnon-availability of the multiple keys in response.\n\nChange-Id: Ic21d3bda6bf776377bfa92b43f5d705de26851ee\n'}, {'number': 5, 'created': '2014-05-23 05:17:46.000000000', 'files': ['tempest/api/network/test_fwaas_extensions.py', 'tempest/api/network/admin/test_quotas.py', 'tempest/api/network/admin/test_floating_ips_admin_actions.py', 'tempest/api/network/test_networks.py', 'tempest/api/network/admin/test_external_network_extension.py', 'tempest/api/network/test_ports.py', 'tempest/api/network/test_extensions.py', 'tempest/api/network/admin/test_dhcp_agent_scheduler.py', 'tempest/api/network/test_routers.py', 'tempest/api/network/test_extra_dhcp_options.py', 'tempest/api/network/test_floating_ips.py', 'tempest/api/network/test_security_groups.py', 'tempest/api/network/admin/test_l3_agent_scheduler.py', 'tempest/api/network/test_metering_extensions.py', 'tempest/api/network/test_load_balancer.py', 'tempest/api/network/test_vpnaas_extensions.py', 'tempest/api/network/admin/test_load_balancer_admin_actions.py', 'tempest/api/network/base.py', 'tempest/test.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/54580bfeaa025eb1cdbe8ee6b43a09b87f4679c5', 'message': 'NEUTRON tests optimization\n\nThis patch optimizes the Neutron tests and removes the\nredundant codes and unused variables.\nThis patch also adds a help method in tempest.test to check\nfor the expected response for multiple keys.\nand other help methods to validate the availability and\nnon-availability of the multiple keys in response.\n\nChange-Id: Ic21d3bda6bf776377bfa92b43f5d705de26851ee\n'}]",36,94174,54580bfeaa025eb1cdbe8ee6b43a09b87f4679c5,49,12,5,10966,,,0,"NEUTRON tests optimization

This patch optimizes the Neutron tests and removes the
redundant codes and unused variables.
This patch also adds a help method in tempest.test to check
for the expected response for multiple keys.
and other help methods to validate the availability and
non-availability of the multiple keys in response.

Change-Id: Ic21d3bda6bf776377bfa92b43f5d705de26851ee
",git fetch https://review.opendev.org/openstack/tempest refs/changes/74/94174/3 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/network/test_fwaas_extensions.py', 'tempest/api/network/admin/test_quotas.py', 'tempest/api/network/admin/test_floating_ips_admin_actions.py', 'tempest/api/network/test_networks.py', 'tempest/api/network/admin/test_external_network_extension.py', 'tempest/api/network/test_ports.py', 'tempest/api/network/test_extensions.py', 'tempest/api/network/admin/test_dhcp_agent_scheduler.py', 'tempest/api/network/test_routers.py', 'tempest/api/network/test_extra_dhcp_options.py', 'tempest/api/network/test_floating_ips.py', 'tempest/api/network/test_security_groups.py', 'tempest/api/network/admin/test_l3_agent_scheduler.py', 'tempest/api/network/test_metering_extensions.py', 'tempest/api/network/test_load_balancer.py', 'tempest/api/network/test_vpnaas_extensions.py', 'tempest/api/network/admin/test_load_balancer_admin_actions.py', 'tempest/api/network/base.py']",18,c600f7a1d525a592aaedc583ed9ca84ff9d08720,neutron_tests_optimization," assert 201 == resp.status assert 201 == resp.status assert 201 == resp.status assert 200 == resp.status assert 201 == resp.status assert 201 == resp.status assert 201 == resp.status _, body = cls.client.update_pool(name=name) assert 201 == resp.status _, body = cls.client.update_vip(name=name) assert 201 == resp.status _, body = cls.client.update_member(admin_state_up=admin_state_up) assert 201 == resp.status _, body = cls.client.update_vip(admin_state_up=admin_state_up) assert 200 == resp.status assert 201 == resp.status assert 201 == resp.status assert 201 == resp.status assert 201 == resp.status assert 200 == resp.status assert 201 == resp.status assert 201 == resp.status"," resp, body = cls.client.update_pool(name=name) resp, body = cls.client.update_vip(name=name) resp, body = cls.client.update_member(admin_state_up=admin_state_up) resp, body = cls.client.update_vip(admin_state_up=admin_state_up)",187,184
openstack%2Ftempest~master~I0c5893833e4bbe1cef50b59ec26161215a2819ea,openstack/tempest,master,I0c5893833e4bbe1cef50b59ec26161215a2819ea,Add negative tests for server_group,ABANDONED,2014-06-25 09:14:59.000000000,2014-12-05 15:06:04.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 6167}, {'_account_id': 8021}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-06-25 09:14:59.000000000', 'files': ['tempest/api/compute/servers/test_server_group_negative.py', 'tempest/api/compute/servers/test_server_group.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/d00b2bd305b95db98aa192c18b8d357dfa5cfd17', 'message': ""Add negative tests for server_group\n\nThere's only basic tests for server_group now.\nThe negative ones are also necessary here.\n\nChange-Id: I0c5893833e4bbe1cef50b59ec26161215a2819ea\nCloses-Bug: #1334162\n""}]",2,102472,d00b2bd305b95db98aa192c18b8d357dfa5cfd17,16,6,1,8021,,,0,"Add negative tests for server_group

There's only basic tests for server_group now.
The negative ones are also necessary here.

Change-Id: I0c5893833e4bbe1cef50b59ec26161215a2819ea
Closes-Bug: #1334162
",git fetch https://review.opendev.org/openstack/tempest refs/changes/72/102472/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/compute/servers/test_server_group_negative.py', 'tempest/api/compute/servers/test_server_group.py']",2,d00b2bd305b95db98aa192c18b8d357dfa5cfd17,bug/1334162,," @test.skip_because(bug=""1324348"") @test.attr(type='gate') def test_create_delete_server_group_with_multiple_policies(self): # Create and Delete the server-group with multiple policies policies = ['affinity', 'affinity'] self._create_delete_server_group(policies) ",139,7
openstack%2Ftempest~master~I8a59899feab46b82a0f022a2885832f52e80fb91,openstack/tempest,master,I8a59899feab46b82a0f022a2885832f52e80fb91,Missing Nova CLI tests,ABANDONED,2014-06-09 10:33:42.000000000,2014-12-05 15:06:03.000000000,,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 2750}, {'_account_id': 5174}, {'_account_id': 8205}, {'_account_id': 8556}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-06-09 10:33:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/2b2cb60f84a0f584d179feb6a984289a7307bf81', 'message': 'Missing Nova CLI tests\n\nThis patch adds following missing Nova CLI tests -\n1. quota defaults\n2. bash completion\n3. cell capacities\n\nPartially implements: blueprint missing-nova-cli-tests\n\nChange-Id: I8a59899feab46b82a0f022a2885832f52e80fb91\n'}, {'number': 2, 'created': '2014-06-09 10:47:42.000000000', 'files': ['tempest/cli/simple_read_only/test_nova.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/4ba8228fa1ab43de437b7467d13e9d31e474f391', 'message': 'Missing Nova CLI tests\n\nThis patch adds following missing Nova CLI tests -\n1. quota defaults\n2. bash completion\n3. cell capacities\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: I8a59899feab46b82a0f022a2885832f52e80fb91\n'}]",4,98735,4ba8228fa1ab43de437b7467d13e9d31e474f391,16,9,2,8205,,,0,"Missing Nova CLI tests

This patch adds following missing Nova CLI tests -
1. quota defaults
2. bash completion
3. cell capacities

Partially implements: blueprint missing-cli-tests-in-tempest

Change-Id: I8a59899feab46b82a0f022a2885832f52e80fb91
",git fetch https://review.opendev.org/openstack/tempest refs/changes/35/98735/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/cli/simple_read_only/test_nova.py'],1,2b2cb60f84a0f584d179feb6a984289a7307bf81,bp/missing-cli-tests-in-tempest," def test_quota_defaults(self): self.nova('quota-defaults') self.nova('quota-defaults', flags='--debug') def test_bash_completion(self): self.nova('bash-completion') def test_admin_list_cell_capacities(self): self.nova('cell-capacities') ",,10,0
openstack%2Ftempest~master~I2b01c61162e1b079f518387dbfb56b6ec880797b,openstack/tempest,master,I2b01c61162e1b079f518387dbfb56b6ec880797b,Refactor common code for test_server_cfn_init,ABANDONED,2014-06-19 01:45:58.000000000,2014-12-05 15:06:02.000000000,,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 7428}, {'_account_id': 8556}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10090}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-06-19 01:45:58.000000000', 'files': ['tempest/scenario/orchestration/test_server_cfn_init.py', 'tempest/scenario/orchestration/test_server_base.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/5cc0a5f05227fff06705a99f90e05b71ea420eda', 'message': 'Refactor common code for test_server_cfn_init\n\nIn a subsequent patch we will introduce tests to work with\nos-collect-config that make use of these common functions.\n\nChange-Id: I2b01c61162e1b079f518387dbfb56b6ec880797b\n'}]",1,101072,5cc0a5f05227fff06705a99f90e05b71ea420eda,19,10,1,6488,,,0,"Refactor common code for test_server_cfn_init

In a subsequent patch we will introduce tests to work with
os-collect-config that make use of these common functions.

Change-Id: I2b01c61162e1b079f518387dbfb56b6ec880797b
",git fetch https://review.opendev.org/openstack/tempest refs/changes/72/101072/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/scenario/orchestration/test_server_cfn_init.py', 'tempest/scenario/orchestration/test_server_base.py']",2,5cc0a5f05227fff06705a99f90e05b71ea420eda,,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from tempest import config from tempest.openstack.common import log as logging from tempest.scenario import manager CONF = config.CONF LOG = logging.getLogger(__name__) class ServerScenarioTestBase(manager.OrchestrationScenarioTest): template_name = None def setUp(self): super(ServerScenarioTestBase, self).setUp() self.image_ref = CONF.orchestration.image_ref or 'cirros' self.client = self.orchestration_client def assign_keypair(self): self.stack_name = self._stack_rand_name() if CONF.orchestration.keypair_name: self.keypair = None self.keypair_name = CONF.orchestration.keypair_name else: self.keypair = self.create_keypair() self.keypair_name = self.keypair.id def launch_stack(self): net = self._get_default_network() self.parameters = { 'key_name': self.keypair_name, 'flavor': CONF.orchestration.instance_type, 'image': self.image_ref, 'timeout': CONF.orchestration.build_timeout, 'network': net['id'], } # create the stack self.template = self._load_template(__file__, self.template_name) self.client.stacks.create( stack_name=self.stack_name, template=self.template, parameters=self.parameters) self.stack = self.client.stacks.get(self.stack_name) self.stack_identifier = '%s/%s' % (self.stack_name, self.stack.id) self.addCleanup(self._stack_delete, self.stack_identifier) ",,62,34
openstack%2Ftempest~master~I4cee376e67bd58b7da6778467808e4309fea56a9,openstack/tempest,master,I4cee376e67bd58b7da6778467808e4309fea56a9,Add tests for neutron firewall resource,ABANDONED,2014-05-21 11:46:45.000000000,2014-12-05 15:06:01.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 7139}, {'_account_id': 7428}, {'_account_id': 8824}, {'_account_id': 8871}, {'_account_id': 10068}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 10856}]","[{'number': 1, 'created': '2014-05-21 11:46:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ac2f0d13d001ee4cd2020d4e3abc20f19d139bb2', 'message': 'Add tests for neutron firewall resource\n\nTests for OS::Neutron::Firewall,OS::Neutron::FirewallPolicy and OS::Neutron::FirewallRule\n\nChange-Id: I4cee376e67bd58b7da6778467808e4309fea56a9\nblueprint: tempest-heat-integration\n'}, {'number': 2, 'created': '2014-05-21 11:57:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/44f25ee615388c8f57b40d686295de96cbeb2bc9', 'message': 'Add tests for neutron firewall resource\n\nTests for OS::Neutron::Firewall,OS::Neutron::FirewallPolicy and OS::Neutron::FirewallRule\n\nChange-Id: I4cee376e67bd58b7da6778467808e4309fea56a9\nblueprint: tempest-heat-integration\n'}, {'number': 3, 'created': '2014-05-26 10:18:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/4c7433b73b4d729987c2b10dfbd7db099ad754ef', 'message': 'Add tests for neutron firewall resource\n\nTests for OS::Neutron::Firewall,OS::Neutron::FirewallPolicy and\nOS::Neutron::FirewallRule\n\nChange-Id: I4cee376e67bd58b7da6778467808e4309fea56a9\nblueprint: tempest-heat-integration\n'}, {'number': 4, 'created': '2014-05-30 08:37:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/9bdf97ba10a0aa4479fd623c3296a59f6f791925', 'message': 'Add tests for neutron firewall resource\n\nTests for OS::Neutron::Firewall,OS::Neutron::FirewallPolicy and\nOS::Neutron::FirewallRule\n\nChange-Id: I4cee376e67bd58b7da6778467808e4309fea56a9\nblueprint: tempest-heat-integration\n'}, {'number': 5, 'created': '2014-05-31 03:17:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/13d592555b061716d07b03930e73c40b05aea0c8', 'message': 'Add tests for neutron firewall resource\n\nTests for OS::Neutron::Firewall,OS::Neutron::FirewallPolicy and\nOS::Neutron::FirewallRule\n\nChange-Id: I4cee376e67bd58b7da6778467808e4309fea56a9\nblueprint: tempest-heat-integration\n'}, {'number': 6, 'created': '2014-06-03 00:42:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/eb15219b77e1769c080f3dd7a2513e357ea21eb6', 'message': 'Add tests for neutron firewall resource\n\nTests for OS::Neutron::Firewall,OS::Neutron::FirewallPolicy and\nOS::Neutron::FirewallRule\n\nChange-Id: I4cee376e67bd58b7da6778467808e4309fea56a9\nPartially implements: blueprint tempest-heat-integration\n'}, {'number': 7, 'created': '2014-06-03 01:48:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/41a77b9fc241e2d2e9b6ff1cfb19fb4c0fe6480d', 'message': 'Add tests for neutron firewall resource\n\nTests for OS::Neutron::Firewall,OS::Neutron::FirewallPolicy and\nOS::Neutron::FirewallRule\n\nChange-Id: I4cee376e67bd58b7da6778467808e4309fea56a9\nPartially implements: blueprint tempest-heat-integration\n'}, {'number': 8, 'created': '2014-06-05 07:07:32.000000000', 'files': ['tempest/api/orchestration/stacks/templates/neutron_basic.yaml', 'tempest/api/orchestration/stacks/test_neutron_resources.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/d07b3e995fe777008c43a06cc6a427ac838b0c7d', 'message': 'Add tests for neutron firewall resource\n\nTests for OS::Neutron::Firewall,OS::Neutron::FirewallPolicy and\nOS::Neutron::FirewallRule\n\nChange-Id: I4cee376e67bd58b7da6778467808e4309fea56a9\nPartially implements: blueprint tempest-heat-integration\n'}]",14,94590,d07b3e995fe777008c43a06cc6a427ac838b0c7d,64,10,8,10856,,,0,"Add tests for neutron firewall resource

Tests for OS::Neutron::Firewall,OS::Neutron::FirewallPolicy and
OS::Neutron::FirewallRule

Change-Id: I4cee376e67bd58b7da6778467808e4309fea56a9
Partially implements: blueprint tempest-heat-integration
",git fetch https://review.opendev.org/openstack/tempest refs/changes/90/94590/8 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/orchestration/stacks/templates/neutron_basic.yaml', 'tempest/api/orchestration/stacks/test_neutron_resources.py']",2,ac2f0d13d001ee4cd2020d4e3abc20f19d139bb2,bp/tempest-heat-integration," ('Server', 'OS::Nova::Server'), ('Firewall', 'OS::Neutron::Firewall'), ('FirewallPolicy', 'OS::Neutron::FirewallPolicy'), ('FirewallRule1', 'OS::Neutron::FirewallRule'), ('FirewallRule2', 'OS::Neutron::FirewallRule')] @test.attr(type='slow') def test_created_firewallrule(self): """"""Verifies created firewallrule."""""" firewallrule_id = self.test_resources.get('FirewallRule1')['physical_resource_id'] resp, body = self.network_client.show_firewall_rule(firewallrule_id) self.assertEqual('200', resp['status']) firewallrule = body['firewallrule'] self.assertIsInstance(firewallrule, dict) self.assertEqual(firewallrule_id, firewallrule['id']) self.assertEqual('allow', firewallrule['action']) self.assertEqual('Allow SSH connection', firewallrule['description']) self.assertEqual('10.0.3.0/24', firewallrule['destination_ip_address']) self.assertEqual(True, firewallrule['enabled']) self.assertEqual('4', firewallrule['ip_version']) self.assertEqual('tcp', firewallrule['protocol']) self.assertEqual(False, firewallrule['shared']) self.assertEqual('10.10.1.0/24', firewallrule['source_ip_address']) self.assertEqual('22', firewallrule['source_port']) @test.attr(type='slow') def test_created_firewallpolicy(self): """"""Verifies created firewallpolicy."""""" firewallrule_id1 = self.test_resources.get('FirewallRule1')['physical_resource_id'] firewallrule_id2 = self.test_resources.get('FirewallRule2')['physical_resource_id'] firewallpolicy_id = self.test_resources.get('FirewallPolicy')['physical_resource_id'] resp, body = self.network_client.show_firewall_policy(firewallpolicy_id) self.assertEqual('200', resp['status']) firewallpolicy = body['firewallpolicy'] self.assertIsInstance(firewallpolicy, dict) self.assertEqual(firewallpolicy_id, firewallpolicy['id']) self.assertEqual(False, firewallpolicy['audited']) self.aseertEqual(2, len(firewallpolicy['firewall_rules'])) self.assertIn(firewallrule_id1, firewallpolicy['firewall_rules']) self.assertIn(firewallrule_id2, firewallpolicy['firewall_rules']) self.assertEqual('testFirewallPolicy', firewallpolicy['name']) self.assertEqual(False, firewallpolicy['shared']) @test.attr(type='slow') def test_created_firewall(self): """"""Verifies created firewall."""""" firewallpolicy_id = self.test_resources.get('FirewallPolicy')['physical_resource_id'] firewall_id = self.test_resources.get('Firewall')['physical_resource_id'] resp, body = self.network_client.show_firewall(firewall_id) self.assertEqual('200', resp['status']) firewall = body['firewall'] self.assertIsInstance(firewall, dict) self.assertEqual(firewall_id, firewall['id']) self.assertEqual(False, firewall['admin_state_up']) self.assertEqual('A Test Firewall', firewall['description']) self.assertEqual(firewallpolicy_id, firewall['firewall_policy_id']) self.assertEqual('testFirewall', firewall['name'])"," ('Server', 'OS::Nova::Server')]",100,1
openstack%2Ftempest~master~I559398d6ff716164712084170d569f6ba546b191,openstack/tempest,master,I559398d6ff716164712084170d569f6ba546b191,Cinder stress and Continuous Hours (CHO) test,ABANDONED,2014-05-21 18:20:18.000000000,2014-12-05 15:06:00.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 7428}, {'_account_id': 7872}, {'_account_id': 8871}, {'_account_id': 9533}, {'_account_id': 9624}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-05-21 18:20:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/3098123f27e9f3a35ce596fb9d1db8fdacb5095f', 'message': 'Cinder stress and Continuous Hours (CHO) test\n\nStress test and Continous hours of operation test for cinder.\n\nChange-Id: I559398d6ff716164712084170d569f6ba546b191\nCo-Authored-By: Walter A. Boring IV <walter.boring@hp.com>\n'}, {'number': 2, 'created': '2014-05-21 23:33:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ad2011fb5506e2ecf2525d9bb21f1bc9061c3a39', 'message': 'Cinder stress and Continuous Hours (CHO) test\n\nStress test and Continous hours of operation test for cinder.\n\nChange-Id: I559398d6ff716164712084170d569f6ba546b191\nCo-Authored-By: Walter A. Boring IV <walter.boring@hp.com>\n'}, {'number': 3, 'created': '2014-06-04 00:18:53.000000000', 'files': ['tempest/stress/actions/cinder_stress.py', 'tempest/stress/etc/cinder-cho.json', 'tempest/stress/etc/cinder-stress.json'], 'web_link': 'https://opendev.org/openstack/tempest/commit/452d8ebe0f2ead1b8846d1719d373d05c6169c2d', 'message': 'Cinder stress and Continuous Hours (CHO) test\n\nStress test and Continous hours of operation test for cinder.\n\nThe Stress test runs a sequence of tests for n volumes:\n    1. Create n volumes\n    2. Create n snapshots\n    3. Attach n volumes\n    4. Detach n volumes\n    5. Delete n snapshots\n    6. Delete n volumes\n\nThe Continous hours test runs a random sequence of the\nabove actions for a specified ""minutes"" duration when specified\nin the kwargs.\n\nChange-Id: I559398d6ff716164712084170d569f6ba546b191\nCo-Authored-By: Walter A. Boring IV <walter.boring@hp.com>\n'}]",3,94690,452d8ebe0f2ead1b8846d1719d373d05c6169c2d,30,9,3,9624,,,0,"Cinder stress and Continuous Hours (CHO) test

Stress test and Continous hours of operation test for cinder.

The Stress test runs a sequence of tests for n volumes:
    1. Create n volumes
    2. Create n snapshots
    3. Attach n volumes
    4. Detach n volumes
    5. Delete n snapshots
    6. Delete n volumes

The Continous hours test runs a random sequence of the
above actions for a specified ""minutes"" duration when specified
in the kwargs.

Change-Id: I559398d6ff716164712084170d569f6ba546b191
Co-Authored-By: Walter A. Boring IV <walter.boring@hp.com>
",git fetch https://review.opendev.org/openstack/tempest refs/changes/90/94690/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/stress/actions/cinder_stress.py', 'tempest/stress/etc/cinder-cho.json', 'tempest/stress/etc/cinder-stress.json']",3,3098123f27e9f3a35ce596fb9d1db8fdacb5095f,cinder-stress,"[{""action"": ""tempest.stress.actions.cinder_stress.CinderTest"", ""threads"": 1, ""use_admin"": false, ""use_isolated_tenants"": false, ""kwargs"": {""num_servers"": 1, ""num_volumes"": 5, ""cleanup"": true} } ] ",,555,0
openstack%2Ftempest~master~I6deb996fe2506e5659179eee7a7c0d46dd606568,openstack/tempest,master,I6deb996fe2506e5659179eee7a7c0d46dd606568,Test that CFN API works with os-collect-config,ABANDONED,2014-06-19 01:45:58.000000000,2014-12-05 15:05:59.000000000,,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 6488}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-06-19 01:45:58.000000000', 'files': ['requirements.txt', 'tempest/scenario/orchestration/test_os_collect_config.py', 'tempest/scenario/orchestration/structured_deployment.yaml'], 'web_link': 'https://opendev.org/openstack/tempest/commit/a9590611b89ad96c5476cd3cceab3fc06d297545', 'message': 'Test that CFN API works with os-collect-config\n\nThis test simulates the way os-collect-config would be called from\ninside of an instance from the local machine. That will allow us to run\nthis as part of the smoke tests rather than slow.\n\nIn the future we will add slow tests which expect this process to be\nduplicated inside the booted instance.\n\nChange-Id: I6deb996fe2506e5659179eee7a7c0d46dd606568\n'}]",0,101073,a9590611b89ad96c5476cd3cceab3fc06d297545,16,7,1,6488,,,0,"Test that CFN API works with os-collect-config

This test simulates the way os-collect-config would be called from
inside of an instance from the local machine. That will allow us to run
this as part of the smoke tests rather than slow.

In the future we will add slow tests which expect this process to be
duplicated inside the booted instance.

Change-Id: I6deb996fe2506e5659179eee7a7c0d46dd606568
",git fetch https://review.opendev.org/openstack/tempest refs/changes/73/101073/1 && git format-patch -1 --stdout FETCH_HEAD,"['requirements.txt', 'tempest/scenario/orchestration/test_os_collect_config.py', 'tempest/scenario/orchestration/structured_deployment.yaml']",3,a9590611b89ad96c5476cd3cceab3fc06d297545,,"heat_template_version: '2013-05-23' description: Uses software deployments and tests os-collect-config integration. parameters: key_name: Type: String flavor: Type: String image: Type: String network: Type: String timeout: Type: Number resources: smoke_security_group: type: OS::Neutron::SecurityGroup properties: description: Enable only ICMP and SSH access rules: - {protocol: icmp} - {protocol: tcp, port_range_min: 22, port_range_max: 22} smoke_server: type: OS::Nova::Server properties: image: {Ref: image} flavor: {Ref: flavor} key_name: {Ref: key_name} security_groups: [ {get_resource: smoke_security_group} ] networks: [ uuid: {get_param: network} ] user_data_format: SOFTWARE_CONFIG smoke_config: type: OS::Heat::StructuredConfig properties: config: root_level_dict: list: [1, 2, 3] string: 'the quick brown fox' int: 42 from_deploy: {get_input: the_input} smoke_deploy: type: OS::Heat::StructuredDeployment properties: signal_transport: NO_SIGNAL config: {get_resource: smoke_config} server: {get_resource: smoke_server} input_values: from_deploy: 'string from deploy' outputs: stdout: description: stdout of deployment value: get_attribute: [ smoke_deploy, stdout ] ",,117,0
openstack%2Ftempest~master~Ie0f9ec3bedbca0d39ebe3d6964b2c0a57014188c,openstack/tempest,master,Ie0f9ec3bedbca0d39ebe3d6964b2c0a57014188c,Test to verify updated description in alarm,ABANDONED,2014-06-09 11:49:22.000000000,2014-12-05 15:05:58.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 5174}, {'_account_id': 6167}, {'_account_id': 6553}, {'_account_id': 8556}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 10736}]","[{'number': 1, 'created': '2014-06-09 11:49:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f2d4890190159bc69634ea353569eb737c4954df', 'message': 'Added unit test for bug#1325436\n\nAdded test_create_update_get_verify_description_delete_alarm unit test case in test_telemetry_alarming_api.py script.\nIt creates an alarm with some specified threshold using default description then update threshold and then verify that default description  get updated accordingly.\n\nChange-Id: Ie0f9ec3bedbca0d39ebe3d6964b2c0a57014188c\nCloses-Bug: #1325436\n'}, {'number': 2, 'created': '2014-06-11 08:55:07.000000000', 'files': ['tempest/api/telemetry/test_telemetry_alarming_api.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/c1bde70d8c8b27d5b70bf18890b3574e2be81ac8', 'message': 'Test to verify updated description in alarm\n\nTest case creates a threshold-type-alarm with default values,\nperform update operation and update threshold, comparision operator, statistic,\nperiod and then verify if default description get updated according to the updated values.\n\nChange-Id: Ie0f9ec3bedbca0d39ebe3d6964b2c0a57014188c\nCloses-Bug: #1325436\n'}]",13,98751,c1bde70d8c8b27d5b70bf18890b3574e2be81ac8,19,9,2,10736,,,0,"Test to verify updated description in alarm

Test case creates a threshold-type-alarm with default values,
perform update operation and update threshold, comparision operator, statistic,
period and then verify if default description get updated according to the updated values.

Change-Id: Ie0f9ec3bedbca0d39ebe3d6964b2c0a57014188c
Closes-Bug: #1325436
",git fetch https://review.opendev.org/openstack/tempest refs/changes/51/98751/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/telemetry/test_telemetry_alarming_api.py'],1,f2d4890190159bc69634ea353569eb737c4954df,bug/1325436," @test.attr(type=""gate"") def test_create_update_get_verify_description_delete_alarm(self): # Create an alarm with default description alarm_name = data_utils.rand_name('telemetry_alarm') resp, body = self.telemetry_client.create_alarm( name=alarm_name, type='threshold', threshold_rule=self.rule) self.assertEqual(201, resp.status) self.assertEqual(alarm_name, body['name']) alarm_id = body['alarm_id'] self.assertDictContainsSubset(self.rule, body['threshold_rule']) # Update alarm with new rule and new name new_rule = {'meter_name': 'cpu', 'comparison_operator': 'eq', 'threshold': 70.0, 'statistic': 'avg', 'period': 60} updated_description = 'Alarm when %s is %s a %s of %d.0 over %d seconds' % \ (new_rule['meter_name'], new_rule['comparison_operator'], new_rule['statistic'], new_rule['threshold'], new_rule['period']) alarm_name = data_utils.rand_name('telemetry-alarm-update') resp, body = self.telemetry_client.update_alarm( alarm_id, threshold_rule=new_rule, name=alarm_name, type='threshold', description=body['description']) self.assertEqual(200, resp.status) self.assertEqual(alarm_name, body['name']) self.assertDictContainsSubset(new_rule, body['threshold_rule']) # Get and verify details of an alarm after update resp, body = self.telemetry_client.get_alarm(alarm_id) self.assertEqual(200, resp.status) self.assertEqual(alarm_name, body['name']) self.assertEqual(updated_description, body['description']) self.assertDictContainsSubset(new_rule, body['threshold_rule']) # Delete alarm and verify if deleted resp, _ = self.telemetry_client.delete_alarm(alarm_id) self.assertEqual(204, resp.status) self.assertRaises(exceptions.NotFound, self.telemetry_client.get_alarm, alarm_id)",,44,0
openstack%2Ftempest~master~Ibbde5d8380189b80f6490201eed5098face45fef,openstack/tempest,master,Ibbde5d8380189b80f6490201eed5098face45fef,Add test for LBaaS API version 2,ABANDONED,2014-07-11 18:31:33.000000000,2014-12-05 15:05:57.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 7010}, {'_account_id': 9008}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-07-11 18:31:33.000000000', 'files': ['tempest/api/network/test_load_balancer_v2.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/0757fb02f0ccb73bf1def33e6314ec7414eac8a1', 'message': 'Add test for LBaaS API version 2\n\nChange-Id: Ibbde5d8380189b80f6490201eed5098face45fef\n'}]",0,106462,0757fb02f0ccb73bf1def33e6314ec7414eac8a1,8,5,1,7010,,,0,"Add test for LBaaS API version 2

Change-Id: Ibbde5d8380189b80f6490201eed5098face45fef
",git fetch https://review.opendev.org/openstack/tempest refs/changes/62/106462/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/network/test_load_balancer_v2.py'],1,0757fb02f0ccb73bf1def33e6314ec7414eac8a1,lbaas,"# Copyright 2013 OpenStack Foundation # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from tempest.api.network import base from tempest.common.utils import data_utils from tempest import test class LoadBalancerV2TestJSON(base.BaseNetworkTest): _interface = 'json' """""" Tests the following operations in the Neutron API using the TEST client for Neutron: """""" @classmethod @test.safe_setup def setUpClass(cls): super(LoadBalancerV2TestJSON, cls).setUpClass() if not test.is_extension_enabled('lbaas', 'network'): msg = ""lbaas extension not enabled."" raise cls.skipException(msg) cls.network = cls.create_network() cls.name = cls.network['name'] cls.subnet = cls.create_subnet(cls.network) cls.address = cls.subnet['allocation_pools'][0]['end'] loadbalancer_name = data_utils.rand_name('loadbalancer-') listener_name = data_utils.rand_name('listener-') pool_name = data_utils.rand_name('pool-') resp, body = cls.client.create_loadbalancer( name=loadbalancer_name, vip_subnet=cls.subnet['id']) cls.loadbalancer = body['loadbalancer'] resp, body = cls.client.create_healthmonitor( type=""HTTP"", delay=4, timeout=1, max_retries=2) cls.healthmonitor = body['healthmonitor'] resp, body = cls.client.create_pool_v2( name=pool_name, protocol=""HTTP"", lb_algorithm=""ROUND_ROBIN"", healthmonitor_id=cls.healthmonitor['id']) cls.pool = body['pool'] resp, body = cls.client.create_member_v2( pool_id=cls.pool['id'], address=cls.address, protocol_port=8080, subnet_id=cls.subnet['id']) resp, body = cls.client.create_listener( name=listener_name, protocol=""HTTP"", protocol_port=80, default_pool_id=cls.pool['id'], load_balancer_id=cls.loadbalancer['id']) cls.listener = body['listener'] def _check_list_with_filter(self, obj_name, attr_exceptions, **kwargs): if obj_name in ['pool', 'member']: create_obj = getattr(self.client, 'create_' + obj_name + '_v2') delete_obj = getattr(self.client, 'delete_' + obj_name + '_v2') list_objs = getattr(self.client, 'list_' + obj_name + 's_v2') else: create_obj = getattr(self.client, 'create_' + obj_name) delete_obj = getattr(self.client, 'delete_' + obj_name) list_objs = getattr(self.client, 'list_' + obj_name + 's') resp, body = create_obj(**kwargs) self.assertEqual('201', resp['status']) obj = body[obj_name] self.addCleanup(delete_obj, obj['id']) for key, value in obj.iteritems(): # It is not relevant to filter by all arguments. That is why # there is a list of attr exceptions. if key not in attr_exceptions: resp, body = list_objs(**{key: value}) self.assertEqual('200', resp['status']) objs = [v[key] for v in body[obj_name + 's']] self.assertIn(value, objs) @test.attr(type='smoke') def test_list_loadbalancers(self): resp, body = self.client.list_loadbalancers() self.assertEqual('200', resp['status']) loadbalancers = body['loadbalancers'] self.assertIn(self.loadbalancer['id'], [b['id'] for b in loadbalancers]) @test.attr(type='smoke') def test_list_loadbalancers_with_filter(self): attr_exceptions = ['status', 'description'] self._check_list_with_filter('loadbalancer', attr_exceptions) @test.attr(type='smoke') def test_show_loadbalancer(self): resp, body = self.client.show_loadbalancer(self.loadbalancer['id']) self.assertEqual('200', resp['status']) loadbalancer = body['loadbalancer'] for key, value in loadbalancer.iteritems(): if key != 'status': self.assertEqual(self.loadbalancer[key], value) @test.attr(type='smoke') def test_create_update_delete_loadbalancer(self): # Create the loadbalancer name = data_utils.rand_name('old-name-') description = data_utils.rand_name('old-description-') resp, body = self.client.create_loadbalancer( name=old_name, description=old_description, admin_state_up=True) self.assertEqual('200', resp['status']) old_loadbalancer = body['loadbalancer'] # Update the loadbalancer updata_params = { 'name': data_utils.rand_name('new-name-'), 'description': data_utils.rand_name('new-description-'), 'admin_state_up': False } resp, body = self.client.update_loadbalancer(old_loadbalancer['id'], **update_params) self.assertEqual('200', resp['status']) # Get the updated loadbalancer resp, body = self.client.show_loadbalancer(old_loadbalancer['id']) self.assertEqual('200', resp['status']) new_loadbalancer = body['loadbalancer'] # Verify the update for key, value in update_params.iteritems(): self.assertIn(key, new_loadbalancer) self.assertEqual(value, new_loadbalancer[key]) # Delete the loadbalancer resp, body = self.client.delete_loadbalancer(new_loadbalancer['id']) self.assertEqual('204', resp['status']) @test.attr(type='smoke') def test_list_listeners(self): resp, body = self.client.list_listeners() self.assertEqual('200', resp['status']) listeners = body['listeners'] self.assertIn(self.listeners['id'], [s['id'] for s in listeners]) @test.attr(type='smoke') def test_list_listeners_with_filter(self): attr_exceptions = ['status', 'description'] self._check_list_with_filter('listener', attr_exceptions) @test.attr(type='smoke') def test_show_listener(self): resp, body = self.client.show_listener(self.listener['id']) self.assertEqual('200', resp['status']) listener = body['listener'] for key, value in listener.iteritems(): if key != 'status': self.assertEqual(self.listener[key], value) @test.attr(type='smoke') def test_create_update_delete_listener(self): # Create the listener old_name = data_utils.rand_name('old-name-') old_description = data_utils.rand_name('old-description-') resp, body = self.client.create_listener( name=old_name, description=old_description, admin_state_up=True) self.assertEqual('200', resp['status']) old_listener = body['listener'] # Update the listener update_params = { 'name': data_utils.rand_name('new-name-'), 'description': data_utils.rand_name('new-description-'), 'admin_state_up': False } resp, body = self.client.update_listener(old_listener['id'], **update_params) self.assertEqual('200', resp['status']) # Get the updated listener resp, body = self.client.show_listener(old_listener['id']) self.assertEqual('200', resp['status']) new_listener = body['listener'] # Verify the update for key, value in update_params.iteritems(): self.assertIn(key, new_listener) self.assertEqual(value, new_listener[key]) # Delete the listener resp, body = self.client.delete_listener(new_listener['id']) self.assertEqual('204', resp['status']) @test.attr(type='smoke') def test_list_pools_v2(self): resp, body = self.client.list_pools_v2() self.assertEqual('200', resp['status']) pools = body['pool'] self.assertIn(self.pool['id'], [p['id'] for p in pools]) @test.attr(type='smoke') def test_list_pools_with_filters_v2(self): attr_expcetions = ['status', 'description'] self._check_list_with_filter('pool', attr_exceptions) @test.attr(type='smoke') def test_show_pool_v2(self): resp, body = self.client.show_pool_v2(self.pool['id']) self.assertEqual('200', resp['status']) pool = body['pool'] for key, value in pool.iteritems(): if key != 'status': self.assertEqual(self.pool[key], value) @test.attr(type='smoke') def test_create_update_delete_pool_v2(self): # Create the pool old_name = data_utils.rand_name('old-name-') old_description = data_utils.rand_name('old-description-') resp, body = self.client.create_pool_v2( name=old_name, description=old_description, admin_state_up=True) self.assertEqual('200', resp['status']) old_pool = body['pool'] # Update the pool update_params = { 'name': data_utils.rand_name('new-name-'), 'description': data_utils.rand_name('new-description-'), 'admin_state_up': False } resp, body = self.client.update_pool_v2(old_pool['id'], **update_params) self.assertEqual('200', resp['status']) # Get the updated pool resp, body = self.client.show_pool_v2(old_pool['id']) self.assertEqual('200', resp['status']) new_pool = body['pool'] # Verify the update for key, value in update_params.iteritems(): self.assertIn(key, new_pool) self.assertEqual(value, new_pool[key]) # Delete the pool resp, body = self.client.delete_pool_v2(new_listener['id']) self.assertEqual('204', resp['status']) @test.attr(type='smoke') def test_list_members_v2(self): resp, body = self.client.list_members_v2() self.assertEqual('200', resp['status']) members = body['members'] self.assertIn(self.member['id'], [m['id'] for m in members]) @test.attr(type='smoke') def test_list_members_with_filter_v2(self): attr_exceptions = ['status'] self._check_list_with_filter('member', attr_exceptions) @test.attr(type='smoke') def test_show_member_v2(self): resp, body = self.client.show_member_v2(self.member['id']) self.assertEqual('200', resp['status']) member = body['member'] for key, value in member.iteritems(): if key != 'status': self.assertEqual(self.member[key], value) @test.attr(type='smoke') def test_create_update_delete_member_v2(self): # Create the member resp, body = self.client.create_member_v2( pool_id=self.pool['id'], subnet_id=self.subnet['id'], address=self.address, protocol_port=8181, weight=10, admin_state_up=True) self.assertEqual('200', resp['status']) old_member = body['member'] # Update the member update_params = {'weight': 20, 'admin_state_up': False} resp, body = self.client.update_member_v2(old_member['id'], **update_params) self.assertEqual('200', resp['status']) # Get the updated memeber resp, body = self.client.show_member_v2(old_member['id']) self.assertEqual('200', resp['status']) new_member = body['member'] # Verify the update for key, value in update_params.iteritems(): self.assertIn(key, new_member) self.asserEqual(value, new_member[key]) # Delete the member resp, body = self.client.delete_member_v2(new_member['id']) self.assertEqual('204', resp['status']) # Verify the delete resp, body = self.client.list_members_v2() members = [m['id'] for m in body['members']] self.assertNotIn(new_member['id'], members) @test.attr(type='smoke') def test_list_healthmonitors(self): resp, body = self.client.list_healthmonitors() self.assertEqual('200', resp['status']) healthmonitors = body['healthmonitors'] self.assertIn(self.healthmonitor['id'], [h['id'] for h in healthmonitors]) @test.attr(type='smoke') def test_list_healthmonitors_with_filters(self): attr_exceptions = ['status'] self._check_list_with_filter('healthmonitor', attr_exceptions) @test.attr(type='smoke') def test_show_healthmonitor(self): resp, body = self.client.show_healthmonitor(self.healthmonitor['id']) self.assertEqual('200', resp['status']) healthmonitor = body['healthmonitor'] for key, value in healthmonitor.iteritems(): if key != 'status': self.assertEqual(self.healthmonitor[key], value) @test.attr(type='smoke') def test_create_update_delete_healthmonitor(self): # Create the healthmonitor resp, body = self.client.create_healthmonitor( type=""TCP"", delay=5, timeout=4, max_retries=3) self.assertEqual('200', resp['status']) old_healthmonitor = body['healthmonitor'] # Update the healthmonitor update_params = { 'delay': 9, 'timeout': 8, 'max_retries': 7, 'http_method': ""POST"", 'url_path': ""/healthcheck"", 'expected_error_codes': ""404"", 'admin_state_up': False } resp, body = self.client.update_healthmonitor( old_healthmonitor['id'], **new_healthmonitor) self.assertEqual('200', resp['status']) # Get the updated healthmonitor resp, body = self.client.show_healthmonitor(old_healthmonitor['id']) self.assertEqual('200', resp['status']) new_healthmonitor = body['healthmonitor'] # Verify the update for key, value in update_params.iteritems(): self.assertIn(key, new_healthmonitor) self.assertEqual(value, new_healthmonitor[key]) # Delete the healthmonitor resp, body = self.client.delete_healthmonitor(new_healthmonitor['id']) self.assertEqual('204', resp['status']) # Verify the delete resp, body = self.client.list_healthmonitors() healthmonitors = [h['id'] for h in body['healthmonitors']] self.assertNotIn(new_healthmonitor['id'], healthmonitors) class LoadBalancerV2TestXML(LoadBalancerV2TestJSON): _interface = 'xml' ",,403,0
openstack%2Ftempest~master~I6651081d5033aa94ca05ae98365d5db7e277a0ae,openstack/tempest,master,I6651081d5033aa94ca05ae98365d5db7e277a0ae,Fixes for test_swift_basic_ops multiregion support,ABANDONED,2014-06-26 13:44:10.000000000,2014-12-05 15:05:56.000000000,,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 4656}, {'_account_id': 8871}, {'_account_id': 9392}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-06-26 13:44:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5e3cdc1fbe090c26098967f878e4f4d9dcd047d5', 'message': 'Fixes for test_swift_basic_ops multiregion support\n\nThe change was to pass region information into swiftclient.\n\nChange-Id: I6651081d5033aa94ca05ae98365d5db7e277a0ae\nCloses-Bug: #1334650\n'}, {'number': 2, 'created': '2014-06-30 16:16:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/12bc99788a8661ad4aee1335bbb307839cee4f95', 'message': 'Fixes for test_swift_basic_ops multiregion support\n\nThe change was to pass region information into swiftclient.\n\nChange-Id: I6651081d5033aa94ca05ae98365d5db7e277a0ae\nCloses-Bug: #1334650\n'}, {'number': 3, 'created': '2014-06-30 18:37:34.000000000', 'files': ['tempest/clients.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/4effa8d83ea822338ccea0934bd39b01b708bbca', 'message': 'Fixes for test_swift_basic_ops multiregion support\n\nThe change was to pass region information into swiftclient.\n\nChange-Id: I6651081d5033aa94ca05ae98365d5db7e277a0ae\nCloses-Bug: #1334650\n'}]",7,102827,4effa8d83ea822338ccea0934bd39b01b708bbca,34,8,3,9392,,,0,"Fixes for test_swift_basic_ops multiregion support

The change was to pass region information into swiftclient.

Change-Id: I6651081d5033aa94ca05ae98365d5db7e277a0ae
Closes-Bug: #1334650
",git fetch https://review.opendev.org/openstack/tempest refs/changes/27/102827/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/clients.py'],1,5e3cdc1fbe090c26098967f878e4f4d9dcd047d5,bug/1334650," region = CONF.identity.region os_options = {'endpoint_type': endpoint_type, 'region_name': region}", os_options = {'endpoint_type': endpoint_type},2,1
openstack%2Ftempest~master~Ic95ab6b0592f1fd1e6efe8b3842dc41b2b109eb8,openstack/tempest,master,Ic95ab6b0592f1fd1e6efe8b3842dc41b2b109eb8,Add support to Neutron Group Policies extension,ABANDONED,2014-07-20 03:43:35.000000000,2014-12-05 15:05:55.000000000,,"[{'_account_id': 3}, {'_account_id': 490}, {'_account_id': 2750}, {'_account_id': 4694}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-07-20 03:43:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/291cf5f44c29fd11973ab4097f54e6eb52a59126', 'message': 'Add support to Neutron Group Policies extension\n\nThe ReST client for Neutron is modified to handle the calls for the Group\nPolicies API extension\n\nChange-Id: Ic95ab6b0592f1fd1e6efe8b3842dc41b2b109eb8\nPartially-implements: blueprint group-based-policy-abstraction\n'}, {'number': 2, 'created': '2014-07-20 21:51:21.000000000', 'files': ['tempest/services/network/network_client_base.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/bebf0480af93d24e098265b6c59c8c94b096296f', 'message': 'Add support to Neutron Group Policies extension\n\nThe ReST client for Neutron is modified to handle the calls for the Group\nPolicies API extension\n\nChange-Id: Ic95ab6b0592f1fd1e6efe8b3842dc41b2b109eb8\nPartially-implements: blueprint group-based-policy-abstraction\n'}]",2,108222,bebf0480af93d24e098265b6c59c8c94b096296f,15,6,2,4694,,,0,"Add support to Neutron Group Policies extension

The ReST client for Neutron is modified to handle the calls for the Group
Policies API extension

Change-Id: Ic95ab6b0592f1fd1e6efe8b3842dc41b2b109eb8
Partially-implements: blueprint group-based-policy-abstraction
",git fetch https://review.opendev.org/openstack/tempest refs/changes/22/108222/2 && git format-patch -1 --stdout FETCH_HEAD,['tempest/services/network/network_client_base.py'],1,291cf5f44c29fd11973ab4097f54e6eb52a59126,bp/group-based-policy-abstraction," 'firewalls': 'fw', 'endpoints': 'grouppolicy', 'endpoint_groups': 'grouppolicy', 'l2_policies': 'grouppolicy', 'l3_policies': 'grouppolicy', 'firewall_policy': 'firewall_policies', 'l2_policy': 'l2_policies', 'l3_policy': 'l3_policies'", 'firewalls': 'fw' 'firewall_policy': 'firewall_policies',8,2
openstack%2Ftempest~master~Ibf4a0bc04e46cdd9c474e1e89673d9d5a773a7c5,openstack/tempest,master,Ibf4a0bc04e46cdd9c474e1e89673d9d5a773a7c5,Neutron Group Policy extension tests,ABANDONED,2014-07-20 10:41:16.000000000,2014-12-05 15:05:54.000000000,,"[{'_account_id': 3}, {'_account_id': 490}, {'_account_id': 2750}, {'_account_id': 4694}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9828}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-07-20 10:41:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c108f3a4a42ff9cfc7a15c3e1eaeacd9458b1041', 'message': 'Neutron Group Policy extension tests\n\nAdding CRUD test cases for Neutron Group Policy extension.\n\nChange-Id: Ibf4a0bc04e46cdd9c474e1e89673d9d5a773a7c5\nPartially-implements: blueprint group-based-policy-abstraction\n'}, {'number': 2, 'created': '2014-07-21 04:43:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c17ec19f473f8c55a6577c8cb410c389fc85e4cd', 'message': 'Neutron Group Policy extension tests\n\nAdding CRUD test cases for Neutron Group Policy extension.\n\nChange-Id: Ibf4a0bc04e46cdd9c474e1e89673d9d5a773a7c5\nPartially-implements: blueprint group-based-policy-abstraction\n'}, {'number': 3, 'created': '2014-07-21 05:51:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/1a7dbd0d4cd6f74b31ceda153e9f9119f6ac418e', 'message': 'Neutron Group Policy extension tests\n\nAdding CRUD test cases for Neutron Group Policy extension.\n\nChange-Id: Ibf4a0bc04e46cdd9c474e1e89673d9d5a773a7c5\nPartially-implements: blueprint group-based-policy-abstraction\n'}, {'number': 4, 'created': '2014-07-21 06:44:21.000000000', 'files': ['tempest/services/network/network_client_base.py', 'tempest/api/network/base.py', 'tempest/api/network/test_group_policy_extensions.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/3f78b763214c1df49c430991ba07c063fdc91d4f', 'message': 'Neutron Group Policy extension tests\n\nAdding CRUD test cases for Neutron Group Policy extension.\n\nChange-Id: Ibf4a0bc04e46cdd9c474e1e89673d9d5a773a7c5\nPartially-implements: blueprint group-based-policy-abstraction\n'}]",21,108234,3f78b763214c1df49c430991ba07c063fdc91d4f,31,9,4,490,,,0,"Neutron Group Policy extension tests

Adding CRUD test cases for Neutron Group Policy extension.

Change-Id: Ibf4a0bc04e46cdd9c474e1e89673d9d5a773a7c5
Partially-implements: blueprint group-based-policy-abstraction
",git fetch https://review.opendev.org/openstack/tempest refs/changes/34/108234/2 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/services/network/network_client_base.py', 'tempest/api/network/base.py', 'tempest/api/network/test_group_policy_extensions.py']",3,c108f3a4a42ff9cfc7a15c3e1eaeacd9458b1041,bp/group-based-policy-abstraction,"# Copyright 2013 OpenStack Foundation # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from tempest.api.network import base from tempest.common.utils import data_utils from tempest import config from tempest import exceptions from tempest import test CONF = config.CONF class GroupPolicyTestJSON(base.BaseAdminNetworkTest): _interface = 'json' """""" Tests the following operations in the Neutron API using the REST client for Neutron: List, Show, Create, Delete, and Update Endpoint List, Show, Create, Delete, and Update Endpoint Group List, Show, Create, Delete, and Update L2 Policy List, Show, Create, Delete, and Update L3 Policy """""" @classmethod @test.safe_setup def setUpClass(cls): if not test.is_extension_enabled('group-policy', 'network'): msg = ""group-policy extension not enabled."" raise cls.skipException(msg) super(GroupPolicyTestJSON, cls).setUpClass() cls.endpoint_group = cls.create_endpoint_group() def _delete_endpoint_group(self, endpoint_group_id): resp, _ = self.client.delete_endpoint_group(endpoint_group_id) self.assertEqual('204', resp['status']) # Asserting if endpoint_group is found in the list after deletion _, body = self.client.list_endpoint_groups() endpoint_groups = [epg['id'] for epg in body['endpoint_groups']] self.assertNotIn(endpoint_group_id, endpoint_groups) @test.attr(type='smoke') def test_list_endpoint_groups(self): # Verify the EPG exists in the list of all EPGs resp, body = self.client.list_endpoint_groups() self.assertEqual('200', resp['status']) endpoint_groups = body['endpoint_groups'] self.assertIn(self.endpoint_group['id'], [epg['id'] for epg in endpoint_groups]) @test.attr(type='smoke') def test_create_update_delete_endpoint_group(self): # Creates an EPG and sets up deletion name = data_utils.rand_name('endpoint-group') resp, body = self.client.create_endpoint_group(name=name) self.assertEqual('201', resp['status']) endpoint_group = body['endpoint_group'] self.addCleanup(self._delete_endpoint_group, endpoint_group['id']) # Assert if created endpoint_groups are not found in endpoint_groups list resp, body = self.client.list_endpoint_groups() endpoint_groups = [epg['id'] for epg in body['endpoint_groups']] self.assertIsNotNone(endpoint_group['id']) self.assertIn(endpoint_group['id'], endpoint_groups) @test.attr(type='smoke') def test_show_endpoint_group(self): # Verifies the details of a vpn service resp, body = self.client.show_endpoint_group(self.endpoint_group['id']) self.assertEqual('200', resp['status']) endpoint_group = body['endpoint_group'] self.assertEqual(self.endpoint_group['id'], endpoint_group['id']) self.assertEqual(self.endpoint_group['name'], endpoint_group['name']) self.assertEqual(self.endpoint_group['description'], endpoint_group['description']) self.assertEqual(endpoint_group['l2_policy_id'], endpoint_group['l2_policy_id']) self.assertEqual(self.endpoint_group['endpoints'], endpoint_group['endpoints']) self.assertEqual(self.endpoint_group['tenant_id'], endpoint_group['tenant_id']) self.assertEqual(self.endpoint_group['provided_contracts'], endpoint_group['provided_contracts']) self.assertEqual(self.endpoint_group['consumed_contracts'], endpoint_group['consumed_contracts']) ",,112,1
openstack%2Ftempest~master~I65d4d2bea79e1a8641bb09d8035f14003ba2737f,openstack/tempest,master,I65d4d2bea79e1a8641bb09d8035f14003ba2737f,Add alarm scenario test,ABANDONED,2014-06-19 11:59:43.000000000,2014-12-05 15:05:53.000000000,,"[{'_account_id': 3}, {'_account_id': 2284}, {'_account_id': 2750}, {'_account_id': 3012}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 10488}]","[{'number': 1, 'created': '2014-06-19 11:59:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/cf5d12c4b9a46b3beca10bf02d2815cf44504f10', 'message': ""Add alarm scenario test\n\nThis test checks alarm functionality.\n\nThe following is the scenario outline:\n 1. Create alarm which triggered to alarm and send\n    callback by alarm_actions if in last period\n    more then one server had been created.\n 2. Create one server.\n 3. Check that alarm triggered to state 'ok'.\n 4. Create two servers.\n 5. Check that alarm triggered to state 'alarm'.\n 6. Check that alarms callback received.\n\nblueprint add-ceilometer-scenario-tests\n\nChange-Id: I65d4d2bea79e1a8641bb09d8035f14003ba2737f\n""}, {'number': 2, 'created': '2014-06-19 12:03:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/bb59976845e984f3b72c60e6428593fd6fa89496', 'message': ""Add alarm scenario test\n\nThis test checks alarm functionality.\n\nThe following is the scenario outline:\n 1. Create alarm which triggered to alarm and send\n    callback by alarm_actions if in last period\n    more then one server had been created.\n 2. Create one server.\n 3. Check that alarm triggered to state 'ok'.\n 4. Create two servers.\n 5. Check that alarm triggered to state 'alarm'.\n 6. Check that alarms callback received.\n\nblueprint add-ceilometer-scenario-tests\n\nChange-Id: I65d4d2bea79e1a8641bb09d8035f14003ba2737f\n""}, {'number': 3, 'created': '2014-06-26 14:51:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ec7245e405d6c025a1aedd0303bbc1f1bb163334', 'message': ""Add alarm scenario test\n\nThis test checks alarm functionality.\n\nThe following is the scenario outline:\n 1. Create alarm which triggered to 'alarm' state and send\n    callback by alarm_actions if in last period\n    more then one server had been created.\n 2. Create one server.\n 3. Check that alarm triggered to state 'ok'.\n 4. Create two servers.\n 5. Check that alarm triggered to state 'alarm'.\n 6. Check that alarms callback received.\n\nblueprint add-ceilometer-scenario-tests\n\nChange-Id: I65d4d2bea79e1a8641bb09d8035f14003ba2737f\n""}, {'number': 4, 'created': '2014-06-26 15:52:02.000000000', 'files': ['tempest/scenario/test_threshold_alarm.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/6cce083ab776511b53a522abca361edef53ce88f', 'message': ""Add alarm scenario test\n\nThis test checks alarm functionality.\n\nThe following is the scenario outline:\n 1. Create alarm which triggered to 'alarm' state and send\n    callback by alarm_actions if in last period\n    more then one server had been created.\n 2. Create one server.\n 3. Check that alarm triggered to state 'ok'.\n 4. Create two servers.\n 5. Check that alarm triggered to state 'alarm'.\n 6. Check that alarms callback received.\n\nblueprint add-ceilometer-scenario-tests\n\nChange-Id: I65d4d2bea79e1a8641bb09d8035f14003ba2737f\n""}]",3,101186,6cce083ab776511b53a522abca361edef53ce88f,34,9,4,10488,,,0,"Add alarm scenario test

This test checks alarm functionality.

The following is the scenario outline:
 1. Create alarm which triggered to 'alarm' state and send
    callback by alarm_actions if in last period
    more then one server had been created.
 2. Create one server.
 3. Check that alarm triggered to state 'ok'.
 4. Create two servers.
 5. Check that alarm triggered to state 'alarm'.
 6. Check that alarms callback received.

blueprint add-ceilometer-scenario-tests

Change-Id: I65d4d2bea79e1a8641bb09d8035f14003ba2737f
",git fetch https://review.opendev.org/openstack/tempest refs/changes/86/101186/2 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/test_threshold_alarm.py'],1,cf5d12c4b9a46b3beca10bf02d2815cf44504f10,bp/add-ceilometer-scenario-tests,"# Copyright 2014 Mirantis.inc # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import socket import threading import time from tempest.common.utils import data_utils from tempest import config from tempest.scenario import manager from tempest import test CONF = config.CONF class TestThresholdAlarm(manager.OfficialClientTest): """""" This test checks alarm functionality. The following is the scenario outline: 1. Create alarm which triggered to alarm and send callback by alarm_actions if in last period more then one server had been created. 2. Create one server. 3. Check that alarm triggered to state 'ok'. 4. Create two servers. 5. Check that alarm triggered to state 'alarm'. 6. Check that alarms callback received. """""" THRESHOLD_PERIOD = 30 PORT = 61482 alarm_callback_received = False @classmethod def SetUpClass(cls): super(TestThresholdAlarm, cls).setUpClass() if cls.ceilometer_client is not None: skip_msg = (""%s skip as telemetry is not available"" % cls.__name__) raise cls.skipException(skip_msg) cls.alarm_callback_received = False def cleanup_wrapper(self, resource): self.cleanup_resource(resource, self.__class__.__name__) def _create_server(self): create_kwargs = {} server = self.create_server(create_kwargs=create_kwargs) self.addCleanup(self.cleanup_wrapper, server) return server def _create_alarm(self): create_kwargs = { 'alarm_actions': [""http://localhost:%s/"" % self.PORT, ], 'period': self.THRESHOLD_PERIOD, 'evaluation_periods': 1, 'threshold': 2, 'statistic': 'sum', 'comparison_operator': 'ge', 'meter_name': 'instance', 'matching_metadata': { 'metadata.event_type': 'compute.instance.create.end', }, } alarm = self.ceilometer_client.alarms.create( name=data_utils.rand_name('threshold_alarm'), type='threshold', **create_kwargs) self.addCleanup(self.cleanup_wrapper, alarm) return alarm def _is_alarm_state_triggered(self, alarm, expected_state): for i in range(2 * self.THRESHOLD_PERIOD): state = alarm.get_state() if state == expected_state: return True break time.sleep(1) return False @test.attr(type='gate') @test.services('compute', 'ceilometer') def test_threshold_alarm(self): alarm = self._create_alarm() self.assertIsNotNone(alarm, ""Alarm is not created"") self.assertIsNotNone(self._create_server(), ""Server is not created"") self.assertTrue(self._is_alarm_state_triggered(alarm, 'ok'), ""Alarm is not triggered to state 'ok'"") self.assertIsNotNone(self._create_server(), ""Server is not created"") self.assertIsNotNone(self._create_server(), ""Server is not created"") thread = threading.Thread(target=self._receive_alarm_callback, args=(self, alarm)) thread.start() self.assertTrue(thread.isAlive(), ""Alarm callback receiver is not running"") self.assertTrue(self._is_alarm_state_triggered(alarm, 'alarm'), ""Alarm is not triggered to state 'alarm'"") self.assertTrue(self.alarm_callback_received, ""Alarm callback is not received"") def _receive_alarm_callback(*args): testClass = args[1] alarm = args[2] socket.setdefaulttimeout(2.5 * testClass.THRESHOLD_PERIOD) sock = socket.socket() sock.bind(('', testClass.PORT)) sock.listen(1) conn, addr = sock.accept() data = conn.recv(1024) if alarm.alarm_id in data: testClass.alarm_callback_received = True conn.close() ",,131,0
openstack%2Ftempest~master~I805b0a3f5de65a4931abb23ab9e467857766f08b,openstack/tempest,master,I805b0a3f5de65a4931abb23ab9e467857766f08b,ran pep8 Change-Id: Ie1c90b7e17521e1c36c63a2c66e10e8c31f903c1,ABANDONED,2014-07-23 10:56:26.000000000,2014-12-05 15:05:52.000000000,,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-07-23 10:56:26.000000000', 'files': ['tempest/scenario/test_network_multi_tenant.py', 'tempest/scenario/managerMulti.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/4d747d70cff9d974cce6ffccc77a1415361b4697', 'message': 'ran pep8 Change-Id: Ie1c90b7e17521e1c36c63a2c66e10e8c31f903c1\n\nChange-Id: I805b0a3f5de65a4931abb23ab9e467857766f08b\n'}]",0,108953,4d747d70cff9d974cce6ffccc77a1415361b4697,9,6,1,1868,,,0,"ran pep8 Change-Id: Ie1c90b7e17521e1c36c63a2c66e10e8c31f903c1

Change-Id: I805b0a3f5de65a4931abb23ab9e467857766f08b
",git fetch https://review.opendev.org/openstack/tempest refs/changes/53/108953/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/scenario/test_network_multi_tenant.py', 'tempest/scenario/managerMulti.py']",2,4d747d70cff9d974cce6ffccc77a1415361b4697,tempest-892839,"# Copyright 2013 Hewlett-Packard Development Company, L.P. class OfficialClientTestA(tempest.test.BaseTestCase): class NetworkScenarioMultitATest(OfficialClientTestA): rules = self._create_loginable_secgroup_rule_neutronA(\ secgroup=secgroup) rules = self._create_loginable_secgroup_rule_neutronB(\ secgroup=secgroup)",# Copyright 2013 IBM Corp.class OfficialClientTestA(tempest.test.BaseTestCase): class NetworkScenarioMultitATest(OfficialClientTestA): rules = self._create_loginable_secgroup_rule_neutronA(secgroup=secgroup) rules = self._create_loginable_secgroup_rule_neutronB(secgroup=secgroup) ,16,17
openstack%2Ftempest~master~I861f2626cce3b325f24a5aadd67436722ce48e96,openstack/tempest,master,I861f2626cce3b325f24a5aadd67436722ce48e96,"Make tempest rely on _member_ role, not Member",ABANDONED,2014-06-15 22:24:35.000000000,2014-12-05 15:05:51.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 6772}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10234}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-06-15 22:24:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0b33df9e39c49ce3f65a250ebce84e67a3e43649', 'message': 'Make tempest rely on _member_ role, not Member\n\nKeystone has created a role named _member_ since the Grizzly release,\nHorizon added support for using this as the default role in Havana.\n\nDevstack has not been updated to use _member_, instead creating its own.\nAs a result devstack installations have both _member_ and Member roles\ndefined.\n\nThis change updates Tempest to use the _member_ role, which is a\nprerequisite to removing the code in devstack that creates the Member\nrole.\n\nCloses-Bug: #1330132\nChange-Id: I861f2626cce3b325f24a5aadd67436722ce48e96\n'}, {'number': 2, 'created': '2014-06-28 01:26:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f7114b868607e113b9f3f7936062ae4b652ce38c', 'message': 'Make tempest rely on _member_ role, not Member\n\nKeystone has created a role named _member_ since the Grizzly release,\nHorizon added support for using this as the default role in Havana.\n\nDevstack has not been updated to use _member_, instead creating its own.\nAs a result devstack installations have both _member_ and Member roles\ndefined.\n\nThis change updates Tempest to use the _member_ role, which is a\nprerequisite to removing the code in devstack that creates the Member\nrole.\n\nCloses-Bug: #1330132\nChange-Id: I861f2626cce3b325f24a5aadd67436722ce48e96\n'}, {'number': 3, 'created': '2014-06-28 01:35:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8a85b58438a58344c39e7d15cc50a6d6caa11ef7', 'message': 'Make tempest rely on _member_ role, not Member\n\nKeystone has created a role named _member_ since the Grizzly release,\nHorizon added support for using this as the default role in Havana.\n\nDevstack has not been updated to use _member_, instead creating its own.\nAs a result devstack installations have both _member_ and Member roles\ndefined.\n\nThis change updates Tempest to use the _member_ role, which is a\nprerequisite to removing the code in devstack that creates the Member\nrole.\n\nCloses-Bug: #1330132\nChange-Id: I861f2626cce3b325f24a5aadd67436722ce48e96\n'}, {'number': 4, 'created': '2014-07-26 01:44:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/597b9afb7bf05c9a875264d8d881ce578bdf1c59', 'message': 'Make tempest rely on _member_ role, not Member\n\nKeystone has created a role named _member_ since the Grizzly release,\nHorizon added support for using this as the default role in Havana.\n\nDevstack has not been updated to use _member_, instead creating its own.\nAs a result devstack installations have both _member_ and Member roles\ndefined.\n\nThis change updates Tempest to use the _member_ role, which is a\nprerequisite to removing the code in devstack that creates the Member\nrole.\n\nCloses-Bug: #1330132\nChange-Id: I861f2626cce3b325f24a5aadd67436722ce48e96\n'}, {'number': 5, 'created': '2014-07-26 02:13:15.000000000', 'files': ['tempest/cmd/javelin.py', 'tempest/common/isolated_creds.py', 'etc/tempest.conf.sample', 'tempest/config.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/7273ca88ef07f1dcf00135d5c6714f4c376b0ce5', 'message': 'Make tempest rely on _member_ role, not Member\n\nKeystone has created a role named _member_ since the Grizzly release,\nHorizon added support for using this as the default role in Havana.\n\nDevstack has not been updated to use _member_, instead creating its own.\nAs a result devstack installations have both _member_ and Member roles\ndefined.\n\nThis change updates Tempest to use the _member_ role, which is a\nprerequisite to removing the code in devstack that creates the Member\nrole.\n\nCloses-Bug: #1330132\nChange-Id: I861f2626cce3b325f24a5aadd67436722ce48e96\n'}]",5,100113,7273ca88ef07f1dcf00135d5c6714f4c376b0ce5,31,7,5,6772,,,0,"Make tempest rely on _member_ role, not Member

Keystone has created a role named _member_ since the Grizzly release,
Horizon added support for using this as the default role in Havana.

Devstack has not been updated to use _member_, instead creating its own.
As a result devstack installations have both _member_ and Member roles
defined.

This change updates Tempest to use the _member_ role, which is a
prerequisite to removing the code in devstack that creates the Member
role.

Closes-Bug: #1330132
Change-Id: I861f2626cce3b325f24a5aadd67436722ce48e96
",git fetch https://review.opendev.org/openstack/tempest refs/changes/13/100113/4 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/cmd/javelin.py', 'etc/tempest.conf.sample', 'tempest/config.py']",3,0b33df9e39c49ce3f65a250ebce84e67a3e43649,," default='_member_',"," default='Member',",3,3
openstack%2Ftempest~master~I4562a80620f31bea79bbbdbb7157dccf6bfcfef1,openstack/tempest,master,I4562a80620f31bea79bbbdbb7157dccf6bfcfef1,Add orchestration scenario test for stack-abandon and stack-adopt,ABANDONED,2014-06-27 17:23:50.000000000,2014-12-05 15:05:49.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 4328}, {'_account_id': 4571}, {'_account_id': 6488}, {'_account_id': 6498}, {'_account_id': 6899}, {'_account_id': 7293}, {'_account_id': 8576}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10068}, {'_account_id': 10090}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 10856}, {'_account_id': 11956}, {'_account_id': 12437}]","[{'number': 1, 'created': '2014-06-27 17:23:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/3fd25cd6240c0ffde5db713bbd2a1ffa04cb4e0c', 'message': 'Add orchestration scenario test for stack-abandon and stack-adopt\n\n- Creates a stack with multiple resources\n- Abandon the stack and verify that stack is deleted but resources exists\n- Adopt the stack back and verify all the resources are adopted\n- Delete the adopted stack and verify all the resources are deleted\n\nSome refactoring is done for the earlier test case to make code more\nreusable\n\nChange-Id: I4562a80620f31bea79bbbdbb7157dccf6bfcfef1\nCloses-Bug: #1324406\n'}, {'number': 2, 'created': '2014-06-27 17:27:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/21d2c305e8112e42ce5bfca978f21fa712b4063e', 'message': 'Add orchestration scenario test for stack-abandon and stack-adopt\n\n- Creates a stack with multiple resources\n- Abandon the stack and verify that stack is deleted but resources exists\n- Adopt the stack back and verify all the resources are adopted\n- Delete the adopted stack and verify all the resources are deleted\n\nSome refactoring is done for the earlier test case to make code more\nreusable\n\nChange-Id: I4562a80620f31bea79bbbdbb7157dccf6bfcfef1\nCloses-Bug: #1324406\n'}, {'number': 3, 'created': '2014-06-27 17:30:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d2e6fcbfaa10aa0b80977355769e285cb5a15280', 'message': 'Add orchestration scenario test for stack-abandon and stack-adopt\n\n- Creates a stack with multiple resources\n- Abandon the stack and verify that stack is deleted but resources exists\n- Adopt the stack back and verify all the resources are adopted\n- Delete the adopted stack and verify all the resources are deleted\n\nSome refactoring is done for the earlier test case to make code more\nreusable\n\nChange-Id: I4562a80620f31bea79bbbdbb7157dccf6bfcfef1\nCloses-Bug: #1324406\n'}, {'number': 4, 'created': '2014-06-27 18:31:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/b8e6664afbe5089082d31602b58a4c8927be805b', 'message': 'Add orchestration scenario test for stack-abandon and stack-adopt\n\n- Creates a stack with multiple resources\n- Abandon the stack and verify that stack is deleted but resources exists\n- Adopt the stack back and verify all the resources are adopted\n- Delete the adopted stack and verify all the resources are deleted\n\nSome refactoring is done in earlier test cases to make code more\nreusable\n\nChange-Id: I4562a80620f31bea79bbbdbb7157dccf6bfcfef1\nCloses-Bug: #1324406\n'}, {'number': 5, 'created': '2014-07-03 09:37:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/31d8906b2151f1072ad04ea98344e2a42761b44a', 'message': 'Add orchestration scenario test for stack-abandon and stack-adopt\n\n- Creates a stack with multiple resources\n- Abandon the stack and verify that stack is deleted but resources exists\n- Adopt the stack back and verify all the resources are adopted\n- Delete the adopted stack and verify all the resources are deleted\n\nSome refactoring is done in earlier test cases to make code more\nreusable\n\nChange-Id: I4562a80620f31bea79bbbdbb7157dccf6bfcfef1\nCloses-Bug: #1324406\n'}, {'number': 6, 'created': '2014-07-03 14:29:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5f77b219b650280b03fbe8e085050eff1c45e90c', 'message': 'Add orchestration scenario test for stack-abandon and stack-adopt\n\n- Creates a stack with multiple resources\n- Abandon the stack and verify that stack is deleted but resources exists\n- Adopt the stack back and verify all the resources are adopted\n- Delete the adopted stack and verify all the resources are deleted\n\nChange-Id: I4562a80620f31bea79bbbdbb7157dccf6bfcfef1\nCloses-Bug: #1324406\n'}, {'number': 7, 'created': '2014-07-04 06:51:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/2942aca8a4ffb7217deab077747329b8b1fdc9a9', 'message': 'Add orchestration scenario test for stack-abandon and stack-adopt\n\n- Creates a stack with multiple resources\n- Abandon the stack and verify that stack is deleted but resources exists\n- Adopt the stack back and verify all the resources are adopted\n- Delete the adopted stack and verify all the resources are deleted\n\nChange-Id: I4562a80620f31bea79bbbdbb7157dccf6bfcfef1\nCloses-Bug: #1324406\n'}, {'number': 8, 'created': '2014-07-04 07:05:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/cb46c097b9cd8ef4cd972da9d3e347d95e0ae749', 'message': 'Add orchestration scenario test for stack-abandon and stack-adopt\n\n- Creates a stack with multiple resources\n- Abandon the stack and verify that stack is deleted but resources exists\n- Adopt the stack back and verify all the resources are adopted\n- Delete the adopted stack and verify all the resources are deleted\n\nChange-Id: I4562a80620f31bea79bbbdbb7157dccf6bfcfef1\nCloses-Bug: #1324406\n'}, {'number': 9, 'created': '2014-07-15 11:14:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/4a5f1365abe69baa3f57bc74fb5737830f55ff40', 'message': 'Add orchestration scenario test for stack-abandon and stack-adopt\n\n- Creates a stack with multiple resources\n- Abandon the stack and verify that stack is deleted but resources exists\n- Adopt the stack back and verify all the resources are adopted\n- Delete the adopted stack and verify all the resources are deleted\n\nChange-Id: I4562a80620f31bea79bbbdbb7157dccf6bfcfef1\nCloses-Bug: #1324406\n'}, {'number': 10, 'created': '2014-07-15 11:23:53.000000000', 'files': ['tempest/scenario/orchestration/templates/test_stack_abandon_and_adopt.yaml', 'tempest/scenario/orchestration/test_stack_abandon_and_adopt.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/30e4124fa8c84f1006583e23eca169193c9e8f92', 'message': 'Add orchestration scenario test for stack-abandon and stack-adopt\n\n- Creates a stack with multiple resources\n- Abandon the stack and verify that stack is deleted but resources exists\n- Adopt the stack back and verify all the resources are adopted\n- Delete the adopted stack and verify all the resources are deleted\n\nChange-Id: I4562a80620f31bea79bbbdbb7157dccf6bfcfef1\nCloses-Bug: #1324406\n'}]",19,103198,30e4124fa8c84f1006583e23eca169193c9e8f92,73,18,10,11956,,,0,"Add orchestration scenario test for stack-abandon and stack-adopt

- Creates a stack with multiple resources
- Abandon the stack and verify that stack is deleted but resources exists
- Adopt the stack back and verify all the resources are adopted
- Delete the adopted stack and verify all the resources are deleted

Change-Id: I4562a80620f31bea79bbbdbb7157dccf6bfcfef1
Closes-Bug: #1324406
",git fetch https://review.opendev.org/openstack/tempest refs/changes/98/103198/3 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/scenario/orchestration/templates/cfn_init_signal.yaml', 'tempest/scenario/orchestration/templates/test_stack_abandon_and_adopt.yaml', 'tempest/scenario/orchestration/test_server_cfn_init.py', 'tempest/scenario/manager.py', 'tempest/scenario/orchestration/test_stack_abandon.py', 'tempest/scenario/orchestration/templates/test_autoscaling.yaml', 'tempest/scenario/orchestration/test_autoscaling.py']",7,3fd25cd6240c0ffde5db713bbd2a1ffa04cb4e0c,103198," self.template = self._load_template('test_autoscaling.yaml') self.create_stack(self.stack_name, self.template, self.parameters)"," def assign_keypair(self): self.stack_name = self._stack_rand_name() if CONF.orchestration.keypair_name: self.keypair_name = CONF.orchestration.keypair_name else: self.keypair = self.create_keypair() self.keypair_name = self.keypair.id self.template = self._load_template(__file__, 'test_autoscaling.yaml') self.client.stacks.create( stack_name=self.stack_name, template=self.template, parameters=self.parameters) self.stack = self.client.stacks.get(self.stack_name) self.stack_identifier = '%s/%s' % (self.stack_name, self.stack.id)",235,37
openstack%2Ftempest~master~I121e80cda5bc9a11c6844c417a8eb0f68e073792,openstack/tempest,master,I121e80cda5bc9a11c6844c417a8eb0f68e073792,Add Trove (database) Instance API Tests,ABANDONED,2014-07-15 22:22:15.000000000,2014-12-05 15:05:48.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-07-15 22:22:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ceafe912d6d30f58d4fe7a3eaf64cbe38e23110c', 'message': 'Add Trove (database) Instance API Tests\n\nPartially implements blueprint: trove-tempest\n\nChange-Id: I121e80cda5bc9a11c6844c417a8eb0f68e073792\n'}, {'number': 2, 'created': '2014-07-16 21:47:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/28d98872d994023962fad5ef97f73ffc233bc95f', 'message': 'Add Trove (database) Instance API Tests\n\nPartially implements blueprint: trove-tempest\n\nChange-Id: I121e80cda5bc9a11c6844c417a8eb0f68e073792\n'}, {'number': 3, 'created': '2014-07-17 20:17:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d056038b4f45dbf6c7c55f4a542c2236bcd96d73', 'message': 'Add Trove (database) Instance API Tests\n\nPartially implements blueprint: trove-tempest\n\nChange-Id: I121e80cda5bc9a11c6844c417a8eb0f68e073792\n'}, {'number': 4, 'created': '2014-07-22 01:30:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/44a49b3b25813243594293b08cdf4b94e0e58bd5', 'message': 'Add Trove (database) Instance API Tests\n\nPartially implements blueprint: trove-tempest\n\nChange-Id: I121e80cda5bc9a11c6844c417a8eb0f68e073792\n'}, {'number': 5, 'created': '2014-07-24 18:16:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5cb99c3b82622f36e79b3e6d4131c8a2541d16e3', 'message': 'Add Trove (database) Instance API Tests\n\nPartially implements blueprint: trove-tempest\n\nChange-Id: I121e80cda5bc9a11c6844c417a8eb0f68e073792\n'}, {'number': 6, 'created': '2014-07-25 20:03:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f2160e85b5239d4cec2d58e4ec1c59d7a5ec3067', 'message': 'Add Trove (database) Instance API Tests\n\nPartially implements blueprint: trove-tempest\n\nChange-Id: I121e80cda5bc9a11c6844c417a8eb0f68e073792\n'}, {'number': 7, 'created': '2014-07-29 17:38:28.000000000', 'files': ['tempest/api/database/instances/test_instances.py', 'tempest/api/database/instances/__init__.py', 'tempest/common/waiters.py', 'tempest/clients.py', 'tempest/config.py', 'tempest/api/database/base.py', 'tempest/exceptions.py', 'tempest/services/database/json/instances_client.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/3e9488b22072e0a3c8d3197988599b416197aaad', 'message': 'Add Trove (database) Instance API Tests\n\nPartially implements blueprint: trove-tempest\n\nChange-Id: I121e80cda5bc9a11c6844c417a8eb0f68e073792\n'}]",0,107193,3e9488b22072e0a3c8d3197988599b416197aaad,42,6,7,1870,,,0,"Add Trove (database) Instance API Tests

Partially implements blueprint: trove-tempest

Change-Id: I121e80cda5bc9a11c6844c417a8eb0f68e073792
",git fetch https://review.opendev.org/openstack/tempest refs/changes/93/107193/3 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/database/instances/test_instances.py', 'tempest/api/database/instances/__init__.py', 'etc/tempest.conf.sample', 'tempest/clients.py', 'tempest/config.py', 'tempest/api/database/base.py', 'tempest/services/database/json/instances_client.py']",7,ceafe912d6d30f58d4fe7a3eaf64cbe38e23110c,bp/trove-tempest,"# Copyright 2014 Hewlett-Packard Development Company, L.P. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # import urllib from tempest.common import rest_client from tempest.common import waiters from tempest import config CONF = config.CONF class DatabaseInstancesClientJSON(rest_client.RestClient): def __init__(self, auth_provider): super(DatabaseInstancesClientJSON, self).__init__(auth_provider) self.service = CONF.database.catalog_type def list_db_instances(self, params=None): """""" List database instances for one tenent. """""" url = 'instances' if params: url += '?%s' % urllib.urlencode(params) resp, body = self.get(url) return resp, self._parse_resp(body) def get_db_instance_details(self, db_instance_id): """""" Show specific database instance. """""" resp, body = self.get(""instances/%s"" % str(db_instance_id)) return resp, self._parse_resp(body) def create_db_instance(self, name, flavor, **kwargs): """""" Boot database instance. """""" url = 'instances' post_body = { 'name': name, 'flavorRef': flavor } if 'volume_size' in kwargs: post_body['volume'] = {""size"": kwargs['volume_size']} resp, body = self.post(url, post_body) return resp, self._parse_resp(body) def delete_db_instance(self, db_instance_id): """""" Delete database instance per given id. """""" resp, body = self.delete('instances/%s' % str(db_instance_id)) return resp, self._parse_resp(body) def enable_root_user(self, db_instance_id): """""" Enable root access for specific databsae instance. """""" resp, body = self.post(""instances/%s/root"" % str(db_instance_id)) return resp, self._parse_resp(body) def get_root_status(self, db_instance_id): """""" Show root status for specific database instance. """""" resp, body = self.get(""instances/%s/root"" % str(db_instance_id)) return resp, self._parse_resp(body) def wait_for_instance_status(self, instance_id, status, extra_timeout=0, raise_on_error=True): """"""Waits for a server to reach a given status."""""" return waiters.wait_for_server_status(self, instance_id, status, extra_timeout=extra_timeout, raise_on_error=raise_on_error) ",,182,0
openstack%2Ftempest~master~I7433f091027a69360a6f654a45d25beb0859a9fe,openstack/tempest,master,I7433f091027a69360a6f654a45d25beb0859a9fe,Test script for CRUD operations on trove instances,ABANDONED,2014-07-28 09:13:52.000000000,2014-12-05 15:05:47.000000000,,"[{'_account_id': 3}, {'_account_id': 1795}, {'_account_id': 2238}, {'_account_id': 2750}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10068}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-07-28 09:13:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/62d28eebdc654ea8457f78eccdac29e5c3ca630f', 'message': ""Test script for CRUD operations on trove instances\n\nAdds a directory instances with test script 'test_instances.py'\nto database api tests.\nAdds related JSON supporting methods in client file\n'instance_client.py' present in 'tempest/services/database/json'.\nAdds api_schema for database instances in 'tempest/api_schema'\ndirectory.\nAddition of api_schema includes creation of a folder for database\nwith two files 'instances.py' and 'parameter_types.py'.\nModified 'tempest.conf.sample' and 'config.py' to add parameters\nbuild_interval and build_timeout.\nModified 'base.py' of api database tests and clients.py to add the\ndetails of instance_client.py.\nAdds a method 'wait_for_db_instance_status' to waiters.py present\nin 'tempest/common' so as to implement logic to wait till db\ninstance reaches the desired status.\n\nChange-Id: I7433f091027a69360a6f654a45d25beb0859a9fe\n""}, {'number': 2, 'created': '2014-07-28 13:47:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/09f31cadc87113c9be5140c93f384b08554db06f', 'message': 'Test script for CRUD operations on trove instances\n\nAdds a directory instances with test script \'test_instances.py\'\nto database api tests.\nAdds related JSON supporting methods in client file\n\'instance_client.py\' present in \'tempest/services/database/json\'.\nAdds api_schema for database instances in \'tempest/api_schema\'\ndirectory.\nAddition of api_schema includes creation of a folder for database\nwith two files \'instances.py\' and \'parameter_types.py\'.\nModified \'tempest.conf.sample\' and \'config.py\' to add parameters\nbuild_interval and build_timeout.\nModified \'base.py\' of api database tests and clients.py to add the\ndetails of instance_client.py.\nAdds a method \'wait_for_db_instance_status\' to waiters.py present\nin \'tempest/common\' so as to implement logic to wait till db\ninstance reaches the desired status.\nAlso corrected a minor pep8 validation in ""wait_for_server_status""\nmethod of waiters.py script.\n\nChange-Id: I7433f091027a69360a6f654a45d25beb0859a9fe\n'}, {'number': 3, 'created': '2014-07-29 18:01:43.000000000', 'files': ['tempest/api_schema/database/parameter_types.py', 'tempest/api/database/instances/__init__.py', 'etc/tempest.conf.sample', 'tempest/common/waiters.py', 'tempest/api_schema/database/__init__.py', 'tempest/services/database/json/instances_client.py', 'tempest/api/database/instances/test_instances.py', 'tempest/clients.py', 'tempest/config.py', 'tempest/api/database/base.py', 'tempest/api_schema/database/instances.py', 'tempest/exceptions.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/486319fc0f4ee841d4e0a07ae9548146b495d047', 'message': 'Test script for CRUD operations on trove instances\n\nAdds a directory instances with test script \'test_instances.py\'\nto database api tests.\nAdds related JSON supporting methods in client file\n\'instance_client.py\' present in \'tempest/services/database/json\'.\nAdds api_schema for database instances in \'tempest/api_schema\'\ndirectory.\nAddition of api_schema includes creation of a folder for database\nwith two files \'instances.py\' and \'parameter_types.py\'.\nModified \'tempest.conf.sample\' and \'config.py\' to add parameters\nbuild_interval and build_timeout.\nModified \'base.py\' of api database tests and clients.py to add the\ndetails of instance_client.py.\nAdds a method \'wait_for_db_instance_status\' to waiters.py present\nin \'tempest/common\' so as to implement logic to wait till db\ninstance reaches the desired status.\nAdds a class \'DbInstanceBuildErrorException\' to exceptions.py present\nin \'tempest/\' to raise an exception message when database instance\nreaches ERROR state.\nAlso corrected minor pep8 validations in ""wait_for_server_status""\nmethod of waiters.py script and ""__str__"" method of class\n""CommandFailed"" in exceptions.py\n\nChange-Id: I7433f091027a69360a6f654a45d25beb0859a9fe\n'}]",0,109935,486319fc0f4ee841d4e0a07ae9548146b495d047,22,9,3,1845,,,0,"Test script for CRUD operations on trove instances

Adds a directory instances with test script 'test_instances.py'
to database api tests.
Adds related JSON supporting methods in client file
'instance_client.py' present in 'tempest/services/database/json'.
Adds api_schema for database instances in 'tempest/api_schema'
directory.
Addition of api_schema includes creation of a folder for database
with two files 'instances.py' and 'parameter_types.py'.
Modified 'tempest.conf.sample' and 'config.py' to add parameters
build_interval and build_timeout.
Modified 'base.py' of api database tests and clients.py to add the
details of instance_client.py.
Adds a method 'wait_for_db_instance_status' to waiters.py present
in 'tempest/common' so as to implement logic to wait till db
instance reaches the desired status.
Adds a class 'DbInstanceBuildErrorException' to exceptions.py present
in 'tempest/' to raise an exception message when database instance
reaches ERROR state.
Also corrected minor pep8 validations in ""wait_for_server_status""
method of waiters.py script and ""__str__"" method of class
""CommandFailed"" in exceptions.py

Change-Id: I7433f091027a69360a6f654a45d25beb0859a9fe
",git fetch https://review.opendev.org/openstack/tempest refs/changes/35/109935/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/database/instances/test_instances.py', 'tempest/api_schema/database/parameter_types.py', 'tempest/api/database/instances/__init__.py', 'etc/tempest.conf.sample', 'tempest/common/waiters.py', 'tempest/clients.py', 'tempest/config.py', 'tempest/api/database/base.py', 'tempest/api_schema/database/instances.py', 'tempest/api_schema/database/__init__.py', 'tempest/services/database/json/instances_client.py']",11,62d28eebdc654ea8457f78eccdac29e5c3ca630f,separate/trove_instances,"# Copyright 2014 Hewlett-Packard Development Company, L.P # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import json import time import urllib from tempest.api_schema.database import instances as common_schema from tempest.common import rest_client from tempest.common import waiters from tempest import config from tempest import exceptions CONF = config.CONF class DatabaseInstanceClientJSON(rest_client.RestClient): def __init__(self, auth_provider): super(DatabaseInstanceClientJSON, self).__init__(auth_provider) self.service = CONF.database.catalog_type self.build_interval = CONF.database.build_interval self.build_timeout = CONF.database.build_timeout def list_db_instances(self, params=None): """"""List all db_instances."""""" url = 'instances' if params: url += '?%s' % urllib.urlencode(params) resp, body = self.get(url) body = json.loads(body) self.validate_response(common_schema.list_db_instances, resp, body) return resp, body['instances'] def get_db_instance(self, db_instance_id): """"""Get db_instance details."""""" resp, body = self.get(""instances/%s"" % str(db_instance_id)) body = json.loads(body) self.validate_response(common_schema.get_db_instance, resp, body) return resp, body['instance'] def delete_db_instance(self, db_instance_id): """"""Deletes a db instance."""""" resp, body = self.delete(""/instances/%s"" % str(db_instance_id)) self.validate_response(common_schema.delete_instance, resp, body) return resp, body def create_db_instance(self, flavor_id, name, size, **kwargs): """"""Creates an instance"""""" post_body = {'flavorRef': flavor_id, 'name': name, 'volume': {'size': size}} for option in ['size', 'databases', 'users', 'backup', 'availability_zone']: if isinstance(option, tuple): post_param = option[0] key = option[1] else: post_param = option key = option value = kwargs.get(key) if value is not None: post_body[post_param] = value post_body = json.dumps({'instance': post_body}) resp, body = self.post('instances', post_body) body = json.loads(body) self.validate_response(common_schema.create_db_instance, resp, body) return resp, body['instance'] def wait_for_db_instance_status(self, instance_id, status, extra_timeout=0, raise_on_error=True): """"""Waits for a instance to reach a given status."""""" return waiters.wait_for_db_instance_status( self, instance_id, status, extra_timeout=extra_timeout, raise_on_error=raise_on_error) def wait_for_db_instance_termination(self, instance_id, ignore_error=False): """"""Waits for instance to reach termination."""""" start_time = int(time.time()) while True: try: resp, body = self.get_db_instance(instance_id) except exceptions.NotFound: return instance_status = body['status'] if instance_status == 'ERROR' and not ignore_error: raise exceptions.BuildErrorException(instance_id=instance_id) if int(time.time()) - start_time >= self.build_timeout: raise exceptions.TimeoutException time.sleep(self.build_interval) ",,399,1
openstack%2Ftempest~master~I156290f859dcc075c0ccb85d1f0425efc1577e97,openstack/tempest,master,I156290f859dcc075c0ccb85d1f0425efc1577e97,Changes comments back to doc strings.,ABANDONED,2014-08-04 01:12:36.000000000,2014-12-05 15:05:46.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-04 01:12:36.000000000', 'files': ['tempest/api/identity/admin/test_roles_negative.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/9afb6da64ae91555d2a39b2ff09701b1f30e582a', 'message': 'Changes comments back to doc strings.\n\nNose would take doc strings and replace test names with them so\ndoc strings had been turned into comments. Testr does not replace\ntest names with doc strings so this patch turns some comments back\nto doc strings.\n\nCo-Authored With: Jamie Lennox <jamielennox@redhat.com>\nCo-Authored With: Matthew Treinish <mtreinish@kortar.org>\n\nChange-Id: I156290f859dcc075c0ccb85d1f0425efc1577e97\n'}]",0,111604,9afb6da64ae91555d2a39b2ff09701b1f30e582a,8,4,1,6316,,,0,"Changes comments back to doc strings.

Nose would take doc strings and replace test names with them so
doc strings had been turned into comments. Testr does not replace
test names with doc strings so this patch turns some comments back
to doc strings.

Co-Authored With: Jamie Lennox <jamielennox@redhat.com>
Co-Authored With: Matthew Treinish <mtreinish@kortar.org>

Change-Id: I156290f859dcc075c0ccb85d1f0425efc1577e97
",git fetch https://review.opendev.org/openstack/tempest refs/changes/04/111604/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/identity/admin/test_roles_negative.py'],1,9afb6da64ae91555d2a39b2ff09701b1f30e582a,doc-strings," """"""Non-administrator user should not be able to list roles."""""" """"""Request to list roles without a valid token should fail."""""" """"""Should not be able to create a role with a blank name."""""" """"""Non-administrator user should not be able to create role."""""" """"""Request to create role without a valid token should fail."""""" """"""Role names should be unique."""""" """"""Non-administrator user should not be able to delete role."""""" """"""Request to delete role without a valid token should fail."""""" """"""Attempt to delete a non existent role should fail."""""" """"""Non-administrator user should not be authorized to assign a role to user."""""" """"""Request to assign a role to a user without a valid token."""""" """"""Attempt to assign a non existent role to user should fail."""""" """"""Attempt to assign a role on a non existent tenant should fail."""""" """"""Duplicate user role should not get assigned."""""" """"""Non-administrator user should not be authorized to # remove a user's role."""""" """"""Request to remove a user's role without a valid token."""""" """"""Attempt to delete a non existent role from a user should fail."""""" """"""Attempt to remove a role from a non existent tenant should fail."""""" """"""Non-administrator user should not be authorized to list a user's roles."""""" """"""Request to list user's roles without a valid token should fail.""""""", # Non-administrator user should not be able to list roles # Request to list roles without a valid token should fail # Should not be able to create a role with a blank name # Non-administrator user should not be able to create role # Request to create role without a valid token should fail # Role names should be unique # Non-administrator user should not be able to delete role # Request to delete role without a valid token should fail # Attempt to delete a non existent role should fail # Non-administrator user should not be authorized to # assign a role to user # Request to assign a role to a user without a valid token # Attempt to assign a non existent role to user should fail # Attempt to assign a role on a non existent tenant should fail # Duplicate user role should not get assigned # Non-administrator user should not be authorized to # remove a user's role # Request to remove a user's role without a valid token # Attempt to delete a non existent role from a user should fail # Attempt to remove a role from a non existent tenant should fail # Non-administrator user should not be authorized to list # a user's roles # Request to list user's roles without a valid token should fail,23,23
openstack%2Ftempest~master~Ic4f33e962761549f5c8cea8f94f74c29a2199a63,openstack/tempest,master,Ic4f33e962761549f5c8cea8f94f74c29a2199a63,Comments to doc strings.,ABANDONED,2014-08-04 01:35:31.000000000,2014-12-05 15:05:45.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-04 01:35:31.000000000', 'files': ['tempest/api/identity/admin/test_tenants.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/a497a18c86034dab066fd93942779567d5f2af2d', 'message': 'Comments to doc strings.\n\nThis patch changes comments to doc strings.\n\nCo-Authored With: Jamie Lennox <jamielennox@redhat.com>\n\nChange-Id: Ic4f33e962761549f5c8cea8f94f74c29a2199a63\n'}]",0,111610,a497a18c86034dab066fd93942779567d5f2af2d,7,4,1,6316,,,0,"Comments to doc strings.

This patch changes comments to doc strings.

Co-Authored With: Jamie Lennox <jamielennox@redhat.com>

Change-Id: Ic4f33e962761549f5c8cea8f94f74c29a2199a63
",git fetch https://review.opendev.org/openstack/tempest refs/changes/10/111610/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/identity/admin/test_tenants.py'],1,a497a18c86034dab066fd93942779567d5f2af2d,doc-strings," """"""Create several tenants and delete them."""""" """"""Create tenant with a description."""""" """"""Create a tenant that is enabled."""""" """"""Create a tenant that is not enabled."""""" """"""Update name attribute of a tenant."""""" """"""Update description attribute of a tenant."""""" """"""Update the enabled attribute of a tenant.""""""", # Create several tenants and delete them # Create tenant with a description # Create a tenant that is enabled # Create a tenant that is not enabled # Update name attribute of a tenant # Update description attribute of a tenant # Update the enabled attribute of a tenant,7,7
openstack%2Ftempest~master~Ie10d6779c54e585163c517d447550822dd32b9fe,openstack/tempest,master,Ie10d6779c54e585163c517d447550822dd32b9fe,"Comments to doc-strings, still more",ABANDONED,2014-08-04 01:31:48.000000000,2014-12-05 15:05:44.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-04 01:31:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d769798b971512cb5d19841683fa39b5c21f920c', 'message': 'Comments to doc-strings, still more\n\nThis patches changes comments to doc strings.\n\nChange-Id: Ie10d6779c54e585163c517d447550822dd32b9fe\n'}, {'number': 2, 'created': '2014-08-04 01:35:59.000000000', 'files': ['tempest/api/identity/admin/test_tenant_negative.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/b3c315e30c0a5e4a0c924fd06b4e7b29ae264463', 'message': 'Comments to doc-strings, still more\n\nThis patches changes comments to doc strings.\n\nCo-Authored With: Jamie Lennox <jamielennox@redhat.com>\n\nChange-Id: Ie10d6779c54e585163c517d447550822dd32b9fe\n'}]",0,111609,b3c315e30c0a5e4a0c924fd06b4e7b29ae264463,9,4,2,6316,,,0,"Comments to doc-strings, still more

This patches changes comments to doc strings.

Co-Authored With: Jamie Lennox <jamielennox@redhat.com>

Change-Id: Ie10d6779c54e585163c517d447550822dd32b9fe
",git fetch https://review.opendev.org/openstack/tempest refs/changes/09/111609/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/identity/admin/test_tenant_negative.py'],1,d769798b971512cb5d19841683fa39b5c21f920c,doc-strings," """"""Non-administrator user should not be able to list tenants."""""" """"""Request to list tenants without a valid token should fail."""""" """"""Non-administrator user should not be able to delete a tenant."""""" """"""Request to delete a tenant without a valid token should fail."""""" """"""Attempt to delete a non existent tenant should fail."""""" """"""Tenant names should be unique."""""" """"""Non-administrator user should not be authorized to create a tenant."""""" """"""Create tenant request without a token should not be authorized."""""" """"""Tenant name should not be empty."""""" """"""Tenant name length should not be greater than 64 characters."""""" """"""Attempt to update a non existent tenant should fail."""""" """"""Non-administrator user should not be able to update a tenant."""""" """"""Request to update a tenant without a valid token should fail.""""""", # Non-administrator user should not be able to list tenants # Request to list tenants without a valid token should fail # Non-administrator user should not be able to delete a tenant # Request to delete a tenant without a valid token should fail # Attempt to delete a non existent tenant should fail # Tenant names should be unique # Non-administrator user should not be authorized to create a tenant # Create tenant request without a token should not be authorized # Tenant name should not be empty # Tenant name length should not be greater than 64 characters # Attempt to update a non existent tenant should fail # Non-administrator user should not be able to update a tenant # Request to update a tenant without a valid token should fail,13,13
openstack%2Ftempest~master~I10829c47bb0d73e238fe0e6629761b53fec5ea57,openstack/tempest,master,I10829c47bb0d73e238fe0e6629761b53fec5ea57,Keystone V2 API tests optimization,ABANDONED,2014-05-07 06:29:06.000000000,2014-12-05 15:05:43.000000000,,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 1921}, {'_account_id': 2238}, {'_account_id': 2750}, {'_account_id': 6455}, {'_account_id': 6553}, {'_account_id': 6796}, {'_account_id': 7249}, {'_account_id': 7428}, {'_account_id': 8556}, {'_account_id': 8824}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 10966}]","[{'number': 1, 'created': '2014-05-07 06:29:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/cf5a4f6b2bc505fb1d5047e66bc252160de3f724', 'message': 'Keystone V2 API tests optimization\n\nThis patch optimizes the Keystone V2 tests and removes the\nduplicate codes and unused variables.\n\nChange-Id: I10829c47bb0d73e238fe0e6629761b53fec5ea57\n'}, {'number': 2, 'created': '2014-05-09 05:09:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/bfb15ecd6f760549f53b15773e79cff77d5a93d5', 'message': 'Keystone V2 API tests optimization\n\nThis patch optimizes the Keystone V2 tests and removes the\nduplicate codes and unused variables.\n\nChange-Id: I10829c47bb0d73e238fe0e6629761b53fec5ea57\n'}, {'number': 3, 'created': '2014-05-09 06:27:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ec73a4aad09d774de0dc4281b911ee5fa0fb2c55', 'message': 'Keystone V2 API tests optimization\n\nThis patch optimizes the Keystone V2 tests and removes the\nduplicate codes and unused variables.\n\nChange-Id: I10829c47bb0d73e238fe0e6629761b53fec5ea57\n'}, {'number': 4, 'created': '2014-05-21 03:28:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c763108e9b81354fef06cd8f7a5119c2109c8dab', 'message': 'Keystone V2 API tests optimization\n\nThis patch optimizes the Keystone V2 tests and removes the\nduplicate codes and unused variables.\n\nChange-Id: I10829c47bb0d73e238fe0e6629761b53fec5ea57\n'}, {'number': 5, 'created': '2014-05-21 03:31:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ed8871f6d6fc6536d7d9234c1164f7b9ad425b90', 'message': 'Keystone V2 API tests optimization\n\nThis patch optimizes the Keystone V2 tests and removes the\nduplicate codes and unused variables.\n\nChange-Id: I10829c47bb0d73e238fe0e6629761b53fec5ea57\n'}, {'number': 6, 'created': '2014-05-21 09:47:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c55e50c87f510ab1e3a93b811c11f0000d5ff2ab', 'message': 'Keystone V2 API tests optimization\n\nThis patch optimizes the Keystone V2 tests and removes the\nduplicate codes and unused variables.\n\nChange-Id: I10829c47bb0d73e238fe0e6629761b53fec5ea57\n'}, {'number': 7, 'created': '2014-05-21 09:58:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8d601c30a0ffd7305b84380fbaab6706a435db2c', 'message': 'Keystone V2 API tests optimization\n\nThis patch optimizes the Keystone V2 tests and removes the\nduplicate codes and unused variables.\n\nChange-Id: I10829c47bb0d73e238fe0e6629761b53fec5ea57\n'}, {'number': 8, 'created': '2014-05-22 10:45:23.000000000', 'files': ['tempest/api/identity/admin/test_roles.py', 'tempest/api/identity/admin/test_tenant_negative.py', 'tempest/api/identity/admin/test_services.py', 'tempest/api/identity/admin/test_users.py', 'tempest/api/identity/admin/test_roles_negative.py', 'tempest/api/identity/admin/test_tokens.py', 'tempest/test.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/88cf45af55c81c8a3252e14a1bf3f01aa9a04400', 'message': 'Keystone V2 API tests optimization\n\nThis patch optimizes the Keystone V2 tests and removes the\nduplicate codes and unused variables.\nThis patch also adds a help method in tempest.test to check\nfor the expected response for multiple keys.\nand another help method to validate the availability of the\nrequired multiple keys in response.\n\nChange-Id: I10829c47bb0d73e238fe0e6629761b53fec5ea57\n'}]",33,92524,88cf45af55c81c8a3252e14a1bf3f01aa9a04400,63,16,8,10966,,,0,"Keystone V2 API tests optimization

This patch optimizes the Keystone V2 tests and removes the
duplicate codes and unused variables.
This patch also adds a help method in tempest.test to check
for the expected response for multiple keys.
and another help method to validate the availability of the
required multiple keys in response.

Change-Id: I10829c47bb0d73e238fe0e6629761b53fec5ea57
",git fetch https://review.opendev.org/openstack/tempest refs/changes/24/92524/4 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/identity/admin/test_roles.py', 'tempest/api/identity/admin/test_tenant_negative.py', 'tempest/api/identity/admin/test_services.py', 'tempest/api/identity/admin/test_users.py', 'tempest/api/identity/admin/test_roles_negative.py', 'tempest/api/identity/admin/test_tokens.py']",6,cf5a4f6b2bc505fb1d5047e66bc252160de3f724,keystone_test_optimization," tenants = [] for i in range(2): tenant_name = data_utils.rand_name(name='tenant-') resp, tenant = self.client.create_tenant(tenant_name) self.assertEqual(200, resp.status) self.data.tenants.append(tenant) tenants.append(tenant) for i in range(2): resp, _ = self.client.assign_user_role(tenants[i]['id'], user['id'], role['id']) self.assertEqual(200, resp.status) rsp, body = self.token_client.auth_token(token_id, tenant=tenants[0]['name']) resp, _ = self.client.delete_token(scoped_token_id) rsp, _ = self.token_client.auth_token(token_id, tenant=tenants[1]['name'])"," tenant1_name = data_utils.rand_name(name='tenant-') resp, tenant1 = self.client.create_tenant(tenant1_name) self.assertEqual(200, resp.status) self.data.tenants.append(tenant1) tenant2_name = data_utils.rand_name(name='tenant-') resp, tenant2 = self.client.create_tenant(tenant2_name) self.assertEqual(200, resp.status) self.data.tenants.append(tenant2) resp, _ = self.client.assign_user_role(tenant1['id'], user['id'], role['id']) self.assertEqual(200, resp.status) resp, _ = self.client.assign_user_role(tenant2['id'], user['id'], role['id']) self.assertEqual(200, resp.status) rsp, body = self.token_client.auth_token(token_id, tenant=tenant1_name) resp, body = self.client.delete_token(scoped_token_id) rsp, body = self.token_client.auth_token(token_id, tenant=tenant2_name)",79,95
openstack%2Ftempest~master~Idd99e5546c9339eed611cd17c8f335f39c2753a3,openstack/tempest,master,Idd99e5546c9339eed611cd17c8f335f39c2753a3,Changes more comments to doc strings,ABANDONED,2014-08-04 01:51:19.000000000,2014-12-05 15:05:42.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-04 01:51:19.000000000', 'files': ['tempest/api/identity/admin/test_users_negative.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/9bb775b3e10f319d5bf1aec69cac17596776ac04', 'message': 'Changes more comments to doc strings\n\nThis patch changes more comments to doc strings.\n\nCo-Authored With: Jamie Lennox <jamielennox@redhat.com>\n\nChange-Id: Idd99e5546c9339eed611cd17c8f335f39c2753a3\n'}]",0,111612,9bb775b3e10f319d5bf1aec69cac17596776ac04,7,4,1,6316,,,0,"Changes more comments to doc strings

This patch changes more comments to doc strings.

Co-Authored With: Jamie Lennox <jamielennox@redhat.com>

Change-Id: Idd99e5546c9339eed611cd17c8f335f39c2753a3
",git fetch https://review.opendev.org/openstack/tempest refs/changes/12/111612/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/identity/admin/test_users_negative.py'],1,9bb775b3e10f319d5bf1aec69cac17596776ac04,doc-strings," """"""Non-administrator should not be authorized to create a user."""""" """"""User with an empty name should not be created."""""" """"""Length of user name filed should be restricted to 255 characters."""""" """"""Duplicate user should not be created."""""" """"""Attempt to create a user in a non-existent tenant should fail."""""" """"""Request to create a user without a valid token should fail."""""" """"""Attempt to create a user with valid enabled para should fail."""""" """"""Attempt to update a user non-existent user should fail."""""" """"""Request to update a user without a valid token should fail."""""" """"""Non-administrator should not be authorized to update user."""""" """"""Non-administrator user should not be authorized to delete a user."""""" """"""Attempt to delete a non-existent user should fail."""""" """"""Request to delete a user without a valid token should fail."""""" """"""Disabled user's token should not get authenticated."""""" """"""User's token for a disabled tenant should not be authenticated."""""" """"""User's token for an invalid tenant should not be authenticated."""""" """"""Non-existent user's token should not get authenticated."""""" """"""User's token with invalid password should not be authenticated."""""" """"""Non-administrator user should not be authorized to get user list."""""" """"""Request to get list of users without a valid token should fail."""""" """"""Should not be able to return a list of all users for a non-existent tenant.""""""", # Non-administrator should not be authorized to create a user # User with an empty name should not be created # Length of user name filed should be restricted to 255 characters # Duplicate user should not be created # Attempt to create a user in a non-existent tenant should fail # Request to create a user without a valid token should fail # Attempt to create a user with valid enabled para should fail # Attempt to update a user non-existent user should fail # Request to update a user without a valid token should fail # Non-administrator should not be authorized to update user # Non-administrator user should not be authorized to delete a user # Attempt to delete a non-existent user should fail # Request to delete a user without a valid token should fail # Disabled user's token should not get authenticated # User's token for a disabled tenant should not be authenticated # User's token for an invalid tenant should not be authenticated # Non-existent user's token should not get authenticated # User's token with invalid password should not be authenticated # Non-administrator user should not be authorized to get user list # Request to get list of users without a valid token should fail # Should not be able to return a list of all # users for a non-existent tenant,22,22
openstack%2Ftempest~master~I0476b8638a74e583f7a4eb1b884ef2b7aee0539c,openstack/tempest,master,I0476b8638a74e583f7a4eb1b884ef2b7aee0539c,Add scenario test for Trove (database) backup and restore API.,ABANDONED,2014-07-30 20:55:49.000000000,2014-12-05 15:05:41.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 9008}, {'_account_id': 9194}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-07-30 20:55:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/2dc34cef57a36b485b08b0c243d1c6b655dfdadd', 'message': 'Add scenario test for Trove (database) backup and restore API.\n\nPartially implements blueprint: trove-tempest\n\nChange-Id: I0476b8638a74e583f7a4eb1b884ef2b7aee0539c\n'}, {'number': 2, 'created': '2014-07-31 00:38:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/bee42cb813f0b5c745d92af03db7d1745f2babc9', 'message': 'Add scenario test for Trove (database) backup and restore API.\n\nPartially implements blueprint: trove-tempest\n\nChange-Id: I0476b8638a74e583f7a4eb1b884ef2b7aee0539c\n'}, {'number': 3, 'created': '2014-07-31 22:36:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/debdef6afc7bee99b5ed868dc2f9b1114663222b', 'message': 'Add scenario test for Trove (database) backup and restore API.\n\nPartially implements blueprint: trove-tempest\n\nChange-Id: I0476b8638a74e583f7a4eb1b884ef2b7aee0539c\n'}, {'number': 4, 'created': '2014-07-31 23:09:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/4bbefe0f0802321a173efabd26788849f15a5605', 'message': 'Add scenario test for Trove (database) backup and restore API.\n\nPartially implements blueprint: trove-tempest\n\nChange-Id: I0476b8638a74e583f7a4eb1b884ef2b7aee0539c\n'}, {'number': 5, 'created': '2014-08-04 19:50:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/68e903ff8808cced78ad5ba5646198c94002d5a5', 'message': 'Add scenario test for Trove (database) backup and restore API.\n\nPartially implements blueprint: trove-tempest\n\nChange-Id: I0476b8638a74e583f7a4eb1b884ef2b7aee0539c\n'}, {'number': 6, 'created': '2014-08-04 20:10:40.000000000', 'files': ['tempest/scenario/test_database_backup_restore_ops.py', 'requirements.txt', 'tempest/scenario/manager.py', 'tempest/clients.py', 'tempest/config.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/79e689b19efee9fd15799a665eae6437de2995dd', 'message': 'Add scenario test for Trove (database) backup and restore API.\n\nPartially implements blueprint: tempest-guest-tests\n\nChange-Id: I0476b8638a74e583f7a4eb1b884ef2b7aee0539c\n'}]",1,110785,79e689b19efee9fd15799a665eae6437de2995dd,31,6,6,1870,,,0,"Add scenario test for Trove (database) backup and restore API.

Partially implements blueprint: tempest-guest-tests

Change-Id: I0476b8638a74e583f7a4eb1b884ef2b7aee0539c
",git fetch https://review.opendev.org/openstack/tempest refs/changes/85/110785/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/scenario/test_database_backup_restore_ops.py', 'tempest/scenario/manager.py', 'tempest/clients.py']",3,2dc34cef57a36b485b08b0c243d1c6b655dfdadd,bp/tempest-guest-tests," self.database_client = self._get_database_client(credentials) def _get_database_client(self, credentials): if not CONF.service_available.trove: return None import troveclient.v1.client auth_url = CONF.identity.uri return troveclient.v1.client.Client( username=credentials.username, password=credentials.password, project_id=credentials.tenant_name, auth_url=auth_url) ",,154,2
openstack%2Ftempest~master~I17a9e64cc42ddfbaf97a90e23fa5d17adfe86a7b,openstack/tempest,master,I17a9e64cc42ddfbaf97a90e23fa5d17adfe86a7b,Move orchestration scenario test templates in separate directory,ABANDONED,2014-07-03 13:26:39.000000000,2014-12-05 15:05:40.000000000,,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 4328}, {'_account_id': 5803}, {'_account_id': 6498}, {'_account_id': 6899}, {'_account_id': 7293}, {'_account_id': 8556}, {'_account_id': 8576}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10090}, {'_account_id': 10385}, {'_account_id': 11956}, {'_account_id': 12437}]","[{'number': 1, 'created': '2014-07-03 13:26:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/975ba0e13ebdc635384eb7de7c13872ed0af44f2', 'message': 'Move orchestration scenario test templates in seperate directory\n\nSeperate out templates from test modules and also refactor orchestration\nscenario test cases to use the new template location\n\nChange-Id: I17a9e64cc42ddfbaf97a90e23fa5d17adfe86a7b\n'}, {'number': 2, 'created': '2014-07-03 14:42:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/213df56ead83b17acce6e3f0227402fa37150cdb', 'message': 'Move orchestration scenario test templates in separate directory\n\nSeparate out templates from test modules and also refactor orchestration\nscenario test cases to use the new template location\n\nChange-Id: I17a9e64cc42ddfbaf97a90e23fa5d17adfe86a7b\n'}, {'number': 3, 'created': '2014-07-04 06:45:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/9c4bc3493ed5eeef64892233ba0b52bc9ba73275', 'message': 'Move orchestration scenario test templates in seperate directory\n\nSeperate out templates from test modules and also refactor orchestration\nscenario test cases to use the new template location\n\nChange-Id: I17a9e64cc42ddfbaf97a90e23fa5d17adfe86a7b\n'}, {'number': 4, 'created': '2014-07-15 08:39:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/a049682c262df3a5190966bf7675ae5689d3c884', 'message': 'Move orchestration scenario test templates in separate directory\n\nSeparate out templates from test modules and also refactor orchestration\nscenario test cases to use the new template location\n\nChange-Id: I17a9e64cc42ddfbaf97a90e23fa5d17adfe86a7b\n'}, {'number': 5, 'created': '2014-07-15 11:14:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/3a3bbdaa9665b48ccc389ad9a3370b0dc82c486d', 'message': 'Move orchestration scenario test templates in seperate directory\n\nSeperate out templates from test modules and also refactor orchestration\nscenario test cases to use the new template location\n\nChange-Id: I17a9e64cc42ddfbaf97a90e23fa5d17adfe86a7b\n'}, {'number': 6, 'created': '2014-07-15 11:22:57.000000000', 'files': ['tempest/scenario/orchestration/templates/cfn_init_signal.yaml', 'tempest/scenario/orchestration/test_server_cfn_init.py', 'tempest/scenario/manager.py', 'tempest/scenario/orchestration/test_autoscaling.py', 'tempest/scenario/orchestration/templates/test_autoscaling.yaml'], 'web_link': 'https://opendev.org/openstack/tempest/commit/13e9f18206a3a617a5ad0e06590fa952d3b3119b', 'message': 'Move orchestration scenario test templates in separate directory\n\nSeparate out templates from test modules and also refactor orchestration\nscenario test cases to use the new template location\n\nChange-Id: I17a9e64cc42ddfbaf97a90e23fa5d17adfe86a7b\n'}]",7,104546,13e9f18206a3a617a5ad0e06590fa952d3b3119b,83,16,6,11956,,,0,"Move orchestration scenario test templates in separate directory

Separate out templates from test modules and also refactor orchestration
scenario test cases to use the new template location

Change-Id: I17a9e64cc42ddfbaf97a90e23fa5d17adfe86a7b
",git fetch https://review.opendev.org/openstack/tempest refs/changes/46/104546/4 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/scenario/orchestration/templates/cfn_init_signal.yaml', 'tempest/scenario/orchestration/test_server_cfn_init.py', 'tempest/scenario/manager.py', 'tempest/scenario/orchestration/templates/test_autoscaling.yaml', 'tempest/scenario/orchestration/test_autoscaling.py']",5,975ba0e13ebdc635384eb7de7c13872ed0af44f2,103198, self.template = self._load_template('test_autoscaling.yaml')," self.template = self._load_template(__file__, 'test_autoscaling.yaml')",5,5
openstack%2Ftempest~master~I7e662fd8aa8ca2716cc78b156ee0eed9289e6c7c,openstack/tempest,master,I7e662fd8aa8ca2716cc78b156ee0eed9289e6c7c,Adding neutron-debug CLI tests,ABANDONED,2014-06-23 08:41:25.000000000,2014-12-05 15:05:39.000000000,,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 2750}, {'_account_id': 8205}, {'_account_id': 8556}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 11667}]","[{'number': 1, 'created': '2014-06-23 08:41:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/854e4133a8be02fd6b913f0bfef7fe38dbc30dc2', 'message': 'Adding neutron-debug CLI tests\n\nThis patch adds following neutron-debug CLI tests -\n1. probe-list\n2. probe-clear\n3. probe-list with given conf file\n4. net-list\n5. subnet-list\n7. port-list\n\nIn addition to this following optional arguments are also verified.\n1. --version\n2. --debug\n3. --help\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: I7e662fd8aa8ca2716cc78b156ee0eed9289e6c7c\n'}, {'number': 2, 'created': '2014-06-23 09:14:44.000000000', 'files': ['tempest/cli/__init__.py', 'tempest/cli/simple_read_only/test_neutron_debug.py', 'etc/tempest.conf.sample', 'tempest/config.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/8286143de2e8f911804375149c563cfdb7187664', 'message': 'Adding neutron-debug CLI tests\n\nThis patch adds following neutron-debug CLI tests -\n1. probe-list\n2. probe-clear\n3. probe-list with given conf file\n4. net-list\n5. subnet-list\n7. port-list\n\nIn addition to this following optional arguments are also verified.\n1. --version\n2. --debug\n3. --help\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: I7e662fd8aa8ca2716cc78b156ee0eed9289e6c7c\n'}]",0,101826,8286143de2e8f911804375149c563cfdb7187664,38,10,2,8205,,,0,"Adding neutron-debug CLI tests

This patch adds following neutron-debug CLI tests -
1. probe-list
2. probe-clear
3. probe-list with given conf file
4. net-list
5. subnet-list
7. port-list

In addition to this following optional arguments are also verified.
1. --version
2. --debug
3. --help

Partially implements: blueprint missing-cli-tests-in-tempest

Change-Id: I7e662fd8aa8ca2716cc78b156ee0eed9289e6c7c
",git fetch https://review.opendev.org/openstack/tempest refs/changes/26/101826/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/cli/__init__.py', 'tempest/cli/simple_read_only/test_neutron_debug.py', 'etc/tempest.conf.sample', 'tempest/config.py']",4,854e4133a8be02fd6b913f0bfef7fe38dbc30dc2,bp/missing-cli-tests-in-tempest," cfg.StrOpt('NEUTRON_TEST_CONFIG_FILE', default=""/etc/neutron/l3_agent.ini"", help=""default configuration file for neutron-debug""),",,108,1
openstack%2Ftempest~master~I23ebcc0def48b1b31d35cd92e06c35ef296b7704,openstack/tempest,master,I23ebcc0def48b1b31d35cd92e06c35ef296b7704,Missing Trove CLI tests,ABANDONED,2014-06-13 10:15:07.000000000,2014-12-05 15:05:38.000000000,,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 2750}, {'_account_id': 8205}, {'_account_id': 8556}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 11667}]","[{'number': 1, 'created': '2014-06-13 10:15:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/886833fbf56c7eb79a26f51c15cd08cc712a4c60', 'message': 'Missing Trove CLI tests\n\nThis patch adds following missing Trove CLI tests -\n1. backup list\n2. datastore list\n3. flavor list\n4. list\n5. limit list\n6. secgroup list\n7. bash completion\n8. help\n\nIn addition to this following optional arguments are also verified.\n1. --version\n2. --debug\n3. --retries\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: I23ebcc0def48b1b31d35cd92e06c35ef296b7704\n'}, {'number': 2, 'created': '2014-06-16 05:18:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/eb598723c9b116bc2a04bac55382d4224238db89', 'message': 'Missing Trove CLI tests\n\nThis patch adds following missing Trove CLI tests -\n1. backup list\n2. datastore list\n3. flavor list\n4. list\n5. limit list\n6. secgroup list\n7. bash completion\n8. help\n\nIn addition to this following optional arguments are also verified.\n1. --version\n2. --debug\n3. --retries\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: I23ebcc0def48b1b31d35cd92e06c35ef296b7704\n'}, {'number': 3, 'created': '2014-06-17 04:43:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/979ac180a7f95b3ddee47fe9d5eebcc1ee37bba6', 'message': 'Missing Trove CLI tests\n\nThis patch adds following missing Trove CLI tests -\n1. backup list\n2. datastore list\n3. flavor list\n4. list\n5. limit list\n6. secgroup list\n7. bash completion\n8. help\n\nIn addition to this following optional arguments are also verified.\n1. --version\n2. --debug\n3. --retries\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: I23ebcc0def48b1b31d35cd92e06c35ef296b7704\n'}, {'number': 4, 'created': '2014-06-17 07:01:12.000000000', 'files': ['tempest/cli/__init__.py', 'etc/tempest.conf.sample', 'tempest/cli/simple_read_only/test_trove.py', 'tempest/config.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/d42fcb4370bf84812b1d19a57175cc1460458110', 'message': 'Missing Trove CLI tests\n\nThis patch adds following missing Trove CLI tests -\n1. backup list\n2. datastore list\n3. flavor list\n4. list\n5. limit list\n6. secgroup list\n7. bash completion\n8. help\n\nIn addition to this following optional arguments are also verified.\n1. --version\n2. --debug\n3. --retries\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: I23ebcc0def48b1b31d35cd92e06c35ef296b7704\n'}]",2,99895,d42fcb4370bf84812b1d19a57175cc1460458110,64,10,4,8205,,,0,"Missing Trove CLI tests

This patch adds following missing Trove CLI tests -
1. backup list
2. datastore list
3. flavor list
4. list
5. limit list
6. secgroup list
7. bash completion
8. help

In addition to this following optional arguments are also verified.
1. --version
2. --debug
3. --retries

Partially implements: blueprint missing-cli-tests-in-tempest

Change-Id: I23ebcc0def48b1b31d35cd92e06c35ef296b7704
",git fetch https://review.opendev.org/openstack/tempest refs/changes/95/99895/4 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/cli/__init__.py', 'etc/tempest.conf.sample', 'tempest/cli/simple_read_only/test_trove.py', 'tempest/config.py']",4,886833fbf56c7eb79a26f51c15cd08cc712a4c60,bp/missing-cli-tests-in-tempest," cfg.StrOpt('endpoint_type', default='publicURL', choices=['public', 'admin', 'internal', 'publicURL', 'adminURL', 'internalURL'], help=""The endpoint type to use for the trove service.""), cfg.BoolOpt('trove', default=False, help=""Whether or not Trove is expected to be available""),",,124,2
openstack%2Ftempest~master~Icc6a575be1f9f1fdb5e310aa56e2587e443a8e46,openstack/tempest,master,Icc6a575be1f9f1fdb5e310aa56e2587e443a8e46,Mising CLI test for Heat,ABANDONED,2014-06-16 05:54:16.000000000,2014-12-05 15:05:37.000000000,,"[{'_account_id': 3}, {'_account_id': 1849}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 8205}, {'_account_id': 8556}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 11667}]","[{'number': 1, 'created': '2014-06-16 05:54:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/04b4b1730dacaa152c3bcd2a62be73f43e1c4ef4', 'message': 'Mising CLI test for Heat\n\nThis patch adds following missing Heat CLI test -\n1. heat build info\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: Icc6a575be1f9f1fdb5e310aa56e2587e443a8e46\n'}, {'number': 2, 'created': '2014-06-18 04:56:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/7e9b8be5f1a58c970cbec9da40f73f78bd3fa386', 'message': 'Mising CLI test for Heat\n\nThis patch adds following missing Heat CLI test -\n1. heat build info\n\nIt also implements following optional argument -\n1. --verbose\n\nAddtionally it also adds output verification of some tests.\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: Icc6a575be1f9f1fdb5e310aa56e2587e443a8e46\n'}, {'number': 3, 'created': '2014-06-18 08:41:15.000000000', 'files': ['tempest/cli/__init__.py', 'tempest/cli/simple_read_only/test_heat.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/7b822b15397851ffa0cf7984e77e8188c2960eb0', 'message': 'Mising CLI test for Heat\n\nThis patch adds following missing Heat CLI test -\n1. heat build info\n\nIt also implements following optional argument -\n1. --verbose\n\nAddtionally it also adds output verification of below tests -\n1. test_heat_stack_list\n2. test_heat_version\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: Icc6a575be1f9f1fdb5e310aa56e2587e443a8e46\n'}]",15,100142,7b822b15397851ffa0cf7984e77e8188c2960eb0,45,9,3,8205,,,0,"Mising CLI test for Heat

This patch adds following missing Heat CLI test -
1. heat build info

It also implements following optional argument -
1. --verbose

Addtionally it also adds output verification of below tests -
1. test_heat_stack_list
2. test_heat_version

Partially implements: blueprint missing-cli-tests-in-tempest

Change-Id: Icc6a575be1f9f1fdb5e310aa56e2587e443a8e46
",git fetch https://review.opendev.org/openstack/tempest refs/changes/42/100142/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/cli/simple_read_only/test_heat.py'],1,04b4b1730dacaa152c3bcd2a62be73f43e1c4ef4,bp/missing-cli-tests-in-tempest," def test_heat_build_info(self): result = self.heat('build-info') build_info = self.parser.listing(result) self.assertTableStruct(build_info, ['Property', 'Value'])",,5,0
openstack%2Ftempest~master~Ib33b42a4bf8ae70db5c1ab3ac12cdbba64ca1765,openstack/tempest,master,Ib33b42a4bf8ae70db5c1ab3ac12cdbba64ca1765,Convert identity v3 comments to doc strings,ABANDONED,2014-08-04 01:59:44.000000000,2014-12-05 15:05:36.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 7191}, {'_account_id': 7227}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-04 01:59:44.000000000', 'files': ['tempest/api/identity/admin/v3/test_users.py', 'tempest/api/identity/admin/v3/test_services.py', 'tempest/api/identity/admin/v3/test_tokens.py', 'tempest/api/identity/admin/v3/test_projects_negative.py', 'tempest/api/identity/admin/v3/test_projects.py', 'tempest/api/identity/admin/v3/test_list_users.py', 'tempest/api/identity/admin/v3/test_domains.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/d0561e7c1521f6fbf85ede7996154b874288e020', 'message': ""Convert identity v3 comments to doc strings\n\nMost of these aren't particularly useful comments, however fixing them\ncan come later and we'll just convert those that do exist back to a doc\nstring.\n\nChange-Id: Ib33b42a4bf8ae70db5c1ab3ac12cdbba64ca1765\nCo-Authored-By: Anita Kuno <anteaya@anteaya.info>\n""}]",0,111613,d0561e7c1521f6fbf85ede7996154b874288e020,14,7,1,7191,,,0,"Convert identity v3 comments to doc strings

Most of these aren't particularly useful comments, however fixing them
can come later and we'll just convert those that do exist back to a doc
string.

Change-Id: Ib33b42a4bf8ae70db5c1ab3ac12cdbba64ca1765
Co-Authored-By: Anita Kuno <anteaya@anteaya.info>
",git fetch https://review.opendev.org/openstack/tempest refs/changes/13/111613/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/identity/admin/v3/test_services.py', 'tempest/api/identity/admin/v3/test_users.py', 'tempest/api/identity/admin/v3/test_tokens.py', 'tempest/api/identity/admin/v3/test_projects_negative.py', 'tempest/api/identity/admin/v3/test_projects.py', 'tempest/api/identity/admin/v3/test_list_users.py', 'tempest/api/identity/admin/v3/test_domains.py']",7,d0561e7c1521f6fbf85ede7996154b874288e020,doc-strings," """"""Test to list domains.""""""", # Test to list domains,25,25
openstack%2Ftempest~master~Ia258f32270325153361c19af647a16e7703b08a3,openstack/tempest,master,Ia258f32270325153361c19af647a16e7703b08a3,Remove double-dash from the names,ABANDONED,2014-06-15 10:50:34.000000000,2014-12-05 15:05:35.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 5803}, {'_account_id': 6167}, {'_account_id': 7350}, {'_account_id': 8556}, {'_account_id': 8576}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9828}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-06-15 10:50:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/42eb1db1298828b2100239ebadac3cd6a6204344', 'message': 'Remove double-dash in the names\n\nThe rand_name function already adds the dash.\n\nThe change made by:\nfind -name ""*.py"" | xargs -n 1 sed -i\n\'s/rand_name([\'\\\'\'""]\\(.*\\)-[\'\\\'\'""])/rand_name(\'\\\'\'\\1\'\\\'\')/\'\n\nChange-Id: Ia258f32270325153361c19af647a16e7703b08a3\n'}, {'number': 2, 'created': '2014-06-16 11:49:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/7809110ed609e0dfb26c7870d7c4d538ea4f92cc', 'message': 'Remove double-dash from the names\n\nThe rand_name function already adds the dash.\n\nThe change made by:\nfind -name ""*.py"" | xargs -n 1 sed -i \\\n\'s/rand_name([\'\\\'\'""]\\(.*\\)[_-][_-]*[\'\\\'\'""])/rand_name(\'\\\'\'\\1\'\\\'\')/\'\n\nChange-Id: Ia258f32270325153361c19af647a16e7703b08a3\n'}, {'number': 3, 'created': '2014-06-24 12:19:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/16799521ccb018b307b4ac21ede18301ea9b28fa', 'message': 'Remove double-dash from the names\n\nThe rand_name function already adds the dash.\n\nThe change made by:\nfind -name ""*.py"" | xargs -n 1 sed -i \\\n\'s/rand_name([\'\\\'\'""]\\(.*\\)[_-][_-]*[\'\\\'\'""])/rand_name(\'\\\'\'\\1\'\\\'\')/\'\n\nChange-Id: Ia258f32270325153361c19af647a16e7703b08a3\n'}, {'number': 4, 'created': '2014-06-26 10:57:53.000000000', 'files': ['tempest/api/compute/test_authorization.py', 'tempest/api/identity/admin/v3/test_users.py', 'tempest/api/compute/v3/test_live_block_migration_negative.py', 'tempest/api/identity/admin/v3/test_trusts.py', 'tempest/api/identity/admin/v3/test_tokens.py', 'tempest/scenario/manager.py', 'tempest/api/identity/admin/test_users_negative.py', 'tempest/api/volume/test_volumes_actions.py', 'tempest/thirdparty/boto/test_ec2_keys.py', 'tempest/api/compute/admin/test_quotas.py', 'tempest/api/compute/base.py', 'tempest/api/compute/v3/admin/test_aggregates.py', 'tempest/api/identity/admin/v3/test_projects.py', 'tempest/scenario/test_network_advanced_server_ops.py', 'tempest/api/volume/admin/test_multi_backend.py', 'tempest/api/identity/base.py', 'tempest/api/network/base_security_groups.py', 'tempest/api/network/test_fwaas_extensions.py', 'tempest/thirdparty/boto/test_s3_objects.py', 'tempest/stress/actions/ssh_floating.py', 'tempest/api/image/v2/test_images.py', 'tempest/thirdparty/boto/test_ec2_instance_run.py', 'tempest/api/identity/admin/v3/test_projects_negative.py', 'tempest/api/image/v2/test_images_tags.py', 'tempest/api/network/test_ports.py', 'tempest/api/volume/admin/test_volume_types_extra_specs.py', 'tempest/api/volume/test_volumes_negative.py', 'tempest/api/compute/admin/test_flavors_negative_xml.py', 'tempest/api/network/test_routers.py', 'tempest/api/network/test_security_groups.py', 'tempest/api/identity/admin/v3/test_roles.py', 'tempest/api/network/admin/test_l3_agent_scheduler.py', 'tempest/api/compute/images/test_images_oneserver_negative.py', 'tempest/api/compute/admin/test_security_groups.py', 'tempest/thirdparty/boto/test_ec2_security_groups.py', 'tempest/api/compute/admin/test_quotas_negative.py', 'tempest/api/identity/admin/v3/test_groups.py', 'tempest/api/image/v2/test_images_tags_negative.py', 'tempest/api/compute/v3/admin/test_aggregates_negative.py', 'tempest/scenario/test_large_ops.py', 'tempest/api/identity/admin/v3/test_policies.py', 'tempest/api/compute/v3/images/test_images_oneserver_negative.py', 'tempest/api/compute/security_groups/test_security_groups_negative.py', 'tempest/scenario/test_network_basic_ops.py', 'tempest/api/network/admin/test_external_network_extension.py', 'tempest/common/utils/data_utils.py', 'tempest/api/baremetal/base.py', 'tempest/api/compute/images/test_images_negative.py', 'tempest/api/compute/v3/images/test_images.py', 'tempest/api/identity/admin/test_services.py', 'tempest/api/compute/admin/test_aggregates.py', 'tempest/api/network/admin/test_lbaas_agent_scheduler.py', 'tempest/api/compute/admin/test_servers_negative.py', 'tempest/api/identity/admin/v3/test_regions.py', 'tempest/api/network/test_routers_negative.py', 'tempest/api/identity/admin/v3/test_credentials.py', 'tempest/api/network/test_metering_extensions.py', 'tempest/api/volume/admin/test_volume_quotas.py', 'tempest/api/volume/test_volumes_snapshots.py', 'tempest/api/network/test_load_balancer.py', 'tempest/api/queuing/test_queues.py', 'tempest/stress/actions/volume_attach_verify.py', 'tempest/api/compute/v3/admin/test_flavors_negative.py', 'tempest/scenario/test_stamp_pattern.py', 'tempest/thirdparty/boto/test_s3_buckets.py', 'tempest/api/identity/admin/v3/test_endpoints_negative.py', 'tempest/api/compute/v3/admin/test_servers_negative.py', 'tempest/api/network/admin/test_quotas.py', 'tempest/api/network/test_networks.py', 'tempest/api/compute/security_groups/test_security_groups.py', 'tempest/api/compute/images/test_images.py', 'tempest/api/compute/keypairs/test_keypairs_negative.py', 'tempest/scenario/test_security_groups_basic_ops.py', 'tempest/api/compute/test_live_block_migration_negative.py', 'tempest/api/compute/volumes/test_volumes_negative.py', 'tempest/api/network/test_floating_ips.py', 'tempest/api/volume/admin/test_volume_types.py', 'tempest/api/compute/admin/test_aggregates_negative.py', 'tempest/api/identity/admin/v3/test_services.py', 'tempest/api/compute/keypairs/test_keypairs.py', 'tempest/api/compute/v3/admin/test_quotas.py', 'tempest/scenario/test_volume_boot_pattern.py', 'tempest/api/compute/v3/keypairs/test_keypairs_negative.py', 'tempest/api/compute/volumes/test_volumes_get.py', 'tempest/api/network/test_vpnaas_extensions.py', 'tempest/api/volume/admin/test_volume_types_extra_specs_negative.py', 'tempest/api/compute/v3/images/test_images_negative.py', 'tempest/api/baremetal/test_chassis.py', 'tempest/api/identity/admin/test_users.py', 'tempest/api/identity/admin/v3/test_endpoints.py', 'tempest/api/network/admin/test_load_balancer_admin_actions.py', 'tempest/api/network/base.py', 'tempest/thirdparty/boto/test_s3_ec2_images.py', 'tempest/api/identity/admin/v3/test_domains.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/2261c23ccf3dabec0aa095a6c1065a1aa725e007', 'message': 'Remove double-dash from the names\n\nThe rand_name function already adds the dash.\n\nThe change mostly created by:\nfind -name ""*.py"" | xargs -n 1 sed -i \\\n\'s/rand_name([\'\\\'\'""]\\(.*\\)[_-][_-]*[\'\\\'\'""])/rand_name(\'\\\'\'\\1\'\\\'\')/\'\n\nIn tempest/scenario/manager.py the \'-\' suffix also removed from\nthe namestart.\n\nThe rand_name now forbids the names ending with \'-\'.\n\nChange-Id: Ia258f32270325153361c19af647a16e7703b08a3\n'}]",11,100086,2261c23ccf3dabec0aa095a6c1065a1aa725e007,43,12,4,5803,,,0,"Remove double-dash from the names

The rand_name function already adds the dash.

The change mostly created by:
find -name ""*.py"" | xargs -n 1 sed -i \
's/rand_name(['\''""]\(.*\)[_-][_-]*['\''""])/rand_name('\''\1'\'')/'

In tempest/scenario/manager.py the '-' suffix also removed from
the namestart.

The rand_name now forbids the names ending with '-'.

Change-Id: Ia258f32270325153361c19af647a16e7703b08a3
",git fetch https://review.opendev.org/openstack/tempest refs/changes/86/100086/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/compute/test_authorization.py', 'tempest/api/identity/admin/v3/test_users.py', 'tempest/api/compute/v3/test_live_block_migration_negative.py', 'tempest/api/identity/admin/v3/test_trusts.py', 'tempest/api/identity/admin/v3/test_tokens.py', 'tempest/scenario/manager.py', 'tempest/api/identity/admin/test_users_negative.py', 'tempest/api/volume/test_volumes_actions.py', 'tempest/thirdparty/boto/test_ec2_keys.py', 'tempest/api/compute/admin/test_quotas.py', 'tempest/api/compute/base.py', 'tempest/api/identity/admin/v3/test_projects.py', 'tempest/scenario/test_network_advanced_server_ops.py', 'tempest/api/volume/admin/test_multi_backend.py', 'tempest/api/network/base_security_groups.py', 'tempest/api/network/test_fwaas_extensions.py', 'tempest/thirdparty/boto/test_s3_objects.py', 'tempest/stress/actions/ssh_floating.py', 'tempest/api/image/v2/test_images.py', 'tempest/thirdparty/boto/test_ec2_instance_run.py', 'tempest/api/identity/admin/v3/test_projects_negative.py', 'tempest/api/image/v2/test_images_tags.py', 'tempest/api/network/test_ports.py', 'tempest/api/volume/admin/test_volume_types_extra_specs.py', 'tempest/api/volume/test_volumes_negative.py', 'tempest/api/compute/admin/test_flavors_negative_xml.py', 'tempest/api/network/test_routers.py', 'tempest/api/network/test_security_groups.py', 'tempest/api/identity/admin/v3/test_roles.py', 'tempest/api/network/admin/test_l3_agent_scheduler.py', 'tempest/api/compute/images/test_images_oneserver_negative.py', 'tempest/api/compute/admin/test_security_groups.py', 'tempest/thirdparty/boto/test_ec2_security_groups.py', 'tempest/api/compute/admin/test_quotas_negative.py', 'tempest/api/identity/admin/v3/test_groups.py', 'tempest/api/image/v2/test_images_tags_negative.py', 'tempest/scenario/test_large_ops.py', 'tempest/api/identity/admin/v3/test_policies.py', 'tempest/api/compute/v3/images/test_images_oneserver_negative.py', 'tempest/api/compute/security_groups/test_security_groups_negative.py', 'tempest/scenario/test_network_basic_ops.py', 'tempest/api/network/admin/test_external_network_extension.py', 'tempest/api/baremetal/base.py', 'tempest/api/compute/images/test_images_negative.py', 'tempest/api/compute/v3/images/test_images.py', 'tempest/api/identity/admin/test_services.py', 'tempest/api/network/admin/test_lbaas_agent_scheduler.py', 'tempest/api/compute/admin/test_servers_negative.py', 'tempest/api/identity/admin/v3/test_regions.py', 'tempest/api/network/test_routers_negative.py', 'tempest/api/identity/admin/v3/test_credentials.py', 'tempest/api/network/test_metering_extensions.py', 'tempest/api/volume/test_volumes_snapshots.py', 'tempest/api/network/test_load_balancer.py', 'tempest/api/queuing/test_queues.py', 'tempest/stress/actions/volume_attach_verify.py', 'tempest/api/compute/v3/admin/test_flavors_negative.py', 'tempest/scenario/test_stamp_pattern.py', 'tempest/thirdparty/boto/test_s3_buckets.py', 'tempest/api/identity/admin/v3/test_endpoints_negative.py', 'tempest/api/compute/v3/admin/test_servers_negative.py', 'tempest/api/network/test_networks.py', 'tempest/api/compute/security_groups/test_security_groups.py', 'tempest/api/compute/images/test_images.py', 'tempest/api/compute/keypairs/test_keypairs_negative.py', 'tempest/scenario/test_security_groups_basic_ops.py', 'tempest/api/compute/test_live_block_migration_negative.py', 'tempest/api/compute/volumes/test_volumes_negative.py', 'tempest/api/network/test_floating_ips.py', 'tempest/api/volume/admin/test_volume_types.py', 'tempest/api/identity/admin/v3/test_services.py', 'tempest/api/compute/keypairs/test_keypairs.py', 'tempest/api/compute/v3/admin/test_quotas.py', 'tempest/scenario/test_volume_boot_pattern.py', 'tempest/api/compute/v3/keypairs/test_keypairs_negative.py', 'tempest/api/compute/volumes/test_volumes_get.py', 'tempest/api/network/test_vpnaas_extensions.py', 'tempest/api/volume/admin/test_volume_types_extra_specs_negative.py', 'tempest/api/compute/v3/images/test_images_negative.py', 'tempest/api/baremetal/test_chassis.py', 'tempest/api/identity/admin/test_users.py', 'tempest/api/identity/admin/v3/test_endpoints.py', 'tempest/api/network/admin/test_load_balancer_admin_actions.py', 'tempest/api/network/base.py', 'tempest/thirdparty/boto/test_s3_ec2_images.py', 'tempest/api/identity/admin/v3/test_domains.py']",86,42eb1db1298828b2100239ebadac3cd6a6204344,double-dash," data_utils.rand_name('domain'), description=data_utils.rand_name('domain-desc')) d_name = data_utils.rand_name('domain') d_desc = data_utils.rand_name('domain-desc') new_desc = data_utils.rand_name('new-desc') new_name = data_utils.rand_name('new-name')"," data_utils.rand_name('domain-'), description=data_utils.rand_name('domain-desc-')) d_name = data_utils.rand_name('domain-') d_desc = data_utils.rand_name('domain-desc-') new_desc = data_utils.rand_name('new-desc-') new_name = data_utils.rand_name('new-name-')",295,295
openstack%2Ftempest~master~I2ae0421758a77014d19bef98d03ba6787792ec1d,openstack/tempest,master,I2ae0421758a77014d19bef98d03ba6787792ec1d,Comments to doc strings,ABANDONED,2014-08-04 01:26:48.000000000,2014-12-05 15:05:34.000000000,,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 2750}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-04 01:26:48.000000000', 'files': ['tempest/api/identity/admin/test_services.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/a2bfad3e64b0356815f4cae79267c403ab2cb98b', 'message': 'Comments to doc strings\n\nThis patch changes some more comments to doc strings.\n\nChange-Id: I2ae0421758a77014d19bef98d03ba6787792ec1d\n'}]",0,111608,a2bfad3e64b0356815f4cae79267c403ab2cb98b,10,5,1,6316,,,0,"Comments to doc strings

This patch changes some more comments to doc strings.

Change-Id: I2ae0421758a77014d19bef98d03ba6787792ec1d
",git fetch https://review.opendev.org/openstack/tempest refs/changes/08/111608/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/identity/admin/test_services.py'],1,a2bfad3e64b0356815f4cae79267c403ab2cb98b,doc-strings," """"""Create a service only with name and type."""""" """"""Create, List, Verify and Delete Services."""""""," # Create a service only with name and type # Create, List, Verify and Delete Services",2,2
openstack%2Ftempest~master~Ie1c90b7e17521e1c36c63a2c66e10e8c31f903c1,openstack/tempest,master,Ie1c90b7e17521e1c36c63a2c66e10e8c31f903c1,multitenant test,ABANDONED,2014-07-22 06:39:14.000000000,2014-12-05 15:05:33.000000000,,"[{'_account_id': 3}, {'_account_id': 1795}, {'_account_id': 1868}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 8576}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-07-22 06:39:14.000000000', 'files': ['tempest/scenario/test_network_multi_tenant.py', 'tempest/scenario/managerMulti.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/bef491de9b1f943eecc2e247394b10b33e58ad71', 'message': 'multitenant test\n\nChange-Id: Ie1c90b7e17521e1c36c63a2c66e10e8c31f903c1\n'}]",0,108589,bef491de9b1f943eecc2e247394b10b33e58ad71,15,9,1,1868,,,0,"multitenant test

Change-Id: Ie1c90b7e17521e1c36c63a2c66e10e8c31f903c1
",git fetch https://review.opendev.org/openstack/tempest refs/changes/89/108589/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/scenario/test_network_multi_tenant.py', 'tempest/scenario/managerMulti.py']",2,bef491de9b1f943eecc2e247394b10b33e58ad71,tempest-892839,"# Copyright 2012 OpenStack Foundation # Copyright 2013 IBM Corp. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import logging import os import re import six import subprocess import time from heatclient import exc as heat_exceptions import netaddr from neutronclient.common import exceptions as exc from novaclient import exceptions as nova_exceptions from tempest.api.network import common as net_common from tempest import auth from tempest import clients from tempest.common import debug from tempest.common import isolated_creds from tempest.common.utils import data_utils from tempest.common.utils.linux import remote_client from tempest import config from tempest import exceptions from tempest.openstack.common import log from tempest.openstack.common import timeutils import tempest.test CONF = config.CONF LOG = log.getLogger(__name__) # NOTE(afazekas): Workaround for the stdout logging LOG_nova_client = logging.getLogger('novaclient.client') LOG_nova_client.addHandler(log.NullHandler()) LOG_cinder_client = logging.getLogger('cinderclient.client') LOG_cinder_client.addHandler(log.NullHandler()) class OfficialClientTestA(tempest.test.BaseTestCase): """""" Official Client test base class for scenario testing. Official Client tests are tests that have the following characteristics: * Test basic operations of an API, typically in an order that a regular user would perform those operations * Test only the correct inputs and action paths -- no fuzz or random input data is sent, only valid inputs. * Use only the default client tool for calling an API """""" @classmethod def setUpClass(cls): super(OfficialClientTestA, cls).setUpClass() cls.isolated_credsA = isolated_creds.IsolatedCreds( cls.__name__, tempest_client=False, network_resources=cls.network_resources) cls.managerA = clients.OfficialClientManager( credentials=cls.credentialsA()) cls.compute_client = cls.managerA.compute_client cls.image_client = cls.managerA.image_client cls.baremetal_client = cls.managerA.baremetal_client cls.identity_client = cls.managerA.identity_client cls.network_client = cls.managerA.network_client cls.volume_client = cls.managerA.volume_client cls.object_storage_client = cls.managerA.object_storage_client cls.orchestration_client = cls.managerA.orchestration_client cls.data_processing_client = cls.managerA.data_processing_client cls.ceilometer_client = cls.managerA.ceilometer_client cls.resource_keys = {} cls.os_resources = [] @classmethod def _get_credentialsA(cls, get_creds, ctype): if CONF.compute.allow_tenant_isolation: creds = get_creds() else: creds = auth.get_default_credentials(ctype) return creds @classmethod def credentialsA(cls): return cls._get_credentialsA(cls.isolated_credsA.get_primary_creds, 'user') @classmethod def alt_credentialsA(cls): return cls._get_credentialsA(cls.isolated_credsA.get_alt_creds, 'alt_user') @classmethod def admin_credentialsA(cls): return cls._get_credentialsA(cls.isolated_credsA.get_admin_creds, 'identity_admin') @staticmethod def cleanup_resource(resource, test_name): LOG.debug(""Deleting %r from shared resources of %s"" % (resource, test_name)) try: # OpenStack resources are assumed to have a delete() # method which destroys the resource... resource.delete() except Exception as e: # If the resource is already missing, mission accomplished. # - Status code tolerated as a workaround for bug 1247568 # - HTTPNotFound tolerated as this is currently raised when # attempting to delete an already-deleted heat stack. if (e.__class__.__name__ in ('NotFound', 'HTTPNotFound') or (hasattr(e, 'status_code') and e.status_code == 404)): return raise def is_deletion_complete(): # Deletion testing is only required for objects whose # existence cannot be checked via retrieval. if isinstance(resource, dict): return True try: resource.get() except Exception as e: # Clients are expected to return an exception # called 'NotFound' if retrieval fails. if e.__class__.__name__ == 'NotFound': return True raise return False # Block until resource deletion has completed or timed-out tempest.test.call_until_true(is_deletion_complete, 10, 1) @classmethod def tearDownClass(cls): # NOTE(jaypipes): Because scenario tests are typically run in a # specific order, and because test methods in scenario tests # generally create resources in a particular order, we destroy # resources in the reverse order in which resources are added to # the scenario test class object while cls.os_resources: thing = cls.os_resources.pop() cls.cleanup_resource(thing, cls.__name__) cls.isolated_credsA.clear_isolated_creds() super(OfficialClientTestA, cls).tearDownClass() @classmethod def set_resource(cls, key, thing): LOG.debug(""Adding %r to shared resources of %s"" % (thing, cls.__name__)) cls.resource_keys[key] = thing cls.os_resources.append(thing) @classmethod def get_resource(cls, key): return cls.resource_keys[key] @classmethod def remove_resource(cls, key): thing = cls.resource_keys[key] cls.os_resources.remove(thing) del cls.resource_keys[key] def status_timeout(self, things, thing_id, expected_status, error_status='ERROR', not_found_exception=nova_exceptions.NotFound): """""" Given a thing and an expected status, do a loop, sleeping for a configurable amount of time, checking for the expected status to show. At any time, if the returned status of the thing is ERROR, fail out. """""" self._status_timeout(things, thing_id, expected_status=expected_status, error_status=error_status, not_found_exception=not_found_exception) def delete_timeout(self, things, thing_id, error_status='ERROR', not_found_exception=nova_exceptions.NotFound): """""" Given a thing, do a loop, sleeping for a configurable amount of time, checking for the deleted status to show. At any time, if the returned status of the thing is ERROR, fail out. """""" self._status_timeout(things, thing_id, allow_notfound=True, error_status=error_status, not_found_exception=not_found_exception) def _status_timeout(self, things, thing_id, expected_status=None, allow_notfound=False, error_status='ERROR', not_found_exception=nova_exceptions.NotFound): log_status = expected_status if expected_status else '' if allow_notfound: log_status += ' or NotFound' if log_status != '' else 'NotFound' def check_status(): # python-novaclient has resources available to its client # that all implement a get() method taking an identifier # for the singular resource to retrieve. try: thing = things.get(thing_id) except not_found_exception: if allow_notfound: return True else: raise new_status = thing.status # Some components are reporting error status in lower case # so case sensitive comparisons can really mess things # up. if new_status.lower() == error_status.lower(): message = (""%s failed to get to expected status (%s). "" ""In %s state."") % (thing, expected_status, new_status) raise exceptions.BuildErrorException(message, server_id=thing_id) elif new_status == expected_status and expected_status is not None: return True # All good. LOG.debug(""Waiting for %s to get to %s status. "" ""Currently in %s status"", thing, log_status, new_status) if not tempest.test.call_until_true( check_status, CONF.compute.build_timeout, CONF.compute.build_interval): message = (""Timed out waiting for thing %s "" ""to become %s"") % (thing_id, log_status) raise exceptions.TimeoutException(message) def _create_loginable_secgroup_rule_nova(self, client=None, secgroup_id=None): if client is None: client = self.compute_client if secgroup_id is None: sgs = client.security_groups.list() for sg in sgs: if sg.name == 'default': secgroup_id = sg.id # These rules are intended to permit inbound ssh and icmp # traffic from all sources, so no group_id is provided. # Setting a group_id would only permit traffic from ports # belonging to the same security group. rulesets = [ { # ssh 'ip_protocol': 'tcp', 'from_port': 22, 'to_port': 22, 'cidr': '0.0.0.0/0', }, { # ping 'ip_protocol': 'icmp', 'from_port': -1, 'to_port': -1, 'cidr': '0.0.0.0/0', } ] rules = list() for ruleset in rulesets: sg_rule = client.security_group_rules.create(secgroup_id, **ruleset) rules.append(sg_rule) return rules def _create_security_group_nova(self, client=None, namestart='secgroup-smoke-'): if client is None: client = self.compute_client # Create security group sg_name = data_utils.rand_name(namestart) sg_desc = sg_name + "" description"" secgroup = client.security_groups.create(sg_name, sg_desc) self.assertEqual(secgroup.name, sg_name) self.assertEqual(secgroup.description, sg_desc) self.set_resource(sg_name, secgroup) # Add rules to the security group self._create_loginable_secgroup_rule_nova(client, secgroup.id) return secgroup def create_server(self, client=None, name=None, image=None, flavor=None, wait=True, create_kwargs={}): if client is None: client = self.compute_client if name is None: name = data_utils.rand_name('scenario-server-') if image is None: image = CONF.compute.image_ref if flavor is None: flavor = CONF.compute.flavor_ref fixed_network_name = CONF.compute.fixed_network_name if 'nics' not in create_kwargs and fixed_network_name: networks = client.networks.list() # If several networks found, set the NetID on which to connect the # server to avoid the following error ""Multiple possible networks # found, use a Network ID to be more specific."" # See Tempest #1250866 if len(networks) > 1: for network in networks: if network.label == fixed_network_name: create_kwargs['nics'] = [{'net-id': network.id}] break # If we didn't find the network we were looking for : else: msg = (""The network on which the NIC of the server must "" ""be connected can not be found : "" ""fixed_network_name=%s. Starting instance without "" ""specifying a network."") % fixed_network_name LOG.info(msg) LOG.debug(""Creating a server (name: %s, image: %s, flavor: %s)"", name, image, flavor) server = client.servers.create(name, image, flavor, **create_kwargs) self.assertEqual(server.name, name) self.set_resource(name, server) if wait: self.status_timeout(client.servers, server.id, 'ACTIVE') # The instance retrieved on creation is missing network # details, necessitating retrieval after it becomes active to # ensure correct details. server = client.servers.get(server.id) self.set_resource(name, server) LOG.debug(""Created server: %s"", server) return server def create_volume(self, client=None, size=1, name=None, snapshot_id=None, imageRef=None): if client is None: client = self.volume_client if name is None: name = data_utils.rand_name('scenario-volume-') LOG.debug(""Creating a volume (size: %s, name: %s)"", size, name) volume = client.volumes.create(size=size, display_name=name, snapshot_id=snapshot_id, imageRef=imageRef) self.set_resource(name, volume) self.assertEqual(name, volume.display_name) self.status_timeout(client.volumes, volume.id, 'available') LOG.debug(""Created volume: %s"", volume) return volume def create_server_snapshot(self, server, compute_client=None, image_client=None, name=None): if compute_client is None: compute_client = self.compute_client if image_client is None: image_client = self.image_client if name is None: name = data_utils.rand_name('scenario-snapshot-') LOG.debug(""Creating a snapshot image for server: %s"", server.name) image_id = compute_client.servers.create_image(server, name) self.addCleanup(image_client.images.delete, image_id) self.status_timeout(image_client.images, image_id, 'active') snapshot_image = image_client.images.get(image_id) self.assertEqual(name, snapshot_image.name) LOG.debug(""Created snapshot image %s for server %s"", snapshot_image.name, server.name) return snapshot_image def create_keypair(self, client=None, name=None): if client is None: client = self.compute_client if name is None: name = data_utils.rand_name('scenario-keypair-') keypair = client.keypairs.create(name) self.assertEqual(keypair.name, name) self.set_resource(name, keypair) return keypair def get_remote_client(self, server_or_ip, username=None, private_key=None): if isinstance(server_or_ip, six.string_types): ip = server_or_ip else: network_name_for_ssh = CONF.compute.network_for_ssh ip = server_or_ip.networks[network_name_for_ssh][0] if username is None: username = CONF.scenario.ssh_user if private_key is None: private_key = self.keypair.private_key linux_client = remote_client.RemoteClient(ip, username, pkey=private_key) try: linux_client.validate_authentication() except exceptions.SSHTimeout: LOG.exception('ssh connection to %s failed' % ip) debug.log_net_debug() raise return linux_client def _log_console_output(self, servers=None): if not servers: servers = self.compute_client.servers.list() for server in servers: LOG.debug('Console output for %s', server.id) LOG.debug(server.get_console_output()) def wait_for_volume_status(self, status): volume_id = self.volume.id self.status_timeout( self.volume_client.volumes, volume_id, status) def _image_create(self, name, fmt, path, properties={}): name = data_utils.rand_name('%s-' % name) image_file = open(path, 'rb') self.addCleanup(image_file.close) params = { 'name': name, 'container_format': fmt, 'disk_format': fmt, 'is_public': 'False', } params.update(properties) image = self.image_client.images.create(**params) self.addCleanup(self.image_client.images.delete, image) self.assertEqual(""queued"", image.status) image.update(data=image_file) return image.id def glance_image_create(self): qcow2_img_path = (CONF.scenario.img_dir + ""/"" + CONF.scenario.qcow2_img_file) aki_img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.aki_img_file ari_img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.ari_img_file ami_img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.ami_img_file LOG.debug(""paths: img: %s, ami: %s, ari: %s, aki: %s"" % (qcow2_img_path, ami_img_path, ari_img_path, aki_img_path)) try: self.image = self._image_create('scenario-img', 'bare', qcow2_img_path, properties={'disk_format': 'qcow2'}) except IOError: LOG.debug(""A qcow2 image was not found. Try to get a uec image."") kernel = self._image_create('scenario-aki', 'aki', aki_img_path) ramdisk = self._image_create('scenario-ari', 'ari', ari_img_path) properties = { 'properties': {'kernel_id': kernel, 'ramdisk_id': ramdisk} } self.image = self._image_create('scenario-ami', 'ami', path=ami_img_path, properties=properties) class OfficialClientTestB(tempest.test.BaseTestCase): """""" Official Client test base class for scenario testing. Official Client tests are tests that have the following characteristics: * Test basic operations of an API, typically in an order that a regular user would perform those operations * Test only the correct inputs and action paths -- no fuzz or random input data is sent, only valid inputs. * Use only the default client tool for calling an API """""" @classmethod def setUpClass(cls): super(OfficialClientTestB, cls).setUpClass() cls.isolated_credsB = isolated_creds.IsolatedCreds( cls.__name__, tempest_client=False, network_resources=cls.network_resources) cls.managerB = clients.OfficialClientManager( credentials=cls.credentialsB()) cls.compute_client = cls.managerB.compute_client cls.image_client = cls.managerB.image_client cls.baremetal_client = cls.managerB.baremetal_client cls.identity_client = cls.managerB.identity_client cls.network_client = cls.managerB.network_client cls.volume_client = cls.managerB.volume_client cls.object_storage_client = cls.managerB.object_storage_client cls.orchestration_client = cls.managerB.orchestration_client cls.data_processing_client = cls.managerB.data_processing_client cls.ceilometer_client = cls.managerB.ceilometer_client cls.resource_keys = {} cls.os_resources = [] @classmethod def _get_credentialsB(cls, get_creds, ctype): if CONF.compute.allow_tenant_isolation: creds = get_creds() else: creds = auth.get_default_credentials(ctype) return creds @classmethod def credentialsB(cls): return cls._get_credentialsB(cls.isolated_credsB.get_primary_creds, 'user') @classmethod def alt_credentialsiB(cls): return cls._get_credentialsB(cls.isolated_credsB.get_alt_creds, 'alt_user') @classmethod def admin_credentialsB(cls): return cls._get_credentialsB(cls.isolated_credsB.get_admin_creds, 'identity_admin') @staticmethod def cleanup_resource(resource, test_name): LOG.debug(""Deleting %r from shared resources of %s"" % (resource, test_name)) try: # OpenStack resources are assumed to have a delete() # method which destroys the resource... resource.delete() except Exception as e: # If the resource is already missing, mission accomplished. # - Status code tolerated as a workaround for bug 1247568 # - HTTPNotFound tolerated as this is currently raised when # attempting to delete an already-deleted heat stack. if (e.__class__.__name__ in ('NotFound', 'HTTPNotFound') or (hasattr(e, 'status_code') and e.status_code == 404)): return raise def is_deletion_complete(): # Deletion testing is only required for objects whose # existence cannot be checked via retrieval. if isinstance(resource, dict): return True try: resource.get() except Exception as e: # Clients are expected to return an exception # called 'NotFound' if retrieval fails. if e.__class__.__name__ == 'NotFound': return True raise return False # Block until resource deletion has completed or timed-out tempest.test.call_until_true(is_deletion_complete, 10, 1) @classmethod def tearDownClass(cls): # NOTE(jaypipes): Because scenario tests are typically run in a # specific order, and because test methods in scenario tests # generally create resources in a particular order, we destroy # resources in the reverse order in which resources are added to # the scenario test class object while cls.os_resources: thing = cls.os_resources.pop() cls.cleanup_resource(thing, cls.__name__) cls.isolated_credsB.clear_isolated_creds() super(OfficialClientTestB, cls).tearDownClass() @classmethod def set_resource(cls, key, thing): LOG.debug(""Adding %r to shared resources of %s"" % (thing, cls.__name__)) cls.resource_keys[key] = thing cls.os_resources.append(thing) @classmethod def get_resource(cls, key): return cls.resource_keys[key] @classmethod def remove_resource(cls, key): thing = cls.resource_keys[key] cls.os_resources.remove(thing) del cls.resource_keys[key] def status_timeout(self, things, thing_id, expected_status, error_status='ERROR', not_found_exception=nova_exceptions.NotFound): """""" Given a thing and an expected status, do a loop, sleeping for a configurable amount of time, checking for the expected status to show. At any time, if the returned status of the thing is ERROR, fail out. """""" self._status_timeout(things, thing_id, expected_status=expected_status, error_status=error_status, not_found_exception=not_found_exception) def delete_timeout(self, things, thing_id, error_status='ERROR', not_found_exception=nova_exceptions.NotFound): """""" Given a thing, do a loop, sleeping for a configurable amount of time, checking for the deleted status to show. At any time, if the returned status of the thing is ERROR, fail out. """""" self._status_timeout(things, thing_id, allow_notfound=True, error_status=error_status, not_found_exception=not_found_exception) def _status_timeout(self, things, thing_id, expected_status=None, allow_notfound=False, error_status='ERROR', not_found_exception=nova_exceptions.NotFound): log_status = expected_status if expected_status else '' if allow_notfound: log_status += ' or NotFound' if log_status != '' else 'NotFound' def check_status(): # python-novaclient has resources available to its client # that all implement a get() method taking an identifier # for the singular resource to retrieve. try: thing = things.get(thing_id) except not_found_exception: if allow_notfound: return True else: raise new_status = thing.status # Some components are reporting error status in lower case # so case sensitive comparisons can really mess things # up. if new_status.lower() == error_status.lower(): message = (""%s failed to get to expected status (%s). "" ""In %s state."") % (thing, expected_status, new_status) raise exceptions.BuildErrorException(message, server_id=thing_id) elif new_status == expected_status and expected_status is not None: return True # All good. LOG.debug(""Waiting for %s to get to %s status. "" ""Currently in %s status"", thing, log_status, new_status) if not tempest.test.call_until_true( check_status, CONF.compute.build_timeout, CONF.compute.build_interval): message = (""Timed out waiting for thing %s "" ""to become %s"") % (thing_id, log_status) raise exceptions.TimeoutException(message) def _create_loginable_secgroup_rule_nova(self, client=None, secgroup_id=None): if client is None: client = self.compute_client if secgroup_id is None: sgs = client.security_groups.list() for sg in sgs: if sg.name == 'default': secgroup_id = sg.id # These rules are intended to permit inbound ssh and icmp # traffic from all sources, so no group_id is provided. # Setting a group_id would only permit traffic from ports # belonging to the same security group. rulesets = [ { # ssh 'ip_protocol': 'tcp', 'from_port': 22, 'to_port': 22, 'cidr': '0.0.0.0/0', }, { # ping 'ip_protocol': 'icmp', 'from_port': -1, 'to_port': -1, 'cidr': '0.0.0.0/0', } ] rules = list() for ruleset in rulesets: sg_rule = client.security_group_rules.create(secgroup_id, **ruleset) rules.append(sg_rule) return rules def _create_security_group_nova(self, client=None, namestart='secgroup-smoke-'): if client is None: client = self.compute_client # Create security group sg_name = data_utils.rand_name(namestart) sg_desc = sg_name + "" description"" secgroup = client.security_groups.create(sg_name, sg_desc) self.assertEqual(secgroup.name, sg_name) self.assertEqual(secgroup.description, sg_desc) self.set_resource(sg_name, secgroup) # Add rules to the security group self._create_loginable_secgroup_rule_nova(client, secgroup.id) return secgroup def create_server(self, client=None, name=None, image=None, flavor=None, wait=True, create_kwargs={}): if client is None: client = self.compute_client if name is None: name = data_utils.rand_name('scenario-server-') if image is None: image = CONF.compute.image_ref if flavor is None: flavor = CONF.compute.flavor_ref fixed_network_name = CONF.compute.fixed_network_name if 'nics' not in create_kwargs and fixed_network_name: networks = client.networks.list() # If several networks found, set the NetID on which to connect the # server to avoid the following error ""Multiple possible networks # found, use a Network ID to be more specific."" # See Tempest #1250866 if len(networks) > 1: for network in networks: if network.label == fixed_network_name: create_kwargs['nics'] = [{'net-id': network.id}] break # If we didn't find the network we were looking for : else: msg = (""The network on which the NIC of the server must "" ""be connected can not be found : "" ""fixed_network_name=%s. Starting instance without "" ""specifying a network."") % fixed_network_name LOG.info(msg) LOG.debug(""Creating a server (name: %s, image: %s, flavor: %s)"", name, image, flavor) server = client.servers.create(name, image, flavor, **create_kwargs) self.assertEqual(server.name, name) self.set_resource(name, server) if wait: self.status_timeout(client.servers, server.id, 'ACTIVE') # The instance retrieved on creation is missing network # details, necessitating retrieval after it becomes active to # ensure correct details. server = client.servers.get(server.id) self.set_resource(name, server) LOG.debug(""Created server: %s"", server) return server def create_volume(self, client=None, size=1, name=None, snapshot_id=None, imageRef=None): if client is None: client = self.volume_client if name is None: name = data_utils.rand_name('scenario-volume-') LOG.debug(""Creating a volume (size: %s, name: %s)"", size, name) volume = client.volumes.create(size=size, display_name=name, snapshot_id=snapshot_id, imageRef=imageRef) self.set_resource(name, volume) self.assertEqual(name, volume.display_name) self.status_timeout(client.volumes, volume.id, 'available') LOG.debug(""Created volume: %s"", volume) return volume def create_server_snapshot(self, server, compute_client=None, image_client=None, name=None): if compute_client is None: compute_client = self.compute_client if image_client is None: image_client = self.image_client if name is None: name = data_utils.rand_name('scenario-snapshot-') LOG.debug(""Creating a snapshot image for server: %s"", server.name) image_id = compute_client.servers.create_image(server, name) self.addCleanup(image_client.images.delete, image_id) self.status_timeout(image_client.images, image_id, 'active') snapshot_image = image_client.images.get(image_id) self.assertEqual(name, snapshot_image.name) LOG.debug(""Created snapshot image %s for server %s"", snapshot_image.name, server.name) return snapshot_image def create_keypair(self, client=None, name=None): if client is None: client = self.compute_client if name is None: name = data_utils.rand_name('scenario-keypair-') keypair = client.keypairs.create(name) self.assertEqual(keypair.name, name) self.set_resource(name, keypair) return keypair def get_remote_client(self, server_or_ip, username=None, private_key=None): if isinstance(server_or_ip, six.string_types): ip = server_or_ip else: network_name_for_ssh = CONF.compute.network_for_ssh ip = server_or_ip.networks[network_name_for_ssh][0] if username is None: username = CONF.scenario.ssh_user if private_key is None: private_key = self.keypair.private_key linux_client = remote_client.RemoteClient(ip, username, pkey=private_key) try: linux_client.validate_authentication() except exceptions.SSHTimeout: LOG.exception('ssh connection to %s failed' % ip) debug.log_net_debug() raise return linux_client def _log_console_output(self, servers=None): if not servers: servers = self.compute_client.servers.list() for server in servers: LOG.debug('Console output for %s', server.id) LOG.debug(server.get_console_output()) def wait_for_volume_status(self, status): volume_id = self.volume.id self.status_timeout( self.volume_client.volumes, volume_id, status) def _image_create(self, name, fmt, path, properties={}): name = data_utils.rand_name('%s-' % name) image_file = open(path, 'rb') self.addCleanup(image_file.close) params = { 'name': name, 'container_format': fmt, 'disk_format': fmt, 'is_public': 'False', } params.update(properties) image = self.image_client.images.create(**params) self.addCleanup(self.image_client.images.delete, image) self.assertEqual(""queued"", image.status) image.update(data=image_file) return image.id def glance_image_create(self): qcow2_img_path = (CONF.scenario.img_dir + ""/"" + CONF.scenario.qcow2_img_file) aki_img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.aki_img_file ari_img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.ari_img_file ami_img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.ami_img_file LOG.debug(""paths: img: %s, ami: %s, ari: %s, aki: %s"" % (qcow2_img_path, ami_img_path, ari_img_path, aki_img_path)) try: self.image = self._image_create('scenario-img', 'bare', qcow2_img_path, properties={'disk_format': 'qcow2'}) except IOError: LOG.debug(""A qcow2 image was not found. Try to get a uec image."") kernel = self._image_create('scenario-aki', 'aki', aki_img_path) ramdisk = self._image_create('scenario-ari', 'ari', ari_img_path) properties = { 'properties': {'kernel_id': kernel, 'ramdisk_id': ramdisk} } self.image = self._image_create('scenario-ami', 'ami', path=ami_img_path, properties=properties) # power/provision states as of icehouse class BaremetalPowerStates(object): """"""Possible power states of an Ironic node."""""" POWER_ON = 'power on' POWER_OFF = 'power off' REBOOT = 'rebooting' SUSPEND = 'suspended' class BaremetalProvisionStates(object): """"""Possible provision states of an Ironic node."""""" NOSTATE = None INIT = 'initializing' ACTIVE = 'active' BUILDING = 'building' DEPLOYWAIT = 'wait call-back' DEPLOYING = 'deploying' DEPLOYFAIL = 'deploy failed' DEPLOYDONE = 'deploy complete' DELETING = 'deleting' DELETED = 'deleted' ERROR = 'error' class NetworkScenarioMultitATest(OfficialClientTestA): """""" Base class for network scenario tests """""" @classmethod def check_preconditionsA(cls): if (CONF.service_available.neutron): cls.enabled = True # verify that neutron_available is telling the truth try: cls.network_client.list_networks() except exc.EndpointNotFound: cls.enabled = False raise else: cls.enabled = False msg = 'Neutron not available' raise cls.skipException(msg) @classmethod def setUpClass(cls): super(NetworkScenarioMultitATest, cls).setUpClass() cls.tenant_idA = cls.managerA.identity_client.tenant_id def _create_networkA(self, tenant_id, namestart='network-smoke-'): name = data_utils.rand_name(namestart) body = dict( network=dict( name=name, tenant_id=tenant_id, ), ) result = self.network_client.create_network(body=body) network = net_common.DeletableNetwork(client=self.network_client, **result['network']) self.assertEqual(network.name, name) self.set_resource(name, network) return network def _list_networksA(self, **kwargs): nets = self.network_client.list_networks(**kwargs) return nets['networks'] def _list_subnetsA(self, **kwargs): subnets = self.network_client.list_subnets(**kwargs) return subnets['subnets'] def _list_routersA(self, **kwargs): routers = self.network_client.list_routers(**kwargs) return routers['routers'] def _list_portsA(self, **kwargs): ports = self.network_client.list_ports(**kwargs) return ports['ports'] def _get_tenant_own_network_numA(self, tenant_id): nets = self._list_networks(tenant_id=tenant_id) return len(nets) def _get_tenant_own_subnet_numA(self, tenant_id): subnets = self._list_subnets(tenant_id=tenant_id) return len(subnets) def _get_tenant_own_port_numA(self, tenant_id): ports = self._list_ports(tenant_id=tenant_id) return len(ports) def _create_subnetA(self, network, namestart='subnet-smoke-', **kwargs): """""" Create a subnet for the given network within the cidr block configured for tenant networks. """""" def cidr_in_use(cidr, tenant_id): """""" :return True if subnet with cidr already exist in tenant False else """""" cidr_in_use = self._list_subnetsA(tenant_id=tenant_id, cidr=cidr) return len(cidr_in_use) != 0 tenant_cidr = netaddr.IPNetwork(CONF.network.tenant_network_cidr) result = None # Repeatedly attempt subnet creation with sequential cidr # blocks until an unallocated block is found. for subnet_cidr in tenant_cidr.subnet( CONF.network.tenant_network_mask_bits): str_cidr = str(subnet_cidr) if cidr_in_use(str_cidr, tenant_id=network.tenant_id): continue body = dict( subnet=dict( name=data_utils.rand_name(namestart), ip_version=4, network_id=network.id, tenant_id=network.tenant_id, cidr=str_cidr, ), ) body['subnet'].update(kwargs) try: result = self.network_client.create_subnet(body=body) break except exc.NeutronClientException as e: is_overlapping_cidr = 'overlaps with another subnet' in str(e) if not is_overlapping_cidr: raise self.assertIsNotNone(result, 'Unable to allocate tenant network') subnet = net_common.DeletableSubnet(client=self.network_client, **result['subnet']) self.assertEqual(subnet.cidr, str_cidr) self.set_resource(data_utils.rand_name(namestart), subnet) return subnet def _create_portA(self, network, namestart='port-quotatest-'): name = data_utils.rand_name(namestart) body = dict( port=dict(name=name, network_id=network.id, tenant_id=network.tenant_id)) result = self.network_client.create_port(body=body) self.assertIsNotNone(result, 'Unable to allocate port') port = net_common.DeletablePort(client=self.network_client, **result['port']) self.set_resource(name, port) return port def _get_server_port_idA(self, server, ip_addr=None): ports = self._list_portsA(device_id=server.id, fixed_ip=ip_addr) self.assertEqual(len(ports), 1, ""Unable to determine which port to target."") return ports[0]['id'] def _create_floating_ipA(self, thing, external_network_id, port_id=None): if not port_id: port_id = self._get_server_port_idA(thing) body = dict( floatingip=dict( floating_network_id=external_network_id, port_id=port_id, tenant_id=thing.tenant_id, ) ) result = self.network_client.create_floatingip(body=body) floating_ip = net_common.DeletableFloatingIp( client=self.network_client, **result['floatingip']) self.set_resource(data_utils.rand_name('floatingip-'), floating_ip) return floating_ip def _associate_floating_ipA(self, floating_ip, server): port_id = self._get_server_port_idA(server) floating_ip.update(port_id=port_id) self.assertEqual(port_id, floating_ip.port_id) return floating_ip def _disassociate_floating_ipA(self, floating_ip): """""" :param floating_ip: type DeletableFloatingIp """""" floating_ip.update(port_id=None) self.assertIsNone(floating_ip.port_id) return floating_ip def _ping_ip_addressA(self, ip_address, should_succeed=True): cmd = ['ping', '-c1', '-w1', ip_address] def ping(): proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE) proc.wait() return (proc.returncode == 0) == should_succeed return tempest.test.call_until_true( ping, CONF.compute.ping_timeout, 1) def _create_poolA(self, lb_method, protocol, subnet_id): """"""Wrapper utility that returns a test pool."""""" name = data_utils.rand_name('pool-') body = { ""pool"": { ""protocol"": protocol, ""name"": name, ""subnet_id"": subnet_id, ""lb_method"": lb_method } } resp = self.network_client.create_pool(body=body) pool = net_common.DeletablePool(client=self.network_client, **resp['pool']) self.assertEqual(pool['name'], name) self.set_resource(name, pool) return pool def _create_memberA(self, address, protocol_port, pool_id): """"""Wrapper utility that returns a test member."""""" body = { ""member"": { ""protocol_port"": protocol_port, ""pool_id"": pool_id, ""address"": address } } resp = self.network_client.create_member(body) member = net_common.DeletableMember(client=self.network_client, **resp['member']) self.set_resource(data_utils.rand_name('member-'), member) return member def _create_vipA(self, protocol, protocol_port, subnet_id, pool_id): """"""Wrapper utility that returns a test vip."""""" name = data_utils.rand_name('vip-') body = { ""vip"": { ""protocol"": protocol, ""name"": name, ""subnet_id"": subnet_id, ""pool_id"": pool_id, ""protocol_port"": protocol_port } } resp = self.network_client.create_vip(body) vip = net_common.DeletableVip(client=self.network_client, **resp['vip']) self.assertEqual(vip['name'], name) self.set_resource(name, vip) return vip def _check_vm_connectivityA(self, ip_address, username=None, private_key=None, should_connect=True): """""" :param ip_address: server to test against :param username: server's ssh username :param private_key: server's ssh private key to be used :param should_connect: True/False indicates positive/negative test positive - attempt ping and ssh negative - attempt ping and fail if succeed :raises: AssertError if the result of the connectivity check does not match the value of the should_connect param """""" if should_connect: msg = ""Timed out waiting for %s to become reachable"" % ip_address else: msg = ""ip address %s is reachable"" % ip_address self.assertTrue(self._ping_ip_address(ip_address, should_succeed=should_connect), msg=msg) if should_connect: # no need to check ssh for negative connectivity self.get_remote_client(ip_address, username, private_key) def _check_public_network_connectivityA(self, ip_address, username, private_key, should_connect=True, msg=None, servers=None): # The target login is assumed to have been configured for # key-based authentication by cloud-init. LOG.debug('checking network connections to IP %s with user: %s' % (ip_address, username)) try: self._check_vm_connectivity(ip_address, username, private_key, should_connect=should_connect) except Exception as e: ex_msg = 'Public network connectivity check failed' if msg: ex_msg += "": "" + msg LOG.exception(ex_msg) self._log_console_output(servers) # network debug is called as part of ssh init if not isinstance(e, exceptions.SSHTimeout): debug.log_net_debug() raise def _check_tenant_network_connectivityA(self, server, username, private_key, should_connect=True, servers_for_debug=None): if not CONF.network.tenant_networks_reachable: msg = 'Tenant networks not configured to be reachable.' LOG.info(msg) return # The target login is assumed to have been configured for # key-based authentication by cloud-init. try: for net_name, ip_addresses in server.networks.iteritems(): for ip_address in ip_addresses: self._check_vm_connectivity(ip_address, username, private_key, should_connect=should_connect) except Exception as e: LOG.exception('Tenant network connectivity check failed') self._log_console_output(servers_for_debug) # network debug is called as part of ssh init if not isinstance(e, exceptions.SSHTimeout): debug.log_net_debug() raise def _check_remote_connectivityA(self, source, dest, should_succeed=True): """""" check ping server via source ssh connection :param source: RemoteClient: an ssh connection from which to ping :param dest: and IP to ping against :param should_succeed: boolean should ping succeed or not :returns: boolean -- should_succeed == ping :returns: ping is false if ping failed """""" def ping_remote(): try: source.ping_host(dest) except exceptions.SSHExecCommandFailed: LOG.exception('Failed to ping host via ssh connection') return not should_succeed return should_succeed return tempest.test.call_until_true(ping_remote, CONF.compute.ping_timeout, 1) def _create_security_group_neutronA(self, tenant_id, client=None, namestart='secgroup-smoke-'): if client is None: client = self.network_client secgroup = self._create_empty_security_groupA(namestart=namestart, client=client, tenant_id=tenant_id) # Add rules to the security group rules = self._create_loginable_secgroup_rule_neutronA(secgroup=secgroup) for rule in rules: self.assertEqual(tenant_id, rule.tenant_id) self.assertEqual(secgroup.id, rule.security_group_id) return secgroup def _create_empty_security_groupA(self, tenant_id, client=None, namestart='secgroup-smoke-'): """"""Create a security group without rules. Default rules will be created: - IPv4 egress to any - IPv6 egress to any :param tenant_id: secgroup will be created in this tenant :returns: DeletableSecurityGroup -- containing the secgroup created """""" if client is None: client = self.network_client sg_name = data_utils.rand_name(namestart) sg_desc = sg_name + "" description"" sg_dict = dict(name=sg_name, description=sg_desc) sg_dict['tenant_id'] = tenant_id body = dict(security_group=sg_dict) result = client.create_security_group(body=body) secgroup = net_common.DeletableSecurityGroup( client=client, **result['security_group'] ) self.assertEqual(secgroup.name, sg_name) self.assertEqual(tenant_id, secgroup.tenant_id) self.assertEqual(secgroup.description, sg_desc) self.set_resource(sg_name, secgroup) return secgroup def _default_security_groupA(self, tenant_id, client=None): """"""Get default secgroup for given tenant_id. :returns: DeletableSecurityGroup -- default secgroup for given tenant """""" if client is None: client = self.network_client sgs = [ sg for sg in client.list_security_groups().values()[0] if sg['tenant_id'] == tenant_id and sg['name'] == 'default' ] msg = ""No default security group for tenant %s."" % (tenant_id) self.assertTrue(len(sgs) > 0, msg) if len(sgs) > 1: msg = ""Found %d default security groups"" % len(sgs) raise exc.NeutronClientNoUniqueMatch(msg=msg) return net_common.DeletableSecurityGroup(client=client, **sgs[0]) def _create_security_group_ruleA(self, client=None, secgroup=None, tenant_id=None, **kwargs): """"""Create a rule from a dictionary of rule parameters. Create a rule in a secgroup. if secgroup not defined will search for default secgroup in tenant_id. :param secgroup: type DeletableSecurityGroup. :param secgroup_id: search for secgroup by id default -- choose default secgroup for given tenant_id :param tenant_id: if secgroup not passed -- the tenant in which to search for default secgroup :param kwargs: a dictionary containing rule parameters: for example, to allow incoming ssh: rule = { direction: 'ingress' protocol:'tcp', port_range_min: 22, port_range_max: 22 } """""" if client is None: client = self.network_client if secgroup is None: secgroup = self._default_security_group(tenant_id) ruleset = dict(security_group_id=secgroup.id, tenant_id=secgroup.tenant_id, ) ruleset.update(kwargs) body = dict(security_group_rule=dict(ruleset)) sg_rule = client.create_security_group_rule(body=body) sg_rule = net_common.DeletableSecurityGroupRule( client=client, **sg_rule['security_group_rule'] ) self.assertEqual(secgroup.tenant_id, sg_rule.tenant_id) self.assertEqual(secgroup.id, sg_rule.security_group_id) return sg_rule def _create_loginable_secgroup_rule_neutronA(self, client=None, secgroup=None): """"""These rules are intended to permit inbound ssh and icmp traffic from all sources, so no group_id is provided. Setting a group_id would only permit traffic from ports belonging to the same security group. """""" if client is None: client = self.network_client rules = [] rulesets = [ dict( # ssh protocol='tcp', port_range_min=22, port_range_max=22, ), dict( # ping protocol='icmp', ) ] for ruleset in rulesets: for r_direction in ['ingress', 'egress']: ruleset['direction'] = r_direction try: sg_rule = self._create_security_group_ruleA( client=client, secgroup=secgroup, **ruleset) except exc.NeutronClientException as ex: # if rule already exist - skip rule and continue if not (ex.status_code is 409 and 'Security group rule' ' already exists' in ex.message): raise ex else: self.assertEqual(r_direction, sg_rule.direction) rules.append(sg_rule) return rules def _ssh_to_serverA(self, server, private_key): ssh_login = CONF.compute.image_ssh_user return self.get_remote_client(server, username=ssh_login, private_key=private_key) def _show_quota_networkA(self, tenant_id): quota = self.network_client.show_quota(tenant_id) return quota['quota']['network'] def _show_quota_subnetA(self, tenant_id): quota = self.network_client.show_quota(tenant_id) return quota['quota']['subnet'] def _show_quota_portA(self, tenant_id): quota = self.network_client.show_quota(tenant_id) return quota['quota']['port'] def _get_routerA(self, tenant_id): """"""Retrieve a router for the given tenant id. If a public router has been configured, it will be returned. If a public router has not been configured, but a public network has, a tenant router will be created and returned that routes traffic to the public network. """""" router_id = CONF.network.public_router_id network_id = CONF.network.public_network_id if router_id: result = self.network_client.show_router(router_id) return net_common.AttributeDict(**result['router']) elif network_id: router = self._create_router(tenant_id) router.add_gateway(network_id) return router else: raise Exception(""Neither of 'public_router_id' or "" ""'public_network_id' has been defined."") def _create_routerA(self, tenant_id, namestart='router-smoke-'): name = data_utils.rand_name(namestart) body = dict( router=dict( name=name, admin_state_up=True, tenant_id=tenant_id, ), ) result = self.network_client.create_router(body=body) router = net_common.DeletableRouter(client=self.network_client, **result['router']) self.assertEqual(router.name, name) self.set_resource(name, router) return router def _create_networksA(self, tenant_id=None): """"""Create a network with a subnet connected to a router. :returns: network, subnet, router """""" if tenant_id is None: tenant_id = self.tenant_id network = self._create_network(tenant_id) router = self._get_router(tenant_id) subnet = self._create_subnet(network) subnet.add_to_router(router.id) return network, subnet, router class NetworkScenarioMultitBTest(OfficialClientTestB): """""" Base class for network scenario tests """""" @classmethod def check_preconditionsB(cls): if (CONF.service_available.neutron): cls.enabled = True # verify that neutron_available is telling the truth try: cls.network_client.list_networks() except exc.EndpointNotFound: cls.enabled = False raise else: cls.enabled = False msg = 'Neutron not available' raise cls.skipException(msg) @classmethod def setUpClass(cls): super(NetworkScenarioMultitBTest, cls).setUpClass() cls.tenant_idB = cls.managerB.identity_client.tenant_id def _create_networkB(self, tenant_id, namestart='network-smoke-'): name = data_utils.rand_name(namestart) body = dict( network=dict( name=name, tenant_id=tenant_id, ), ) result = self.network_client.create_network(body=body) network = net_common.DeletableNetwork(client=self.network_client, **result['network']) self.assertEqual(network.name, name) self.set_resource(name, network) return network def _list_networksB(self, **kwargs): nets = self.network_client.list_networks(**kwargs) return nets['networks'] def _list_subnetsB(self, **kwargs): subnets = self.network_client.list_subnets(**kwargs) return subnets['subnets'] def _list_routersB(self, **kwargs): routers = self.network_client.list_routers(**kwargs) return routers['routers'] def _list_portsB(self, **kwargs): ports = self.network_client.list_ports(**kwargs) return ports['ports'] def _get_tenant_own_network_numB(self, tenant_id): nets = self._list_networks(tenant_id=tenant_id) return len(nets) def _get_tenant_own_subnet_numB(self, tenant_id): subnets = self._list_subnets(tenant_id=tenant_id) return len(subnets) def _get_tenant_own_port_numB(self, tenant_id): ports = self._list_ports(tenant_id=tenant_id) return len(ports) def _create_subnetB(self, network, namestart='subnet-smoke-', **kwargs): """""" Create a subnet for the given network within the cidr block configured for tenant networks. """""" def cidr_in_use(cidr, tenant_id): """""" :return True if subnet with cidr already exist in tenant False else """""" cidr_in_use = self._list_subnetsB(tenant_id=tenant_id, cidr=cidr) return len(cidr_in_use) != 0 tenant_cidr = netaddr.IPNetwork(CONF.network.tenant_network_cidr) result = None # Repeatedly attempt subnet creation with sequential cidr # blocks until an unallocated block is found. for subnet_cidr in tenant_cidr.subnet( CONF.network.tenant_network_mask_bits): str_cidr = str(subnet_cidr) if cidr_in_use(str_cidr, tenant_id=network.tenant_id): continue body = dict( subnet=dict( name=data_utils.rand_name(namestart), ip_version=4, network_id=network.id, tenant_id=network.tenant_id, cidr=str_cidr, ), ) body['subnet'].update(kwargs) try: result = self.network_client.create_subnet(body=body) break except exc.NeutronClientException as e: is_overlapping_cidr = 'overlaps with another subnet' in str(e) if not is_overlapping_cidr: raise self.assertIsNotNone(result, 'Unable to allocate tenant network') subnet = net_common.DeletableSubnet(client=self.network_client, **result['subnet']) self.assertEqual(subnet.cidr, str_cidr) self.set_resource(data_utils.rand_name(namestart), subnet) return subnet def _create_portB(self, network, namestart='port-quotatest-'): name = data_utils.rand_name(namestart) body = dict( port=dict(name=name, network_id=network.id, tenant_id=network.tenant_id)) result = self.network_client.create_port(body=body) self.assertIsNotNone(result, 'Unable to allocate port') port = net_common.DeletablePort(client=self.network_client, **result['port']) self.set_resource(name, port) return port def _get_server_port_idB(self, server, ip_addr=None): ports = self._list_portsB(device_id=server.id, fixed_ip=ip_addr) self.assertEqual(len(ports), 1, ""Unable to determine which port to target."") return ports[0]['id'] def _create_floating_ipB(self, thing, external_network_id, port_id=None): if not port_id: port_id = self._get_server_port_idB(thing) body = dict( floatingip=dict( floating_network_id=external_network_id, port_id=port_id, tenant_id=thing.tenant_id, ) ) result = self.network_client.create_floatingip(body=body) floating_ip = net_common.DeletableFloatingIp( client=self.network_client, **result['floatingip']) self.set_resource(data_utils.rand_name('floatingip-'), floating_ip) return floating_ip def _associate_floating_ipB(self, floating_ip, server): port_id = self._get_server_port_idB(server) floating_ip.update(port_id=port_id) self.assertEqual(port_id, floating_ip.port_id) return floating_ip def _disassociate_floating_ipB(self, floating_ip): """""" :param floating_ip: type DeletableFloatingIp """""" floating_ip.update(port_id=None) self.assertIsNone(floating_ip.port_id) return floating_ip def _ping_ip_addressB(self, ip_address, should_succeed=True): cmd = ['ping', '-c1', '-w1', ip_address] def ping(): proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE) proc.wait() return (proc.returncode == 0) == should_succeed return tempest.test.call_until_true( ping, CONF.compute.ping_timeout, 1) def _create_poolB(self, lb_method, protocol, subnet_id): """"""Wrapper utility that returns a test pool."""""" name = data_utils.rand_name('pool-') body = { ""pool"": { ""protocol"": protocol, ""name"": name, ""subnet_id"": subnet_id, ""lb_method"": lb_method } } resp = self.network_client.create_pool(body=body) pool = net_common.DeletablePool(client=self.network_client, **resp['pool']) self.assertEqual(pool['name'], name) self.set_resource(name, pool) return pool def _create_memberB(self, address, protocol_port, pool_id): """"""Wrapper utility that returns a test member."""""" body = { ""member"": { ""protocol_port"": protocol_port, ""pool_id"": pool_id, ""address"": address } } resp = self.network_client.create_member(body) member = net_common.DeletableMember(client=self.network_client, **resp['member']) self.set_resource(data_utils.rand_name('member-'), member) return member def _create_vipB(self, protocol, protocol_port, subnet_id, pool_id): """"""Wrapper utility that returns a test vip."""""" name = data_utils.rand_name('vip-') body = { ""vip"": { ""protocol"": protocol, ""name"": name, ""subnet_id"": subnet_id, ""pool_id"": pool_id, ""protocol_port"": protocol_port } } resp = self.network_client.create_vip(body) vip = net_common.DeletableVip(client=self.network_client, **resp['vip']) self.assertEqual(vip['name'], name) self.set_resource(name, vip) return vip def _check_vm_connectivityB(self, ip_address, username=None, private_key=None, should_connect=True): """""" :param ip_address: server to test against :param username: server's ssh username :param private_key: server's ssh private key to be used :param should_connect: True/False indicates positive/negative test positive - attempt ping and ssh negative - attempt ping and fail if succeed :raises: AssertError if the result of the connectivity check does not match the value of the should_connect param """""" if should_connect: msg = ""Timed out waiting for %s to become reachable"" % ip_address else: msg = ""ip address %s is reachable"" % ip_address self.assertTrue(self._ping_ip_address(ip_address, should_succeed=should_connect), msg=msg) if should_connect: # no need to check ssh for negative connectivity self.get_remote_client(ip_address, username, private_key) def _check_public_network_connectivityB(self, ip_address, username, private_key, should_connect=True, msg=None, servers=None): # The target login is assumed to have been configured for # key-based authentication by cloud-init. LOG.debug('checking network connections to IP %s with user: %s' % (ip_address, username)) try: self._check_vm_connectivity(ip_address, username, private_key, should_connect=should_connect) except Exception as e: ex_msg = 'Public network connectivity check failed' if msg: ex_msg += "": "" + msg LOG.exception(ex_msg) self._log_console_output(servers) # network debug is called as part of ssh init if not isinstance(e, exceptions.SSHTimeout): debug.log_net_debug() raise def _check_tenant_network_connectivityB(self, server, username, private_key, should_connect=True, servers_for_debug=None): if not CONF.network.tenant_networks_reachable: msg = 'Tenant networks not configured to be reachable.' LOG.info(msg) return # The target login is assumed to have been configured for # key-based authentication by cloud-init. try: for net_name, ip_addresses in server.networks.iteritems(): for ip_address in ip_addresses: self._check_vm_connectivity(ip_address, username, private_key, should_connect=should_connect) except Exception as e: LOG.exception('Tenant network connectivity check failed') self._log_console_output(servers_for_debug) # network debug is called as part of ssh init if not isinstance(e, exceptions.SSHTimeout): debug.log_net_debug() raise def _check_remote_connectivityB(self, source, dest, should_succeed=True): """""" check ping server via source ssh connection :param source: RemoteClient: an ssh connection from which to ping :param dest: and IP to ping against :param should_succeed: boolean should ping succeed or not :returns: boolean -- should_succeed == ping :returns: ping is false if ping failed """""" def ping_remote(): try: source.ping_host(dest) except exceptions.SSHExecCommandFailed: LOG.exception('Failed to ping host via ssh connection') return not should_succeed return should_succeed return tempest.test.call_until_true(ping_remote, CONF.compute.ping_timeout, 1) def _create_security_group_neutronB(self, tenant_id, client=None, namestart='secgroup-smoke-'): if client is None: client = self.network_client secgroup = self._create_empty_security_groupB(namestart=namestart, client=client, tenant_id=tenant_id) # Add rules to the security group rules = self._create_loginable_secgroup_rule_neutronB(secgroup=secgroup) for rule in rules: self.assertEqual(tenant_id, rule.tenant_id) self.assertEqual(secgroup.id, rule.security_group_id) return secgroup def _create_empty_security_groupB(self, tenant_id, client=None, namestart='secgroup-smoke-'): """"""Create a security group without rules. Default rules will be created: - IPv4 egress to any - IPv6 egress to any :param tenant_id: secgroup will be created in this tenant :returns: DeletableSecurityGroup -- containing the secgroup created """""" if client is None: client = self.network_client sg_name = data_utils.rand_name(namestart) sg_desc = sg_name + "" description"" sg_dict = dict(name=sg_name, description=sg_desc) sg_dict['tenant_id'] = tenant_id body = dict(security_group=sg_dict) result = client.create_security_group(body=body) secgroup = net_common.DeletableSecurityGroup( client=client, **result['security_group'] ) self.assertEqual(secgroup.name, sg_name) self.assertEqual(tenant_id, secgroup.tenant_id) self.assertEqual(secgroup.description, sg_desc) self.set_resource(sg_name, secgroup) return secgroup def _default_security_groupB(self, tenant_id, client=None): """"""Get default secgroup for given tenant_id. :returns: DeletableSecurityGroup -- default secgroup for given tenant """""" if client is None: client = self.network_client sgs = [ sg for sg in client.list_security_groups().values()[0] if sg['tenant_id'] == tenant_id and sg['name'] == 'default' ] msg = ""No default security group for tenant %s."" % (tenant_id) self.assertTrue(len(sgs) > 0, msg) if len(sgs) > 1: msg = ""Found %d default security groups"" % len(sgs) raise exc.NeutronClientNoUniqueMatch(msg=msg) return net_common.DeletableSecurityGroup(client=client, **sgs[0]) def _create_security_group_ruleB(self, client=None, secgroup=None, tenant_id=None, **kwargs): """"""Create a rule from a dictionary of rule parameters. Create a rule in a secgroup. if secgroup not defined will search for default secgroup in tenant_id. :param secgroup: type DeletableSecurityGroup. :param secgroup_id: search for secgroup by id default -- choose default secgroup for given tenant_id :param tenant_id: if secgroup not passed -- the tenant in which to search for default secgroup :param kwargs: a dictionary containing rule parameters: for example, to allow incoming ssh: rule = { direction: 'ingress' protocol:'tcp', port_range_min: 22, port_range_max: 22 } """""" if client is None: client = self.network_client if secgroup is None: secgroup = self._default_security_group(tenant_id) ruleset = dict(security_group_id=secgroup.id, tenant_id=secgroup.tenant_id, ) ruleset.update(kwargs) body = dict(security_group_rule=dict(ruleset)) sg_rule = client.create_security_group_rule(body=body) sg_rule = net_common.DeletableSecurityGroupRule( client=client, **sg_rule['security_group_rule'] ) self.assertEqual(secgroup.tenant_id, sg_rule.tenant_id) self.assertEqual(secgroup.id, sg_rule.security_group_id) return sg_rule def _create_loginable_secgroup_rule_neutronB(self, client=None, secgroup=None): """"""These rules are intended to permit inbound ssh and icmp traffic from all sources, so no group_id is provided. Setting a group_id would only permit traffic from ports belonging to the same security group. """""" if client is None: client = self.network_client rules = [] rulesets = [ dict( # ssh protocol='tcp', port_range_min=22, port_range_max=22, ), dict( # ping protocol='icmp', ) ] for ruleset in rulesets: for r_direction in ['ingress', 'egress']: ruleset['direction'] = r_direction try: sg_rule = self._create_security_group_ruleB( client=client, secgroup=secgroup, **ruleset) except exc.NeutronClientException as ex: # if rule already exist - skip rule and continue if not (ex.status_code is 409 and 'Security group rule' ' already exists' in ex.message): raise ex else: self.assertEqual(r_direction, sg_rule.direction) rules.append(sg_rule) return rules def _ssh_to_serverB(self, server, private_key): ssh_login = CONF.compute.image_ssh_user return self.get_remote_client(server, username=ssh_login, private_key=private_key) def _show_quota_networkB(self, tenant_id): quota = self.network_client.show_quota(tenant_id) return quota['quota']['network'] def _show_quota_subnetB(self, tenant_id): quota = self.network_client.show_quota(tenant_id) return quota['quota']['subnet'] def _show_quota_portB(self, tenant_id): quota = self.network_client.show_quota(tenant_id) return quota['quota']['port'] def _get_routerB(self, tenant_id): """"""Retrieve a router for the given tenant id. If a public router has been configured, it will be returned. If a public router has not been configured, but a public network has, a tenant router will be created and returned that routes traffic to the public network. """""" router_id = CONF.network.public_router_id network_id = CONF.network.public_network_id if router_id: result = self.network_client.show_router(router_id) return net_common.AttributeDict(**result['router']) elif network_id: router = self._create_router(tenant_id) router.add_gateway(network_id) return router else: raise Exception(""Neither of 'public_router_id' or "" ""'public_network_id' has been defined."") def _create_routerB(self, tenant_id, namestart='router-smoke-'): name = data_utils.rand_name(namestart) body = dict( router=dict( name=name, admin_state_up=True, tenant_id=tenant_id, ), ) result = self.network_client.create_router(body=body) router = net_common.DeletableRouter(client=self.network_client, **result['router']) self.assertEqual(router.name, name) self.set_resource(name, router) return router def _create_networksB(self, tenant_id=None): """"""Create a network with a subnet connected to a router. :returns: network, subnet, router """""" if tenant_id is None: tenant_id = self.tenant_id network = self._create_network(tenant_id) router = self._get_router(tenant_id) subnet = self._create_subnet(network) subnet.add_to_router(router.id) ",,2225,0
openstack%2Ftempest~master~I3b9df9f7761a49a636489c4413d37775772dd7ae,openstack/tempest,master,I3b9df9f7761a49a636489c4413d37775772dd7ae,Add event scenario test,ABANDONED,2014-07-07 16:18:24.000000000,2014-12-05 15:05:32.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 8576}, {'_account_id': 9008}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-07-07 16:18:24.000000000', 'files': ['tempest/scenario/test_store_events.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/64297184feb7085c62c56c31c82e878ed33deb0f', 'message': 'Add event scenario test\n\nThis test checks events storing.\n\nThe following is the scenario outline:\n 1. Create keypiar via nova.\n 2. Get list of all events and filter them by\n    generated time and type.\n 3. Check that keypair.create.start and keypair.create.end\n    events are exists.\n 4. Check that this events has correct trait names.\n\nblueprint add-ceilometer-scenario-tests\n\nChange-Id: I3b9df9f7761a49a636489c4413d37775772dd7ae\n'}]",2,105223,64297184feb7085c62c56c31c82e878ed33deb0f,8,5,1,10488,,,0,"Add event scenario test

This test checks events storing.

The following is the scenario outline:
 1. Create keypiar via nova.
 2. Get list of all events and filter them by
    generated time and type.
 3. Check that keypair.create.start and keypair.create.end
    events are exists.
 4. Check that this events has correct trait names.

blueprint add-ceilometer-scenario-tests

Change-Id: I3b9df9f7761a49a636489c4413d37775772dd7ae
",git fetch https://review.opendev.org/openstack/tempest refs/changes/23/105223/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/test_store_events.py'],1,64297184feb7085c62c56c31c82e878ed33deb0f,bp/add-ceilometer-scenario-tests,"# Copyright 2014 Mirantis.inc # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import datetime from tempest import clients from tempest import config from tempest.scenario import manager from tempest import test CONF = config.CONF class TestStoreEvents(manager.OfficialClientTest): """""" This test checks events storing. The following is the scenario outline: 1. Create keypiar via nova. 2. Get list of all events and filter them by generated time and type. 3. Check that keypair.create.start and keypair.create.end events are exists. 4. Check that this events has correct trait names. """""" CEILOMETERCLIENT_VERSION = clients.OfficialClientManager.\ CEILOMETERCLIENT_VERSION @classmethod def SetUpClass(cls): super(TestStoreEvents, cls).setUpClass() if cls.ceilometer_client is not None: skip_msg = (""%s skip as telemetry is not available"" % cls.__name__) raise cls.skipException(skip_msg) def cleanup_wrapper(self, resource): self.cleanup_resource(resource, self.__class__.__name__) def _get_admin_ceilometer_client(self, credentials): import ceilometerclient.client endpoint_type = CONF.telemetry.endpoint_type service_type = CONF.telemetry.catalog_type auth_url = CONF.identity.uri return ceilometerclient.client.get_client( self.CEILOMETERCLIENT_VERSION, os_username=credentials.username, os_password=credentials.password, os_tenant_name=credentials.tenant_name, os_auth_url=auth_url, os_service_type=service_type, os_endpoint_type=endpoint_type) def _get_events(self): admin_ceilometer_client = self._get_admin_ceilometer_client( self.admin_credentials()) return admin_ceilometer_client.events.list() def _check_traits(self, event, expected_trait_names): trait_names = [] for trait in event.traits: trait_names.append(trait['name']) trait_names = sorted(trait_names) self.assertEqual(expected_trait_names, trait_names, ""Expected %s trait names, found %s"" % (expected_trait_names, trait_names)) @test.services('ceilometer', 'compute') def test_store_events(self): create_time = datetime.datetime.utcnow() self.assertIsNotNone(self.create_keypair(), ""Keypair was not created"") create_start_event = None create_end_event = None for event in self._get_events(): event_generated = datetime.datetime.strptime(event.generated, '%Y-%m-%dT%H:%M:%S.%f') if event_generated < create_time: continue if event.event_type in ['keypair.create.start']: create_start_event = event if event.event_type in ['keypair.create.end']: create_end_event = event self.assertIsNotNone(create_start_event, ""keypair.create.start event was not found"") self.assertIsNotNone(create_end_event, ""keypair.create.end event was not found"") trait_names = sorted(['request_id', 'service', 'tenant_id']) self._check_traits(create_start_event, trait_names) self._check_traits(create_end_event, trait_names) ",,109,0
openstack%2Ftempest~master~Iee8ba86a49584e1c49e28237b336c6468469f07d,openstack/tempest,master,Iee8ba86a49584e1c49e28237b336c6468469f07d,Add support for LBaaS v2.0 API to Tempest clients,ABANDONED,2014-07-10 16:08:17.000000000,2014-12-05 15:05:31.000000000,,"[{'_account_id': 3}, {'_account_id': 105}, {'_account_id': 1687}, {'_account_id': 1795}, {'_account_id': 2750}, {'_account_id': 4694}, {'_account_id': 6951}, {'_account_id': 7249}, {'_account_id': 8576}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 10980}]","[{'number': 1, 'created': '2014-07-10 16:08:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/17bf349ee33b6e231641fb600b5cac56b52cd99a', 'message': 'Add support for LBaaS v2.0 API to Tempest clients\n\nThe LBaaS v2.0 API has 5 resources names: loadbalancers, listeners, pools,\nmembers and healthmonitors. Out of this, loadbalancers, listeners and\nhealthmonitors are new resources names in the Neutron API and therefore can\nbe automatically supported by tempest.services.network.network_client_base,\nrequiring only minor changes.\n\npools and members are resources names that also exist in the LBaaS v1.0 API. To\nsupport them, this patchset adds methods to the JSON and XML network clients\nwith names that include the suffix ""_v2"", e.g. ""create_pool_v2""\n\nChange-Id: Iee8ba86a49584e1c49e28237b336c6468469f07d\n'}, {'number': 2, 'created': '2014-07-28 00:50:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/dbec3d62421b096cb92c269f282ed4e4a92c1be9', 'message': 'Add support for LBaaS v2.0 API to Tempest clients\n\nThe LBaaS v2.0 API has 5 resources names: loadbalancers, listeners, pools,\nmembers and healthmonitors. Out of this, loadbalancers, listeners and\nhealthmonitors are new resources names in the Neutron API and therefore can\nbe automatically supported by tempest.services.network.network_client_base,\nrequiring only minor changes.\n\npools and members are resources names that also exist in the LBaaS v1.0 API. To\nsupport them, this patchset adds methods to the JSON and XML network clients\nwith names that include the suffix ""_v2"", e.g. ""create_pool_v2""\n\nChange-Id: Iee8ba86a49584e1c49e28237b336c6468469f07d\n'}, {'number': 3, 'created': '2014-08-10 22:47:55.000000000', 'files': ['tempest/services/network/json/network_client.py', 'tempest/services/network/network_client_base.py', 'tempest/services/network/xml/network_client.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/f91421e4a08a521715635fe869e6b5903eddfe81', 'message': 'Add support for LBaaS v2.0 API to Tempest clients\n\nThe LBaaS v2.0 API has 5 resources names: loadbalancers, listeners, pools,\nmembers and healthmonitors. Out of this, loadbalancers, listeners and\nhealthmonitors are new resources names in the Neutron API and therefore can\nbe automatically supported by tempest.services.network.network_client_base,\nrequiring only minor changes.\n\npools and members are resources names that also exist in the LBaaS v1.0 API. To\nsupport them, this patchset adds methods to the JSON and XML network clients\nwith names that include the suffix ""_v2"", e.g. ""create_pool_v2""\n\nThe client implementation in this patchset has been verified to work correctly\nwith the LBaaS V2.0 implementation\n\nChange-Id: Iee8ba86a49584e1c49e28237b336c6468469f07d\n'}]",2,106089,f91421e4a08a521715635fe869e6b5903eddfe81,26,13,3,4694,,,0,"Add support for LBaaS v2.0 API to Tempest clients

The LBaaS v2.0 API has 5 resources names: loadbalancers, listeners, pools,
members and healthmonitors. Out of this, loadbalancers, listeners and
healthmonitors are new resources names in the Neutron API and therefore can
be automatically supported by tempest.services.network.network_client_base,
requiring only minor changes.

pools and members are resources names that also exist in the LBaaS v1.0 API. To
support them, this patchset adds methods to the JSON and XML network clients
with names that include the suffix ""_v2"", e.g. ""create_pool_v2""

The client implementation in this patchset has been verified to work correctly
with the LBaaS V2.0 implementation

Change-Id: Iee8ba86a49584e1c49e28237b336c6468469f07d
",git fetch https://review.opendev.org/openstack/tempest refs/changes/89/106089/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/services/network/json/network_client.py', 'tempest/services/network/network_client_base.py', 'tempest/services/network/xml/network_client.py']",3,17bf349ee33b6e231641fb600b5cac56b52cd99a,new-lbaas," def create_pool_v2(self, name, protocol, lb_algorithm, **kwargs): uri = '%s/pools' % (self.uri_prefix) pool = common.Element(""pool"") pool.append(common.Element(""name"", name)) pool.append(common.Element(""protocol"", protocol)) pool.append(common.Element(""lb_algorithm"", lb_algorithm)) common.deep_dict_to_xml(pool, kwargs) resp, body = self.post(uri, str(common.Document(pool))) body = _root_tag_fetcher_and_xml_to_json_parse(body) return resp, body def list_pools_v2(self): uri = '%s/pools' % (self.uri_prefix) resp, body = self.get(uri) pools = common.parse_array(etree.fromstring(body)) body = {'pools': pools} return resp, body def show_pool_v2(self, pool_id): uri = '%s/pools/%s' % (self.uri_prefix, pool_id) resp, body = self.get(uri) body = _root_tag_fetcher_and_xml_to_json_parse(body) return resp, body def update_pool_v2(self, pool_id, **kwargs): uri = '%s/pools/%s' % (self.uri_prefix, pool_id) pool = common.Element(""pool"") common.deep_dict_to_xml(pool, kwargs) resp, body = self.put(uri, str(common.Document(pool))) body = _root_tag_fetcher_and_xml_to_json_parse(body) return resp, body def delete_pool_v2(self, pool_id): uri = '%s/pools/%s' % (self.uri_prefix, pool_id) return self.delete(uri) def create_member_v2(self, pool_id, address, protocol_port, subnet_id, **kwargs): uri = '%s/pool/%s/members' % (self.uri_prefix, pool_id) member = common.Element(""member"") member.append(common.Element(""address"", address)) member.append(common.Element(""protocol_port"", protocol_port)) member.append(common.Element(""subnet_id"", subnet_id)) common.deep_dict_to_xml(member, kwargs) resp, body = self.post(uri, str(common.Document(member))) body = _root_tag_fetcher_and_xml_to_json_parse(body) return resp, body def list_members_v2(self, pool_id): uri = '%s/pool/%s/members' % (self.uri_prefix, pool_id) resp, body = self.get(uri) members = common.parse_array(etree.fromstring(body)) body = {'members': members} return resp, body def show_member_v2(self, pool_id, member_id): uri = '%s/pool/%s/members/%s' % (self.uri_prefix, pool_id, member_id) resp, body = self.get(uri) body = _root_tag_fetcher_and_xml_to_json_parse(body) return resp, body def update_member_v2(self, pool_id, member_id, **kwargs): uri = '%s/pool/%s/members/%s' % (self.uri_prefix, pool_id, member_id) member = common.Element(""member"") common.deep_dict_to_xml(member, kwargs) resp, body = self.put(uri, str(common.Document(member))) body = _root_tag_fetcher_and_xml_to_json_parse(body) return resp, body def delete_member_v2(self, pool_id, member_id): uri = '%s/pool/%s/members/%s' % (self.uri_prefix, pool_id, member_id) return self.delete(uri) ",,149,1
openstack%2Ftempest~master~I37753578cf6dfea4d05bc76bcaa1a0c305201434,openstack/tempest,master,I37753578cf6dfea4d05bc76bcaa1a0c305201434,Added a nova CLI test,ABANDONED,2014-08-18 12:53:57.000000000,2014-12-05 15:05:30.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-18 12:53:57.000000000', 'files': ['tempest/cli/simple_read_only/test_nova.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/0d41ff0500c17c0c9490d8575d4217407f8c356e', 'message': 'Added a nova CLI test\n\nTest cases added for CLI:-\ninstance-action-list\ninstance-action\nAfter nova listing, if image get deleted by test cases running in parallel,\nso error handling is done for the same\n\nChange-Id: I37753578cf6dfea4d05bc76bcaa1a0c305201434\n'}]",0,114956,0d41ff0500c17c0c9490d8575d4217407f8c356e,6,3,1,12837,,,0,"Added a nova CLI test

Test cases added for CLI:-
instance-action-list
instance-action
After nova listing, if image get deleted by test cases running in parallel,
so error handling is done for the same

Change-Id: I37753578cf6dfea4d05bc76bcaa1a0c305201434
",git fetch https://review.opendev.org/openstack/tempest refs/changes/56/114956/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/cli/simple_read_only/test_nova.py'],1,0d41ff0500c17c0c9490d8575d4217407f8c356e,novaCLI," def test_instance_action_list(self): nova_list = self.parser.listing(self.nova('list', params='--all-tenants 1')) try: for lists in nova_list: lists = self.parser.listing(self.nova('instance-action-list', params=lists['ID'])) field_names = ['Action', 'Request_ID', 'Message', 'Start_Time'] self.assertTableStruct(lists, field_names) except exceptions.CommandFailed as e: if ""No server with a name or ID of"" in e.stderr: pass else: raise e def test_instance_action(self): nova_list = self.parser.listing(self.nova('list', params='--all-tenants 1')) try: for lists in nova_list: action_list = self.parser.listing(self.nova ('instance-action-list', params=lists['ID'])) parameter = lists['ID'] parameter += "" "" parameter += action_list[0]['Request_ID'] action = self.parser.listing(self.nova ('instance-action', params=parameter)) field_names = ['Property', 'Value'] self.assertTableStruct(action, field_names) except exceptions.CommandFailed as e: if ""No server with a name or ID of"" in e.stderr: pass else: raise e ",,37,0
openstack%2Ftempest~master~Id8e6ea33a54b252431eded741c29aae18c4d1683,openstack/tempest,master,Id8e6ea33a54b252431eded741c29aae18c4d1683,Testcase GET/LIST datastore and datastore versions,ABANDONED,2014-08-18 18:13:25.000000000,2014-12-05 15:05:29.000000000,,"[{'_account_id': 3}, {'_account_id': 1795}, {'_account_id': 2750}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-18 18:13:25.000000000', 'files': ['tempest/api_schema/response/database/__init__.py', 'tempest/services/database/json/datastores_client.py', 'tempest/clients.py', 'tempest/api/database/base.py', 'tempest/api_schema/response/database/datastores.py', 'tempest/api/database/datastore/test_datastores.py', 'tempest/api/database/datastore/__init__.py', 'tempest/api_schema/response/database/parameter_types.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/8296f575bbe75fc8d324f3ef45f9e178afe5afe9', 'message': 'Testcase GET/LIST datastore and datastore versions\n\nThis submission adds a test case\n""test_list_datastores_and_datastore_versions"" for list and get operations\non datastores and datastore versions. Thus a new directory datastore is\nadded under database folder of ""tempest/api/""\n\nThe supporting functions are added in the client file at\n""tempest/tempest/services/database/json/datastores_client.py"".\n\nApi schema related to datastores is added at\n""tempest/api_schema/response/database/datastores.py"" and\n""tempest/api_schema/response/database/parameter_types.py"".\n\nAlso corresponding modifications are done in ""tempest/api/database/base.py""\nand ""tempest/clients.py""\n\nChange-Id: Id8e6ea33a54b252431eded741c29aae18c4d1683\n'}]",0,115049,8296f575bbe75fc8d324f3ef45f9e178afe5afe9,6,4,1,1687,,,0,"Testcase GET/LIST datastore and datastore versions

This submission adds a test case
""test_list_datastores_and_datastore_versions"" for list and get operations
on datastores and datastore versions. Thus a new directory datastore is
added under database folder of ""tempest/api/""

The supporting functions are added in the client file at
""tempest/tempest/services/database/json/datastores_client.py"".

Api schema related to datastores is added at
""tempest/api_schema/response/database/datastores.py"" and
""tempest/api_schema/response/database/parameter_types.py"".

Also corresponding modifications are done in ""tempest/api/database/base.py""
and ""tempest/clients.py""

Change-Id: Id8e6ea33a54b252431eded741c29aae18c4d1683
",git fetch https://review.opendev.org/openstack/tempest refs/changes/49/115049/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api_schema/response/database/__init__.py', 'tempest/services/database/json/datastores_client.py', 'tempest/clients.py', 'tempest/api/database/base.py', 'tempest/api_schema/response/database/datastores.py', 'tempest/api/database/datastore/test_datastores.py', 'tempest/api/database/datastore/__init__.py', 'tempest/api_schema/response/database/parameter_types.py']",8,8296f575bbe75fc8d324f3ef45f9e178afe5afe9,separate/trove_datastores,"# Copyright 2014 Hewlett-Packard Development Company, L.P # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. links = { 'type': 'array', 'items': { 'type': 'object', 'properties': { 'href': { 'type': 'string', 'format': 'uri' }, 'rel': {'type': 'string'} }, 'required': ['href', 'rel'] } } versions = { 'type': 'object', 'properties': { 'id': {'type': 'string'}, 'name': {'type': 'string'}, 'links': links, 'datastore': {'type': 'string'} }, 'required': ['links', 'id', 'name'] } ",,237,0
openstack%2Ftempest~master~Ie027e505e99c52dfb707f618c356d44ea60c2607,openstack/tempest,master,Ie027e505e99c52dfb707f618c356d44ea60c2607,Missing baremetal API tests,ABANDONED,2014-03-27 11:23:31.000000000,2014-12-05 15:05:28.000000000,,"[{'_account_id': 3}, {'_account_id': 97}, {'_account_id': 2750}, {'_account_id': 7428}, {'_account_id': 7882}, {'_account_id': 7926}, {'_account_id': 8205}, {'_account_id': 8556}, {'_account_id': 8824}, {'_account_id': 8871}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-03-27 11:23:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/fa05dbd9ce72eafa188fc0b5fcef7fa6171b7f09', 'message': 'Missing baremetal API tests\n\nAs a part of blueprint, this patch implements following API test -\n\nChassis APIs -\n1. Retrieve a list of chassis with detail.\n\nNodes APIs -\n1. Retrieve a list of nodes with detail.\n\nPorts -\n1. Retrieve a list of ports with detail.\n\npart of blueprint: missing-baremetal-api-test\n\nChange-Id: Ie027e505e99c52dfb707f618c356d44ea60c2607\n'}, {'number': 2, 'created': '2014-03-27 11:23:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/1909b02eee7ea62fd9f5b32706bb9885746b61cc', 'message': 'Missing baremetal API tests\n\nAs a part of blueprint, this patch implements following API test -\n\nChassis APIs -\n1. Retrieve a list of chassis with detail.\n\nNodes APIs -\n1. Retrieve a list of nodes with detail.\n\nPorts -\n1. Retrieve a list of ports with detail.\n\npart of blueprint: missing-baremetal-api-test\n\nChange-Id: Ie027e505e99c52dfb707f618c356d44ea60c2607\n'}, {'number': 3, 'created': '2014-03-28 02:58:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5b2947af07af4a69d6560ac49beb33f4d50ea575', 'message': 'Missing baremetal API tests\n\nAs a part of blueprint, this patch implements following API test -\n\nChassis APIs -\n1. Retrieve a list of chassis with detail.\n\nNodes APIs -\n1. Retrieve a list of nodes with detail.\n\nPorts -\n1. Retrieve a list of ports with detail.\n\npart of blueprint: missing-baremetal-api-test\n\nChange-Id: Ie027e505e99c52dfb707f618c356d44ea60c2607\n'}, {'number': 4, 'created': '2014-03-28 07:08:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ddbd5c9f24d23780c632a028dc2c88d41e9ead0d', 'message': 'Missing baremetal API tests\n\nAs a part of blueprint, this patch implements following API test -\n\nChassis APIs -\n1. Retrieve a list of chassis with detail.\n\nNodes APIs -\n1. Retrieve a list of nodes with detail.\n\nPorts -\n1. Retrieve a list of ports with detail.\n\npart of blueprint: missing-baremetal-api-test\n\nChange-Id: Ie027e505e99c52dfb707f618c356d44ea60c2607\n'}, {'number': 5, 'created': '2014-03-28 07:08:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d5b8f4eb6612660264f99e5a36370091866e1793', 'message': 'Missing baremetal API tests\n\nAs a part of blueprint, this patch implements following API test -\n\nChassis APIs -\n1. Retrieve a list of chassis with detail.\n\nNodes APIs -\n1. Retrieve a list of nodes with detail.\n\nPorts -\n1. Retrieve a list of ports with detail.\n\npart of blueprint: missing-baremetal-api-test\n\nChange-Id: Ie027e505e99c52dfb707f618c356d44ea60c2607\n'}, {'number': 6, 'created': '2014-04-02 09:33:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/7ad23f825f457c260c3927d48cee55d58c82a94b', 'message': 'Missing baremetal API tests\n\nAs a part of blueprint, this patch implements following API test -\n\nChassis APIs -\n1. Retrieve a list of chassis with detail.\n\nNodes APIs -\n1. Retrieve a list of nodes with detail.\n\nPorts -\n1. Retrieve a list of ports with detail.\n\npart of blueprint: missing-baremetal-api-test\n\nChange-Id: Ie027e505e99c52dfb707f618c356d44ea60c2607\n'}, {'number': 7, 'created': '2014-04-08 06:10:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/377b31db9e3b33b36ed30c54717d42c5d6ac0d85', 'message': 'Missing baremetal API tests\n\nAs a part of blueprint, this patch implements following API test -\n\nChassis APIs -\n1. Retrieve a list of chassis with detail.\n\nNodes APIs -\n1. Retrieve a list of nodes with detail.\n\npart of blueprint: missing-baremetal-api-test\n\nChange-Id: Ie027e505e99c52dfb707f618c356d44ea60c2607\n'}, {'number': 8, 'created': '2014-04-28 07:33:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/68926110cb642c025d361fcb9d0e9780f03a7f83', 'message': 'Missing baremetal API tests\n\nAs a part of blueprint, this patch implements following API test -\n\nChassis APIs -\n1. Retrieve a list of chassis with detail.\n\nNodes APIs -\n1. Retrieve a list of nodes with detail.\n\npart of blueprint: missing-baremetal-api-test\n\nChange-Id: Ie027e505e99c52dfb707f618c356d44ea60c2607\n'}, {'number': 9, 'created': '2014-04-30 09:43:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/b01c068d1e42b92aa3d5a0567f39f93490cd2155', 'message': 'Missing baremetal API tests\n\nAs a part of blueprint, this patch implements following API test -\n\nChassis APIs -\n1. Retrieve a list of chassis with detail.\n\nNodes APIs -\n1. Retrieve a list of nodes with detail.\n\npart of blueprint: missing-baremetal-api-test\n\nChange-Id: Ie027e505e99c52dfb707f618c356d44ea60c2607\n'}, {'number': 10, 'created': '2014-05-01 06:12:52.000000000', 'files': ['tempest/api/baremetal/test_nodes.py', 'tempest/services/baremetal/v1/base_v1.py', 'tempest/api/baremetal/test_chassis.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/3f14dfd04a9b27eaef4c5922b2ede85edfc118e2', 'message': 'Missing baremetal API tests\n\nAs a part of blueprint, this patch implements following API test -\n\nChassis APIs -\n1. Retrieve a list of chassis with detail.\n\nNodes APIs -\n1. Retrieve a list of nodes with detail.\n\npart of blueprint: missing-baremetal-api-test\n\nChange-Id: Ie027e505e99c52dfb707f618c356d44ea60c2607\n'}]",49,83345,3f14dfd04a9b27eaef4c5922b2ede85edfc118e2,125,11,10,8205,,,0,"Missing baremetal API tests

As a part of blueprint, this patch implements following API test -

Chassis APIs -
1. Retrieve a list of chassis with detail.

Nodes APIs -
1. Retrieve a list of nodes with detail.

part of blueprint: missing-baremetal-api-test

Change-Id: Ie027e505e99c52dfb707f618c356d44ea60c2607
",git fetch https://review.opendev.org/openstack/tempest refs/changes/45/83345/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/baremetal/test_nodes.py', 'tempest/services/baremetal/v1/base_v1.py', 'tempest/api/baremetal/test_chassis.py', 'tempest/services/baremetal/base.py', 'tempest/api/baremetal/test_ports.py']",5,fa05dbd9ce72eafa188fc0b5fcef7fa6171b7f09,bp/missing-baremetal-api-test," def test_detailed_list_nodes(self): node_id = self.node['uuid'] uuids = [self.create_port(node_id=node_id)['port']['uuid'] for i in range(0, 5)] resp, body = self.client.list_ports_with_detail() loaded_uuids = [p['uuid'] for p in body['ports']] for u in uuids: self.assertIn(u, loaded_uuids) @test.attr(type='smoke')",,58,7
openstack%2Ftempest~master~I18f6e0c4c5a3535bd02c18f89abcab91e9fb6c63,openstack/tempest,master,I18f6e0c4c5a3535bd02c18f89abcab91e9fb6c63,Missing Neutron CLI tests,ABANDONED,2014-06-10 08:54:04.000000000,2014-12-05 15:05:27.000000000,,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 1849}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 6796}, {'_account_id': 7139}, {'_account_id': 8205}, {'_account_id': 8556}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 11667}]","[{'number': 1, 'created': '2014-06-10 08:54:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/2a59b77b527940d2423ba0429dcb9c952d69d169', 'message': 'Missing Neutron CLI tests\n\nThis patch adds following missing Neutron CLI tests -\n1. firewall list\n2. firewall policy list\n3. firewall rule list\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: I18f6e0c4c5a3535bd02c18f89abcab91e9fb6c63\n'}, {'number': 2, 'created': '2014-06-10 11:23:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/086a8b04f89617374b0263f09715a0ce898bf523', 'message': 'Missing Neutron CLI tests\n\nThis patch adds following missing Neutron CLI tests -\n1. firewall list\n2. firewall policy list\n3. firewall rule list\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: I18f6e0c4c5a3535bd02c18f89abcab91e9fb6c63\n'}, {'number': 3, 'created': '2014-06-18 06:17:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/eedac76c97b500a1258c28a3bc5ed79592200ff1', 'message': 'Missing Neutron CLI tests\n\nThis patch adds following missing Neutron CLI tests -\n1. firewall list\n2. firewall policy list\n3. firewall rule list\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: I18f6e0c4c5a3535bd02c18f89abcab91e9fb6c63\n'}, {'number': 4, 'created': '2014-06-18 06:34:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d2873f306c94b26c0afb108b4016b94ec898fd9e', 'message': 'Missing Neutron CLI tests\n\nThis patch adds following missing Neutron CLI tests -\n1. firewall list\n2. firewall policy list\n3. firewall rule list\n4. service provider list\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: I18f6e0c4c5a3535bd02c18f89abcab91e9fb6c63\n'}, {'number': 5, 'created': '2014-06-18 09:10:53.000000000', 'files': ['tempest/cli/simple_read_only/test_neutron.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/c44b2fea872d3357d13577907bec7643fc726e4f', 'message': 'Missing Neutron CLI tests\n\nThis patch adds following missing Neutron CLI tests -\n1. firewall list\n2. firewall policy list\n3. firewall rule list\n4. service provider list\n\nThis patch also improves code of following method -\n1. _test_neutron_lbaas_command -\nAs by passing error 404 has no significance in case so in this\ntest is modified to avoid such checks.\n\nPartially implements: blueprint missing-cli-tests-in-tempest\n\nChange-Id: I18f6e0c4c5a3535bd02c18f89abcab91e9fb6c63\n'}]",10,98974,c44b2fea872d3357d13577907bec7643fc726e4f,54,13,5,8205,,,0,"Missing Neutron CLI tests

This patch adds following missing Neutron CLI tests -
1. firewall list
2. firewall policy list
3. firewall rule list
4. service provider list

This patch also improves code of following method -
1. _test_neutron_lbaas_command -
As by passing error 404 has no significance in case so in this
test is modified to avoid such checks.

Partially implements: blueprint missing-cli-tests-in-tempest

Change-Id: I18f6e0c4c5a3535bd02c18f89abcab91e9fb6c63
",git fetch https://review.opendev.org/openstack/tempest refs/changes/74/98974/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/cli/simple_read_only/test_neutron.py'],1,2a59b77b527940d2423ba0429dcb9c952d69d169,bp/missing-cli-tests-in-tempest," @test.requires_ext(extension='fwaas', service='network') def _test_neutron_firewall_command(self, command): try: self.neutron(command) except cli.CommandFailed as e: print e.stderr if '404 Not Found' not in e.stderr: self.fail('%s: Unexpected failure.' % command) @test.attr(type='smoke') def test_neutron_firewall_list(self): self._test_neutron_firewall_command('firewall-list') @test.attr(type='smoke') def test_neutron_firewall_policy_list(self): self._test_neutron_firewall_command('firewall-policy-list') @test.attr(type='smoke') def test_neutron_firewall_rule_list(self): self._test_neutron_firewall_command('firewall-rule-list') ",,21,0
openstack%2Fironic-inspector~master~I85fabfd513a8464f31a03e920b69486a845980d8,openstack/ironic-inspector,master,I85fabfd513a8464f31a03e920b69486a845980d8,Do not fail if ipmi_address is not present in discovery data,MERGED,2014-12-05 14:14:38.000000000,2014-12-05 15:05:26.000000000,2014-12-05 15:05:25.000000000,"[{'_account_id': 3}, {'_account_id': 6645}, {'_account_id': 10239}]","[{'number': 1, 'created': '2014-12-05 14:14:38.000000000', 'files': ['ironic_discoverd/test.py', 'ironic_discoverd/discoverd.py'], 'web_link': 'https://opendev.org/openstack/ironic-inspector/commit/d2a35f4d666d178b832a7c1e81ad5e62284ccde0', 'message': 'Do not fail if ipmi_address is not present in discovery data\n\nipmi_address never was a required field actually.\n\nChange-Id: I85fabfd513a8464f31a03e920b69486a845980d8\n'}]",0,139635,d2a35f4d666d178b832a7c1e81ad5e62284ccde0,7,3,1,10239,,,0,"Do not fail if ipmi_address is not present in discovery data

ipmi_address never was a required field actually.

Change-Id: I85fabfd513a8464f31a03e920b69486a845980d8
",git fetch https://review.opendev.org/openstack/ironic-inspector refs/changes/35/139635/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironic_discoverd/test.py', 'ironic_discoverd/discoverd.py']",2,d2a35f4d666d178b832a7c1e81ad5e62284ccde0,optional_ipmi_address," bmc_address = node_info.get('ipmi_address') 'ipmi_address': bmc_address}) uuid = node_cache.pop_node(bmc_address=bmc_address, mac=valid_macs)"," 'ipmi_address': node_info.get('ipmi_address')}) uuid = node_cache.pop_node(bmc_address=node_info['ipmi_address'], mac=valid_macs)",13,5
openstack%2Ftempest~master~Ibb9b6771e161482c70bea8cfccb2fe640df8044d,openstack/tempest,master,Ibb9b6771e161482c70bea8cfccb2fe640df8044d,Prepare to tag tests that require admin creds,ABANDONED,2014-04-17 15:59:59.000000000,2014-12-05 15:05:26.000000000,,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 7139}, {'_account_id': 7872}, {'_account_id': 9008}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-04-17 15:59:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/cb7945f511ae85daa3ff585c057abc1e487b7cde', 'message': 'Prepare to tag tests that require admin creds\n\nFixed OfficialClientManager so it does not do things that require admin\ncreds at import time.\nRemoved unnecessary admin api call from server metadata test.\nAdded has_admin_creds attribute to config object and raise exception\nif that is false and tenant isolation is specified.\nAdded requires_admin decorator.\n\nPartially implements: blueprint run-without-admin\n\nChange-Id: Ibb9b6771e161482c70bea8cfccb2fe640df8044d\n'}, {'number': 2, 'created': '2014-04-17 17:24:35.000000000', 'files': ['tempest/api/compute/servers/test_server_metadata.py', 'tempest/clients.py', 'tempest/config.py', 'tempest/test.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/ebe590e09d7569764a07809c7b928485ca8ef19a', 'message': 'Prepare to tag tests that require admin creds\n\nFixed OfficialClientManager so it does not do things that require admin\ncreds at import time.\nRemoved unnecessary admin api call from server metadata test.\nAdded has_admin_creds attribute to config object and raise exception\nif that is false and tenant isolation is specified.\nAdded requires_admin decorator.\n\nPartially implements: blueprint run-without-admin\n\nChange-Id: Ibb9b6771e161482c70bea8cfccb2fe640df8044d\n'}]",4,88347,ebe590e09d7569764a07809c7b928485ca8ef19a,20,8,2,1192,,,0,"Prepare to tag tests that require admin creds

Fixed OfficialClientManager so it does not do things that require admin
creds at import time.
Removed unnecessary admin api call from server metadata test.
Added has_admin_creds attribute to config object and raise exception
if that is false and tenant isolation is specified.
Added requires_admin decorator.

Partially implements: blueprint run-without-admin

Change-Id: Ibb9b6771e161482c70bea8cfccb2fe640df8044d
",git fetch https://review.opendev.org/openstack/tempest refs/changes/47/88347/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/compute/servers/test_server_metadata.py', 'tempest/clients.py', 'tempest/config.py', 'tempest/test.py']",4,cb7945f511ae85daa3ff585c057abc1e487b7cde,bp/run-without-admin,"def requires_admin(): """"""A decorator to skip tests if admin credentials are not available """""" def decorator(func): @functools.wraps(func) def wrapper(*func_args, **func_kwargs): if not CONF.has_admin_creds: msg = ""Skipped because admin creds are not available"" raise testtools.TestCase.skipException(msg) return func(*func_args, **func_kwargs) return wrapper return decorator ",,31,8
openstack%2Ftempest~master~I05c5d922458f88367ba39f41b64808aa16c70bc5,openstack/tempest,master,I05c5d922458f88367ba39f41b64808aa16c70bc5,Added test to update subnet with dhcp to false,ABANDONED,2014-08-13 08:54:35.000000000,2014-12-05 15:05:24.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 7350}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 12708}]","[{'number': 1, 'created': '2014-08-13 08:54:35.000000000', 'files': ['tempest/api/network/test_networks.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/d69e1130e4b94b3a2830682c0c769224a1538ef2', 'message': 'Added test to update subnet with dhcp to false\n\nChange-Id: I05c5d922458f88367ba39f41b64808aa16c70bc5\n'}]",0,113816,d69e1130e4b94b3a2830682c0c769224a1538ef2,21,7,1,12708,,,0,"Added test to update subnet with dhcp to false

Change-Id: I05c5d922458f88367ba39f41b64808aa16c70bc5
",git fetch https://review.opendev.org/openstack/tempest refs/changes/16/113816/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/network/test_networks.py'],1,d69e1130e4b94b3a2830682c0c769224a1538ef2,enable_dhcp_false_subnet," @test.attr(type='smoke') def test_update_subnet_with_enable_dhcp_to_False(self): subnet_id = self.subnet['id'] resp, body = self.client.update_subnet(subnet_id, enable_dhcp=False) self.assertEqual('200', resp['status']) updated_subnet = body['subnet'] self.assertEqual(updated_subnet['enable_dhcp'], False) ",,8,0
openstack%2Ftempest~master~Ie8dca8e47735c0dd42baa018c2348276ed419a7c,openstack/tempest,master,Ie8dca8e47735c0dd42baa018c2348276ed419a7c,Test resource claim scenario,ABANDONED,2014-08-19 06:01:49.000000000,2014-12-05 15:05:23.000000000,,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-19 06:01:49.000000000', 'files': ['tempest/api/compute/servers/test_server_resources.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/f613140984b32b49ccd089e659ec3f8ad0a0a3de', 'message': 'Test resource claim scenario\n\nThe compute node resource status changed when instance_claim()\nor resize_claim() happens.\n\nWe should make sure that after claim, or claim abortion, the\nresource information is up to date. For example, after a\ndrop_resize_claim() happen, the corresponding compute resource\nwill be added back to compute node resource tracker.\n\nCurrently this does not work, and this test case is to guard this.\nNotice: to make sure the check is correctly, we need make sure no\nother case executed and try to claim resources in the same compute node.\n\nChange-Id: Ie8dca8e47735c0dd42baa018c2348276ed419a7c\n'}]",0,115176,f613140984b32b49ccd089e659ec3f8ad0a0a3de,6,4,1,4573,,,0,"Test resource claim scenario

The compute node resource status changed when instance_claim()
or resize_claim() happens.

We should make sure that after claim, or claim abortion, the
resource information is up to date. For example, after a
drop_resize_claim() happen, the corresponding compute resource
will be added back to compute node resource tracker.

Currently this does not work, and this test case is to guard this.
Notice: to make sure the check is correctly, we need make sure no
other case executed and try to claim resources in the same compute node.

Change-Id: Ie8dca8e47735c0dd42baa018c2348276ed419a7c
",git fetch https://review.opendev.org/openstack/tempest refs/changes/76/115176/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/compute/servers/test_server_resources.py'],1,f613140984b32b49ccd089e659ec3f8ad0a0a3de,,"# Copyright 2012 OpenStack Foundation # Copyright 2014 Intel Corp. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import logging from tempest.api.compute import base from tempest import config CONF = config.CONF LOG = logging.getLogger(__name__) class ServerResourcesTestJSON(base.BaseV2ComputeAdminTest): run_ssh = CONF.compute.run_ssh _host_key = 'OS-EXT-SRV-ATTR:host' _hypervisor_name_key = 'OS-EXT-SRV-ATTR:hypervisor_hostname' def setUp(self): # NOTE(afazekas): Normally we use the same server with all test cases, # but if it has an issue, we build a new one super(ServerResourcesTestJSON, self).setUp() # Check if the server is in a clean state after test try: self.client.wait_for_server_status(self.server_id, 'ACTIVE') except Exception: # Rebuild server if something happened to it during a test self.__class__.server_id = self.rebuild_server(self.server_id) resp, server = self.client.get_server(self.server_id) self.host_id = self._get_hypervisor_id(server) self.clean_hyper = self._get_clean_hypervisor(server) self.server_deleted = False def tearDown(self): if not self.server_deleted: self.server_check_teardown() super(ServerResourcesTestJSON, self).tearDown() @classmethod def setUpClass(cls): super(ServerResourcesTestJSON, cls).setUpClass() cls.client = cls.os_adm.servers_client cls.hyper_client = cls.os_adm.hypervisor_client resp, cls.clean_hypers = cls.hyper_client.get_hypervisor_list_details() cls.server_id = cls.rebuild_server(None) def _get_hypervisor_id(self, server): host = server.get(self._host_key) hypervisor_name = server.get(self._hypervisor_name_key) resp, detail_hypers = self.hyper_client.get_hypervisor_list_details() for hyper in detail_hypers: if (hyper['service']['host'] == host and hyper['hypervisor_hostname'] == hypervisor_name): return hyper['id'] def _get_clean_hypervisor(self, server): host = server.get(self._host_key) hypervisor_name = server.get(self._hypervisor_name_key) for hyper in self.clean_hypers: if (hyper['service']['host'] == host and hyper['hypervisor_hostname'] == hypervisor_name): return hyper def _get_flavor_ramsize(self, flavor_ref): resp, flavor = self.os.flavors_client.get_flavor_details(flavor_ref) return flavor['ram'] def test_resource_boot(self): resp, hyper = self.hyper_client.get_hypervisor_show_details( self.host_id) flavor_ref, foo = self._detect_server_flavor(self.server_id) ramsize = self._get_flavor_ramsize(flavor_ref) self.assertEqual(hyper['free_ram_mb'] + ramsize, self.clean_hyper['free_ram_mb']) self.client.delete_server(self.server_id) self.client.wait_for_server_termination(self.server_id) resp, hyper = self.hyper_client.get_hypervisor_show_details( self.host_id) self.assertEqual(hyper['free_ram_mb'], self.clean_hyper['free_ram_mb']) self.server_deleted = True def _detect_server_flavor(self, server_id): # Detects the current server image flavor ref. resp, server = self.client.get_server(server_id) current_flavor = server['flavor']['id'] new_flavor_ref = (self.flavor_ref_alt if current_flavor == self.flavor_ref else self.flavor_ref) return current_flavor, new_flavor_ref def test_resource_confirm_resize(self): previous_flavor_ref, new_flavor_ref = self._detect_server_flavor( self.server_id) prev_ramsize = self._get_flavor_ramsize(previous_flavor_ref) new_ramsize = self._get_flavor_ramsize(new_flavor_ref) resp, hyper = self.hyper_client.get_hypervisor_show_details( self.host_id) self.assertEqual(hyper['free_ram_mb'] + prev_ramsize, self.clean_hyper['free_ram_mb']) resp, server = self.client.resize(self.server_id, new_flavor_ref) self.client.wait_for_server_status(self.server_id, 'VERIFY_RESIZE') resp, hyper_resize = self.hyper_client.get_hypervisor_show_details( self.host_id) self.assertEqual( hyper_resize['free_ram_mb'] + prev_ramsize + new_ramsize, self.clean_hyper['free_ram_mb']) self.client.confirm_resize(self.server_id) expected_status = 'ACTIVE' self.client.wait_for_server_status(self.server_id, expected_status) rsp, hyper_confirm = self.hyper_client.get_hypervisor_show_details( self.host_id) self.assertEqual(hyper_confirm['free_ram_mb'] + new_ramsize, self.clean_hyper['free_ram_mb']) ",,134,0
openstack%2Ftempest~master~I9ed1891bb3d07d415feca37daba86cab2a8794fc,openstack/tempest,master,I9ed1891bb3d07d415feca37daba86cab2a8794fc,Add tempest test for heat resource AWS::EC2::EIP,ABANDONED,2014-04-22 13:16:50.000000000,2014-12-05 15:05:22.000000000,,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 2750}, {'_account_id': 4328}, {'_account_id': 5196}, {'_account_id': 5292}, {'_account_id': 5689}, {'_account_id': 5803}, {'_account_id': 6698}, {'_account_id': 6796}, {'_account_id': 7428}, {'_account_id': 7872}, {'_account_id': 8871}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-04-22 13:16:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8125b563e4a24a45a367bc737f96c41f70728ec5', 'message': 'Add tempest test for heat resource AWS::EC2::EIP\n\nBug: #1269551\nChange-Id: I9ed1891bb3d07d415feca37daba86cab2a8794fc\n'}, {'number': 2, 'created': '2014-04-23 17:02:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/a6c863b258869ef5a4a2c34b24bee20f9be01497', 'message': 'Add tempest test for heat resource AWS::EC2::EIP\n\nCloses-Bug: #1269551\nChange-Id: I9ed1891bb3d07d415feca37daba86cab2a8794fc\n'}, {'number': 3, 'created': '2014-04-29 17:24:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/307fb6ea0d86c279af9d85ad04037994124c37c1', 'message': 'Add tempest test for heat resource AWS::EC2::EIP\n\nThis adds four tests to test a Heat template with AWS::EC2::EIP.\nBoth YAML and json templates are added and tested. \nThe VPC and AllocationId are tested.\n\nCloses-Bug: #1269551\nChange-Id: I9ed1891bb3d07d415feca37daba86cab2a8794fc\n'}, {'number': 4, 'created': '2014-04-30 15:34:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/cd0594ca5b21b0321ebe90cf97f1dbc2764a73e8', 'message': 'Add tempest test for heat resource AWS::EC2::EIP\n\nThis adds four tests to test a Heat template with AWS::EC2::EIP.\nBoth YAML and json templates are added and tested.\nThe VPC and AllocationId are tested.\n\nCloses-Bug: #1269551\nChange-Id: I9ed1891bb3d07d415feca37daba86cab2a8794fc\n'}, {'number': 5, 'created': '2014-04-30 17:23:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/920dc6da0698e9c5609eff307c2759d014a37a8a', 'message': 'Add tempest test for heat resource AWS::EC2::EIP\n\nThis adds four tests to test a Heat template with AWS::EC2::EIP.\nBoth YAML and json templates are added and tested.\nThe VPC and AllocationId are tested.\n\nCloses-Bug: #1269551\nChange-Id: I9ed1891bb3d07d415feca37daba86cab2a8794fc\n'}, {'number': 6, 'created': '2014-05-02 18:03:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0678eab705e84e2d90dd22e4f2a3ce2a7d9adab1', 'message': 'Add tempest test for heat resource AWS::EC2::EIP\n\nThis adds four tests to test a Heat template with AWS::EC2::EIP.\nBoth YAML and json templates are added and tested.\nThe VPC and AllocationId are tested.\n\nCloses-Bug: #1269551\nChange-Id: I9ed1891bb3d07d415feca37daba86cab2a8794fc\n'}, {'number': 7, 'created': '2014-05-05 14:10:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d4df7a5bcf641f9f06dd65410d258d3cebb37199', 'message': 'Add tempest test for heat resource AWS::EC2::EIP\n\nThis adds four tests to test a Heat template with AWS::EC2::EIP.\nBoth YAML and json templates are added and tested.\nThe VPC and AllocationId are tested.\n\nCloses-Bug: #1269551\nChange-Id: I9ed1891bb3d07d415feca37daba86cab2a8794fc\n'}, {'number': 8, 'created': '2014-05-06 13:28:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/9e79412260677c4e3153a3ce8e64fd8f65d47d31', 'message': 'Add tempest test for heat resource AWS::EC2::EIP\n\nThis adds four tests to test a Heat template with AWS::EC2::EIP.\nBoth YAML and json templates are added and tested.\nThe VPC and AllocationId are tested.\n\nCloses-Bug: #1269551\nChange-Id: I9ed1891bb3d07d415feca37daba86cab2a8794fc\n'}, {'number': 9, 'created': '2014-05-06 14:09:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f4fa61a3396e4f2018c25a7564288c70214c59eb', 'message': 'Add tempest test for heat resource AWS::EC2::EIP\n\nThis adds four tests to test a Heat template with AWS::EC2::EIP.\nBoth YAML and json templates are added and tested.\nThe VPC and AllocationId are tested.\n\nCloses-Bug: #1269551\nChange-Id: I9ed1891bb3d07d415feca37daba86cab2a8794fc\n'}, {'number': 10, 'created': '2014-05-06 15:20:49.000000000', 'files': ['tempest/api/orchestration/stacks/test_eip_negative.py', 'tempest/api/orchestration/stacks/templates/eip.yaml', 'tempest/api/orchestration/stacks/test_eip.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/7e89297261ac1f5cb7e773cef10f408ec998c733', 'message': 'Add tempest test for heat resource AWS::EC2::EIP\n\nThis adds four tests to test a Heat template with AWS::EC2::EIP.\nYAML templates are added and tested.\nThe VPC and AllocationId are tested.\n\nCloses-Bug: #1269551\nChange-Id: I9ed1891bb3d07d415feca37daba86cab2a8794fc\n'}]",36,89564,7e89297261ac1f5cb7e773cef10f408ec998c733,63,14,10,6698,,,0,"Add tempest test for heat resource AWS::EC2::EIP

This adds four tests to test a Heat template with AWS::EC2::EIP.
YAML templates are added and tested.
The VPC and AllocationId are tested.

Closes-Bug: #1269551
Change-Id: I9ed1891bb3d07d415feca37daba86cab2a8794fc
",git fetch https://review.opendev.org/openstack/tempest refs/changes/64/89564/6 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/orchestration/stacks/test_eip_negative.py', 'tempest/api/orchestration/stacks/templates/eip.yaml', 'tempest/api/orchestration/stacks/test_eip.py', 'tempest/api/orchestration/stacks/templates/eip.json']",4,8125b563e4a24a45a367bc737f96c41f70728ec5,rpothier/1269551,"{ ""AWSTemplateFormatVersion"" : ""2010-09-09"", ""Parameters"" : { ""KeyName"" : { ""Type"" : ""String"" }, ""Flavor"" : { ""Type"" : ""String"" }, ""Image"" : { ""Type"" : ""String"" } }, ""Resources"" : { ""Server"" : { ""Type"" : ""AWS::EC2::Instance"", ""Properties"" : { ""ImageId"" : { ""Ref"" : ""Image"" }, ""InstanceType"" : { ""Ref"" : ""Flavor"" }, ""KeyName"" : { ""Ref"" : ""KeyName"" } } }, ""DatabaseIPAddress"" : { ""Type"" : ""AWS::EC2::EIP"", ""Properties"" : { ""Domain"" : ""vpc"", InstanceId: { ""Ref"" : ""Server""} } }, }, ""Outputs"" : { ""allocation_id"" : { ""Description"" : ""EIP Value"", ""Value"" : { ""Fn::GetAtt"" : [ ""DatabaseIPAddress"" , ""AllocationId""] } } } } ",,258,0
openstack%2Ftempest~master~I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1,openstack/tempest,master,I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1,Stress Test for server availability through SSH,ABANDONED,2014-02-17 14:58:40.000000000,2014-12-05 15:05:21.000000000,,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 1849}, {'_account_id': 2750}, {'_account_id': 5689}, {'_account_id': 5803}, {'_account_id': 7227}, {'_account_id': 7872}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 9922}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-02-17 14:58:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/daa39d9580de128cd662d7282e7cfda1d385077d', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 2, 'created': '2014-02-17 16:03:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/3d9c2ee503b41361b71b14a5c36cc596efc0cc12', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 3, 'created': '2014-02-18 09:00:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f4d0388265135766578a818957768df4433783fb', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 4, 'created': '2014-02-18 10:17:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c4443310bd489027936fa740ab0e0a22077ddc8e', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 5, 'created': '2014-02-19 10:42:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f94ce1c729782181874b09313057221597a437df', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 6, 'created': '2014-02-21 16:48:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/bb379969a3a69ff9f79a7eed5414ef6724df3f00', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 7, 'created': '2014-02-24 10:35:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/1a97fca0e5ce22befb6f1fcff6a5db81449340ee', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 8, 'created': '2014-02-24 15:14:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c9f74bd6ddc314bcc1a7aeac2660918d391dfd3f', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 9, 'created': '2014-02-28 15:16:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/35a883b814c3649ef3de8bc421e9adc47d3256fe', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 10, 'created': '2014-03-14 14:47:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/2feef8c1b1e70260cb24cd7aed7cb051dabd633a', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 11, 'created': '2014-03-17 09:35:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ef0f5194a5b72fb969564b610a84166992d6550d', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 12, 'created': '2014-03-17 16:16:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d57c5813f2b50893d863cf02c21e5296ea227291', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 13, 'created': '2014-03-21 15:02:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d4b73557af70010f3c8801776b8da17f025bcb02', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 14, 'created': '2014-03-24 10:20:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c05e6b6ee42307b0147a5bd3bf681971d40676d8', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 15, 'created': '2014-03-25 09:38:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/709defcac69ae60509e597389db1c456c9878ec3', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 16, 'created': '2014-03-25 10:21:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0e268a2e7379090a43f9669216922dfdb9bba6fb', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 17, 'created': '2014-04-03 11:56:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ba3a4c2289da8f3a2273edaa46fe9b4513f1f652', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 18, 'created': '2014-04-03 12:00:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f47133c952ab1585cb00946eeaad63acdb1e6542', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 19, 'created': '2014-05-05 10:07:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/355eb95551328b00ad23a77de58839e29cc8b5b0', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 20, 'created': '2014-05-05 11:45:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/4bd15e6fe892f18b32386e1fb75141af81fa2ea1', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 21, 'created': '2014-05-05 13:44:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/584506a2d7d8d8e929a8102873ff9377625daed7', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 22, 'created': '2014-05-14 09:58:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5ba4039b5873450f2c29f89a25d54ba451791e8c', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 23, 'created': '2014-05-22 09:13:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0e7c01fd5685803c33dad886be829d434874a862', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 24, 'created': '2014-05-30 12:27:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f3fe282abda6ffb124c8877fc2ba1149be963837', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}, {'number': 25, 'created': '2014-06-02 13:18:42.000000000', 'files': ['tempest/api/compute/servers/test_multiple_large_availability.py', 'tempest/config.py', 'tempest/stress/etc/test_large_ssh_scenario.json'], 'web_link': 'https://opendev.org/openstack/tempest/commit/35f0c9ee7f1342944e1b5e11ae4e8062043b47e4', 'message': 'Stress Test for server availability through SSH\n\nThis stress test aims to launch several instances\n(using one or more workers in parallel), attach them\na floating IP and check their availability through SSH.\n\nIt should be used to check the capability of an openstack to\nlaunch several instances in parallel.\n\nChange-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1\nImplements: blueprint stress-test-ssh-floating-ip\n'}]",27,74067,35f0c9ee7f1342944e1b5e11ae4e8062043b47e4,170,13,25,9922,,,0,"Stress Test for server availability through SSH

This stress test aims to launch several instances
(using one or more workers in parallel), attach them
a floating IP and check their availability through SSH.

It should be used to check the capability of an openstack to
launch several instances in parallel.

Change-Id: I2e4a27900ddc83f04c2fcd03ce60a3cf97a029e1
Implements: blueprint stress-test-ssh-floating-ip
",git fetch https://review.opendev.org/openstack/tempest refs/changes/67/74067/25 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/stress/etc/test_large_ssh_scenario.json', 'tempest/stress/driver.py', 'tempest/scenario/test_large_ops.py']",3,daa39d9580de128cd662d7282e7cfda1d385077d,bp/stress-test-ssh-floating-ip,"# vim: tabstop=4 shiftwidth=4 softtabstop=4 import Queuefrom tempest.test import stresstest import threading Tests below: #1: test_large_ops_scenario * spin up multiple instances in one nova call * as a regular user #2: test_large_ssh_scenario (stress test) * call test #1 * attach floating IPs for every instance * check SSH connectivity for every instance TODO: * same test as #1 for cinder aki_img_path = CONF.scenario.img_dir + ""/"" + \ CONF.scenario.aki_img_file ari_img_path = CONF.scenario.img_dir + ""/"" + \ CONF.scenario.ari_img_file ami_img_path = CONF.scenario.img_dir + ""/"" + \ CONF.scenario.ami_img_file self.keypair = self.create_keypair(name='keypair-%s' % name) security_groups=[secgroup.name], key_name=self.keypair.name) [self.set_resource(server.name, server) for server in self.servers] def _attach_and_register_floating_ip(self, server): """""" Attach a floating IP to a server and track this public IP for later use. :param server: Server on which a floating IP will be attached """""" public_network_id = CONF.network.public_network_id floating_ip = self._create_floating_ip(server, public_network_id) server.public_ip = floating_ip.floating_ip_address LOG.debug(""Server '%s' now has the public IP '%s'"", server.name, server.public_ip) def _check_server_ssh_availability(self, server): """""" Check the usability of a server through SSH. The SSH connection will be done using the private key given to the server during boot. :param server: Server to be checked through SSH """""" remote_client = self._ssh_to_server(server.public_ip, self.keypair.private_key) remote_client.validate_authentication() LOG.info(""Uname -a for '%s' : %s"", server.name, remote_client.ssh_client.exec_command('uname -a')) LOG.info(""Server '%s' is available through SSH"", server.name) def check_servers_network(self, queue, errors): """""" Iterate through the queue to get servers and check network availability on them. :param queue: Queue used to store test servers :param errors: List containing errors occuring on tested servers """""" while not queue.empty(): current_server = queue.get() try: LOG.info(""Test ping on server '%s' (timeout %is)"", current_server.public_ip, CONF.compute.ping_timeout) ping_status = self._ping_ip_address(current_server.public_ip, should_succeed=True) if not ping_status: err = ""Ping on '%s' failed"" % current_server.public_ip LOG.error(err) raise Exception(err) LOG.info(""Ping on '%s' successful"", current_server.public_ip) self._check_server_ssh_availability(current_server) except Exception as e: errors.append(str(e)) finally: queue.task_done() raise ValueError(""'%s' is not a valid number of requested servers \ (set as 'large_ops_number' in tempest.conf)"" % CONF.scenario.large_ops_number) @stresstest(class_setup_per='action') @services('compute', 'image', 'network') def test_large_ssh_scenario(self): """""" Stress test based on ""test_large_ops_scenario"". This script aims to test the load of an OpenStack platform when creating batches of instances and getting them available. """""" LOG.info(""Launch '%i' instances"", CONF.scenario.large_ops_number) if CONF.compute.image_ref: self.image = CONF.compute.image_ref else: self.glance_image_create() self.nova_boot() if CONF.stress.leave_dirty_stack: LOG.debug(""Keypair used (private part) :\n%s"", self.keypair.private_key) LOG.info(""Attach floating IPs"") [self._attach_and_register_floating_ip(server) for server in self.servers] # Use a queue to check servers connectivity in separate threads check_queue = Queue.Queue() errors = list() [check_queue.put(server) for server in self.servers] thread_num = min(len(self.servers), 12) LOG.info(""Using %i threads to check servers connectivity"", thread_num) for i in range(thread_num): t = threading.Thread(name='Check-' + str(i), target=self.check_servers_network, args=[check_queue, errors]) t.setDaemon(True) t.start() check_queue.join() LOG.info(""Run results : %i/%i servers available"", len(self.servers) - len(errors), len(self.servers)) if errors: LOG.error(""List of errors : %r"", errors) raise Exception(""Error during SSH stress test"") LOG.info(""End of run"")"," This test below: * Spin up multiple instances in one nova call * as a regular user * TODO: same thing for cinder aki_img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.aki_img_file ari_img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.ari_img_file ami_img_path = CONF.scenario.img_dir + ""/"" + CONF.scenario.ami_img_file security_groups=[secgroup.name]) for server in self.servers: self.set_resource(server.name, server) return",155,17
openstack%2Ftempest~master~Ibead1aae82b65d0a8d4bf73bb816d6b2c322ebfd,openstack/tempest,master,Ibead1aae82b65d0a8d4bf73bb816d6b2c322ebfd,Add IPv4/IPv6 Provider Network Scenario tests,ABANDONED,2014-06-16 05:58:12.000000000,2014-12-05 15:05:19.000000000,,"[{'_account_id': 3}, {'_account_id': 261}, {'_account_id': 2750}, {'_account_id': 4656}, {'_account_id': 8576}, {'_account_id': 9008}, {'_account_id': 9107}, {'_account_id': 10118}, {'_account_id': 10257}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-06-16 05:58:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c258a4145ce211aefdf3a75c23b9b972bf99dc0e', 'message': 'Add IPv4/IPv6 Provider Network Scenario tests\n\nThis patch implements provider network scenario test cases for both\nIPv4 and IPv6 subnets. For the tests to run, devstack should create\na provider network and associate the required subnets (IPv4 and/or IPv6)\nto the network. It should then configure the provider network id in\ntempest.conf (as network.provider_network_id) for tempest to read\nthe information.\n\nChange-Id: Ibead1aae82b65d0a8d4bf73bb816d6b2c322ebfd\n'}, {'number': 2, 'created': '2014-06-16 08:05:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/fbe1917293a3e0c197f6edd948835248df8997ae', 'message': 'Add IPv4/IPv6 Provider Network Scenario tests\n\nThis patch implements provider network scenario test cases for both\nIPv4 and IPv6 subnets. For the tests to run, devstack should create\na provider network and associate the required subnets (IPv4 and/or IPv6)\nto the network. It should then configure the provider network id in\ntempest.conf (as network.provider_network_id) for tempest to read\nthe information.\n\nChange-Id: Ibead1aae82b65d0a8d4bf73bb816d6b2c322ebfd\n'}, {'number': 3, 'created': '2014-06-23 04:36:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8af21390a31cad04329dbde3256c6b0fb8b06fe4', 'message': 'Add IPv4/IPv6 Provider Network Scenario tests\n\nThis patch implements provider network scenario test cases for both\nIPv4 and IPv6 subnets. For the tests to run, devstack should create\na provider network and associate the required subnets (IPv4 and/or IPv6)\nto the network. It should then configure the provider network id in\ntempest.conf (as network.provider_network_id) for tempest to read\nthe information.\n\nChange-Id: Ibead1aae82b65d0a8d4bf73bb816d6b2c322ebfd\n'}, {'number': 4, 'created': '2014-07-02 07:43:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5cf2c06b763196520bb11376ca2ac969e35ea13d', 'message': 'Add IPv4/IPv6 Provider Network Scenario tests\n\nThis patch implements provider network scenario test cases for both\nIPv4 and IPv6 subnets. For the tests to run, devstack should create\na provider network and associate the required subnets (IPv4 and/or IPv6)\nto the network. It should then configure the provider network id in\ntempest.conf (as network.provider_network_id) for tempest to read\nthe information.\n\nImplements: bp neutron-provider-networking\nChange-Id: Ibead1aae82b65d0a8d4bf73bb816d6b2c322ebfd\n'}, {'number': 5, 'created': '2014-08-11 16:06:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/92a58dc38d6765f8c3dcc42e109c4f5f80c61ce4', 'message': 'Add IPv4/IPv6 Provider Network Scenario tests\n\nThis patch implements provider network scenario test cases for both\nIPv4 and IPv6 subnets. For the tests to run, devstack should create\na provider network and associate the required subnets (IPv4 and/or IPv6)\nto the network. It should then configure the provider network id in\ntempest.conf (as network.provider_network_id) for tempest to read\nthe information.\n\nImplements: bp neutron-provider-networking\nChange-Id: Ibead1aae82b65d0a8d4bf73bb816d6b2c322ebfd\n'}, {'number': 6, 'created': '2014-08-12 09:58:11.000000000', 'files': ['tempest/scenario/manager.py', 'tempest/common/utils/linux/remote_client.py', 'tempest/scenario/test_network_basic_ops.py', 'etc/tempest.conf.sample', 'tempest/config.py', 'tempest/scenario/test_provider_net_ops.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/b74588933b86a3ca9cbdcb77f45eda02439e6200', 'message': 'Add IPv4/IPv6 Provider Network Scenario tests\n\nThis patch implements provider network scenario test cases for both\nIPv4 and IPv6 subnets. For the tests to run, devstack should create\na provider network and associate the required subnets (IPv4 and/or IPv6)\nto the network. It should then configure the provider network id in\ntempest.conf (as network.provider_network_id) for tempest to read\nthe information.\n\nImplements: bp neutron-provider-networking\nChange-Id: Ibead1aae82b65d0a8d4bf73bb816d6b2c322ebfd\n'}]",35,100143,b74588933b86a3ca9cbdcb77f45eda02439e6200,53,10,6,10257,,,0,"Add IPv4/IPv6 Provider Network Scenario tests

This patch implements provider network scenario test cases for both
IPv4 and IPv6 subnets. For the tests to run, devstack should create
a provider network and associate the required subnets (IPv4 and/or IPv6)
to the network. It should then configure the provider network id in
tempest.conf (as network.provider_network_id) for tempest to read
the information.

Implements: bp neutron-provider-networking
Change-Id: Ibead1aae82b65d0a8d4bf73bb816d6b2c322ebfd
",git fetch https://review.opendev.org/openstack/tempest refs/changes/43/100143/4 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/scenario/manager.py', 'etc/tempest.conf.sample', 'tempest/config.py', 'tempest/scenario/test_provider_net_ops.py']",4,c258a4145ce211aefdf3a75c23b9b972bf99dc0e,bp/neutron-provider-networking,"# Copyright (C) 2014 eNovance SAS <licensing@enovance.com> # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import netaddr from neutronclient.common import exceptions as exc from tempest.api.network import common as net_common from tempest.common import debug from tempest.common.utils import data_utils from tempest import config from tempest import exceptions from tempest.openstack.common import log as logging from tempest.scenario import manager from tempest import test CONF = config.CONF LOG = logging.getLogger(__name__) class TestProviderNetworkBasicOps(manager.NetworkScenarioTest): """""" This smoke test suite assumes that Nova has been configured to boot VM's with Neutron-managed networking and attempts to verify network connectivity scenarios for provider network. Assumptions: - Provider network is pre-created and its id is configured in the tempest config file as network.provider_network_id - IPv4 and/or IPv6 subnets are pre-created and associated to the provider network. - provider network is reachable from the tempest host. """""" @classmethod def check_preconditions(cls): super(TestProviderNetworkBasicOps, cls).check_preconditions() if not (CONF.network.provider_network_id): msg = ('Provider Network id not configured, skipping.') cls.enabled = False raise cls.skipException(msg) @classmethod def setUpClass(cls): cls.set_network_resources() super(TestProviderNetworkBasicOps, cls).setUpClass() # Skip the tests if the required extensions are missing for ext in ['provider', 'security-group']: if not test.is_extension_enabled(ext, 'network'): msg = ""%s extension not enabled."" % ext raise cls.skipException(msg) cls.check_preconditions() def cleanup_wrapper(self, resource): self.cleanup_resource(resource, self.__class__.__name__) def setUp(self): super(TestProviderNetworkBasicOps, self).setUp() """""" Verify that a provider network and the associated subnets are configured by checking the output of list_networks and list_subnets. """""" self.provider_net = None seen_nets = self._list_networks() for n in seen_nets: if CONF.network.provider_network_id == n['id']: self.provider_net = net_common.DeletableNetwork( client=self.network_client, **n) break msg = (""Provider network [%s] not found when queried. "" ""Check the provider_network_id in tempest config file."" % CONF.network.provider_network_id) self.assertIsNotNone(self.provider_net, msg) self.provider_subnets = [] subnets = self._list_subnets(network_id=self.provider_net.id) msg = ""Subnet not configured for provider network."" self.assertNotEqual(0, len(subnets), msg) for s in subnets: subnet = net_common.DeletableSubnet( client=self.network_client, **s) self.provider_subnets.append(subnet) def _check_network_connectivity(self, server, keypair, should_connect=True): # Verify connectivity using ping and ssh ssh_login = CONF.compute.image_ssh_user try: for net_name, ip_addresses in server.networks.iteritems(): for ip_address in ip_addresses: self._check_vm_connectivity(ip_address, ssh_login, keypair.private_key, should_connect=should_connect) except Exception: LOG.exception('Tenant connectivity check failed') self._log_console_output() debug.log_net_debug() raise def _get_ipv6_subnet(self): for snet in self.provider_subnets: if snet.ip_version == 6: return snet return None def _check_ipv6_eui64_address_assignment(self): """""" verify that IPv6 addresses are based on EUI64 """""" ipv6_subnet = self._get_ipv6_subnet() if ipv6_subnet is None: LOG.debug(""No IPv6 subnet in provider network"") return if not (ipv6_subnet.ipv6_ra_mode is None and ipv6_subnet.ipv6_address_mode in ['slaac', 'dhcpv6-stateless']): LOG.debug(""IPv6 SLAAC not configured in provider n/w."") return fields = ['fixed_ips', 'mac_address', 'device_owner'] port_list = self._list_ports(network_id=self.provider_net.id, fields=fields) for pinfo in port_list: if pinfo['device_owner'].startswith('network:dhcp'): continue mac_addr = pinfo['mac_address'] fixed_ips = [pinfo['fixed_ips'][i]['ip_address'] for i in range(len(pinfo['fixed_ips']))] ipv6_eui64_addr = self._get_ipv6_addr_by_EUI64(ipv6_subnet.cidr, mac_addr) self.assertIn(str(ipv6_eui64_addr), fixed_ips) def _create_server(self, name, network, security_groups, tenant_id): keypair = self.create_keypair(name='keypair-%s' % name) self.addCleanup(self.cleanup_wrapper, keypair) create_kwargs = { 'nics': [ {'net-id': network.id}, ], 'key_name': keypair.name, 'security_groups': security_groups, 'tenant_id': tenant_id } server = self.create_server(name=name, create_kwargs=create_kwargs) self.addCleanup(self.cleanup_wrapper, server) return (server, keypair) def _check_provider_net_connectivity(self, server, keypair, tenant_id, secgroup): # Verify that we are unable to reach the VM, as there are no # inbound Security Group rules. self._check_network_connectivity(server, keypair, should_connect=False) # Add SG ingress rules to the security group associated with the VM rules = self._create_loginable_secgroup_rule_neutron(secgroup=secgroup) for rule in rules: self.assertEqual(tenant_id, rule.tenant_id) self.assertEqual(secgroup.id, rule.security_group_id) # Verify that VM is now reachable via ping and ssh self._check_network_connectivity(server, keypair, should_connect=True) def _check_provider_net_external_connectivity(self, server, keypair): """""" ping provider network default gateway for external connectivity """""" connection_params = {} source_ips = server.networks[self.provider_net.name] for source_ip in source_ips: addr = netaddr.IPAddress(source_ip) for snet in self.provider_subnets: if addr.version == snet.ip_version: connection_params[source_ip] = snet.gateway_ip break for source_ip, external_ip in connection_params.items(): # Get an ssh client connected to the VM@source_ip # and ping external_ip to check the connectivity. self._check_server_connectivity(source_ip, keypair.private_key, external_ip) def _create_inbound_ssh_rule(self, tenant_id): access_sg = self._create_empty_security_group( namestart='secgroup_new-', tenant_id=tenant_id ) self.addCleanup(self.cleanup_wrapper, access_sg) # Security Group rule to allow inbound SSH connection ssh_rule = dict( protocol='tcp', port_range_min=22, port_range_max=22, direction='ingress', ) for ethertype in ['IPv4', 'IPv6']: ssh_rule['ethertype'] = ethertype try: rule = self._create_security_group_rule(secgroup=access_sg, **ssh_rule) except exc.NeutronClientException as ex: # Ignore if the rule already exists. if not (ex.status_code is 409 and 'Security group rule' ' already exists' in ex.message): raise ex self.addCleanup(self.cleanup_wrapper, rule) return access_sg def _check_server_connectivity(self, source_ip, priv_key, dest_ip): # Get an ssh client connected to the VM@source_ip ssh_client = self._ssh_to_server(source_ip, private_key=priv_key) addr = netaddr.IPAddress(source_ip) try: # Some VM images (like Cirros) may not include ping6 utility. # Check for the presence of the utility, skip test if missing. if addr.version == 6: ssh_client.exec_command('ping6 -h') except exceptions.SSHExecCommandFailed: LOG.debug(""Instance does not include ping6 utility, "" ""hence skipping the test."") return should_succeed = True msg = ""Timed out waiting for %s to become reachable"" % dest_ip try: self.assertTrue(self._check_remote_connectivity(ssh_client, dest_ip, should_succeed), msg) except Exception: LOG.exception(""Unable to access {dest} via ssh to "" ""floating-ip {src}"".format(dest=dest_ip, src=source_ip)) debug.log_net_debug() raise def _check_in_tenant_connectivity(self): name = data_utils.rand_name('server-smoke') tenant_id = self.tenant_id # Create an access SG with inbound rule to allow SSH access_sg = self._create_inbound_ssh_rule(tenant_id) default_sg = self._default_security_group(tenant_id) security_groups = [access_sg.name, default_sg.name] # Create a server with access_sg and default security group server1, keypair1 = self._create_server(name, self.provider_net, security_groups, tenant_id) security_groups = [default_sg.name] name = data_utils.rand_name('server-smoke') # Create a second server with the default Security Group. # Note: When VMs are attached to default SG, appropriate SG rules # are automatically added for intra tenant connectivity. server2, keypair2 = self._create_server(name, self.provider_net, security_groups, tenant_id) ip_address = server1.networks[self.provider_net.name] remote_ip = server2.networks[self.provider_net.name] for index in range(len(ip_address)): self._check_server_connectivity(ip_address[index], keypair1.private_key, remote_ip[index]) @test.services('compute', 'network') def test_provider_net_with_sg_rule(self): """""" This testcase validates the following scenarios. - Spawns a VM with an empty security group and verifies that the VM is not reachable from the tempest host as the ingress security group rules are missing. - VM would become reachable after configuring the appropriate ingress SG rules. - The IPv6 addresses assigned to the ports are based on EUI64 - Tempest host can perform key-based authentication to the VM. - VM is able to reach the external ip address (by pinging the default gateway of the provider network. """""" name = data_utils.rand_name('server-smoke') tenant_id = self.tenant_id secgroup = self._create_empty_security_group(namestart=name, tenant_id=tenant_id) self.addCleanup(self.cleanup_wrapper, secgroup) security_groups = [secgroup.name] server, keypair = self._create_server(name, self.provider_net, security_groups, tenant_id) self._check_ipv6_eui64_address_assignment() self._check_provider_net_connectivity(server, keypair, tenant_id, secgroup) self._check_provider_net_external_connectivity(server, keypair) @test.services('compute', 'network') def test_provider_same_tenant_connectivity(self): """""" This testcase validates the following scenario. - Spawns two VMs for the tenant, both with default Security Group. For VM1 an additional SG is created which allows inbound SSH. Connect to VM1 using SSH and obtain an SSHClient handle. Using SSHClient handle, verify that VM2 is reachable from VM1. This testcase is to validate that when VMs are associated with the default Security Group, intra tenant VM communication is allowed by default. """""" self._check_in_tenant_connectivity() ",,365,1
openstack%2Ftempest~master~Ibf9bdab421c11a5904d05d9fe80e997e6a6b46d7,openstack/tempest,master,Ibf9bdab421c11a5904d05d9fe80e997e6a6b46d7,Adds api schema for NEUTRON APIs,ABANDONED,2014-08-25 09:45:05.000000000,2014-12-05 15:05:18.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 10068}, {'_account_id': 10385}, {'_account_id': 13041}]","[{'number': 1, 'created': '2014-08-25 09:45:05.000000000', 'files': ['tempest/api_schema/response/network/networks.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/7cf41fed03ceed1fcd66bfcae26601f32ecb142d', 'message': 'Adds api schema for NEUTRON APIs\n\n1. create_network\n2. delete_network\n\nPartially implements : blueprint api-schema-unification\n\nChange-Id: Ibf9bdab421c11a5904d05d9fe80e997e6a6b46d7\n'}]",0,116576,7cf41fed03ceed1fcd66bfcae26601f32ecb142d,7,5,1,13041,,,0,"Adds api schema for NEUTRON APIs

1. create_network
2. delete_network

Partially implements : blueprint api-schema-unification

Change-Id: Ibf9bdab421c11a5904d05d9fe80e997e6a6b46d7
",git fetch https://review.opendev.org/openstack/tempest refs/changes/76/116576/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api_schema/response/network/networks.py'],1,7cf41fed03ceed1fcd66bfcae26601f32ecb142d,bp/api-schema-unification,"# Copyright 2014 NEC Corporation. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. create_network = { 'status_code': [200], 'response_body': { 'type': 'object', 'properties': { 'network': { 'type': 'object', 'properties': { 'status': {'type': 'string'}, 'subnets': { 'type': 'array', 'items': { 'type': 'string' } }, 'name': {'type': 'string'}, 'admin_state_up': {'type': 'boolean'}, 'tenant_id': {'type': 'string'}, 'segments': { 'type': 'array', 'items': { 'type': 'object', 'properties': { 'provider:segmentation_id': {'type': 'integer'}, 'provider:physical_network': {'type': 'string'}, 'provider:network_type': {'type': 'string'}, }, 'required': ['provider:segmentation_id', 'provider:physical_network', 'provider:network_type'], 'shared': {'type': 'boolean'}, 'port_security_enabled': {'type': 'boolean'}, 'id': {'type': 'string'} }, 'required': ['status', 'name', 'subnets', 'admin_state_up', 'tenant_id', 'segments', 'shared', 'port_security_enabled', 'id'] } }, 'required': ['network'] } } delete_network = { 'status_code': [204] } ",,62,0
openstack%2Ftempest~master~Ib210577cced565cae3c3456fa8d63f843eb15c16,openstack/tempest,master,Ib210577cced565cae3c3456fa8d63f843eb15c16,Added test to check DVR interface add using subnet,ABANDONED,2014-08-12 05:47:38.000000000,2014-12-05 15:05:17.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 7249}, {'_account_id': 8556}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 12714}, {'_account_id': 12881}]","[{'number': 1, 'created': '2014-08-12 05:47:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ddca119facd0e03adf5f3e69eb35d7ab519c590d', 'message': 'Added test to check DVR interface add using subnet\n\nAdded test to check adding dvr present subnet to\n   centralized router vice versa\n\nChange-Id: Ib210577cced565cae3c3456fa8d63f843eb15c16\n'}, {'number': 2, 'created': '2014-08-21 05:19:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0dae8076be350eaf174c384775e057e1cee498ff', 'message': 'Added test to check DVR interface add using subnet\n\nAdded test to check adding dvr present subnet to\n   centralized router vice versa\n\nChange-Id: Ib210577cced565cae3c3456fa8d63f843eb15c16\n'}, {'number': 3, 'created': '2014-08-21 09:36:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f5bd29a4b5e5c951d56a16d53dad03d76e8f7d5c', 'message': 'Added test to check DVR interface add using subnet\n\nAdded test to check adding dvr present subnet to\n   centralized router vice versa\n\nChange-Id: Ib210577cced565cae3c3456fa8d63f843eb15c16\n'}, {'number': 4, 'created': '2014-08-25 05:22:45.000000000', 'files': ['tempest/api/network/base_routers.py', 'tempest/api/network/test_routers.py', 'tempest/api/network/test_routers_negative.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/4e301ecefd709e125cc2524b6d638c4ea5f9edbd', 'message': 'Added test to check DVR interface add using subnet\n\nAdded test to check adding dvr present subnet to\n   centralized router vice versa\n\nChange-Id: Ib210577cced565cae3c3456fa8d63f843eb15c16\n'}]",2,113440,4e301ecefd709e125cc2524b6d638c4ea5f9edbd,25,9,4,12714,,,0,"Added test to check DVR interface add using subnet

Added test to check adding dvr present subnet to
   centralized router vice versa

Change-Id: Ib210577cced565cae3c3456fa8d63f843eb15c16
",git fetch https://review.opendev.org/openstack/tempest refs/changes/40/113440/2 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/network/test_routers.py', 'tempest/api/network/test_routers_negative.py']",2,ddca119facd0e03adf5f3e69eb35d7ab519c590d,dvr_api_test_enhancement," @test.attr(type=['negative', 'smoke']) def test_add_remove_dvr_present_subnet_to_cvr_viceversa_returns_409(self): network01 = self.create_network( network_name=data_utils.rand_name('router-network01-')) subnet01 = self.create_subnet(network01) network02 = self.create_network( network_name=data_utils.rand_name('router-network02-')) sub02_cidr = netaddr.IPNetwork(CONF.network.tenant_network_cidr).next() subnet02 = self.create_subnet(network02, cidr=sub02_cidr) resp, router1_create_body = self.admin_client.create_router( data_utils.rand_name('router'), distributed=True) resp, router2_create_body = self.admin_client.create_router( data_utils.rand_name('router')) self.admin_client.add_router_interface_with_subnet_id( router1_create_body['router']['id'], subnet01['id']) self.admin_client.add_router_interface_with_subnet_id( router2_create_body['router']['id'], subnet02['id']) self.assertRaises(exceptions.Conflict, self.admin_client.add_router_interface_with_subnet_id, router1_create_body['router']['id'], subnet02['id']) self.assertRaises(exceptions.Conflict, self.admin_client.add_router_interface_with_subnet_id, router2_create_body['router']['id'], subnet01['id']) self.addCleanup(self.admin_client.delete_router, router2_create_body['router']['id']) self.addCleanup(self.admin_client.delete_router, router1_create_body['router']['id']) self.addCleanup( self.admin_client.remove_router_interface_with_subnet_id, router1_create_body['router']['id'], subnet01['id']) self.addCleanup( self.admin_client.remove_router_interface_with_subnet_id, router2_create_body['router']['id'], subnet02['id'])",,56,0
openstack%2Ftempest~master~I2e93e79b552450f006c323e229bfbd6fb5386a02,openstack/tempest,master,I2e93e79b552450f006c323e229bfbd6fb5386a02,Fix cleanup_wrapper exception in scenario tests,ABANDONED,2014-05-13 03:36:25.000000000,2014-12-05 15:05:16.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 5689}, {'_account_id': 6593}, {'_account_id': 8556}, {'_account_id': 8576}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-05-13 03:36:25.000000000', 'files': ['tempest/scenario/manager.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/33cd187680c3dcc6ed4431ddf409260275c095e8', 'message': 'Fix cleanup_wrapper exception in scenario tests\n\nWhen public_router_id was configured, all scenario tests\nfail with:\n\nTraceback (most recent call last):\nFile ""tempest/scenario/test_network_basic_ops.py"", line 98, in cleanup_wrapper\n    self.cleanup_resource(resource, self.__class__.__name__)\nFile ""tempest/scenario/manager.py"", line 113, in cleanup_resource\n    resource.delete()\nFile ""tempest/api/network/common.py"", line 27, in __getattr__\n    return super(AttributeDict, self).__getattribute__(name)\nAttributeError: \'AttributeDict\' object has no attribute \'delete\'\n\nThe abstract class AttributeDict cannot be used. Using DeletableRouter\nseems to solve this\n\nChange-Id: I2e93e79b552450f006c323e229bfbd6fb5386a02\n'}]",0,93369,33cd187680c3dcc6ed4431ddf409260275c095e8,38,9,1,6593,,,0,"Fix cleanup_wrapper exception in scenario tests

When public_router_id was configured, all scenario tests
fail with:

Traceback (most recent call last):
File ""tempest/scenario/test_network_basic_ops.py"", line 98, in cleanup_wrapper
    self.cleanup_resource(resource, self.__class__.__name__)
File ""tempest/scenario/manager.py"", line 113, in cleanup_resource
    resource.delete()
File ""tempest/api/network/common.py"", line 27, in __getattr__
    return super(AttributeDict, self).__getattribute__(name)
AttributeError: 'AttributeDict' object has no attribute 'delete'

The abstract class AttributeDict cannot be used. Using DeletableRouter
seems to solve this

Change-Id: I2e93e79b552450f006c323e229bfbd6fb5386a02
",git fetch https://review.opendev.org/openstack/tempest refs/changes/69/93369/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/manager.py'],1,33cd187680c3dcc6ed4431ddf409260275c095e8,deletablerouter," return net_common.DeletableRouter(client=self.network_client, **result['router'])", return net_common.AttributeDict(**result['router']),2,1
openstack%2Ftempest~master~I4e877f7e0befee4d1b4e73588abbc7df2bea768a,openstack/tempest,master,I4e877f7e0befee4d1b4e73588abbc7df2bea768a,Adds tests for CINDER volume retype V1 API,ABANDONED,2014-04-15 05:21:58.000000000,2014-12-05 15:05:15.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 6537}, {'_account_id': 7139}, {'_account_id': 8556}, {'_account_id': 8824}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 10966}]","[{'number': 1, 'created': '2014-04-15 05:21:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/671532eef89c4ca3e57e99ceca2136861919993c', 'message': 'Adds tests for CINDER volume retype v1 API\n\nThis patch adds the JSON/XML tests for CINDER volume retype v1 API.\nIt also adds the JSON/XML clients for them.\n\nChange-Id: I4e877f7e0befee4d1b4e73588abbc7df2bea768a\n'}, {'number': 2, 'created': '2014-04-15 07:38:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/44c04248e78daf536541103ea5fddcf66a1c7a0f', 'message': 'Adds tests for CINDER volume retype v1 API\n\nThis patch adds the JSON/XML tests for CINDER volume retype v1 API.\nIt also adds the JSON/XML clients for them.\n\nChange-Id: I4e877f7e0befee4d1b4e73588abbc7df2bea768a\n'}, {'number': 3, 'created': '2014-04-22 11:29:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/9d2ceff06373b4a9062c88eff486130158d697e6', 'message': 'Adds tests for CINDER volume retype v1 API\n\nThis patch adds the JSON/XML tests for CINDER volume retype v1 API.\nIt also adds the JSON/XML clients for them.\n\nChange-Id: I4e877f7e0befee4d1b4e73588abbc7df2bea768a\n'}, {'number': 4, 'created': '2014-04-22 11:37:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d9e71625745fe699b81ec3330bf49ed83292276e', 'message': 'Adds tests for CINDER volume retype v1 API\n\nThis patch adds the JSON/XML tests for CINDER volume retype v1 API.\nIt also adds the JSON/XML clients for them.\n\nChange-Id: I4e877f7e0befee4d1b4e73588abbc7df2bea768a\n'}, {'number': 5, 'created': '2014-04-25 11:38:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/dc199a3bc9c8cd3cbc5fbba431f6d346afb1278e', 'message': 'Adds tests for CINDER volume retype v1 API\n\nThis patch adds the JSON/XML tests for CINDER volume retype v1 API.\nIt also adds the JSON/XML clients for them.\n\nChange-Id: I4e877f7e0befee4d1b4e73588abbc7df2bea768a\n'}, {'number': 6, 'created': '2014-04-25 11:40:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/6aa4aa08bfef0066ead5e39ea00c350b16424d13', 'message': 'Adds tests for CINDER volume retype V2 API\n\nThis patch adds the JSON/XML tests for CINDER volume retype V2 API.\nIt also adds the JSON/XML clients for them.\n\nChange-Id: I4e877f7e0befee4d1b4e73588abbc7df2bea768a\n'}, {'number': 7, 'created': '2014-04-25 11:42:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/94ddaab74a50105ad9623b22e386eb5f5dd14a99', 'message': 'Adds tests for CINDER volume retype V2 API\n\nThis patch adds the JSON/XML tests for CINDER volume retype V2 API.\nIt also adds the JSON/XML clients for them.\n\nChange-Id: I4e877f7e0befee4d1b4e73588abbc7df2bea768a\n'}, {'number': 8, 'created': '2014-04-28 03:35:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/786409f0276b07823e7961ca34ec8ded760341ac', 'message': 'Adds tests for CINDER volume retype V2 API\n\nThis patch adds the JSON/XML tests for CINDER volume retype V2 API.\nIt also adds the JSON/XML clients for them.\n\nChange-Id: I4e877f7e0befee4d1b4e73588abbc7df2bea768a\n'}, {'number': 9, 'created': '2014-04-30 10:43:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0bdd95a4d9f71b998b093842ee89ed0632691bff', 'message': 'Adds tests for CINDER volume retype V2 API\n\nThis patch adds the JSON/XML tests for CINDER volume retype V2 API.\nIt also adds the JSON/XML clients for them.\n\nChange-Id: I4e877f7e0befee4d1b4e73588abbc7df2bea768a\n'}, {'number': 10, 'created': '2014-05-21 07:30:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/18dfde96fb391c5480c17c341acafad34d383661', 'message': 'Adds tests for CINDER volume retype V1 API\n\nThis patch adds the JSON/XML tests for CINDER volume retype V1 API.\nIt also adds the JSON/XML clients for them.\n\nChange-Id: I4e877f7e0befee4d1b4e73588abbc7df2bea768a\n'}, {'number': 11, 'created': '2014-06-04 08:04:22.000000000', 'files': ['tempest/services/volume/xml/volumes_client.py', 'tempest/services/volume/json/volumes_client.py', 'tempest/api/volume/base.py', 'tempest/api/volume/test_volumes_actions.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/0f576243e3c650f08907864056c815a0913e54a0', 'message': 'Adds tests for CINDER volume retype V1 API\n\nThis patch adds the JSON/XML tests for CINDER volume retype V1 API.\nIt also adds the JSON/XML clients for them.\n\nChange-Id: I4e877f7e0befee4d1b4e73588abbc7df2bea768a\n'}]",28,87444,0f576243e3c650f08907864056c815a0913e54a0,77,10,11,10966,,,0,"Adds tests for CINDER volume retype V1 API

This patch adds the JSON/XML tests for CINDER volume retype V1 API.
It also adds the JSON/XML clients for them.

Change-Id: I4e877f7e0befee4d1b4e73588abbc7df2bea768a
",git fetch https://review.opendev.org/openstack/tempest refs/changes/44/87444/9 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/volume/admin/test_volumes_actions.py', 'tempest/services/volume/xml/volumes_client.py', 'tempest/services/volume/json/volumes_client.py']",3,671532eef89c4ca3e57e99ceca2136861919993c,cinder_v1_volume_retype," def volume_retype(self, volume_id, new_type, policy=""never""): """"""Retype a Volume."""""" post_body = json.dumps({'os-retype': {""new_type"": new_type, ""migration_policy"": policy}}) return self.post('volumes/%s/action' % volume_id, post_body) ",,39,0
openstack%2Ftempest~master~I75e5265bf11c34b5786edc442a177ed674d600b0,openstack/tempest,master,I75e5265bf11c34b5786edc442a177ed674d600b0,Remove duplicate code from orchestration scenario tests,ABANDONED,2014-07-03 14:08:14.000000000,2014-12-05 15:05:13.000000000,,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 6498}, {'_account_id': 8556}, {'_account_id': 8576}, {'_account_id': 9008}, {'_account_id': 10090}, {'_account_id': 10385}, {'_account_id': 11956}]","[{'number': 1, 'created': '2014-07-03 14:08:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/6b94e0395fdfc4ca098448ad9c2115d7469d172e', 'message': 'Remove duplicate code from orchestration scenario tests\n\nRefactor existing scenario tests to remove duplicate code and move\nit to manager\n\nChange-Id: I75e5265bf11c34b5786edc442a177ed674d600b0\n'}, {'number': 2, 'created': '2014-07-04 06:45:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ce05012da2a5103c6ac3963657df5ddfd19799d3', 'message': 'Remove duplicate code from orchestration scenario tests\n\nRefactor existing scenario tests to remove duplicate code and move\nit to manager\n\nChange-Id: I75e5265bf11c34b5786edc442a177ed674d600b0\n'}, {'number': 3, 'created': '2014-07-15 11:14:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0758b93aeda1096b368016ddfde5bca93eaed0ed', 'message': 'Remove duplicate code from orchestration scenario tests\n\nRefactor existing scenario tests to remove duplicate code and move\nit to manager\n\nChange-Id: I75e5265bf11c34b5786edc442a177ed674d600b0\n'}, {'number': 4, 'created': '2014-07-15 11:23:40.000000000', 'files': ['tempest/scenario/orchestration/test_server_cfn_init.py', 'tempest/scenario/manager.py', 'tempest/scenario/orchestration/test_autoscaling.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/b4cd333ea00fb974a1df74ce0871661a13018d9e', 'message': 'Remove duplicate code from orchestration scenario tests\n\nRefactor existing scenario tests to remove duplicate code and move\nit to manager\n\nChange-Id: I75e5265bf11c34b5786edc442a177ed674d600b0\n'}]",13,104555,b4cd333ea00fb974a1df74ce0871661a13018d9e,52,10,4,11956,,,0,"Remove duplicate code from orchestration scenario tests

Refactor existing scenario tests to remove duplicate code and move
it to manager

Change-Id: I75e5265bf11c34b5786edc442a177ed674d600b0
",git fetch https://review.opendev.org/openstack/tempest refs/changes/55/104555/2 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/scenario/orchestration/test_server_cfn_init.py', 'tempest/scenario/manager.py', 'tempest/scenario/orchestration/test_autoscaling.py']",3,6b94e0395fdfc4ca098448ad9c2115d7469d172e,103198," self.create_stack(self.stack_name, self.template, self.parameters)"," self.client = self.orchestration_client def assign_keypair(self): if CONF.orchestration.keypair_name: self.keypair_name = CONF.orchestration.keypair_name else: self.keypair = self.create_keypair() self.keypair_name = self.keypair.id self.client.stacks.create( stack_name=self.stack_name, template=self.template, parameters=self.parameters) self.stack = self.client.stacks.get(self.stack_name) self.stack_identifier = '%s/%s' % (self.stack_name, self.stack.id)",20,31
openstack%2Ftempest~master~I106e15b56dfd36920145731ec68ac6c003703cd3,openstack/tempest,master,I106e15b56dfd36920145731ec68ac6c003703cd3,Add test for OS::Neutron::Port Heat resource,ABANDONED,2014-03-20 20:29:33.000000000,2014-12-05 15:05:12.000000000,,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 4328}, {'_account_id': 4571}, {'_account_id': 5803}, {'_account_id': 7350}, {'_account_id': 7428}, {'_account_id': 9331}, {'_account_id': 10090}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-03-20 20:29:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/43df751b2c62fca306d48bad671acdcec9170b0a', 'message': 'Add test for OS::Neutron::* Heat resource\n\nAdded a test that checks that some OS::Neutron resources are created\ncorrectly:\n    * OS::Neutron::Port\n    * OS::Neutron::FloatingIP\n    * OS::Neutron::FloatingIPAssociation\nReplaced clients.Manager() for clients.OrchestrationManager()\n\nCloses-Bug: #1269569\n\nChange-Id: I106e15b56dfd36920145731ec68ac6c003703cd3\n'}, {'number': 2, 'created': '2014-04-01 19:05:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0de18995bc91ac7396e40d9ef597b3e565cb77f8', 'message': 'Add test for OS::Neutron::* Heat resource\n\nAdded a test that checks that some OS::Neutron resources are created\ncorrectly:\n    * OS::Neutron::Port\n    * OS::Neutron::FloatingIP\n    * OS::Neutron::FloatingIPAssociation\nReplaced clients.Manager() for clients.OrchestrationManager()\n\nCloses-Bug: #1269569\n\nChange-Id: I106e15b56dfd36920145731ec68ac6c003703cd3\n'}, {'number': 3, 'created': '2014-04-03 15:26:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/9e6cda2e640099304c106318b1f9fee96c803040', 'message': 'Add test for OS::Neutron::* Heat resource\n\nAdded a test that checks that some OS::Neutron resources are created\ncorrectly:\n    * OS::Neutron::Port\n    * OS::Neutron::FloatingIP\n    * OS::Neutron::FloatingIPAssociation\nReplaced clients.Manager() for clients.OrchestrationManager()\n\nCloses-Bug: #1269569\n\nChange-Id: I106e15b56dfd36920145731ec68ac6c003703cd3\n'}, {'number': 4, 'created': '2014-04-22 17:43:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/2bd51bc9cc1e722220c6df43c2fb71451d865ce4', 'message': 'Add test for OS::Neutron::Port Heat resource\n\nAdded a test that checks that OS::Neutron::Port resources are created\ncorrectly.\nReplaced clients.Manager() for clients.OrchestrationManager()\n\nPartial-Bug: #1269569\n\nChange-Id: I106e15b56dfd36920145731ec68ac6c003703cd3\n'}, {'number': 5, 'created': '2014-06-26 20:43:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/067699102ffd0d84972b269fc83b6d055ebaef80', 'message': 'Add test for OS::Neutron::Port Heat resource\n\nAdded a test that checks that OS::Neutron::Port resources are created\ncorrectly.\n\nPartial-Bug: #1269569\n\nChange-Id: I106e15b56dfd36920145731ec68ac6c003703cd3\n'}, {'number': 6, 'created': '2014-06-27 19:15:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/cb7a9d70adf5b1b5eb91d9ac0cba4180b554d74e', 'message': 'Add test for OS::Neutron::Port Heat resource\n\nAdded a test that checks that OS::Neutron::Port resources are created\ncorrectly.\n\nPartial-Bug: #1269569\n\nChange-Id: I106e15b56dfd36920145731ec68ac6c003703cd3\n'}, {'number': 7, 'created': '2014-06-30 14:34:29.000000000', 'files': ['tempest/api/orchestration/stacks/templates/neutron_basic.yaml', 'tempest/api/orchestration/stacks/test_neutron_resources.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/97c4ba63d9ff84708816e48c0121eef0a3150af1', 'message': 'Add test for OS::Neutron::Port Heat resource\n\nAdded a test that checks that OS::Neutron::Port resources are created\ncorrectly.\n\nPartial-Bug: #1269569\n\nChange-Id: I106e15b56dfd36920145731ec68ac6c003703cd3\n'}]",13,81909,97c4ba63d9ff84708816e48c0121eef0a3150af1,74,12,7,9331,,,0,"Add test for OS::Neutron::Port Heat resource

Added a test that checks that OS::Neutron::Port resources are created
correctly.

Partial-Bug: #1269569

Change-Id: I106e15b56dfd36920145731ec68ac6c003703cd3
",git fetch https://review.opendev.org/openstack/tempest refs/changes/09/81909/4 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/orchestration/stacks/test_neutron_resources.py'],1,43df751b2c62fca306d48bad671acdcec9170b0a,bug/1269569,"import netaddr Port: type: OS::Neutron::Port properties: name: NewPort network_id: {get_resource: Network} device_id: {get_resource: Router} fixed_ips: - subnet_id: {get_resource: Subnet} - ip_address: 10.0.3.42 PortFloatingIP: type: OS::Neutron::Port properties: name: NewPortFloatingIP network_id: {get_resource: Network} FloatingIP: type: OS::Neutron::FloatingIP properties: floating_network_id: {get_param: ExternalNetworkId} port_id: {get_resource: PortFloatingIP} PortToAssociate: type: OS::Neutron::Port properties: name: NewPortToAssociate network_id: {get_resource: Network} FloatingIPToAssociate: type: OS::Neutron::FloatingIP properties: floating_network_id: {get_param: ExternalNetworkId} FloatingIPAssociation: type: OS::Neutron::FloatingIPAssociation properties: floatingip_id: {get_resource: FloatingIPToAssociate} port_id: {get_resource: PortToAssociate} os = clients.OrchestrationManager() @classmethod def _get_external_subnet(cls): resp, body = cls.network_client.show_network(cls.external_network_id) subnet_id = body['network']['subnets'][0] resp, body = cls.network_client.show_subnet(subnet_id) return body['subnet'] def test_created_port(self): """"""Verifies created port."""""" port_id = self.test_resources.get('Port')['physical_resource_id'] resp, body = self.network_client.show_port(port_id) self.assertEqual('200', resp['status']) port = body['port'] self.assertEqual('NewPort', port['name']) network_id = self.test_resources.get('Network')['physical_resource_id'] self.assertEqual(network_id, port['network_id']) fixed_ips = port['fixed_ips'] subnet_id = self.test_resources.get('Subnet')['physical_resource_id'] subnet_fixed_ips = filter(lambda ip: ip['subnet_id'] == subnet_id and ip['ip_address'] == '10.0.3.42', fixed_ips) self.assertEqual(1, len(subnet_fixed_ips)) router_id = self.test_resources.get('Router')['physical_resource_id'] self.assertEqual(router_id, port['device_id']) @attr(type='slow') def test_created_floating_ip(self): """"""Verifies created floating ip."""""" floating_ip_id = self.test_resources.get('FloatingIP')[ 'physical_resource_id'] resp, body = self.network_client.show_floatingip(floating_ip_id) self.assertEqual('200', resp['status']) floating_ip = body['floatingip'] self.assertEqual(self.external_router_id, floating_ip['router_id']) self.assertEqual(self.external_network_id, floating_ip['floating_network_id']) subnet_id = self.test_resources.get('Subnet')['physical_resource_id'] resp, body = self.network_client.show_subnet(subnet_id) self.assertEqual('200', resp['status']) subnet = body['subnet'] self.assertIn(netaddr.IPAddress(floating_ip['fixed_ip_address']), netaddr.IPNetwork(subnet['cidr'])) port_id = self.test_resources.get('PortFloatingIP')[ 'physical_resource_id'] self.assertEqual(port_id, floating_ip['port_id']) external_subnet = self._get_external_subnet() self.assertIn(netaddr.IPAddress(floating_ip['floating_ip_address']), netaddr.IPNetwork(external_subnet['cidr'])) @attr(type='slow') def test_created_floating_ip_association(self): """"""Verifies created floating ip."""""" floating_ip_id = self.test_resources.get('FloatingIPToAssociate')[ 'physical_resource_id'] resp, body = self.network_client.show_floatingip(floating_ip_id) self.assertEqual('200', resp['status']) floating_ip = body['floatingip'] port_id = self.test_resources.get('PortToAssociate')[ 'physical_resource_id'] self.assertEqual(port_id, floating_ip['port_id']) external_subnet = self._get_external_subnet() self.assertIn(netaddr.IPAddress(floating_ip['floating_ip_address']), netaddr.IPNetwork(external_subnet['cidr'])) @attr(type='slow')", os = clients.Manager(),109,1
openstack%2Ftempest~master~I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9,openstack/tempest,master,I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9,Add partial specs (multi-)provider extension positive test cases,ABANDONED,2014-07-07 16:47:47.000000000,2014-12-05 15:05:11.000000000,,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 2750}, {'_account_id': 5196}, {'_account_id': 5803}, {'_account_id': 6854}, {'_account_id': 7249}, {'_account_id': 8124}, {'_account_id': 8576}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 11105}]","[{'number': 1, 'created': '2014-07-07 16:47:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ab6624dab08e3ec95d1ae29cadfbc83800d6338c', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_scenario:\n\n    partial_specs_network_scenario=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 2, 'created': '2014-07-07 16:54:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/6aee9b9307b55fbf3e9fe6e6ea6548a5fa26c5ba', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_scenario:\n\n    partial_specs_network_scenario=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 3, 'created': '2014-07-08 15:25:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c58d145b2a6dc34a0d29ef1119731d9a93fc3fb7', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_scenario:\n\n    partial_specs_network_scenario=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 4, 'created': '2014-07-09 11:49:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/239e29ddf74eadec041365f4d23ea47b0bcc2b4e', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_scenario:\n\n    partial_specs_network_scenario=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 5, 'created': '2014-07-09 12:05:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8e84b6b60f02c9f720da047c743ee3852395d7b6', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_scenario:\n\n    partial_specs_network_scenario=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 6, 'created': '2014-07-09 14:20:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0fd211b69454f35da1778f6d9d96c01027899404', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_scenario:\n\n    partial_specs_network_scenario=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 7, 'created': '2014-08-13 20:57:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/4aec8f5b233f7af2776cf7f9d08d066a54e08bb9', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_types:\n\n    partial_specs_network_types=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 8, 'created': '2014-08-18 12:26:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/70f68ee0a4e22a8bedd42965e072aa749f3f3da2', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_types:\n\n    partial_specs_network_types=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 9, 'created': '2014-08-19 13:45:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/b25ecdeff042fcf78fb8519d699501680f400fa9', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_types:\n\n    partial_specs_network_types=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 10, 'created': '2014-08-19 16:02:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/596aaf41a21f8a87e71e987fa1d65f76d164b11a', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_types:\n\n    partial_specs_network_types=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 11, 'created': '2014-08-20 21:00:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/e69f4968542541002cbf84da3ad750aa4f3ddfc1', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_types:\n\n    partial_specs_network_types=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 12, 'created': '2014-08-25 11:00:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/a046d58e01694563eeeab8ec985d2f2a7263dc4b', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_types:\n\n    partial_specs_network_types=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 13, 'created': '2014-08-25 14:10:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/84680ea217b5280efc31f8b6d595f49acd891848', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_types:\n\n    partial_specs_network_types=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 14, 'created': '2014-08-26 13:08:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/1e4dc6e5640ff7d75ea9141ec07d25ae58292d2c', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_types:\n\n    partial_specs_network_types=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}, {'number': 15, 'created': '2014-09-02 14:32:37.000000000', 'files': ['etc/tempest.conf.sample', 'tempest/config.py', 'tempest/api/network/admin/test_provider_extension.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/d1d8cdfd5a460c72db31efba6a2f3e9fdaad0059', 'message': 'Add partial specs (multi-)provider extension positive test cases\n\nAdd positive test cases for:\n- partial specs provider networks\n- partial specs multi-provider networks\n\nSegments to use in tests are defined using the new option\npartial_specs_network_types:\n\n    partial_specs_network_types=vlan,gre\n\nDocImpact\n\nRelated to blueprint provider-network-partial-specs\n\nChange-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9\n'}]",5,105234,d1d8cdfd5a460c72db31efba6a2f3e9fdaad0059,127,14,15,8124,,,0,"Add partial specs (multi-)provider extension positive test cases

Add positive test cases for:
- partial specs provider networks
- partial specs multi-provider networks

Segments to use in tests are defined using the new option
partial_specs_network_types:

    partial_specs_network_types=vlan,gre

DocImpact

Related to blueprint provider-network-partial-specs

Change-Id: I0c8aa62ecc097b8d5ab0799a07e4b15612d22eb9
",git fetch https://review.opendev.org/openstack/tempest refs/changes/34/105234/2 && git format-patch -1 --stdout FETCH_HEAD,"['etc/tempest.conf.sample', 'tempest/api/network/admin/test_partial_specs_extension.py', 'tempest/config.py']",3,ab6624dab08e3ec95d1ae29cadfbc83800d6338c,bp/provider-network-partial-specs," cfg.ListOpt( 'partial_specs_network_scenario', default=[], help=""List segments to use in partial_specs (multi-)provider network "" ""scenario: vlan, gre, vxlan partial specs networks: set 'vlan', "" ""'gre', 'vxlan'.""),",,186,0
openstack%2Ftempest~master~Ibe0719b8418f8cfb55a3e2b789f7414c07cf9603,openstack/tempest,master,Ibe0719b8418f8cfb55a3e2b789f7414c07cf9603,"Verify ""show/create interface"" Nova v2/v3 API response attributes",ABANDONED,2014-03-28 06:27:12.000000000,2014-12-05 15:05:09.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 5292}, {'_account_id': 6167}, {'_account_id': 7882}, {'_account_id': 8556}]","[{'number': 1, 'created': '2014-03-28 06:27:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8ed4685b90fa7c11be931dffbf711ab9a5abaa2b', 'message': 'Verify ""show/create interface"" Nova v2/v3 API response attributes\n\nThis patch adds checks whether a response of\nNova show/create interface v2/v3 API includes the attributes\nto block the backward incompatibility change in the future.\n\nThe response body of v2 API is the following:\n{\n    ""interfaceAttachment"": {\n        ""port_state"": ""ACTIVE"",\n        ""fixed_ips"": [\n            {\n                ""subnet_id"": ""66175fc3-1f21-49c9-84e5-f0eca1831b20"",\n                ""ip_address"": ""10.0.0.2""\n            }\n        ],\n        ""port_id"": ""d7a42d37-78ec-4f18-8a9c-1b37c206a733"",\n        ""net_id"": ""18095bfd-0ab8-4207-99bd-dbdc373bd670"",\n        ""mac_addr"": ""fa:16:3e:fc:65:ac""\n    }\n}\n\nThe response body of v3 API is the following:\n{\n    ""interface_attachments"": {\n        ""port_state"": ""ACTIVE"",\n        ""fixed_ips"": [\n            {\n                ""subnet_id"": ""66175fc3-1f21-49c9-84e5-f0eca1831b20"",\n                ""ip_address"": ""10.0.0.2""\n            }\n        ],\n        ""port_id"": ""d7a42d37-78ec-4f18-8a9c-1b37c206a733"",\n        ""net_id"": ""18095bfd-0ab8-4207-99bd-dbdc373bd670"",\n        ""mac_addr"": ""fa:16:3e:fc:65:ac""\n    }\n}\n\nPartially implements blueprint nova-api-attribute-test\n\nChange-Id: Ibe0719b8418f8cfb55a3e2b789f7414c07cf9603\n'}, {'number': 2, 'created': '2014-03-31 03:12:14.000000000', 'files': ['tempest/api_schema/compute/v2/interfaces.py', 'tempest/services/compute/v3/json/interfaces_client.py', 'tempest/services/compute/json/interfaces_client.py', 'tempest/api_schema/compute/v3/interfaces.py', 'tempest/api_schema/compute/interfaces.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/8e5dd0a0004eadaf3817e7e42ec0c1ba935ac34e', 'message': 'Verify ""show/create interface"" Nova v2/v3 API response attributes\n\nThis patch adds checks whether a response of\nNova show/create interface v2/v3 API includes the attributes\nto block the backward incompatibility change in the future.\n\nThe response body of v2 API is the following:\n{\n    ""interfaceAttachment"": {\n        ""port_state"": ""ACTIVE"",\n        ""fixed_ips"": [\n            {\n                ""subnet_id"": ""66175fc3-1f21-49c9-84e5-f0eca1831b20"",\n                ""ip_address"": ""10.0.0.2""\n            }\n        ],\n        ""port_id"": ""d7a42d37-78ec-4f18-8a9c-1b37c206a733"",\n        ""net_id"": ""18095bfd-0ab8-4207-99bd-dbdc373bd670"",\n        ""mac_addr"": ""fa:16:3e:fc:65:ac""\n    }\n}\n\nThe response body of v3 API is the following:\n{\n    ""interface_attachments"": {\n        ""port_state"": ""ACTIVE"",\n        ""fixed_ips"": [\n            {\n                ""subnet_id"": ""66175fc3-1f21-49c9-84e5-f0eca1831b20"",\n                ""ip_address"": ""10.0.0.2""\n            }\n        ],\n        ""port_id"": ""d7a42d37-78ec-4f18-8a9c-1b37c206a733"",\n        ""net_id"": ""18095bfd-0ab8-4207-99bd-dbdc373bd670"",\n        ""mac_addr"": ""fa:16:3e:fc:65:ac""\n    }\n}\n\nPartially implements blueprint nova-api-attribute-test\n\nChange-Id: Ibe0719b8418f8cfb55a3e2b789f7414c07cf9603\n'}]",4,83674,8e5dd0a0004eadaf3817e7e42ec0c1ba935ac34e,32,6,2,7882,,,0,"Verify ""show/create interface"" Nova v2/v3 API response attributes

This patch adds checks whether a response of
Nova show/create interface v2/v3 API includes the attributes
to block the backward incompatibility change in the future.

The response body of v2 API is the following:
{
    ""interfaceAttachment"": {
        ""port_state"": ""ACTIVE"",
        ""fixed_ips"": [
            {
                ""subnet_id"": ""66175fc3-1f21-49c9-84e5-f0eca1831b20"",
                ""ip_address"": ""10.0.0.2""
            }
        ],
        ""port_id"": ""d7a42d37-78ec-4f18-8a9c-1b37c206a733"",
        ""net_id"": ""18095bfd-0ab8-4207-99bd-dbdc373bd670"",
        ""mac_addr"": ""fa:16:3e:fc:65:ac""
    }
}

The response body of v3 API is the following:
{
    ""interface_attachments"": {
        ""port_state"": ""ACTIVE"",
        ""fixed_ips"": [
            {
                ""subnet_id"": ""66175fc3-1f21-49c9-84e5-f0eca1831b20"",
                ""ip_address"": ""10.0.0.2""
            }
        ],
        ""port_id"": ""d7a42d37-78ec-4f18-8a9c-1b37c206a733"",
        ""net_id"": ""18095bfd-0ab8-4207-99bd-dbdc373bd670"",
        ""mac_addr"": ""fa:16:3e:fc:65:ac""
    }
}

Partially implements blueprint nova-api-attribute-test

Change-Id: Ibe0719b8418f8cfb55a3e2b789f7414c07cf9603
",git fetch https://review.opendev.org/openstack/tempest refs/changes/74/83674/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api_schema/compute/v2/interfaces.py', 'tempest/services/compute/v3/json/interfaces_client.py', 'tempest/services/compute/json/interfaces_client.py', 'tempest/api_schema/compute/v3/interfaces.py']",4,8ed4685b90fa7c11be931dffbf711ab9a5abaa2b,bp/nova-api-attribute-test,"# Copyright 2014 NEC Corporation. All rights reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. interface = { 'status_code': [200], 'response_body': { 'type': 'object', 'properties': { 'interface_attachment': { 'type': 'object', 'properties': { 'port_state': {'type': 'string'}, 'fixed_ips': { 'type': 'array', 'items': { 'type': 'object', 'properties': { 'subnet_id': { 'type': 'string', 'format': 'uuid' }, 'ip_address': { 'type': 'string', 'format': 'ipv4' } }, 'required': ['subnet_id', 'ip_address'] } }, 'port_id': {'type': 'string', 'format': 'uuid'}, 'net_id': {'type': 'string', 'format': 'uuid'}, 'mac_addr': {'type': 'string'} }, 'required': ['port_state', 'fixed_ips', 'port_id', 'net_id', 'mac_addr'] } }, 'required': ['interface_attachment'] } } ",,108,0
openstack%2Ftempest~master~Ie3de3517de740a2eb2079ad7e5fde78e8888e603,openstack/tempest,master,Ie3de3517de740a2eb2079ad7e5fde78e8888e603,Verify update agent attributes of V2/V3 APIs,ABANDONED,2014-04-02 08:35:40.000000000,2014-12-05 15:05:07.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 5689}, {'_account_id': 6167}, {'_account_id': 7882}]","[{'number': 1, 'created': '2014-04-02 08:35:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/082db8545e6b731afcd9a93aacf11a1cb00d982f', 'message': 'Verify update agent attributes of V2/V3 APIs\n\nThis patch adds the JSON schema for Nova V2/V3 update agent APIs\nincludes the attributes to block the backward incompatibility change\nin the future.\n\nThe response body of v2 and v3 API is the below:\n{\n    ""agent"": {\n        ""agent_id"": 1,\n        ""version"": ""7.0"",\n        ""url"": ""xxx://xxx/xxx/xxx1"",\n        ""md5hash"": ""add6bb58e139be103324d04d82d8f546""\n    }\n}\n\nPartially implements blueprint nova-api-attribute-test\n\nChange-Id: Ie3de3517de740a2eb2079ad7e5fde78e8888e603\n'}, {'number': 2, 'created': '2014-04-02 11:07:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/61d8ad963c5f9e05c39ec346b139af10ce1ae904', 'message': 'Verify update agent attributes of V2/V3 APIs\n\nThis patch adds the JSON schema for Nova V2/V3 update agent APIs\nincludes the attributes to block the backward incompatibility change\nin the future.\n\nThe response body of v2 and v3 API is the below:\n{\n    ""agent"": {\n        ""agent_id"": 1,\n        ""version"": ""7.0"",\n        ""url"": ""xxx://xxx/xxx/xxx1"",\n        ""md5hash"": ""add6bb58e139be103324d04d82d8f546""\n    }\n}\n\nPartially implements blueprint nova-api-attribute-test\n\nChange-Id: Ie3de3517de740a2eb2079ad7e5fde78e8888e603\n'}, {'number': 7, 'created': '2014-04-02 11:09:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/3d280b01fcdc641fab7389aa37518a4cbccfc24c', 'message': 'Verify update agent attributes of V2/V3 APIs\n\nThis patch adds the JSON schema for Nova V2/V3 update agent APIs\nincludes the attributes to block the backward incompatibility change\nin the future.\n\nThe response body of v2 and v3 API is the below:\n{\n    ""agent"": {\n        ""agent_id"": 1,\n        ""version"": ""7.0"",\n        ""url"": ""xxx://xxx/xxx/xxx1"",\n        ""md5hash"": ""add6bb58e139be103324d04d82d8f546""\n    }\n}\n\nPartially implements blueprint nova-api-attribute-test\n\nChange-Id: Ie3de3517de740a2eb2079ad7e5fde78e8888e603\n'}, {'number': 6, 'created': '2014-04-02 11:09:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c3e927941d9d6498a10ac9728840ec17765cd6bf', 'message': 'Verify update agent attributes of V2/V3 APIs\n\nThis patch adds the JSON schema for Nova V2/V3 update agent APIs\nincludes the attributes to block the backward incompatibility change\nin the future.\n\nThe response body of v2 and v3 API is the below:\n{\n    ""agent"": {\n        ""agent_id"": 1,\n        ""version"": ""7.0"",\n        ""url"": ""xxx://xxx/xxx/xxx1"",\n        ""md5hash"": ""add6bb58e139be103324d04d82d8f546""\n    }\n}\n\nPartially implements blueprint nova-api-attribute-test\n\nChange-Id: Ie3de3517de740a2eb2079ad7e5fde78e8888e603\n'}, {'number': 5, 'created': '2014-04-02 11:09:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/23dd9b35a00138a9880531ce4acaf0532ed64319', 'message': 'Verify update agent attributes of V2/V3 APIs\n\nThis patch adds the JSON schema for Nova V2/V3 update agent APIs\nincludes the attributes to block the backward incompatibility change\nin the future.\n\nThe response body of v2 and v3 API is the below:\n{\n    ""agent"": {\n        ""agent_id"": 1,\n        ""version"": ""7.0"",\n        ""url"": ""xxx://xxx/xxx/xxx1"",\n        ""md5hash"": ""add6bb58e139be103324d04d82d8f546""\n    }\n}\n\nPartially implements blueprint nova-api-attribute-test\n\nChange-Id: Ie3de3517de740a2eb2079ad7e5fde78e8888e603\n'}, {'number': 4, 'created': '2014-04-02 11:09:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/28e4a12f11088eb3be832e9f34674e1ff04b69d1', 'message': 'Verify update agent attributes of V2/V3 APIs\n\nThis patch adds the JSON schema for Nova V2/V3 update agent APIs\nincludes the attributes to block the backward incompatibility change\nin the future.\n\nThe response body of v2 and v3 API is the below:\n{\n    ""agent"": {\n        ""agent_id"": 1,\n        ""version"": ""7.0"",\n        ""url"": ""xxx://xxx/xxx/xxx1"",\n        ""md5hash"": ""add6bb58e139be103324d04d82d8f546""\n    }\n}\n\nPartially implements blueprint nova-api-attribute-test\n\nChange-Id: Ie3de3517de740a2eb2079ad7e5fde78e8888e603\n'}, {'number': 3, 'created': '2014-04-02 11:09:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0f47910476d6d9bbc8468c62444b2313db2b43c1', 'message': 'Verify update agent attributes of V2/V3 APIs\n\nThis patch adds the JSON schema for Nova V2/V3 update agent APIs\nincludes the attributes to block the backward incompatibility change\nin the future.\n\nThe response body of v2 and v3 API is the below:\n{\n    ""agent"": {\n        ""agent_id"": 1,\n        ""version"": ""7.0"",\n        ""url"": ""xxx://xxx/xxx/xxx1"",\n        ""md5hash"": ""add6bb58e139be103324d04d82d8f546""\n    }\n}\n\nPartially implements blueprint nova-api-attribute-test\n\nChange-Id: Ie3de3517de740a2eb2079ad7e5fde78e8888e603\n'}, {'number': 8, 'created': '2014-04-02 11:09:41.000000000', 'files': ['tempest/api_schema/compute/agents.py', 'tempest/services/compute/json/agents_client.py', 'tempest/services/compute/v3/json/agents_client.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/ed1d873e9742b7149d74a8bbd8e482bab4b5c762', 'message': 'Verify update agent attributes of V2/V3 APIs\n\nThis patch adds the JSON schema for Nova V2/V3 update agent APIs\nincludes the attributes to block the backward incompatibility change\nin the future.\n\nThe response body of v2 and v3 API is the below:\n{\n    ""agent"": {\n        ""agent_id"": 1,\n        ""version"": ""7.0"",\n        ""url"": ""xxx://xxx/xxx/xxx1"",\n        ""md5hash"": ""add6bb58e139be103324d04d82d8f546""\n    }\n}\n\nPartially implements blueprint nova-api-attribute-test\n\nChange-Id: Ie3de3517de740a2eb2079ad7e5fde78e8888e603\n'}]",0,84663,ed1d873e9742b7149d74a8bbd8e482bab4b5c762,52,5,8,7882,,,0,"Verify update agent attributes of V2/V3 APIs

This patch adds the JSON schema for Nova V2/V3 update agent APIs
includes the attributes to block the backward incompatibility change
in the future.

The response body of v2 and v3 API is the below:
{
    ""agent"": {
        ""agent_id"": 1,
        ""version"": ""7.0"",
        ""url"": ""xxx://xxx/xxx/xxx1"",
        ""md5hash"": ""add6bb58e139be103324d04d82d8f546""
    }
}

Partially implements blueprint nova-api-attribute-test

Change-Id: Ie3de3517de740a2eb2079ad7e5fde78e8888e603
",git fetch https://review.opendev.org/openstack/tempest refs/changes/63/84663/7 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api_schema/compute/agents.py', 'tempest/services/compute/json/agents_client.py', 'tempest/services/compute/v3/json/agents_client.py']",3,082db8545e6b731afcd9a93aacf11a1cb00d982f,bp/nova-api-attribute-test,"from tempest.api_schema.compute import agents as schema body = json.loads(body) self.validate_response(schema.update_agent, resp, body) return resp, body['agent']"," return resp, self._parse_resp(body)",41,2
openstack%2Ftempest~master~I92fae40750abd8c03645a733f0c0a922d2f495a5,openstack/tempest,master,I92fae40750abd8c03645a733f0c0a922d2f495a5,Provide support for non admin CLI tests,ABANDONED,2014-08-01 11:17:53.000000000,2014-12-05 15:05:06.000000000,,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 2750}, {'_account_id': 5689}, {'_account_id': 5803}, {'_account_id': 6167}, {'_account_id': 7227}, {'_account_id': 8205}, {'_account_id': 8556}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-01 11:17:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/fe6e7f04972d085f3d3c2c151431a2b97d84e1f2', 'message': 'Provide support for non admin CLI tests\n\nCurrent implementation of the tempest is that it only runs all CLI\ntests preparing credentials as of admin tenant and admin username.\n\nThis patch implements a mechanism to support non admin CLI tests,\nin conjunction with admin tests.\n\nA separate directory for admin test cases have been created for admin\nonly test cases.\n\nChange-Id: I92fae40750abd8c03645a733f0c0a922d2f495a5\n'}, {'number': 2, 'created': '2014-08-04 07:19:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/9478e678130ca095ce31ef6f2d4c830604e7bfbf', 'message': 'Provide support for non admin CLI tests\n\nCurrent implementation of the tempest is that,\nit only runs all CLI tests after preparing credentials as of admin tenant\nand admin username but not for primary tenant/user.\n\nThis patch implements a mechanism to support non admin CLI tests,\nin conjunction with admin tests.\n\nA separate directory for admin test cases have been created for admin\nonly test cases.\n\nChange-Id: I92fae40750abd8c03645a733f0c0a922d2f495a5\n'}, {'number': 3, 'created': '2014-08-04 08:59:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/7512459ce87867f616a121fb677ed99e10181e3d', 'message': 'Provide support for non admin CLI tests\n\nCurrent implementation of the tempest is that,\nit only runs all CLI tests after preparing credentials as of admin tenant\nand admin username but not for primary tenant/user.\n\nThis patch implements a mechanism to support non admin CLI tests,\nin conjunction with admin tests.\n\nA separate directory for admin test cases have been created for admin\nonly test cases.\n\nChange-Id: I92fae40750abd8c03645a733f0c0a922d2f495a5\n'}, {'number': 4, 'created': '2014-08-11 06:27:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/413340d86d770557874795e88ecef6338b3bccc3', 'message': 'Provide support for non admin CLI tests\n\nCurrent implementation of the tempest is that,\nit only runs all CLI tests after preparing credentials as of admin tenant\nand admin username but not for primary tenant/user.\n\nThis patch implements a mechanism to support non admin CLI tests,\nin conjunction with admin tests.\n\nA separate directory for admin test cases have been created for admin\nonly test cases.\n\nChange-Id: I92fae40750abd8c03645a733f0c0a922d2f495a5\n'}, {'number': 5, 'created': '2014-08-25 11:32:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/a177be934d4c8c6da3ce25c65613c7d3691965d0', 'message': 'Provide support for non admin CLI tests\n\nCurrent implementation of the tempest is that,\nit only runs all CLI tests after preparing credentials as of admin tenant\nand admin username but not for primary tenant/user.\n\nThis patch implements a mechanism to support non admin CLI tests,\nin conjunction with admin tests.\n\nA separate directory for admin test cases have been created for admin\nonly test cases.\n\nChange-Id: I92fae40750abd8c03645a733f0c0a922d2f495a5\n'}, {'number': 6, 'created': '2014-08-26 03:36:42.000000000', 'files': ['tempest/cli/simple_read_only/admin/test_keystone.py', 'tempest/auth.py', 'tempest/cli/simple_read_only/test_nova_manage.py', 'tempest/cli/simple_read_only/test_keystone.py', 'tempest/cli/simple_read_only/admin/__init__.py', 'tempest/cli/simple_read_only/admin/test_cinder.py', 'tempest/cli/simple_read_only/test_neutron.py', 'tempest/cli/__init__.py', 'tempest/cli/simple_read_only/test_nova.py', 'tempest/cli/simple_read_only/test_cinder.py', 'tempest/cli/simple_read_only/admin/test_neutron.py', 'tempest/cli/simple_read_only/admin/test_nova.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/f35a77b06d7c1ba87419c8db61671ac551d3aaa6', 'message': 'Provide support for non admin CLI tests\n\nCurrent implementation of the tempest is that,\nit only runs all CLI tests after preparing credentials as of admin tenant\nand admin username but not for primary tenant/user.\n\nThis patch implements a mechanism to support non admin CLI tests,\nin conjunction with admin tests.\n\nA separate directory for admin test cases have been created for admin\nonly test cases.\n\nChange-Id: I92fae40750abd8c03645a733f0c0a922d2f495a5\n'}]",8,111241,f35a77b06d7c1ba87419c8db61671ac551d3aaa6,49,13,6,8205,,,0,"Provide support for non admin CLI tests

Current implementation of the tempest is that,
it only runs all CLI tests after preparing credentials as of admin tenant
and admin username but not for primary tenant/user.

This patch implements a mechanism to support non admin CLI tests,
in conjunction with admin tests.

A separate directory for admin test cases have been created for admin
only test cases.

Change-Id: I92fae40750abd8c03645a733f0c0a922d2f495a5
",git fetch https://review.opendev.org/openstack/tempest refs/changes/41/111241/3 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/cli/__init__.py', 'tempest/cli/simple_read_only/admin/test_keystone.py', 'tempest/cli/simple_read_only/test_nova.py', 'tempest/auth.py', 'tempest/cli/simple_read_only/test_cinder.py', 'tempest/cli/simple_read_only/test_keystone.py', 'tempest/cli/simple_read_only/admin/test_neutron.py', 'tempest/cli/simple_read_only/admin/test_nova.py', 'tempest/cli/simple_read_only/admin/__init__.py', 'tempest/cli/simple_read_only/admin/test_cinder.py', 'tempest/cli/simple_read_only/test_neutron.py']",11,fe6e7f04972d085f3d3c2c151431a2b97d84e1f2,Support_for_non_admin_cli_tests,," @test.requires_ext(extension='dhcp_agent_scheduler', service='network') def test_neutron_dhcp_agent_list_hosting_net(self): self.neutron('dhcp-agent-list-hosting-net', params=CONF.compute.fixed_network_name) @test.attr(type='smoke') @test.attr(type='smoke') @test.requires_ext(extension='metering', service='network') def test_neutron_meter_label_list(self): self.neutron('meter-label-list') @test.attr(type='smoke') @test.requires_ext(extension='metering', service='network') def test_neutron_meter_label_rule_list(self): self.neutron('meter-label-rule-list') @test.requires_ext(extension='quotas', service='network') def test_neutron_quota_list(self): self.neutron('quota-list') @test.attr(type='smoke')",323,152
openstack%2Ftempest~master~I7c4ed6866d7f024c35f97e399d8476df6c84ba80,openstack/tempest,master,I7c4ed6866d7f024c35f97e399d8476df6c84ba80,Added a Neutron CLI test,ABANDONED,2014-08-26 12:01:55.000000000,2014-12-05 15:05:05.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 5803}, {'_account_id': 8576}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-26 12:01:55.000000000', 'files': ['tempest/cli/simple_read_only/test_neutron.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/a565385ccffa9b0abf0e39a76f008edbf3889db4', 'message': 'Added a Neutron CLI test\n\nTest case added for CLI:-\nrouter-port-list\n\nChange-Id: I7c4ed6866d7f024c35f97e399d8476df6c84ba80\n'}]",1,116870,a565385ccffa9b0abf0e39a76f008edbf3889db4,9,5,1,12837,,,0,"Added a Neutron CLI test

Test case added for CLI:-
router-port-list

Change-Id: I7c4ed6866d7f024c35f97e399d8476df6c84ba80
",git fetch https://review.opendev.org/openstack/tempest refs/changes/70/116870/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/cli/simple_read_only/test_neutron.py'],1,a565385ccffa9b0abf0e39a76f008edbf3889db4,neutroncli_test,"import testtools @test.attr(type='smoke') @testtools.skipIf(not CONF.network.public_router_id, ""router id doesn't exists"") def test_neutron_router_port_list(self): router_name = CONF.network.public_router_id output = self.parser.listing(self.neutron ('router-port-list', params=router_name)) self.assertTableStruct(output, ['id', 'name', 'mac_address', 'fixed_ips']) ",,12,0
openstack%2Ftempest~master~Ief0bc0a46feac50910a684523260769f2c49f143,openstack/tempest,master,Ief0bc0a46feac50910a684523260769f2c49f143,Passing parameter to create image,ABANDONED,2014-08-25 22:01:09.000000000,2014-12-05 15:05:04.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 4573}, {'_account_id': 8576}, {'_account_id': 9349}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-25 22:01:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5503c1beef8246f12d3578e41a49cb175699416c', 'message': 'Passing parameter to create image\n\nSometime we may want to passing some additional requirement\nto create an image like minimal memory etc. Change the function\nto support extra parameter.\n\nChange-Id: Ief0bc0a46feac50910a684523260769f2c49f143\n'}, {'number': 2, 'created': '2014-08-27 20:52:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/6c4f4187abc30fefd42498f584dc6e5d1602cad7', 'message': 'Passing parameter to create image\n\nSometime we may want to passing some additional requirement\nto create an image like minimal memory etc. Change the function\nto support extra parameter.\n\nChange-Id: Ief0bc0a46feac50910a684523260769f2c49f143\n'}, {'number': 3, 'created': '2014-08-28 20:53:18.000000000', 'files': ['tempest/scenario/manager.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/b71725265041bb6c5b7f03a68945910ceb0d9a14', 'message': 'Passing parameter to create image\n\nSometime we may want to passing some additional requirement\nto create an image like minimal memory etc. Change the function\nto support extra parameter.\n\nChange-Id: Ief0bc0a46feac50910a684523260769f2c49f143\n'}]",7,116738,b71725265041bb6c5b7f03a68945910ceb0d9a14,14,6,3,4573,,,0,"Passing parameter to create image

Sometime we may want to passing some additional requirement
to create an image like minimal memory etc. Change the function
to support extra parameter.

Change-Id: Ief0bc0a46feac50910a684523260769f2c49f143
",git fetch https://review.opendev.org/openstack/tempest refs/changes/38/116738/3 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/manager.py'],1,5503c1beef8246f12d3578e41a49cb175699416c,bug/1360022," def _image_create(self, name, fmt, path, properties=None): if properties is None: properties = {} def glance_image_create(self, params=None): if params is None: params = {} properties = {'disk_format': 'qcow2'} properties.update(params) properties=properties) properties.update(params) def _image_create(self, name, fmt, path, properties=None): if properties is None: properties = {} def glance_image_create(self, params=None): if params is None: params = {} properties = {'disk_format': 'qcow2'} properties.update(params) properties=properties) properties.update(params)"," def _image_create(self, name, fmt, path, properties={}): def glance_image_create(self): properties={'disk_format': 'qcow2'}) def _image_create(self, name, fmt, path, properties={}): def glance_image_create(self): properties={'disk_format': 'qcow2'})",20,8
openstack%2Ftempest~master~Ibf4679154f2c52de304327c8f10975d6820ddb43,openstack/tempest,master,Ibf4679154f2c52de304327c8f10975d6820ddb43,Adds Cross Host Scenario,ABANDONED,2014-03-04 08:06:23.000000000,2014-12-05 15:05:03.000000000,,"[{'_account_id': 3}, {'_account_id': 7}, {'_account_id': 261}, {'_account_id': 1192}, {'_account_id': 2750}, {'_account_id': 5689}, {'_account_id': 7139}, {'_account_id': 8576}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 10257}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-03-04 08:06:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/85f1a4bb7662528a0920c8e59821e0d89761a6eb', 'message': 'Adds Cross Host Scenario\n\nExecutes tests in test_security_groups_basic_ops module, after enforcing VMs\nare created on different compute nodes. Verifying connectivity between nodes\nboth in- and cross- tenants\n\nPartially Implements: blueprint neutron-advanced-scenarios\n\nChange-Id: Ibf4679154f2c52de304327c8f10975d6820ddb43\n'}, {'number': 2, 'created': '2014-03-04 10:03:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8d0035f67349525dd5e80622362eab66e30de022', 'message': 'Adds Cross Host Scenario\n\nExecutes tests in test_security_groups_basic_ops module, after enforcing VMs\nare created on different compute nodes. Verifying connectivity between nodes\nboth in- and cross- tenants\n\nPartially Implements: blueprint neutron-advanced-scenarios\n\nChange-Id: Ibf4679154f2c52de304327c8f10975d6820ddb43\n'}, {'number': 3, 'created': '2014-03-04 10:13:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/fb27ed8a1d6772f8ddeda6adae929f17779b903e', 'message': 'Adds Cross Host Scenario\n\nExecutes tests in test_security_groups_basic_ops module, after enforcing VMs\nare created on different compute nodes. Verifying connectivity between nodes\nboth in- and cross- tenants\n\nPartially Implements: blueprint neutron-advanced-scenarios\n\nChange-Id: Ibf4679154f2c52de304327c8f10975d6820ddb43\n'}, {'number': 4, 'created': '2014-03-04 14:18:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5866598d7f7dcc148dcb5c06e841c5aaf6dad388', 'message': 'Adds Cross Host Scenario\n\nExecutes tests in test_security_groups_basic_ops module, after enforcing VMs\nare created on different compute nodes. Verifying connectivity between nodes\nboth in- and cross- tenants\n\nPartially Implements: blueprint neutron-advanced-scenarios\n\nChange-Id: Ibf4679154f2c52de304327c8f10975d6820ddb43\n'}, {'number': 5, 'created': '2014-03-05 07:46:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/526ac870df560dbc8811fe258216e7a34768fe3e', 'message': 'Adds Cross Host Scenario\n\nExecutes tests in test_security_groups_basic_ops module, after enforcing VMs\nare created on different compute nodes. Verifying connectivity between nodes\nboth in- and cross- tenants\n\nPartially Implements: blueprint neutron-advanced-scenarios\n\nChange-Id: Ibf4679154f2c52de304327c8f10975d6820ddb43\n'}, {'number': 6, 'created': '2014-03-19 09:15:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/bc5deacbdebc0d3a77344eb99c997463f6ab3459', 'message': 'Adds Cross Host Scenario\n\nExecutes tests in test_security_groups_basic_ops module, after enforcing VMs\nare created on different compute nodes. Verifying connectivity between nodes\nboth in- and cross- tenants\n\nPartially Implements: blueprint neutron-advanced-scenarios\n\nChange-Id: Ibf4679154f2c52de304327c8f10975d6820ddb43\n'}, {'number': 7, 'created': '2014-03-26 06:35:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d76343842a673dba25d24b88fa9ef575f3a2fd6e', 'message': 'Adds Cross Host Scenario\n\nExecutes tests in test_security_groups_basic_ops module, after enforcing VMs\nare created on different compute nodes. Verifying connectivity between nodes\nboth in- and cross- tenants\n\nPartially Implements: blueprint neutron-advanced-scenarios\n\nChange-Id: Ibf4679154f2c52de304327c8f10975d6820ddb43\n'}, {'number': 8, 'created': '2014-06-01 12:19:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/6ec66d9bec4623117e71bba49283292393c73bbb', 'message': 'Adds Cross Host Scenario\n\nExecutes tests in test_security_groups_basic_ops module, after enforcing VMs\nare created on different compute nodes. Verifying connectivity between nodes\nboth in- and cross- tenants\n\nPartially Implements: blueprint neutron-advanced-scenarios\n\nChange-Id: Ibf4679154f2c52de304327c8f10975d6820ddb43\n'}, {'number': 9, 'created': '2014-06-08 05:32:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/6afe9faae5aca9a1f0b8f060f5cafcdeafb37934', 'message': 'Adds Cross Host Scenario\n\nExecutes tests in test_security_groups_basic_ops module, after enforcing VMs\nare created on different compute nodes. Verifying connectivity between nodes\nboth in- and cross- tenants\n\nPartially Implements: blueprint neutron-advanced-scenarios\n\nChange-Id: Ibf4679154f2c52de304327c8f10975d6820ddb43\n'}, {'number': 10, 'created': '2014-06-15 08:59:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/75068f61d4a8e3fa399b720eef5e6883ceccd5e3', 'message': 'Adds Cross Host Scenario\n\nExecutes tests in test_security_groups_basic_ops module, after enforcing VMs\nare created on different compute nodes. Verifying connectivity between nodes\nboth in- and cross- tenants\n\nPartially Implements: blueprint neutron-advanced-scenarios\n\nChange-Id: Ibf4679154f2c52de304327c8f10975d6820ddb43\n'}, {'number': 11, 'created': '2014-06-16 06:50:54.000000000', 'files': ['tempest/scenario/test_security_groups_basic_ops.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/e4b7ea3a8635650773d9253f8c6321b6652f70da', 'message': 'Adds Cross Host Scenario\n\nExecutes tests in test_security_groups_basic_ops module, after enforcing VMs\nare created on different compute nodes. Verifying connectivity between nodes\nboth in- and cross- tenants\n\nPartially Implements: blueprint neutron-advanced-scenarios\n\nChange-Id: Ibf4679154f2c52de304327c8f10975d6820ddb43\n'}]",4,77816,e4b7ea3a8635650773d9253f8c6321b6652f70da,87,13,11,8576,,,0,"Adds Cross Host Scenario

Executes tests in test_security_groups_basic_ops module, after enforcing VMs
are created on different compute nodes. Verifying connectivity between nodes
both in- and cross- tenants

Partially Implements: blueprint neutron-advanced-scenarios

Change-Id: Ibf4679154f2c52de304327c8f10975d6820ddb43
",git fetch https://review.opendev.org/openstack/tempest refs/changes/16/77816/9 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/test_cross_hosts.py'],1,85f1a4bb7662528a0920c8e59821e0d89761a6eb,bp/neutron-advanced-scenarios,"# Copyright 2013 Red Hat, Inc. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from tempest.scenario import test_security_groups_basic_ops as base class TestCrossHost(base.TestSecurityGroupsBasicOps): """""" Runs tests in test_security_groups_basic_ops while enforcing VMs are created on different compute nodes. Fails if both VMs are deployed on the same host TODO(yfried): enable migration to in this case tests: 1. Security Groups are enforced between hosts 2. VMs can connect between hosts """""" @classmethod def check_preconditions(cls): adm_usr, adm_pass, adm_ten = cls.admin_credentials() cls.admin_compute_client = cls.manager._get_compute_client( username=adm_usr, tenant_name=adm_ten, password=adm_pass ) hypervisor_list = cls.admin_compute_client.hypervisors.list() if len(hypervisor_list) < 2: raise cls.skipException(""Need at least 2 compute nodes to run"") super(TestCrossHost, cls).check_preconditions() def setUp(self): self.servers = [] super(TestCrossHost, self).setUp() def _create_server(self, name, tenant, security_groups=None): server = super(TestCrossHost, self)._create_server( name, tenant, security_groups, # client=self.admin_compute_client, # availability_zone=availability_zone, ) self.servers.append(server) base.LOG.debug(""Server %s deployed on host %s"", server, self.admin_compute_client.servers.get(server)) if len(self.servers) > 1: serv_a, serv_b = [self.admin_compute_client.servers.get(s) for s in self.servers] self.assertNotEqual( self.gethost(serv_a), self.gethost(serv_b), msg=""servers aren't deployed on different hosts"" ) # self._distribute_servers() return server def gethost(self, server): return getattr(server, 'OS-EXT-SRV-ATTR:host') def _distribute_servers(self): """""" if both servers are on the same compute node, move one to another node """""" self.assertEqual(2, len(self.servers)) serv_a, serv_b = [self.admin_compute_client.servers.get(s) for s in self.servers] if self.gethost(serv_a) != self.gethost(serv_b): return # source_host = self.gethost(serv_b) # target_host = [h for h in # self.admin_compute_client.hypervisors.list() # if h != source_host].pop() # self.admin_compute_client.servers.live_migrate( # serv_b, # target_host.hypervisor_hostname, True, True) self.admin_compute_client.servers.migrate(serv_b) base.LOG.debug(""Server %s migrated to host %s"", serv_b, self.gethost(serv_b)) self.assertNotEqual(self.gethost(serv_a), self.gethost(serv_b), msg=""servers aren't deployed on different hosts"") ",,92,0
openstack%2Ftempest~master~I7011ca354cc80873f63819a85669203cb85d09e3,openstack/tempest,master,I7011ca354cc80873f63819a85669203cb85d09e3,Add allowed_address_pairs check for port show details,ABANDONED,2014-08-26 10:39:34.000000000,2014-12-05 15:05:02.000000000,,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 2750}, {'_account_id': 5292}, {'_account_id': 9349}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-26 10:39:34.000000000', 'files': ['tempest/api/network/test_ports.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/f3091bfd441a8b58f738aa7afc8c6286dd67aff2', 'message': 'Add allowed_address_pairs check for port show details\n\nChange-Id: I7011ca354cc80873f63819a85669203cb85d09e3\n'}]",0,116848,f3091bfd441a8b58f738aa7afc8c6286dd67aff2,9,6,1,12837,,,0,"Add allowed_address_pairs check for port show details

Change-Id: I7011ca354cc80873f63819a85669203cb85d09e3
",git fetch https://review.opendev.org/openstack/tempest refs/changes/48/116848/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/network/test_ports.py'],1,f3091bfd441a8b58f738aa7afc8c6286dd67aff2,testAPI," self.assertEqual(port['allowed_address_pairs'], [])",,1,0
openstack%2Ftempest~master~Ia074106c4d3edccc614324da940f58092c686def,openstack/tempest,master,Ia074106c4d3edccc614324da940f58092c686def,WIP: Enable running Tempest tests without auth system for Swift,ABANDONED,2014-09-16 09:59:06.000000000,2014-12-05 15:05:01.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-16 09:59:06.000000000', 'files': ['tempest/api/object_storage/base.py', 'tempest/auth.py', 'etc/tempest.conf.sample', 'tempest/config.py', 'tempest/manager.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/a13fddd769adf9d2c5f087cd55ca2236e5c5a89a', 'message': 'WIP: Enable running Tempest tests without auth system for Swift\n\nSPEC IS STILL UNAPPROVED. THIS SHOWS HOW TO IMPLEMENT BP.\n\nCurrently, Tempest has no way to run tests for Swift (and other\nOpenStack services) without installing Keystone. Swift can be\ninstalled without applying any auth system, therefore this patch\nenables running Tempest tests to no-auth Swift clusters.\n\nPartially implements blueprint swift-tests-without-auth\nChange-Id: Ia074106c4d3edccc614324da940f58092c686def\n'}]",0,121795,a13fddd769adf9d2c5f087cd55ca2236e5c5a89a,5,3,1,8859,,,0,"WIP: Enable running Tempest tests without auth system for Swift

SPEC IS STILL UNAPPROVED. THIS SHOWS HOW TO IMPLEMENT BP.

Currently, Tempest has no way to run tests for Swift (and other
OpenStack services) without installing Keystone. Swift can be
installed without applying any auth system, therefore this patch
enables running Tempest tests to no-auth Swift clusters.

Partially implements blueprint swift-tests-without-auth
Change-Id: Ia074106c4d3edccc614324da940f58092c686def
",git fetch https://review.opendev.org/openstack/tempest refs/changes/95/121795/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/object_storage/base.py', 'tempest/auth.py', 'etc/tempest.conf.sample', 'tempest/config.py', 'tempest/manager.py']",5,a13fddd769adf9d2c5f087cd55ca2236e5c5a89a,bp/swift-tests-without-auth, if auth_version == 'off': return auth.NoAuthProvider elif auth_version == 'v2':, if auth_version == 'v2':,133,11
openstack%2Ftempest~master~I3966041d4758eec7d54e890718451cb04211d6ee,openstack/tempest,master,I3966041d4758eec7d54e890718451cb04211d6ee,Test boot from volume checking,ABANDONED,2014-08-25 22:01:09.000000000,2014-12-05 15:04:59.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 10385}, {'_account_id': 11075}]","[{'number': 1, 'created': '2014-08-25 22:01:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/a426f81750a6321e281a4ad4d24803de4ff3bd33', 'message': ""Test boot from volume checking\n\nWhen boot from volume and the volume is backed by a image, the\nimage metadata like minimal ram and minimal disk should be\nrespected.\n\nWe'd keep a tempest case to cover this scenario.\n\nChange-Id: I3966041d4758eec7d54e890718451cb04211d6ee\nRelated-Bug: 1360022\n""}, {'number': 2, 'created': '2014-08-27 20:52:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d32087170c01f44a2db1feb7facba253b8d1a0bb', 'message': ""Test boot from volume checking\n\nWhen boot from volume and the volume is backed by a image, the\nimage metadata like minimal ram and minimal disk should be\nrespected.\n\nWe'd keep a tempest case to cover this scenario.\n\nChange-Id: I3966041d4758eec7d54e890718451cb04211d6ee\nRelated-Bug: 1360022\n""}, {'number': 3, 'created': '2014-08-28 20:53:18.000000000', 'files': ['tempest/scenario/test_volume_boot_pattern.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/47ab762125d401b2ce2daafcae88610579536f8e', 'message': ""Test boot from volume checking\n\nWhen boot from volume and the volume is backed by a image, the\nimage metadata like minimal ram and minimal disk should be\nrespected.\n\nWe'd keep a tempest case to cover this scenario.\n\nChange-Id: I3966041d4758eec7d54e890718451cb04211d6ee\nRelated-Bug: 1360022\n""}]",1,116739,47ab762125d401b2ce2daafcae88610579536f8e,11,4,3,4573,,,0,"Test boot from volume checking

When boot from volume and the volume is backed by a image, the
image metadata like minimal ram and minimal disk should be
respected.

We'd keep a tempest case to cover this scenario.

Change-Id: I3966041d4758eec7d54e890718451cb04211d6ee
Related-Bug: 1360022
",git fetch https://review.opendev.org/openstack/tempest refs/changes/39/116739/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/test_volume_boot_pattern.py'],1,a426f81750a6321e281a4ad4d24803de4ff3bd33,bug/1360022,"from novaclient import exceptions as nova_exc def _create_volume_from_image(self, img_uuid=CONF.compute.image_ref): @test.services('compute', 'volume', 'image') def test_volume_boot_check(self): keypair = self.create_keypair() self.security_group = self._create_security_group_nova() flavor = self.compute_client.flavors.get(CONF.compute.flavor_ref) self.glance_image_create({'min_ram': flavor.ram + 16}) volume_origin = self._create_volume_from_image(img_uuid=self.image) self.assertRaises(nova_exc.BadRequest, self._boot_instance_from_volume, volume_origin.id, keypair) ", def _create_volume_from_image(self): img_uuid = CONF.compute.image_ref,14,2
openstack%2Ftempest~master~If421d3d277503b3bb757c0f8b45e6e90901d395b,openstack/tempest,master,If421d3d277503b3bb757c0f8b45e6e90901d395b,Replace _check_remote_connectivity ping with ssh,ABANDONED,2014-09-01 12:58:15.000000000,2014-12-05 15:04:58.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 3185}, {'_account_id': 5196}, {'_account_id': 8576}]","[{'number': 1, 'created': '2014-09-01 12:58:15.000000000', 'files': ['tempest/scenario/test_security_groups_basic_ops.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/5e4a1d80b825d8eb60e9f0b0ed2568f389488f6f', 'message': ""Replace _check_remote_connectivity ping with ssh\n\nThe following test fails on Hyper-Y as Hyper-V networking ACLs\n(the native security group equivalent) doesn't support stateful\nICMP rules, this means that it's not possible to enable an\ninstance to issue outbound ping requests to a second instance\nwithout allowing also inbound ping requests (for the reply).\n\ntempest.scenario.test_security_groups_basic_ops.TestSecurityGroupsBasicOps.test_cross_tenant_traffic\n\nThis issue does not apply to TCP or UDP, so the tests could be\nreplaced by a TCP based tests (e.g testing port 22) instead of ICMP.\n\nChange-Id: If421d3d277503b3bb757c0f8b45e6e90901d395b\nCloses-Bug: #1363986\n""}]",0,118158,5e4a1d80b825d8eb60e9f0b0ed2568f389488f6f,6,5,1,8213,,,0,"Replace _check_remote_connectivity ping with ssh

The following test fails on Hyper-Y as Hyper-V networking ACLs
(the native security group equivalent) doesn't support stateful
ICMP rules, this means that it's not possible to enable an
instance to issue outbound ping requests to a second instance
without allowing also inbound ping requests (for the reply).

tempest.scenario.test_security_groups_basic_ops.TestSecurityGroupsBasicOps.test_cross_tenant_traffic

This issue does not apply to TCP or UDP, so the tests could be
replaced by a TCP based tests (e.g testing port 22) instead of ICMP.

Change-Id: If421d3d277503b3bb757c0f8b45e6e90901d395b
Closes-Bug: #1363986
",git fetch https://review.opendev.org/openstack/tempest refs/changes/58/118158/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/test_security_groups_basic_ops.py'],1,5e4a1d80b825d8eb60e9f0b0ed2568f389488f6f,bug/1363986,"from tempest import exceptions self.ssh_rule = None self._create_sg_ssh_rule(tenant) def _create_sg_ssh_rule(self, tenant): access_sg = tenant.security_groups['access'] sg_ssh_rule = self._create_security_group_rule(secgroup=access_sg, **ssh_rule) tenant.ssh_rule = sg_ssh_rule def _delete_sg_ssh_rule(self, tenant): if tenant.ssh_rule: tenant.ssh_rule.delete() def _check_remote_connectivity(self, source, dest, should_succeed=True): """""" check nc response via source ssh connection :param source: RemoteClient: an ssh connection from which to nc :param dest: and IP to nc against :param should_succeed: boolean should nc succeed or not :returns: boolean -- should_succeed == nc succesful :returns: nc is false if nc failed """""" def ping_remote(): try: # there is no -z parameter for nc in cirros cmd = 'echo 1 | nc -w 2 %s 22 > /dev/null; echo ""$?""' % dest result = source.exec_command(cmd) if result != ""0"": return not should_succeed except exceptions.SSHExecCommandFailed as e: LOG.warn('Failed to ssh IP: %s via a ssh connection from: %s.' % (dest, source.ssh_client.host)) LOG.warn(e) return not should_succeed return should_succeed return test.call_until_true(ping_remote, CONF.compute.ping_timeout, 1) self._create_sg_ssh_rule(dest_tenant) self._delete_sg_ssh_rule(source_tenant) self._create_sg_ssh_rule(source_tenant) self._delete_sg_ssh_rule(dest_tenant) "," self._create_security_group_rule(secgroup=access_sg, **ssh_rule) ruleset = dict( protocol='icmp', direction='ingress' ) self._create_security_group_rule( secgroup=dest_tenant.security_groups['default'], **ruleset ) self._create_security_group_rule( secgroup=source_tenant.security_groups['default'], **ruleset ) ",49,14
openstack%2Ftempest~master~I9ffc77adab29670dd034da5ef7c9f47e9186517f,openstack/tempest,master,I9ffc77adab29670dd034da5ef7c9f47e9186517f,Using credentials of users created on-the-fly,ABANDONED,2014-08-19 13:14:56.000000000,2014-12-05 15:04:57.000000000,,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 8871}, {'_account_id': 10385}, {'_account_id': 12946}]","[{'number': 1, 'created': '2014-08-19 13:14:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/afc5d36c6f717feeb3fa82c40bcb90a5fbf302b2', 'message': ""keystone sanity test, creating a user in a domain and uses his credentials\n\nThis test tries to log in as the new user that it created:\n        creds = KeystoneV3Credentials(username=user_name,\n                                      password=user_name,\n                                      domain_name=dom_name,\n                                      user_domain_name=dom_name,\n                                      tenant_name=proj_name)\n        auth_provider = KeystoneV3AuthProvider(creds)\n        creds = auth_provider.fill_credentials()\n        admin_client = clients.Manager(interface=self._interface, credentials=creds)\n\nThe client is created, however anything you try to do with this user's\ncredentials results in an 'Unauthorized' response...\n\nChange-Id: I9ffc77adab29670dd034da5ef7c9f47e9186517f\n""}, {'number': 2, 'created': '2014-08-19 13:18:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/342211d09297044b1e22c5b2c8f3ccec69d71336', 'message': ""keystone sanity test, creating a user in a domain and uses his credentials\n\nThis test tries to log in as the new user that it created:\n        creds = KeystoneV3Credentials(username=user_name,\n                                      password=user_name,\n                                      domain_name=dom_name,\n                                      user_domain_name=dom_name,\n                                      tenant_name=proj_name)\n        auth_provider = KeystoneV3AuthProvider(creds)\n        creds = auth_provider.fill_credentials()\n        admin_client = clients.Manager(interface=self._interface, credentials=creds)\n\nThe client is created, however anything you try to do with this user's\ncredentials results in an 'Unauthorized' response...\n\nThis is still WIP\n\nChange-Id: I9ffc77adab29670dd034da5ef7c9f47e9186517f\n""}, {'number': 3, 'created': '2014-08-25 08:20:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c34a4a5730411ae2b69ba1bf17fd1b5305eedfeb', 'message': ""keystone sanity test, creating a user in a domain and uses his credentials\n\nThis test tries to log in as the new user that it created:\n        creds = KeystoneV3Credentials(username=user_name,\n                                      password=user_name,\n                                      domain_name=dom_name,\n                                      user_domain_name=dom_name,\n                                      tenant_name=proj_name)\n        auth_provider = KeystoneV3AuthProvider(creds)\n        creds = auth_provider.fill_credentials()\n        admin_client = clients.Manager(interface=self._interface, credentials=creds)\n\nThe client is created, however anything you try to do with this user's\ncredentials results in an 'Unauthorized' response...\n\nThis is still WIP\n\nChange-Id: I9ffc77adab29670dd034da5ef7c9f47e9186517f\n""}, {'number': 4, 'created': '2014-08-25 10:54:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/4b2165241132d273401d23c1fa941563a5c95535', 'message': ""Using credentials of users created on-the-fly\n\nThis test tries to log in as the new user that it created:\n\n  creds = KeystoneV3Credentials(username=user_name,\n                                password=user_name,\n                                domain_name=dom_name,\n                                user_domain_name=dom_name,\n                                tenant_name=proj_name)\n  auth_provider = KeystoneV3AuthProvider(creds)\n  creds = auth_provider.fill_credentials()\n  admin_client = clients.Manager(interface=self._interface, credentials=creds)\n\nThe client is created, however anything you try to do with this user's\ncredentials results in an 'Unauthorized' response...\n\nChange-Id: I9ffc77adab29670dd034da5ef7c9f47e9186517f\n""}, {'number': 5, 'created': '2014-08-25 14:32:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/3885bb5a2646573bf5ceaa6f4688ce4f35e75e74', 'message': ""Using credentials of users created on-the-fly\n\nThis test tries to log in as the new user that it created:\n\n  creds = KeystoneV3Credentials(username=user_name,\n                                password=user_name,\n                                domain_name=dom_name,\n                                user_domain_name=dom_name,\n                                tenant_name=proj_name)\n  auth_provider = KeystoneV3AuthProvider(creds)\n  creds = auth_provider.fill_credentials()\n  admin_client = clients.Manager(interface=self._interface, credentials=creds)\n\nThe client is created, however anything you try to do with this user's\ncredentials results in an 'Unauthorized' response...\n\nChange-Id: I9ffc77adab29670dd034da5ef7c9f47e9186517f\n""}, {'number': 6, 'created': '2014-09-15 10:24:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/dcc177dcea71c06ca4b95794bbc78d29e1ccea8e', 'message': ""Using credentials of users created on-the-fly\n\nThis test tries to log in as the new user that it created:\n\n  creds = KeystoneV3Credentials(username=user_name,\n                                password=user_name,\n                                domain_name=dom_name,\n                                user_domain_name=dom_name,\n                                tenant_name=proj_name)\n  auth_provider = KeystoneV3AuthProvider(creds)\n  creds = auth_provider.fill_credentials()\n  admin_client = clients.Manager(interface=self._interface, credentials=creds)\n\nThe client is created, however anything you try to do with this user's\ncredentials results in an 'Unauthorized' response...\n\nChange-Id: I9ffc77adab29670dd034da5ef7c9f47e9186517f\n""}, {'number': 7, 'created': '2014-09-15 10:25:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/dfad66283ab73c551c9d4a60ba318a094c3eae15', 'message': ""Using credentials of users created on-the-fly\n\nThis test tries to log in as the new user that it created:\n\n  creds = KeystoneV3Credentials(username=user_name,\n                                password=user_name,\n                                domain_name=dom_name,\n                                user_domain_name=dom_name,\n                                tenant_name=proj_name)\n  auth_provider = KeystoneV3AuthProvider(creds)\n  creds = auth_provider.fill_credentials()\n  admin_client = clients.Manager(interface=self._interface, credentials=creds)\n\nThe client is created, however anything you try to do with this user's\ncredentials results in an 'Unauthorized' response...\n\nChange-Id: I9ffc77adab29670dd034da5ef7c9f47e9186517f\n""}, {'number': 8, 'created': '2014-09-17 09:16:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/db11b71c8df54162631a4b717e4074b340dbac74', 'message': ""Using credentials of users created on-the-fly\n\nThis test tries to log in as the new user that it created:\n\n  creds = KeystoneV3Credentials(username=user_name,\n                                password=user_name,\n                                domain_name=dom_name,\n                                user_domain_name=dom_name,\n                                tenant_name=proj_name)\n  auth_provider = KeystoneV3AuthProvider(creds)\n  creds = auth_provider.fill_credentials()\n  admin_client = clients.Manager(interface=self._interface, credentials=creds)\n\nThe client is created, however anything you try to do with this user's\ncredentials results in an 'Unauthorized' response...\n\nChange-Id: I9ffc77adab29670dd034da5ef7c9f47e9186517f\n""}, {'number': 9, 'created': '2014-09-17 14:43:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/d326d9af3ec2f75bfa93b9aa4b25a188c741f00b', 'message': ""Using credentials of users created on-the-fly\n\nThis test tries to log in as the new user that it created:\n\n  creds = KeystoneV3Credentials(username=user_name,\n                                password=user_name,\n                                domain_name=dom_name,\n                                user_domain_name=dom_name,\n                                tenant_name=proj_name)\n  auth_provider = KeystoneV3AuthProvider(creds)\n  creds = auth_provider.fill_credentials()\n  admin_client = clients.Manager(interface=self._interface, credentials=creds)\n\nThe client is created, however anything you try to do with this user's\ncredentials results in an 'Unauthorized' response...\n\nChange-Id: I9ffc77adab29670dd034da5ef7c9f47e9186517f\n""}, {'number': 10, 'created': '2014-09-17 15:36:15.000000000', 'files': ['tempest/scenario/identity/__init__.py', 'tempest/scenario/identity/test_keystone_sanity.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/78d9c346965f690d68de89e951469b1aaffdd6df', 'message': ""Using credentials of users created on-the-fly\n\nThis test tries to log in as the new user that it created:\n\n  creds = KeystoneV3Credentials(username=user_name,\n                                password=user_name,\n                                domain_name=dom_name,\n                                user_domain_name=dom_name,\n                                tenant_name=proj_name)\n  auth_provider = KeystoneV3AuthProvider(creds)\n  creds = auth_provider.fill_credentials()\n  admin_client = clients.Manager(interface=self._interface, credentials=creds)\n\nThe client is created, however anything you try to do with this user's\ncredentials results in an 'Unauthorized' response...\n\nChange-Id: I9ffc77adab29670dd034da5ef7c9f47e9186517f\n""}]",1,115279,78d9c346965f690d68de89e951469b1aaffdd6df,35,6,10,12946,,,0,"Using credentials of users created on-the-fly

This test tries to log in as the new user that it created:

  creds = KeystoneV3Credentials(username=user_name,
                                password=user_name,
                                domain_name=dom_name,
                                user_domain_name=dom_name,
                                tenant_name=proj_name)
  auth_provider = KeystoneV3AuthProvider(creds)
  creds = auth_provider.fill_credentials()
  admin_client = clients.Manager(interface=self._interface, credentials=creds)

The client is created, however anything you try to do with this user's
credentials results in an 'Unauthorized' response...

Change-Id: I9ffc77adab29670dd034da5ef7c9f47e9186517f
",git fetch https://review.opendev.org/openstack/tempest refs/changes/79/115279/2 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/identity/test_keystone_sanity.py'],1,afc5d36c6f717feeb3fa82c40bcb90a5fbf302b2,bug/1369557,"from tempest.test import BaseTestCase from tempest import clients from tempest.common.utils import data_utils from tempest.auth import KeystoneV3Credentials from tempest.auth import KeystoneV3AuthProvider class TestKeystoneSanity (BaseTestCase): _interface = 'json' @classmethod def setUpClass(cls): super(TestKeystoneSanity, cls).setUpClass() cls.os_adm = clients.AdminManager(interface=cls._interface) cls.os = clients.Manager(interface=cls._interface) def setUp(self): super(TestKeystoneSanity, self).setUp() def tearDown(self): super(TestKeystoneSanity, self).tearDown() def _delete_domain(self, domain_id): # It is necessary to disable the domain before deleting, # or else it would result in unauthorized error self.os_adm.identity_v3_client.update_domain(domain_id, enabled=False) self.os_adm.identity_v3_client.delete_domain(domain_id) def test_v3_identity(self): # create a domain dom_name = data_utils.rand_name('dom-') resp, domain = self.os_adm.identity_v3_client.create_domain(dom_name) dom_id = domain['id'] self.addCleanup(self._delete_domain, dom_id) # create a project in the domain proj_name = data_utils.rand_name('proj-') resp, body = self.os_adm.identity_v3_client.create_project(proj_name, domain_id=dom_id) proj_id = body['id'] self.addCleanup(self.os_adm.identity_v3_client.delete_project, proj_id) # create a user in the domain, with the previous project as his default project user_name = data_utils.rand_name('user-') resp, body = self.os_adm.identity_v3_client.create_user(user_name, password=user_name, domain_id=dom_id, project_id=proj_id) #FIXME: use default_project_id user_id = body['id'] self.addCleanup(self.os_adm.identity_v3_client.delete_user, user_id) # get roles and find the admin role resp, body = self.os_adm.identity_v3_client.list_roles() role_ids = [role['id'] for role in body if role['name'] == ""admin""] admin_role_id = role_ids[0] # grant the admin role to the user on his project resp, body = self.os_adm.identity_v3_client.assign_user_role_on_project(proj_id, user_id, admin_role_id) # create a new client with the new user's credentials creds = KeystoneV3Credentials(username=user_name, password=user_name, domain_name=dom_name, user_domain_name=dom_name, tenant_name=proj_name) auth_provider = KeystoneV3AuthProvider(creds) creds = auth_provider.fill_credentials() admin_client = clients.Manager(interface=self._interface, credentials=creds) # list domains with the new credentials resp, body = admin_client.identity_v3_client.list_domains() ",,74,0
openstack%2Ftempest~master~Ib7ced1d718ce62d663991968bf868666780351df,openstack/tempest,master,Ib7ced1d718ce62d663991968bf868666780351df,"Init created_objects,client for BaseBaremetalTest",ABANDONED,2014-08-18 18:11:48.000000000,2014-12-05 15:04:56.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 4523}, {'_account_id': 5803}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-18 18:11:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/1f5889deffe8839b7e003e088a2ed155177d51db', 'message': 'Init created_objects,client for BaseBaremetalTest\n\nThis should prevent an AttributeError in tearDownClass()\nif the test is aborted in setupClass() before these\nvariables are defined.\n\nChange-Id: Ib7ced1d718ce62d663991968bf868666780351df\n'}, {'number': 2, 'created': '2014-08-18 18:32:22.000000000', 'files': ['tempest/api/baremetal/admin/base.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/d3e73510419e10ac3fea1b70331c7d6f50a4bf0e', 'message': 'Init created_objects,client for BaseBaremetalTest\n\nThis should prevent an AttributeError in tearDownClass()\nif the test is aborted in setupClass() before these\nvariables are defined.\n\nChange-Id: Ib7ced1d718ce62d663991968bf868666780351df\n'}]",6,115048,d3e73510419e10ac3fea1b70331c7d6f50a4bf0e,16,5,2,4523,,,0,"Init created_objects,client for BaseBaremetalTest

This should prevent an AttributeError in tearDownClass()
if the test is aborted in setupClass() before these
variables are defined.

Change-Id: Ib7ced1d718ce62d663991968bf868666780351df
",git fetch https://review.opendev.org/openstack/tempest refs/changes/48/115048/2 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/baremetal/admin/base.py'],1,1f5889deffe8839b7e003e088a2ed155177d51db,115048, def __init__(self): self.client = None self.created_objects = {} if cls.client is None: break,,6,0
openstack%2Ftempest~master~I499a11a128b1caa0f92f2392e2f7a511583bf3c9,openstack/tempest,master,I499a11a128b1caa0f92f2392e2f7a511583bf3c9,Enable instance validation with the server_actions v2,ABANDONED,2014-03-20 15:32:42.000000000,2014-12-05 15:04:55.000000000,,"[{'_account_id': 3}, {'_account_id': 1192}, {'_account_id': 1839}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 5174}, {'_account_id': 5196}, {'_account_id': 5803}, {'_account_id': 7227}, {'_account_id': 8824}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-03-20 15:32:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/9446220a678eee14e8d833807d9b06c6fd9ecc2b', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\n fixed and floating ip.\n\nThe after the init_instance_validation method call the\n create_test_server method will help in setting up an\n ssh able server.\nIf the init_instance_validation not called the create_test_server,\n behaves as before.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 2, 'created': '2014-03-20 16:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/e9de09a223376948f4b5929aff34d0e1cd73344c', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\n fixed and floating ip.\n\nThe after the init_instance_validation method call the\n create_test_server method will help in setting up an\n ssh able server.\nIf the init_instance_validation not called the create_test_server,\n behaves as before.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 3, 'created': '2014-03-24 16:03:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/2ea10fa18e10cd298403432c44efef607fcac872', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\n fixed and floating ip.\n\nThe after the init_instance_validation method call the\n create_test_server method will help in setting up an\n ssh able server.\nIf the init_instance_validation not called the create_test_server,\n behaves as before.\n\nNow just the test_rebuild_server will run with instance validation\nas a demo.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 4, 'created': '2014-04-02 09:01:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c3bfd3ece1e483b7ff47936ec7bac2238d50bb88', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\n fixed and floating ip.\n\nThe after the init_instance_validation method call the\n create_test_server method will help in setting up an\n ssh able server.\nIf the init_instance_validation not called the create_test_server,\n behaves as before.\n\nNow just the test_rebuild_server will run with instance validation\nas a demo.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 5, 'created': '2014-04-02 17:31:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0405caca21759c7a65c38eec815f1b3a030c0086', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\n fixed and floating ip.\n\nThe after the init_instance_validation method call the\n create_test_server method will help in setting up an\n ssh able server.\nIf the init_instance_validation not called the create_test_server,\n behaves as before.\n\nNow just the test_rebuild_server will run with instance validation\nas a demo.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 6, 'created': '2014-04-07 08:07:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/02babc5bbb795995ab09f3208dc7ca402b0fe94a', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\n fixed and floating ip.\n\nThe after the init_instance_validation method call the\n create_test_server method will help in setting up an\n ssh able server.\nIf the init_instance_validation not called the create_test_server,\n behaves as before.\n\nNow just the test_rebuild_server will run with instance validation\nas a demo.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 7, 'created': '2014-04-07 09:40:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/0a1d75ce2fedffe2f402c4e5c6cd1c8a65cf976c', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\n fixed and floating ip.\n\nThe after the init_instance_validation method call the\n create_test_server method will help in setting up an\n ssh able server.\nIf the init_instance_validation not called the create_test_server,\n behaves as before.\n\nNow just the test_rebuild_server will run with instance validation\nas a demo.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 8, 'created': '2014-04-08 13:11:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/4d8ecb82453adf0d0b865d6250e90907496d76c1', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\n fixed and floating ip.\n\nThe after the init_instance_validation method call the\n create_test_server method will help in setting up an\n ssh able server.\nIf the init_instance_validation not called the create_test_server,\n behaves as before.\n\nNow just the test_rebuild_server will run with instance validation\nas a demo.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 9, 'created': '2014-04-19 12:54:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/318fdb0d9e60ef00e171df095f5470474a1ec5da', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\n fixed and floating ip.\n\nThe after the init_instance_validation method call the\n create_test_server method will help in setting up an\n ssh able server.\nIf the init_instance_validation not called the create_test_server,\n behaves as before.\n\nNow just the test_rebuild_server will run with instance validation\nas a demo.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 10, 'created': '2014-05-21 16:11:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/800dbd5f52c9fe96803bc7982401b15025c51142', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\n fixed and floating ip.\n\nThe after the init_instance_validation method call the\n create_test_server method will help in setting up an\n ssh able server.\nIf the init_instance_validation not called the create_test_server,\n behaves as before.\n\nNow just the test_rebuild_server will run with instance validation\nas a demo.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 11, 'created': '2014-05-23 13:12:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/3ca0fd9f41c2f352196d879bdf0cfc2578c8e788', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\nfixed and floating ip.\n\nAfter the init_instance_validation method, call to the\ncreate_test_server method will help in setting up an\nssh able server. If the init_instance_validation is not\ncalled the create_test_server, behaves as before.\n\nNow just the test_rebuild_server and test_change_server_password\nwill run with instance validation as a demo.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 12, 'created': '2014-05-23 14:23:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/af683ff1577fd70a73652dee1b56e9bd3639300a', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\nfixed and floating ip.\n\nAfter the init_instance_validation method, call to the\ncreate_test_server method will help in setting up an\nssh able server. If the init_instance_validation is not\ncalled the create_test_server, behaves as before.\n\nNow just the test_rebuild_server and test_change_server_password\nwill run with instance validation as a demo.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 13, 'created': '2014-05-23 14:45:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/b92849029d5a70c694cd88e676cd1f6006410fdb', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\nfixed and floating ip.\n\nAfter the init_instance_validation method, call to the\ncreate_test_server method will help in setting up an\nssh able server. If the init_instance_validation is not\ncalled the create_test_server, behaves as before.\n\nNow just the test_rebuild_server and test_change_server_password\nwill run with instance validation as a demo.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}, {'number': 14, 'created': '2014-05-23 15:47:22.000000000', 'files': ['tempest/api/compute/servers/test_server_actions.py', 'tempest/api/compute/base.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/60da146bac2df925cd8e6777fe625bc90247c863', 'message': 'Enable instance validation with the server_actions v2\n\nAdding the base methods for instance validation on both\nfixed and floating ip.\n\nAfter the init_instance_validation method, call to the\ncreate_test_server method will help in setting up an\nssh able server. If the init_instance_validation is not\ncalled the create_test_server, behaves as before.\n\nNow just the test_rebuild_server and test_change_server_password\nwill run with instance validation as a demo.\n\nChange-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9\nImplements: blueprint ssh-auth-strategy\n'}]",39,81834,60da146bac2df925cd8e6777fe625bc90247c863,105,13,14,5803,,,0,"Enable instance validation with the server_actions v2

Adding the base methods for instance validation on both
fixed and floating ip.

After the init_instance_validation method, call to the
create_test_server method will help in setting up an
ssh able server. If the init_instance_validation is not
called the create_test_server, behaves as before.

Now just the test_rebuild_server and test_change_server_password
will run with instance validation as a demo.

Change-Id: I499a11a128b1caa0f92f2392e2f7a511583bf3c9
Implements: blueprint ssh-auth-strategy
",git fetch https://review.opendev.org/openstack/tempest refs/changes/34/81834/13 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/compute/servers/test_server_actions.py', 'tempest/api/compute/base.py']",2,9446220a678eee14e8d833807d9b06c6fd9ecc2b,bp/ssh-auth-strategy,"from tempest.common.utils.linux import remote_client def create_ssh_sec_group(cls): sec_cli = cls.os.security_groups_client s_name = data_utils.rand_name('securitygroup-') s_description = data_utils.rand_name('description-') _, secgrp = sec_cli.create_security_group(s_name, s_description) _, rule = sec_cli.create_security_group_rule(secgrp['id'], 'tcp', 22, 22) cls.security_groups.append(secgrp) return secgrp @classmethod def init_instance_validation(cls): cls.iv_data = {} cls.iv_data['ssh_user'] = CONF.compute.ssh_user cls.iv_data['secgroup'] = cls.create_ssh_sec_group() if CONF.compute.ssh_auth_method == 'keypair': k_name = data_utils.rand_name('keypair') _, cls.iv_data['keypair'] = cls.os.keypairs_client.create_keypair( k_name) if CONF.compute.ssh_auth_method == 'configured': cls.iv_data['password'] = CONF.compute.image_ssh_password floating_cli = cls.os.floating_ips_client if CONF.compute.ssh_connect_method == 'floating': _, cls.iv_data['floating_ip'] = floating_cli.create_floating_ip() cls.iv_data['floating_ip_associated'] = None @classmethod def destoy_instance_validation(cls): if hasattr(cls, 'iv_data'): if 'floating_ip' in cls.iv_data: floating_cli = cls.os.floating_ips_client fip_id = cls.iv_data['floating_ip']['id'] floating_cli.delete_floating_ip(fip_id) if 'keypair' in cls.iv_data: k_name = cls.iv_data['keypair']['name'] cls.os.keypairs_client.delete_keypair(k_name) @classmethod def get_remote_client_for_server(cls, server_id): remote_client_args = {} remote_client_args['username'] = cls.iv_data['ssh_user'] if CONF.compute.ssh_connect_method == 'floating': fip = cls.iv_data['floating_ip']['ip'] if cls.iv_data['floating_ip_associated'] != server_id: floating_cli = cls.os.floating_ips_client floating_cli.associate_floating_ip_to_server(fip, server_id) cls.iv_data['floating_ip_associated'] = server_id remote_client_args['server'] = fip else: # always getting fresh address info _, server = cls.os.servers_client.get_server(server_id) remote_client_args['server'] = server keypair = cls.iv_data.get('keypair', None) if keypair is not None: remote_client_args['pkey'] = keypair['private_key'] else: remote_client_args['password'] = cls.iv_data['password'] return remote_client.RemoteClient(**remote_client_args) @classmethod cls.destoy_instance_validation() if hasattr(cls, 'iv_data'): keypair = cls.iv_data.get('keypair', None) secgroup = cls.iv_data.get('secgroup', None) server_args = kwargs.copy() if 'key_name' not in server_args and keypair: server_args['key_name'] = keypair['name'] if 'security_groups' not in server_args and secgroup: server_args['security_groups'] = [secgroup] name, image_id, flavor, **server_args) if (hasattr(cls, 'iv_data') and CONF.compute.ssh_auth_method == 'adminpass'): cls.iv_data['password'] = body['adminPass']"," name, image_id, flavor, **kwargs)",91,22
openstack%2Ftempest~master~Ifec5ce4a97a0713bbace449d79d09dc1a418630f,openstack/tempest,master,Ifec5ce4a97a0713bbace449d79d09dc1a418630f,Use processutils and remove duplicate ping code,ABANDONED,2014-09-22 23:54:22.000000000,2014-12-05 15:04:54.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 4395}]","[{'number': 1, 'created': '2014-09-22 23:54:22.000000000', 'files': ['tempest/openstack/common/lockutils.py', 'tempest/openstack/common/fileutils.py', 'tempest/stress/actions/ssh_floating.py', 'tempest/scenario/manager.py', 'tempest/openstack/common/fixture/lockutils.py', 'tempest/openstack/common/fixture/config.py', 'tempest/common/utils/misc.py', 'tempest/openstack/common/versionutils.py', 'tempest/openstack/common/__init__.py', 'tempest/openstack/common/jsonutils.py', 'tempest/openstack/common/log.py', 'tempest/scenario/orchestration/test_server_cfn_init.py', 'tempest/openstack/common/processutils.py', 'tempest/openstack/common/strutils.py', 'tempest/openstack/common/excutils.py', 'tempest/openstack/common/fixture/mockpatch.py', 'tools/install_venv_common.py', 'tempest/openstack/common/fixture/logging.py', 'tempest/openstack/common/fixture/moxstubout.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/9467e5eee07c3c1ef4d2cb18dddc5a886b0f36f2', 'message': 'Use processutils and remove duplicate ping code\n\nSync from oslo-incubator: d09134a965e25a75fc20a883061d5303acbc5d14\n\nChange-Id: Ifec5ce4a97a0713bbace449d79d09dc1a418630f\n'}]",0,123285,9467e5eee07c3c1ef4d2cb18dddc5a886b0f36f2,5,3,1,4395,,,0,"Use processutils and remove duplicate ping code

Sync from oslo-incubator: d09134a965e25a75fc20a883061d5303acbc5d14

Change-Id: Ifec5ce4a97a0713bbace449d79d09dc1a418630f
",git fetch https://review.opendev.org/openstack/tempest refs/changes/85/123285/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/openstack/common/lockutils.py', 'tempest/openstack/common/fileutils.py', 'tempest/stress/actions/ssh_floating.py', 'tempest/scenario/manager.py', 'tempest/openstack/common/fixture/lockutils.py', 'tempest/openstack/common/fixture/config.py', 'tempest/common/utils/misc.py', 'tempest/openstack/common/versionutils.py', 'tempest/openstack/common/__init__.py', 'tempest/openstack/common/jsonutils.py', 'tempest/openstack/common/log.py', 'tempest/scenario/orchestration/test_server_cfn_init.py', 'tempest/openstack/common/processutils.py', 'tempest/openstack/common/strutils.py', 'tempest/openstack/common/excutils.py', 'tempest/openstack/common/fixture/mockpatch.py', 'tools/install_venv_common.py', 'tempest/openstack/common/fixture/logging.py', 'tempest/openstack/common/fixture/moxstubout.py']",19,9467e5eee07c3c1ef4d2cb18dddc5a886b0f36f2,,"############################################################################## ############################################################################## # # DO NOT MODIFY THIS FILE # # This file is being graduated to the oslotest library. Please make all # changes there, and only backport critical fixes here. - dhellmann # ############################################################################## ############################################################################## from six.moves import mox",import mox,729,228
openstack%2Ftempest~master~Ia0c54caf72d9519a2605bf007aa89bc43c0c7134,openstack/tempest,master,Ia0c54caf72d9519a2605bf007aa89bc43c0c7134,Add stack-update arguments for patching/reset parameters,ABANDONED,2014-09-09 21:37:14.000000000,2014-12-05 15:04:53.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 8871}, {'_account_id': 9577}]","[{'number': 1, 'created': '2014-09-09 21:37:14.000000000', 'files': ['tempest/services/orchestration/json/orchestration_client.py', 'etc/tempest.conf.sample', 'tempest/config.py', 'tempest/api/orchestration/stacks/test_update.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/0b769217413aa1b73b58358580f40b02bbc600e5', 'message': 'Add stack-update arguments for patching/reset parameters\n\nTwo new arguments are added for Heat CLI stack-update\ncommand:\n1.Argument named existing will be supported by the heat\nengine to use the parameters stored in the Db.\n2.Argument named clear-parameter will be supported by\nthe heat engine to reset the value of the parameter back\nto the default value. This argument can be specified\nmultiple times.\n\nImplements: partial-blueprint troubleshooting-low-level-control\nPartial-Bug: 1224828\nChange-Id: Ia0c54caf72d9519a2605bf007aa89bc43c0c7134\n'}]",0,120247,0b769217413aa1b73b58358580f40b02bbc600e5,7,4,1,9577,,,0,"Add stack-update arguments for patching/reset parameters

Two new arguments are added for Heat CLI stack-update
command:
1.Argument named existing will be supported by the heat
engine to use the parameters stored in the Db.
2.Argument named clear-parameter will be supported by
the heat engine to reset the value of the parameter back
to the default value. This argument can be specified
multiple times.

Implements: partial-blueprint troubleshooting-low-level-control
Partial-Bug: 1224828
Change-Id: Ia0c54caf72d9519a2605bf007aa89bc43c0c7134
",git fetch https://review.opendev.org/openstack/tempest refs/changes/47/120247/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/services/orchestration/json/orchestration_client.py', 'etc/tempest.conf.sample', 'tempest/config.py', 'tempest/api/orchestration/stacks/test_update.py']",4,0b769217413aa1b73b58358580f40b02bbc600e5,bug/1224828,"import testtoolsfrom tempest import configCONF = config.CONF keypair_template = ''' heat_template_version: 2013-05-23 parameters: KeyPairName1: type: string default: testkey KeyPairName2: type: string default: testkey2 KeyPairName3: type: string default: testkey3 resources: KeyPair1: type: OS::Nova::KeyPair properties: name: { get_param: KeyPairName1 } KeyPair2: type: OS::Nova::KeyPair properties: name: { get_param: KeyPairName2 } KeyPair3: type: OS::Nova::KeyPair properties: name: { get_param: KeyPairName3 } ''' def update_stack(self, stack_identifier, template, parameters=None, existing=False, clear_parameters=None): template=template, parameters=parameters, existing=existing, clear_parameters=clear_parameters) @testtools.skipUnless(CONF.orchestration_feature_enabled. update_with_existing_or_reset_params, 'stack update with existing or reset ' + 'parameters are not available.') @test.attr(type='gate') def test_stack_update_with_existing_parameters(self): try: stack_name = data_utils.rand_name('heat') params = {'KeyPairName1': 'key1', 'KeyPairName2': 'key2', 'KeyPairName3': 'key3'} stack_identifier = self.create_stack(stack_name, self.keypair_template, params) self.client.wait_for_stack_status(stack_identifier, 'CREATE_COMPLETE') expected_resources = [('KeyPair1', 'key1'), ('KeyPair2', 'key2'), ('KeyPair3', 'key3')] for er_name, er_id in expected_resources: _, rsc = self.client.get_resource(stack_identifier, er_name) self.assertEqual(er_id, rsc.get('physical_resource_id')) # Update with existing_parameters flag only self.update_stack(stack_identifier, self.keypair_template, existing=True) # expect the physical resource id remain the same for er_name, er_id in expected_resources: _, rsc = self.client.get_resource(stack_identifier, er_name) self.assertEqual(er_id, rsc.get('physical_resource_id')) finally: # delete the stack self.client.delete_stack(stack_identifier) @testtools.skipUnless(CONF.orchestration_feature_enabled. update_with_existing_or_reset_params, 'stack update with existing or reset ' + 'parameters are not available.') @test.attr(type='gate') def test_stack_update_with_patched_existing_parameters(self): try: stack_name = data_utils.rand_name('heat') params = {'KeyPairName1': 'key1', 'KeyPairName2': 'key2', 'KeyPairName3': 'key3'} stack_identifier = self.create_stack(stack_name, self.keypair_template, params) self.client.wait_for_stack_status(stack_identifier, 'CREATE_COMPLETE') expected_resources = [('KeyPair1', 'key1'), ('KeyPair2', 'key2'), ('KeyPair3', 'key3')] for er_name, er_id in expected_resources: _, rsc = self.client.get_resource(stack_identifier, er_name) self.assertEqual(er_id, rsc.get('physical_resource_id')) # Update with existing_parameters flag set and # a list of parameters updated_params = {'KeyPairName2': 'updated_key2'} self.update_stack(stack_identifier, self.keypair_template, updated_params, existing=True) updated_resources = [('KeyPair1', 'key1'), ('KeyPair2', 'updated_key2'), ('KeyPair3', 'key3')] # expect the physical resource id changed for parameters in the # parameter list for er_name, er_id in updated_resources: _, rsc = self.client.get_resource(stack_identifier, er_name) self.assertEqual(er_id, rsc.get('physical_resource_id')) finally: # delete the stack self.client.delete_stack(stack_identifier) @testtools.skipUnless(CONF.orchestration_feature_enabled. update_with_existing_or_reset_params, 'stack update with existing or reset ' + 'parameters are not available.') @test.attr(type='gate') def test_stack_update_with_existing_and_default_parameters(self): try: stack_name = data_utils.rand_name('heat') params = {'KeyPairName1': 'key1', 'KeyPairName2': 'key2', 'KeyPairName3': 'key3'} stack_identifier = self.create_stack(stack_name, self.keypair_template, params) self.client.wait_for_stack_status(stack_identifier, 'CREATE_COMPLETE') expected_resources = [('KeyPair1', 'key1'), ('KeyPair2', 'key2'), ('KeyPair3', 'key3')] for er_name, er_id in expected_resources: _, rsc = self.client.get_resource(stack_identifier, er_name) self.assertEqual(er_id, rsc.get('physical_resource_id')) # Update with existing_parameters flag set and # a list of clear parameters clear_params = ['KeyPairName1'] self.update_stack(stack_identifier, self.keypair_template, existing=True, clear_parameters=clear_params) updated_resources = [('KeyPair1', 'testkey'), ('KeyPair2', 'key2'), ('KeyPair3', 'key3')] # expect the physical resource id set back to default value # for parameters in the clear list for er_name, er_id in updated_resources: _, rsc = self.client.get_resource(stack_identifier, er_name) self.assertEqual(er_id, rsc.get('physical_resource_id')) finally: # delete the stack self.client.delete_stack(stack_identifier) @testtools.skipUnless(CONF.orchestration_feature_enabled. update_with_existing_or_reset_params, 'stack update with existing or reset ' + 'parameters are not available.') @test.attr(type='gate') def test_stack_update_with_patched_and_default_parameters(self): try: stack_name = data_utils.rand_name('heat') params = {'KeyPairName1': 'key1', 'KeyPairName2': 'key2', 'KeyPairName3': 'key3'} stack_identifier = self.create_stack(stack_name, self.keypair_template, params) self.client.wait_for_stack_status(stack_identifier, 'CREATE_COMPLETE') expected_resources = [('KeyPair1', 'key1'), ('KeyPair2', 'key2'), ('KeyPair3', 'key3')] for er_name, er_id in expected_resources: _, rsc = self.client.get_resource(stack_identifier, er_name) self.assertEqual(er_id, rsc.get('physical_resource_id')) # Update with existing_parameters flag set, # a list of parameters and a list of clear parameters updated_params = {'KeyPairName2': 'updated_key2'} clear_params = ['KeyPairName3'] self.update_stack(stack_identifier, self.keypair_template, updated_params, existing=True, clear_parameters=clear_params) updated_resources = [('KeyPair1', 'key1'), ('KeyPair2', 'updated_key2'), ('KeyPair3', 'testkey3')] # expect the physical_ resource id remain the same for er_name, er_id in updated_resources: _, rsc = self.client.get_resource(stack_identifier, er_name) self.assertEqual(er_id, rsc.get('physical_resource_id')) finally: # delete the stack self.client.delete_stack(stack_identifier)"," def update_stack(self, stack_identifier, template): template=template)",233,6
openstack%2Ftempest~master~Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81,openstack/tempest,master,Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81,VPN/Firewall integration test scenario,ABANDONED,2014-07-11 19:17:11.000000000,2014-12-05 15:04:51.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 6659}, {'_account_id': 7227}, {'_account_id': 7293}, {'_account_id': 7350}, {'_account_id': 8576}, {'_account_id': 8871}, {'_account_id': 9008}, {'_account_id': 10068}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 11361}]","[{'number': 1, 'created': '2014-07-11 19:17:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/55697806079889a4ed5c6c1e64c818d33b22178c', 'message': ""VPN/Firewall integration test scenario\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}, {'number': 2, 'created': '2014-07-11 21:03:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/b95f5a88603463c143226cc45e7f607c49bd6c5b', 'message': ""VPN/Firewall integration test scenario\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}, {'number': 3, 'created': '2014-07-11 23:16:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/15503ff1035b1c0c053bd3198628293baf920a02', 'message': ""VPN/Firewall integration test scenario\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}, {'number': 4, 'created': '2014-07-12 06:20:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/449a1525142d6a5532fc3431ef3d20800b82c84a', 'message': ""VPN/Firewall integration test scenario\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}, {'number': 5, 'created': '2014-07-20 19:20:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/eb3cf5f3a5baf3518f00570e02b7895112f40da9', 'message': ""VPN/Firewall integration test scenario\n\nThere are three tests run as part of this test scenario:\ntest vpn between two sites, test firewall between two\nsites and test vpn+firewall between two sites.\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nIn the VPN only test, pings between the two sites are tested only.\n\nIn the Firewall only test, ping is tested between the sites'\ninstances on their floating ips.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}, {'number': 6, 'created': '2014-07-22 13:04:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/7690e85dfe1e1fc0c60f48c222724e6a10e31d1f', 'message': ""VPN/Firewall integration test scenario\n\nThere are three tests run as part of this test scenario:\ntest vpn between two sites, test firewall between two\nsites and test vpn+firewall between two sites.\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nIn the VPN only test, pings between the two sites are tested only.\n\nIn the Firewall only test, ping is tested between the sites'\ninstances on their floating ips.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}, {'number': 7, 'created': '2014-07-22 16:13:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/9210c13bccba908ee35f18dfd33c8e7c3502a802', 'message': ""VPN/Firewall integration test scenario\n\nThere are three tests run as part of this test scenario:\ntest vpn between two sites, test firewall between two\nsites and test vpn+firewall between two sites.\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nIn the VPN only test, pings between the two sites are tested only.\n\nIn the Firewall only test, ping is tested between the sites'\ninstances on their floating ips.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}, {'number': 8, 'created': '2014-07-23 16:22:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/7a2b0f01b4cd497f99f09fd202e58350882d447f', 'message': ""VPN/Firewall integration test scenario\n\nThere are three tests run as part of this test scenario:\ntest vpn between two sites, test firewall between two\nsites and test vpn+firewall between two sites.\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nIn the VPN only test, pings between the two sites are tested only.\n\nIn the Firewall only test, ping is tested between the sites'\ninstances on their floating ips.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}, {'number': 9, 'created': '2014-07-24 11:48:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/9f47fbb2ae10564eaad5f34e90e4be40d662e95d', 'message': ""VPN/Firewall integration test scenario\n\nThere are three tests run as part of this test scenario:\ntest vpn between two sites, test firewall between two\nsites and test vpn+firewall between two sites.\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nIn the VPN only test, pings between the two sites are tested only.\n\nIn the Firewall only test, ping is tested between the sites'\ninstances on their floating ips.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}, {'number': 10, 'created': '2014-07-27 08:16:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/2da44cdfa142e1e49330433dc615554875cd78c9', 'message': ""VPN/Firewall integration test scenario\n\nThere are three tests run as part of this test scenario:\ntest vpn between two sites, test firewall between two\nsites and test vpn+firewall between two sites.\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nIn the VPN only test, pings between the two sites are tested only.\n\nIn the Firewall only test, ping is tested between the sites'\ninstances on their floating ips.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}, {'number': 11, 'created': '2014-08-06 10:28:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/fe7ca1f949b6fa4859a308aa2b3fa4db577a9fd1', 'message': ""VPN/Firewall integration test scenario\n\nThere are three tests run as part of this test scenario:\ntest vpn between two sites, test firewall between two\nsites and test vpn+firewall between two sites.\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nIn the VPN only test, pings between the two sites are tested only.\n\nIn the Firewall only test, ping is tested between the sites'\ninstances on their floating ips.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}, {'number': 12, 'created': '2014-08-12 18:00:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/a248524e0a08ac00c96ae749bff9ca823406d0ce', 'message': ""VPN/Firewall integration test scenario\n\nThere are three tests run as part of this test scenario:\ntest vpn between two sites, test firewall between two\nsites and test vpn+firewall between two sites.\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nIn the VPN only test, pings between the two sites are tested only.\n\nIn the Firewall only test, ping is tested between the sites'\ninstances on their floating ips.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}, {'number': 13, 'created': '2014-08-12 19:51:58.000000000', 'files': ['tempest/scenario/test_network_vpn_firewall.py', 'tempest/scenario/manager.py', 'tempest/scenario/test_load_balancer_basic.py', 'tempest/api/network/common.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/0e8c86393d0d13c4671b34d6d2d56dc36d59ba5b', 'message': ""VPN/Firewall integration test scenario\n\nThere are three tests run as part of this test scenario:\ntest vpn between two sites, test firewall between two\nsites and test vpn+firewall between two sites.\n\nThis test scenario creates vpn connection between two sites\n(two separate tenants), then tests the connection between the\ntenants's private networks across the vpn using pings between\nvm instances running on the two tenant's networks. Then\nfirewalls are added with rules to deny icmp traffic. Connectivity\nis tested again between the VMs to make sure firewall rules\nwork.\n\nIn the VPN only test, pings between the two sites are tested only.\n\nIn the Firewall only test, ping is tested between the sites'\ninstances on their floating ips.\n\nSteps in this scenario:\n\n1. Two tenants are created with new networks/subnetworks which\nare added to the routers.\n2. VPN connection is estabilished between the two tenant's\nnetworks.\n3. VM instances are created under each tenant\n4. Ping is executed between the VMs on the two tenants. Ping should\nwork\n5. Firewall is created with rule to deny icmp\n6. Ping is executed between the VMs on the two tenants. Ping should\nfail\n\nChange-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81\n""}]",57,106473,0e8c86393d0d13c4671b34d6d2d56dc36d59ba5b,124,13,13,11361,,,0,"VPN/Firewall integration test scenario

There are three tests run as part of this test scenario:
test vpn between two sites, test firewall between two
sites and test vpn+firewall between two sites.

This test scenario creates vpn connection between two sites
(two separate tenants), then tests the connection between the
tenants's private networks across the vpn using pings between
vm instances running on the two tenant's networks. Then
firewalls are added with rules to deny icmp traffic. Connectivity
is tested again between the VMs to make sure firewall rules
work.

In the VPN only test, pings between the two sites are tested only.

In the Firewall only test, ping is tested between the sites'
instances on their floating ips.

Steps in this scenario:

1. Two tenants are created with new networks/subnetworks which
are added to the routers.
2. VPN connection is estabilished between the two tenant's
networks.
3. VM instances are created under each tenant
4. Ping is executed between the VMs on the two tenants. Ping should
work
5. Firewall is created with rule to deny icmp
6. Ping is executed between the VMs on the two tenants. Ping should
fail

Change-Id: Ic69f599a66ecb12b92eaa1aad3f1d36783d0ac81
",git fetch https://review.opendev.org/openstack/tempest refs/changes/73/106473/6 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/scenario/test_network_vpn_firewall.py', 'tempest/scenario/manager.py', 'tempest/api/network/common.py']",3,55697806079889a4ed5c6c1e64c818d33b22178c,vpn_firewall_scenario_test," class DeletableFirewallRule(DeletableResource): def delete(self): self.client.delete_firewall_rule(self.id) class DeletableFirewallPolicy(DeletableResource): def delete(self): self.client.delete_firewall_policy(self.id) def insert_rule(self, **kwargs): # body = dict(firewall_rule_id=firewall_rule_id,insert_after=insert_after,insert_before=insert_before) body = kwargs self.client.firewall_policy_insert_rule(self.id, body=body) def remove_rule(self, firewall_rule_id): body = dict(firewall_rule_id=firewall_rule_id) self.client.firewall_policy_insert_rule(self.id, body=body) class DeletableFirewall(DeletableResource): def delete(self): self.client.delete_firewall(self.id) def get(self): return self.client.show_firewall(self.id) class DeletableIkePolicy(DeletableResource): def delete(self): self.client.delete_ikepolicy(self.id) class DeletableIpsecPolicy(DeletableResource): def delete(self): self.client.delete_ipsecpolicy(self.id) class DeletableVPNService(DeletableResource): def delete(self): self.client.delete_vpnservice(self.id) class DeletableIpsecSiteConnection(DeletableResource): def delete(self): self.client.delete_ipsec_site_connection(self.id)",,469,0
openstack%2Ftempest~master~I1379e84751818d0f558f28ba7b0bc9b5e4c76053,openstack/tempest,master,I1379e84751818d0f558f28ba7b0bc9b5e4c76053,Baremetal nodestate APIs,ABANDONED,2014-04-22 04:44:33.000000000,2014-12-05 15:04:48.000000000,,"[{'_account_id': 3}, {'_account_id': 97}, {'_account_id': 2750}, {'_account_id': 5803}, {'_account_id': 7428}, {'_account_id': 7882}, {'_account_id': 8205}, {'_account_id': 8556}, {'_account_id': 8824}, {'_account_id': 8871}, {'_account_id': 10118}, {'_account_id': 10385}, {'_account_id': 10966}]","[{'number': 1, 'created': '2014-04-22 04:44:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f2b31e32a97656b7cf670f6ea5f4267745233d43', 'message': 'Baremetal node console mode set API test\n\nThis patch implements API test for missing baaremetal node API test -\nnode-set-console-mode.\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 2, 'created': '2014-04-22 06:29:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/8431392609cc2bf58e40a677149a3dc7904934c9', 'message': 'Baremetal node console mode set API test\n\nThis patch implements API test for missing baaremetal node API test -\nnode-set-console-mode.\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 3, 'created': '2014-04-22 09:36:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/afb6fc403f3bd08ee043dcb677c776feeb383019', 'message': 'Baremetal node console mode set API test\n\nThis patch implements API test for missing baaremetal node API test -\nnode-set-console-mode.\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 4, 'created': '2014-04-22 10:18:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/cec69d36fcadceb2fd24f62686b6a4c3fb877931', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 6, 'created': '2014-04-22 10:21:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/f0a3c1cec012f81309655c094c162ba8351db6af', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 5, 'created': '2014-04-22 10:21:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/5ecc6196d700c36db45fd04f972ca1c27c405d03', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 7, 'created': '2014-04-28 11:11:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/75aff59c254833cee9a8335c84fcb33f53651839', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 8, 'created': '2014-04-30 04:37:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/1c4642f3d27063603111ab2a224f2a62663930d1', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 9, 'created': '2014-04-30 04:42:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/26fcd34f02297f82ced739a37dab0010cb981148', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 10, 'created': '2014-04-30 07:27:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/abc15060c9f45d830d440e0e1f536c9973c07ec9', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 11, 'created': '2014-04-30 08:50:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/24b61168ea7eab2be243e8de87cbd731ee523b98', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 12, 'created': '2014-05-01 09:06:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/1853ee2cfffa877edffdf48b638ebfb5172b788b', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 13, 'created': '2014-06-03 11:23:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/c2a390fb0bd6b4d752a9bc11e65fff6c7065714b', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 14, 'created': '2014-06-03 11:34:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/fa136237724cc3c1a7c8d3f3388002e68d1e8c36', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 15, 'created': '2014-06-04 06:44:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/43acab7d1a73175d8394e0f3c2b85614d46c1cf6', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}, {'number': 16, 'created': '2014-06-23 04:21:49.000000000', 'files': ['tempest/services/baremetal/v1/base_v1.py', 'tempest/api/baremetal/test_nodestates.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/732e056e30f90c153782f227fa086c36b1508579', 'message': 'Baremetal nodestate APIs\n\nThis patch implements API test for missing baaremetal node API test -\n1. Console mode set of a given node\n2. Set provision state of a given node\n\nConsole mode can be set to either True or False.\n\nChange-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053\n'}]",22,89472,732e056e30f90c153782f227fa086c36b1508579,114,13,16,8205,,,0,"Baremetal nodestate APIs

This patch implements API test for missing baaremetal node API test -
1. Console mode set of a given node
2. Set provision state of a given node

Console mode can be set to either True or False.

Change-Id: I1379e84751818d0f558f28ba7b0bc9b5e4c76053
",git fetch https://review.opendev.org/openstack/tempest refs/changes/72/89472/16 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/api/baremetal/test_nodes.py', 'tempest/services/baremetal/v1/base_v1.py']",2,f2b31e32a97656b7cf670f6ea5f4267745233d43,bp/missing-baremetal-api-test," @base.handle_errors def set_node_console_mode(self, node_uuid, console_state): """""" Set console state of the specified node. :param node_uuid: The unique identifier of the node. :console_state: desired state to set (true/false). """""" target = {'enabled': console_state} return self._put_request('nodes/%s/states/console' % node_uuid, target)",,26,0
openstack%2Ftempest~master~Ie81620e06d77979751cccdc78d9290a5705bb074,openstack/tempest,master,Ie81620e06d77979751cccdc78d9290a5705bb074,Log output from ping command,ABANDONED,2014-09-23 17:56:41.000000000,2014-12-05 15:04:47.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 4395}, {'_account_id': 5196}, {'_account_id': 8576}]","[{'number': 1, 'created': '2014-09-23 17:56:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/813d5dc98c2dadea2a6a3f190ba305c09d7ce406', 'message': ""Log output from ping command\n\nToday we don't log this information which could be useful to see that this\nis actually occurring successfully.\n\nCloses-bug: 1373055\n\nChange-Id: Ie81620e06d77979751cccdc78d9290a5705bb074\n""}, {'number': 2, 'created': '2014-09-23 17:57:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/ef76ee89f94b301563dc716e37aaac8e61676821', 'message': ""Log output from ping command\n\nToday we don't log this information which could be useful to see that this\nis actually occurring successfully.\n\nCloses-bug: 1373055\n\nChange-Id: Ie81620e06d77979751cccdc78d9290a5705bb074\n""}, {'number': 3, 'created': '2014-09-24 23:22:17.000000000', 'files': ['tempest/scenario/manager.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/c38bd8477e513b23d9b61e02d78b6265b0d8a66b', 'message': ""Log output from ping command\n\nToday we don't log this information which could be useful to see that this\nis actually occurring successfully.\n\nCloses-bug: 1373055\n\nChange-Id: Ie81620e06d77979751cccdc78d9290a5705bb074\n""}]",0,123526,c38bd8477e513b23d9b61e02d78b6265b0d8a66b,19,5,3,4395,,,0,"Log output from ping command

Today we don't log this information which could be useful to see that this
is actually occurring successfully.

Closes-bug: 1373055

Change-Id: Ie81620e06d77979751cccdc78d9290a5705bb074
",git fetch https://review.opendev.org/openstack/tempest refs/changes/26/123526/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/manager.py'],1,813d5dc98c2dadea2a6a3f190ba305c09d7ce406,bug/1373055," out, err = proc.communicate() LOG.debug('Running command: %(cmd)s stdout: %(stdout)s ' 'stderr %(stderr)s' % {'cmd': cmd, 'stdout': out, 'stderr': err})", proc.communicate(),4,1
openstack%2Ftempest~master~I969bd149c2f5ba2e2c13ddb0b3087092596c131e,openstack/tempest,master,I969bd149c2f5ba2e2c13ddb0b3087092596c131e,Add a test for object PUT API with If-None-Match,ABANDONED,2014-08-25 04:28:21.000000000,2014-12-05 15:04:46.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 5196}, {'_account_id': 5689}, {'_account_id': 7350}, {'_account_id': 8859}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-08-25 04:28:21.000000000', 'files': ['tempest/api/object_storage/test_object_services.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/260997ad7d6f4d47df160caa5e0c501087977138', 'message': 'Add a test for object PUT API with If-None-Match\n\nFrom swift Icehouse version, object PUT API supports ""If-None-Match: *""\nheader. This improves effectiveness if users need to check if an object\nexists in requested URL.\n\nPartially implements blueprint add-icehouse-swift-tests\n\nChange-Id: I969bd149c2f5ba2e2c13ddb0b3087092596c131e\n'}]",4,116548,260997ad7d6f4d47df160caa5e0c501087977138,11,7,1,8859,,,0,"Add a test for object PUT API with If-None-Match

From swift Icehouse version, object PUT API supports ""If-None-Match: *""
header. This improves effectiveness if users need to check if an object
exists in requested URL.

Partially implements blueprint add-icehouse-swift-tests

Change-Id: I969bd149c2f5ba2e2c13ddb0b3087092596c131e
",git fetch https://review.opendev.org/openstack/tempest refs/changes/48/116548/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/object_storage/test_object_services.py'],1,260997ad7d6f4d47df160caa5e0c501087977138,bp/add-icehouse-swift-tests," def test_create_object_with_if_none_match(self): # create object with ""If-None-Match: *"" to be accepted request only # when a target object does not exist object_name = data_utils.rand_name(name='TestObject') data = data_utils.arbitrary_string() metadata = {'If-None-Match': ""*""} resp, _ = self.object_client.create_object( self.container_name, object_name, data, metadata=metadata) self.assertEqual(resp['status'], '201') self.assertHeaders(resp, 'Object', 'PUT') # check uploaded content _, body = self.object_client.get_object(self.container_name, object_name) self.assertEqual(data, body) @test.attr(type='gate')",,20,0
openstack%2Ftempest~master~Ia909a39950397502cff0fbcbe7917a587849ea08,openstack/tempest,master,Ia909a39950397502cff0fbcbe7917a587849ea08,Remove duplicate call to clear_isolated_creds(),ABANDONED,2014-09-29 12:12:51.000000000,2014-12-05 15:04:45.000000000,,"[{'_account_id': 3}, {'_account_id': 1921}, {'_account_id': 2750}, {'_account_id': 9828}]","[{'number': 1, 'created': '2014-09-29 12:12:51.000000000', 'files': ['tempest/thirdparty/boto/test.py', 'tempest/api/volume/base.py', 'tempest/api/data_processing/base.py', 'tempest/api/network/base.py', 'tempest/api/telemetry/base.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/d207725f6f770e5672649cad5f689bd859d3d487', 'message': 'Remove duplicate call to clear_isolated_creds()\n\nMethod clear_isolated_creds() is called twice, first in specific test\nclass and then in the base class. This results in warnings about user\nor tenant not found for delete. The solution is to keep cleanup in\nbase class only.\n\nCloses bug 1375230\n\nChange-Id: Ia909a39950397502cff0fbcbe7917a587849ea08\n'}]",0,124721,d207725f6f770e5672649cad5f689bd859d3d487,6,4,1,5950,,,0,"Remove duplicate call to clear_isolated_creds()

Method clear_isolated_creds() is called twice, first in specific test
class and then in the base class. This results in warnings about user
or tenant not found for delete. The solution is to keep cleanup in
base class only.

Closes bug 1375230

Change-Id: Ia909a39950397502cff0fbcbe7917a587849ea08
",git fetch https://review.opendev.org/openstack/tempest refs/changes/21/124721/1 && git format-patch -1 --stdout FETCH_HEAD,"['tempest/thirdparty/boto/test.py', 'tempest/api/volume/base.py', 'tempest/api/data_processing/base.py', 'tempest/api/network/base.py', 'tempest/api/telemetry/base.py']",5,d207725f6f770e5672649cad5f689bd859d3d487,bug/1375230,, cls.clear_isolated_creds(),0,5
openstack%2Ftempest~master~Iebcc8aa806a693c55643752e4e4ff52c01ebfc6e,openstack/tempest,master,Iebcc8aa806a693c55643752e4e4ff52c01ebfc6e,See whats on the console before resize to see if it's different later,ABANDONED,2014-09-30 21:31:05.000000000,2014-12-05 15:04:44.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 4395}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-30 21:31:05.000000000', 'files': ['tempest/scenario/test_network_advanced_server_ops.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/8229f5cf3c512ac5debd5b49ab26474ab95652c5', 'message': ""See whats on the console before resize to see if it's different later\n\nChange-Id: Iebcc8aa806a693c55643752e4e4ff52c01ebfc6e\n""}]",0,125205,8229f5cf3c512ac5debd5b49ab26474ab95652c5,10,4,1,4395,,,0,"See whats on the console before resize to see if it's different later

Change-Id: Iebcc8aa806a693c55643752e4e4ff52c01ebfc6e
",git fetch https://review.opendev.org/openstack/tempest refs/changes/05/125205/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/test_network_advanced_server_ops.py'],1,8229f5cf3c512ac5debd5b49ab26474ab95652c5,, LOG.warn('CURRENT CONSOLE_LOG') self._log_console_output(),,2,0
openstack%2Ftempest~master~I64b1509a179eae40ea62e5f348e96c77abb2855e,openstack/tempest,master,I64b1509a179eae40ea62e5f348e96c77abb2855e,Test that we can ssh to instance in server_advanced_ops,ABANDONED,2014-09-30 23:56:42.000000000,2014-12-05 15:04:43.000000000,,"[{'_account_id': 3}, {'_account_id': 2750}, {'_account_id': 4395}, {'_account_id': 10385}]","[{'number': 1, 'created': '2014-09-30 23:56:42.000000000', 'files': ['tempest/scenario/test_server_advanced_ops.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/cbd6536ff1b2a1450e08cd892265a56a85a29c9d', 'message': 'Test that we can ssh to instance in server_advanced_ops\n\nChange-Id: I64b1509a179eae40ea62e5f348e96c77abb2855e\n'}]",0,125235,cbd6536ff1b2a1450e08cd892265a56a85a29c9d,8,4,1,4395,,,0,"Test that we can ssh to instance in server_advanced_ops

Change-Id: I64b1509a179eae40ea62e5f348e96c77abb2855e
",git fetch https://review.opendev.org/openstack/tempest refs/changes/35/125235/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/scenario/test_server_advanced_ops.py'],1,cbd6536ff1b2a1450e08cd892265a56a85a29c9d,," keypair = self.create_keypair() security_group = self._create_security_group() security_groups = [{'name': security_group['name']}] create_kwargs = { 'key_name': keypair['name'], 'security_groups': security_groups } instance = self.create_server(create_kwargs=create_kwargs) self._ssh_to_server(instance, keypair) self._ssh_to_server(instance, keypair) keypair = self.create_keypair() security_group = self._create_security_group() security_groups = [{'name': security_group['name']}] create_kwargs = { 'key_name': keypair['name'], 'security_groups': security_groups } instance = self.create_server(create_kwargs=create_kwargs) self._ssh_to_server(instance, keypair) self._ssh_to_server(instance, keypair) def _ssh_to_server(self, server, keypair): if CONF.compute.use_floatingip_for_ssh: _, floating_ip = self.floating_ips_client.create_floating_ip() self.addCleanup(self.delete_wrapper, self.floating_ips_client.delete_floating_ip, floating_ip['id']) self.floating_ips_client.associate_floating_ip_to_server( floating_ip['ip'], server['id']) ip = floating_ip['ip'] else: network_name_for_ssh = CONF.compute.network_for_ssh ip = server.networks[network_name_for_ssh][0] try: return self.get_remote_client( ip, private_key=keypair['private_key']) except Exception: LOG.exception('ssh to server failed') self._log_console_output(servers=[server]) raise", instance = self.create_server() instance = self.create_server(),42,2
