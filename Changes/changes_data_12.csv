id,project,branch,change_id,subject,status,created,updated,submitted,reviewers,revisions,total_comment_count,number,current_revision,discussion_messages_count,reviewers_count,revisions_count,owner_account_id,owner_name,owner_username,is_owner_bot,commit_message,git_command,changed_files,files_count,commit_id,topic,added_lines,deleted_lines,insertions,deletions
openstack%2Fironic~master~Ie1e2a4150d4ee4521290737612780c02506f4a9e,openstack/ironic,master,Ie1e2a4150d4ee4521290737612780c02506f4a9e,Add DB API for Firmware and Object,MERGED,2023-05-12 12:22:09.000000000,2023-07-11 03:19:20.000000000,2023-07-11 03:17:47.000000000,"[{'_account_id': 10239}, {'_account_id': 10342}, {'_account_id': 11655}, {'_account_id': 22348}, {'_account_id': 23851}]","[{'number': 1, 'created': '2023-05-12 12:22:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3a5ea05031be16c9dc9285085740101593a8d81e', 'message': 'Add DB API for Firmware\n\nAdds the following methods to DB API:\n\n* create_firmware_component_list\n* update_firmware_component_list\n* delete_firmware_component_list\n* get_firmware_component\n* get_firmware_component_list\n\nAdds three exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n* FirmwareComponentListNotFound\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 2, 'created': '2023-05-13 14:23:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c1b01d03a53e6790dabd144724c06b622a39c5b4', 'message': 'Add DB API for Firmware\n\nAdds the following methods to DB API:\n\n* create_firmware_component_list\n* update_firmware_component_list\n* delete_firmware_component_list\n* get_firmware_component\n* get_firmware_component_list\n\nAdds three exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n* FirmwareComponentListNotFound\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 3, 'created': '2023-05-23 21:41:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d22a312e38f68cec9d42913da18b26e76599ee29', 'message': 'Add DB API for Firmware\n\nAdds the following methods to DB API:\n\n* create_firmware_component_list\n* update_firmware_component_list\n* delete_firmware_component_list\n* get_firmware_component\n* get_firmware_component_list\n\nAdds three exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n* FirmwareComponentListNotFound\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 4, 'created': '2023-05-25 01:06:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/d0e53a2f956d9dafcfd66bc979a7beb43630ec80', 'message': 'Add DB API for Firmware\n\nAdds the following methods to DB API:\n\n* create_firmware_component_list\n* update_firmware_component_list\n* delete_firmware_component_list\n* get_firmware_component\n* get_firmware_component_list\n\nAdds three exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n* FirmwareComponentListNotFound\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 5, 'created': '2023-05-26 06:24:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9458bcbe81be57fb904ed8bb01b7b11f11879cf8', 'message': 'Add DB API for Firmware\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 6, 'created': '2023-05-26 13:33:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f4340b5c925ca78f282b8260a40fa56cf2930506', 'message': 'Add DB API for Firmware\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 7, 'created': '2023-05-31 17:51:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0e0bf9a4567f7a911ec890cc3a174a5d53c8186f', 'message': 'Add DB API for Firmware and Object\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nFirmwareComponent\n* create | save | get\n\nFirmwareComponentList\n* get_by_node id\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nTests for db and objects\n\nChanges were required in models, the class name should match the\nobject name we will create\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 8, 'created': '2023-06-01 14:06:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/dbe5f659dcb8c4069e3b614cc5f2e804791cc48a', 'message': 'Add DB API for Firmware and Object\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nFirmwareComponent\n* create | save | get\n\nFirmwareComponentList\n* get_by_node id\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nTests for db and objects\n\nChanges were required in models, the class name should match the\nobject name we will create\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 9, 'created': '2023-06-02 12:30:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cdb51038557afc61e88c89cee128d13f4d5aff16', 'message': 'Add DB API for Firmware and Object\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nFirmwareComponent\n* create | save | get\n\nFirmwareComponentList\n* get_by_node id\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nTests for db and objects\n\nChanges were required in models, the class name should match the\nobject name we will create\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 10, 'created': '2023-06-06 03:24:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b745b4c29d0275d9ea727f61b831b3ab17e400ec', 'message': 'Add DB API for Firmware and Object\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nFirmwareComponent\n* create | save | get\n\nFirmwareComponentList\n* get_by_node id | sync_firmware_components\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nTests for db and objects\n\nChanges were required in models, the class name should match the\nobject name we will create\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 11, 'created': '2023-06-06 03:41:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f97007efed75a27d38d03d3f74ba6e5db4e78238', 'message': 'Add DB API for Firmware and Object\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nFirmwareComponent\n* create | save | get\n\nFirmwareComponentList\n* get_by_node id | sync_firmware_components\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nTests for db and objects\n\nChanges were required in models, the class name should match the\nobject name we will create\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 12, 'created': '2023-06-06 23:50:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2ea2e636c5627dbb373bd442c16071ba249888af', 'message': 'Add DB API for Firmware and Object\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nFirmwareComponent\n* create | save | get\n\nFirmwareComponentList\n* get_by_node id | sync_firmware_components\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nTests for db and objects\n\nChanges were required in models, the class name should match the\nobject name we will create\n\nStory: 2010659\nTask: 47977\n\nDepends-On: https://review.opendev.org/c/openstack/ironic/+/885372\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 13, 'created': '2023-06-07 01:54:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/1d97f2a37d8a47e03891664dc179efcc8a1618e9', 'message': 'Add DB API for Firmware and Object\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nFirmwareComponent\n* create | save | get\n\nFirmwareComponentList\n* get_by_node id | sync_firmware_components\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nTests for db and objects\n\nChanges were required in models, the class name should match the\nobject name we will create\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 14, 'created': '2023-06-20 11:51:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/94213d470ea547c22b0cc8f0d77fca737d9c5398', 'message': 'Add DB API for Firmware and Object\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nFirmwareComponent\n* create | save | get\n\nFirmwareComponentList\n* get_by_node id | sync_firmware_components\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nTests for db and objects\n\nChanges were required in models, the class name should match the\nobject name we will create\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 15, 'created': '2023-06-20 13:50:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/2d22b6a089467f07f341c73c2d48fa3a39c0afba', 'message': 'Add DB API for Firmware and Object\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nFirmwareComponent\n* create | save | get\n\nFirmwareComponentList\n* get_by_node id | sync_firmware_components\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nTests for db and objects\n\nChanges were required in models, the class name should match the\nobject name we will create\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 16, 'created': '2023-06-21 17:45:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b01a285dee0565bbdea25740f6088eb5c192a752', 'message': 'Add DB API for Firmware and Object\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nFirmwareComponent\n* create | save | get\n\nFirmwareComponentList\n* get_by_node id | sync_firmware_components\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nTests for db and objects\n\nChanges were required in models, the class name should match the\nobject name we will create\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}, {'number': 17, 'created': '2023-06-28 17:05:30.000000000', 'files': ['ironic/tests/unit/common/test_release_mappings.py', 'ironic/tests/unit/objects/test_firmware.py', 'ironic/common/exception.py', 'ironic/objects/firmware.py', 'ironic/db/sqlalchemy/api.py', 'ironic/tests/unit/db/utils.py', 'ironic/tests/unit/db/test_nodes.py', 'ironic/common/release_mappings.py', 'ironic/tests/unit/db/test_firmware_component.py', 'ironic/tests/unit/objects/test_objects.py', 'ironic/db/sqlalchemy/models.py', 'ironic/tests/unit/db/sqlalchemy/test_migrations.py', 'ironic/db/api.py', 'ironic/objects/__init__.py', 'ironic/tests/unit/objects/test_node.py', 'ironic/tests/unit/objects/utils.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/dad6724292b57085a5234efba4441c754657e865', 'message': 'Add DB API for Firmware and Object\n\nAdds the following methods to DB API:\n\n* create_firmware_component\n* update_firmware_component\n* get_firmware_component\n* get_firmware_component_list\n\nFirmwareComponent\n* create | save | get\n\nFirmwareComponentList\n* get_by_node id | sync_firmware_components\n\nAdds two exceptions:\n\n* FirmwareComponentAlreadyExists\n* FirmwareComponentNotFound\n\nTests for db and objects\n\nChanges were required in models, the class name should match the\nobject name we will create\n\nStory: 2010659\nTask: 47977\n\nChange-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e\n'}]",101,883062,dad6724292b57085a5234efba4441c754657e865,161,5,17,15519,,,0,"Add DB API for Firmware and Object

Adds the following methods to DB API:

* create_firmware_component
* update_firmware_component
* get_firmware_component
* get_firmware_component_list

FirmwareComponent
* create | save | get

FirmwareComponentList
* get_by_node id | sync_firmware_components

Adds two exceptions:

* FirmwareComponentAlreadyExists
* FirmwareComponentNotFound

Tests for db and objects

Changes were required in models, the class name should match the
object name we will create

Story: 2010659
Task: 47977

Change-Id: Ie1e2a4150d4ee4521290737612780c02506f4a9e
",git fetch https://review.opendev.org/openstack/ironic refs/changes/62/883062/3 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/common/exception.py', 'ironic/db/sqlalchemy/api.py', 'ironic/tests/unit/db/utils.py', 'ironic/tests/unit/db/test_firmware_component.py', 'ironic/db/api.py']",5,3a5ea05031be16c9dc9285085740101593a8d81e,fw_db_api," @abc.abstractclassmethod def create_firmware_component_list(self, node_id, components, version): """"""Create a list of FirmwareComponent records for a given node. :param node_id: The node id. :param components: A list of Firmware Components to be created. :: [ { 'component': String, 'initial_version': String, 'current_version': String, 'last_version_flashed': String }, ... ] :param version: the version of the object.FirmwareComponent :returns: A list of FirmwareComponent object. :raises: NodeNotFound if the node is not found. :raises: FirmwareComponentAlreadyExists if any of the component records already exists. """""" @abc.abstractclassmethod def update_firmware_component_list(self, node_id, components, version): """"""Update a list of FirmwareComponent records. :param node_id: The node id. :param components: A list of Firmware Components to be created. :: [ { 'component': String, 'initial_version': String, 'current_version': String, 'last_version_flashed': String }, ... ] :param version: the version of the object.FirmwareComponent :returns: A list of FirmwareComponent object. :raises: NodeNotFound if the node is not found. :raises: FirmwareComponentNotFound if any of the components is not found. """""" @abc.abstractclassmethod def delete_firmware_component_list(self, node_id, names): """"""Delete a list of Firmware Component :param node_id: The node id. :param names: List of firmware components names to be deleted. :raises: NodeNotFound if the node is not found. :raises: FirmwareComponentNotFound if any of the components is not found. """""" @abc.abstractmethod def get_firmware_component(self, node_id, name): """"""Retrieve Firmware Component. :param node_id: The node id. :param name: String containing name of Firmware component. :returns: The FirmwareComponent object. :raises: NodeNotFound if the node is not found. :raises: FirmwareComponentNotFound if the BIOS setting is not found. """""" @abc.abstractclassmethod def get_firmware_component_list(self, node_id): """"""TRetrieve Firmware Components of a given node. :param node_id: The node id. :returns: A list of FirmwareComponent objects. :raises: NodeNotFound if the node is not found. """"""",,359,0
openstack%2Fsahara-dashboard~master~I29227341b9b278d205fc9a27ba0296ed65482f2c,openstack/sahara-dashboard,master,I29227341b9b278d205fc9a27ba0296ed65482f2c,setup.cfg: Replace dashes with underscores,ABANDONED,2023-06-08 12:20:52.000000000,2023-07-11 03:07:37.000000000,,"[{'_account_id': 22348}, {'_account_id': 29313}]","[{'number': 1, 'created': '2023-06-08 12:20:52.000000000', 'files': ['setup.cfg'], 'web_link': 'https://opendev.org/openstack/sahara-dashboard/commit/ff9aeb711d16ab75faa082f1f4825dca89d24a18', 'message': ""setup.cfg: Replace dashes with underscores\n\nSetuptools v54.1.0 introduces a warning that the use of dash-separated\noptions in 'setup.cfg' will not be supported in a future version [1].\nGet ahead of the issue by replacing the dashes with underscores. Without\nthis, we see 'UserWarning' messages like the following on new enough\nversions of setuptools:\n\n  UserWarning: Usage of dash-separated 'description-file' will not be\n  supported in future versions. Please use the underscore name\n  'description_file' instead\n\n[1] https://github.com/pypa/setuptools/commit/a2e9ae4cb\n\nChange-Id: I29227341b9b278d205fc9a27ba0296ed65482f2c\n""}]",1,885446,ff9aeb711d16ab75faa082f1f4825dca89d24a18,4,2,1,29423,,,0,"setup.cfg: Replace dashes with underscores

Setuptools v54.1.0 introduces a warning that the use of dash-separated
options in 'setup.cfg' will not be supported in a future version [1].
Get ahead of the issue by replacing the dashes with underscores. Without
this, we see 'UserWarning' messages like the following on new enough
versions of setuptools:

  UserWarning: Usage of dash-separated 'description-file' will not be
  supported in future versions. Please use the underscore name
  'description_file' instead

[1] https://github.com/pypa/setuptools/commit/a2e9ae4cb

Change-Id: I29227341b9b278d205fc9a27ba0296ed65482f2c
",git fetch https://review.opendev.org/openstack/sahara-dashboard refs/changes/46/885446/1 && git format-patch -1 --stdout FETCH_HEAD,['setup.cfg'],1,ff9aeb711d16ab75faa082f1f4825dca89d24a18,,description_file =author_email = openstack-discuss@lists.openstack.org home_page = https://docs.openstack.org/sahara/latest/ python_requires = >=3.8,description-file =author-email = openstack-discuss@lists.openstack.org home-page = https://docs.openstack.org/sahara/latest/ python-requires = >=3.8,4,4
openstack%2Fneutron~master~Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec,openstack/neutron,master,Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec,Add meter bandwidth limit support,NEW,2022-10-09 01:50:35.000000000,2023-07-11 02:56:50.000000000,,"[{'_account_id': 8313}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-10-09 01:50:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/a4f64a346e2d6994760e6afbd5885ae820c2f930', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 2, 'created': '2022-10-09 06:57:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/679b0070365a41ab700ab6e49c290bf2cf844345', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 3, 'created': '2022-10-18 03:05:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/f50588ac5f611c9b125ab73c23c787b36efbf554', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 4, 'created': '2022-10-19 01:44:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/83c72e94ed1e5934e8172678ec565d228c5c3561', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 5, 'created': '2022-11-14 01:40:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/6ef3762662d8fb07a89e92d7ee5dfa8bc83689ec', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 6, 'created': '2022-11-14 03:40:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e322fb114e68275e3e0bdce3e15f9eaf45bb4810', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 7, 'created': '2022-11-14 06:47:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/aaaba2e02882ce6c7f5bb906954a2ba52f14fa75', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 8, 'created': '2022-11-15 02:21:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ee47b19d8138764a17f30d77f3042c4b13e56dda', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 9, 'created': '2022-11-16 01:33:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ceb18b263757ae8da461ad81d81e1a6faceffe8c', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 10, 'created': '2022-11-17 02:11:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/af2b952e821f03ba6d542ab106001e2b9be1442c', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 11, 'created': '2022-11-17 06:36:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ba5ff9b7855750e762d22284c7a5adcf486bfb6b', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 12, 'created': '2022-11-24 01:31:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/33c67dc4d2087fc6bd0e2685e71b244e38e1db1c', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 13, 'created': '2022-12-05 04:18:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/41d50350fbe5c1e9cfc5bbe4a197992523165e42', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 14, 'created': '2022-12-05 04:20:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c3087d9a07e7900d587f4694e420e807de252a6b', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 15, 'created': '2022-12-05 11:49:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/f8c0571a37400bdb71eb438538ca59c83101dd66', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 16, 'created': '2022-12-06 01:14:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/935e0419657b73f701280638e7c103f44cf04cc3', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 17, 'created': '2022-12-14 03:07:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ba3d77ee301690025e7a0fe5cb04010ad15972ec', 'message': 'Add meter bandwidth limit support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 18, 'created': '2022-12-14 03:09:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/5c0647843e81bf4babceb875538148641bf65f68', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n111\n'}, {'number': 19, 'created': '2022-12-14 03:09:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/6600b32db2257016292b5710165063b4263048a5', 'message': 'Add meter bandwidth bandwidth support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 20, 'created': '2022-12-16 00:55:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/33ba3e40acb3ecdb23b1bb39e5843800d617e1f9', 'message': 'Add meter bandwidth limit support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 21, 'created': '2022-12-21 02:05:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/5dc5dc1d121218a6dda5953f516cc3ebb4fcfc96', 'message': 'Add meter bandwidth limit support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 22, 'created': '2023-03-09 01:34:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/41c5945f1d2bd09c6f5f8e4befe5b6d0d869568f', 'message': 'Add meter bandwidth limit support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 23, 'created': '2023-04-11 01:52:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c3c54a76f4f13091cd01c2f94a2889a060a34239', 'message': 'Add meter bandwidth limit support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 24, 'created': '2023-06-01 07:14:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/d7a1874e6fc3ece8f6f154d0f44d66168ac021a2', 'message': 'Add meter bandwidth limit support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 25, 'created': '2023-07-05 09:51:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/13850abc1063650a4374be9ab72d046f75c68fbb', 'message': 'Add meter bandwidth limit support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}, {'number': 26, 'created': '2023-07-11 00:45:21.000000000', 'files': ['neutron/tests/fullstack/test_qos.py', 'neutron/plugins/ml2/drivers/openvswitch/agent/extension_drivers/qos_driver.py', 'neutron/tests/fullstack/resources/environment.py', 'neutron/tests/fullstack/resources/config.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/extension_drivers/test_qos_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/openflow/native/test_br_int.py', 'neutron/conf/plugins/ml2/drivers/ovs_conf.py', 'neutron/plugins/ml2/drivers/openvswitch/agent/openflow/native/br_int.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/22313f4655b49b7c37db9e9fd5df490d4f0cbff4', 'message': 'Add meter bandwidth limit support\n\nBecasue many tables are still goto table PACKET_RATE_LIMIT\nby default, in order to reduce the refator work, packet will\nstill go to packet rate limit table=58 first, then\nbandwidth rate limit table=59.\n\nDepends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4\n\nCloses-Bug: #1964342\nChange-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec\n'}]",19,860767,22313f4655b49b7c37db9e9fd5df490d4f0cbff4,109,4,26,9531,,,0,"Add meter bandwidth limit support

Becasue many tables are still goto table PACKET_RATE_LIMIT
by default, in order to reduce the refator work, packet will
still go to packet rate limit table=58 first, then
bandwidth rate limit table=59.

Depends-On: I87da4960a92bb807203534965bac2b0a42c5d4a4

Closes-Bug: #1964342
Change-Id: Ia7f842f7e2fb6b218f8b1dc930bf1450e6f810ec
",git fetch https://review.opendev.org/openstack/neutron refs/changes/67/860767/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ml2/drivers/openvswitch/agent/extension_drivers/qos_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/extension_drivers/test_qos_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/openflow/native/test_br_int.py', 'neutron/conf/plugins/ml2/drivers/ovs_conf.py', 'neutron/plugins/ml2/drivers/openvswitch/agent/openflow/native/br_int.py']",5,a4f64a346e2d6994760e6afbd5885ae820c2f930,bug/1964342,"from neutron.plugins.ml2.common import constants as comm_constsMETER_FLAG_PPS = comm_consts.METER_FLAG_PPS METER_FLAG_BPS = comm_consts.METER_FLAG_BPS # TODO(liuyulong): reuse neutron-lib if it released. # packet rate limit table PACKET_RATE_LIMIT = 58 # bandwidth rate limit table BANDWIDTH_RATE_LIMIT = 59 self.install_goto(dest_table_id=PACKET_RATE_LIMIT) self.install_goto(dest_table_id=BANDWIDTH_RATE_LIMIT, table_id=PACKET_RATE_LIMIT) table_id=BANDWIDTH_RATE_LIMIT) self.install_goto(dest_table_id=PACKET_RATE_LIMIT, self.install_goto(dest_table_id=PACKET_RATE_LIMIT, ofpp.OFPInstructionGotoTable(table_id=PACKET_RATE_LIMIT), ofpp.OFPInstructionGotoTable(table_id=PACKET_RATE_LIMIT), dest_table_id=PACKET_RATE_LIMIT) def create_meter(self, meter_id, rate, burst=0, type_=METER_FLAG_PPS): if type_ == METER_FLAG_PPS: if burst != 0: flags = ofp.OFPMF_PKTPS | ofp.OFPMF_BURST else: flags = ofp.OFPMF_PKTPS elif type_ == METER_FLAG_BPS: if burst != 0: flags = ofp.OFPMF_KBPS | ofp.OFPMF_BURST else: flags = ofp.OFPMF_KBPS else: return flags=flags, meter_id=meter_id, def update_meter(self, meter_id, rate, burst=0, type_=METER_FLAG_PPS): if type_ == METER_FLAG_PPS: if burst != 0: flags = ofp.OFPMF_PKTPS | ofp.OFPMF_BURST else: flags = ofp.OFPMF_PKTPS elif type_ == METER_FLAG_BPS: if burst != 0: flags = ofp.OFPMF_KBPS | ofp.OFPMF_BURST else: flags = ofp.OFPMF_KBPS else: return flags=flags, meter_id=meter_id, in_port=None, local_vlan=None, type_=METER_FLAG_PPS): if type_ == METER_FLAG_PPS: table_id = PACKET_RATE_LIMIT dest_table = BANDWIDTH_RATE_LIMIT elif type_ == METER_FLAG_BPS: table_id = BANDWIDTH_RATE_LIMIT dest_table = constants.TRANSIENT_TABLE else: return ofpp.OFPInstructionGotoTable(table_id=dest_table)] self.install_instructions(table_id=table_id, in_port=None, local_vlan=None, type_=METER_FLAG_PPS): if type_ == METER_FLAG_PPS: table_id = PACKET_RATE_LIMIT elif type_ == METER_FLAG_BPS: table_id = BANDWIDTH_RATE_LIMIT else: return self.uninstall_flows(table_id=table_id, self.install_goto(dest_table_id=PACKET_RATE_LIMIT,"," self.install_goto(dest_table_id=constants.PACKET_RATE_LIMIT) table_id=constants.PACKET_RATE_LIMIT) self.install_goto(dest_table_id=constants.PACKET_RATE_LIMIT, self.install_goto(dest_table_id=constants.PACKET_RATE_LIMIT, ofpp.OFPInstructionGotoTable(table_id=constants.PACKET_RATE_LIMIT), ofpp.OFPInstructionGotoTable(table_id=constants.PACKET_RATE_LIMIT), dest_table_id=constants.PACKET_RATE_LIMIT) def create_meter(self, meter_id, rate, burst=0): flags=ofp.OFPMF_PKTPS, meter_id=meter_id, def update_meter(self, meter_id, rate, burst=0): flags=ofp.OFPMF_PKTPS, meter_id=meter_id, in_port=None, local_vlan=None): ofpp.OFPInstructionGotoTable(table_id=constants.TRANSIENT_TABLE)] self.install_instructions(table_id=constants.PACKET_RATE_LIMIT, in_port=None, local_vlan=None): self.uninstall_flows(table_id=constants.PACKET_RATE_LIMIT, self.install_goto(dest_table_id=constants.PACKET_RATE_LIMIT,",237,42
openstack%2Fneutron~master~I4fe5e123662d13e74a7bdee39c1fc40c606c4de2,openstack/neutron,master,I4fe5e123662d13e74a7bdee39c1fc40c606c4de2,Add metadata path extension openflows,NEW,2023-07-11 00:47:34.000000000,2023-07-11 02:46:06.000000000,,"[{'_account_id': 9845}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-11 00:47:34.000000000', 'files': ['neutron/agent/l2/extensions/metadata/metadata_flows_process.py', 'neutron/tests/unit/agent/l2/extensions/metadata/test_metadata_path.py', 'neutron/tests/unit/agent/l2/extensions/metadata/test_metadata_flows_process.py', 'neutron/agent/l2/extensions/metadata/metadata_path.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/openflow/native/ovs_bridge_test_base.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/75fba3f59a3a417b193cb3746a51fe191934fefe', 'message': 'Add metadata path extension openflows\n\nThis patch adds the openflows for for Neutron\nopenvswitch metadata datapath agent extension.\n\nThe extension will do the following works when\nhandle port:\n1. create related flows for this port\n\nThe extension will do the following works when\ndelete port:\n1. deleting port metadata path related flows\n\nPartially-Implements: blueprint distributed-metadata-datapath\nChange-Id: I4fe5e123662d13e74a7bdee39c1fc40c606c4de2\n'}]",0,888097,75fba3f59a3a417b193cb3746a51fe191934fefe,4,2,1,9531,,,0,"Add metadata path extension openflows

This patch adds the openflows for for Neutron
openvswitch metadata datapath agent extension.

The extension will do the following works when
handle port:
1. create related flows for this port

The extension will do the following works when
delete port:
1. deleting port metadata path related flows

Partially-Implements: blueprint distributed-metadata-datapath
Change-Id: I4fe5e123662d13e74a7bdee39c1fc40c606c4de2
",git fetch https://review.opendev.org/openstack/neutron refs/changes/97/888097/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/agent/l2/extensions/metadata/metadata_flows_process.py', 'neutron/tests/unit/agent/l2/extensions/metadata/test_metadata_path.py', 'neutron/tests/unit/agent/l2/extensions/metadata/test_metadata_flows_process.py', 'neutron/agent/l2/extensions/metadata/metadata_path.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/openflow/native/ovs_bridge_test_base.py']",5,75fba3f59a3a417b193cb3746a51fe191934fefe,distributed_metadata_data_path,"class OVSBridgeTestMixin(ovs_test_base.OVSOSKenTestBase): def mock_bridge_cls(self, name, cls): br = cls(name) self.stamp = br.default_cookie mock.patch.object(br, '_get_dp', autospec=True, mock__send_msg = mock.patch.object(br, '_send_msg').start() mock_delete_flows = mock.patch.object(br, return br class OVSBridgeTestBase(OVSBridgeTestMixin): def setup_bridge_mock(self, name, cls): self.br = self.mock_bridge_cls(name, cls) ","class OVSBridgeTestBase(ovs_test_base.OVSOSKenTestBase): def setup_bridge_mock(self, name, cls): self.br = cls(name) self.stamp = self.br.default_cookie mock.patch.object(self.br, '_get_dp', autospec=True, mock__send_msg = mock.patch.object(self.br, '_send_msg').start() mock_delete_flows = mock.patch.object(self.br,",1076,14
openstack%2Fnova~stable%2Fyoga~I4ab97626c10052c7af9934a80ff8db9ddab82738,openstack/nova,stable/yoga,I4ab97626c10052c7af9934a80ff8db9ddab82738,Enforce quota usage from placement when unshelving,NEW,2023-07-11 00:38:06.000000000,2023-07-11 02:37:38.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-11 00:38:06.000000000', 'files': ['releasenotes/notes/quota-unshelve-offloaded-e4ea2d6a1449f549.yaml', 'nova/api/openstack/compute/shelve.py', 'nova/tests/unit/conductor/test_conductor.py', 'nova/limit/placement.py', 'nova/conductor/manager.py', 'nova/tests/functional/test_servers.py', 'nova/compute/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/da4b7d1e70486402635a009724b263981cd22861', 'message': 'Enforce quota usage from placement when unshelving\n\nWhen [quota]count_usage_from_placement = true or\n[quota]driver = nova.quota.UnifiedLimitsDriver, cores and ram quota\nusage are counted from placement. When an instance is SHELVED_OFFLOADED,\nit will not have allocations in placement, so its cores and ram should\nnot count against quota during that time.\n\nThis means however that when an instance is unshelved, there is a\npossibility of going over quota if the cores and ram it needs were\nallocated by some other instance(s) while it was SHELVED_OFFLOADED.\n\nThis fixes a bug where quota was not being properly enforced during\nunshelve of a SHELVED_OFFLOADED instance when quota usage is counted\nfrom placement. Test coverage is also added for the ""recheck"" quota\ncases.\n\nConflicts:\n    nova/compute/api.py\n\nNOTE(melwitt): The conflict is because change\nIeb4766fdd88c469574fad823e05fe401537cdc30 (Allow unshelve to a specific\nhost (Compute API part)) is not in Yoga.\n\nCloses-Bug: #2003991\n\nChange-Id: I4ab97626c10052c7af9934a80ff8db9ddab82738\n(cherry picked from commit 6f79d6321e7c3edaab2eb911198b7b7f851371b3)\n(cherry picked from commit c67e69c0e35d837fcefd7e4ea010a956569c3a55)\n(cherry picked from commit 490d5de6bb9ef3123cbbcea50e08ad0ad2edcbbe)\n'}]",0,888096,da4b7d1e70486402635a009724b263981cd22861,3,1,1,4690,,,0,"Enforce quota usage from placement when unshelving

When [quota]count_usage_from_placement = true or
[quota]driver = nova.quota.UnifiedLimitsDriver, cores and ram quota
usage are counted from placement. When an instance is SHELVED_OFFLOADED,
it will not have allocations in placement, so its cores and ram should
not count against quota during that time.

This means however that when an instance is unshelved, there is a
possibility of going over quota if the cores and ram it needs were
allocated by some other instance(s) while it was SHELVED_OFFLOADED.

This fixes a bug where quota was not being properly enforced during
unshelve of a SHELVED_OFFLOADED instance when quota usage is counted
from placement. Test coverage is also added for the ""recheck"" quota
cases.

Conflicts:
    nova/compute/api.py

NOTE(melwitt): The conflict is because change
Ieb4766fdd88c469574fad823e05fe401537cdc30 (Allow unshelve to a specific
host (Compute API part)) is not in Yoga.

Closes-Bug: #2003991

Change-Id: I4ab97626c10052c7af9934a80ff8db9ddab82738
(cherry picked from commit 6f79d6321e7c3edaab2eb911198b7b7f851371b3)
(cherry picked from commit c67e69c0e35d837fcefd7e4ea010a956569c3a55)
(cherry picked from commit 490d5de6bb9ef3123cbbcea50e08ad0ad2edcbbe)
",git fetch https://review.opendev.org/openstack/nova refs/changes/96/888096/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/quota-unshelve-offloaded-e4ea2d6a1449f549.yaml', 'nova/api/openstack/compute/shelve.py', 'nova/limit/placement.py', 'nova/tests/unit/conductor/test_conductor.py', 'nova/conductor/manager.py', 'nova/tests/functional/test_servers.py', 'nova/compute/api.py']",7,da4b7d1e70486402635a009724b263981cd22861,bug/2003991,"from nova.limit import utils as limit_utils @staticmethod def _check_quota_unshelve_offloaded( context: nova_context.RequestContext, instance: 'objects.Instance', request_spec: 'objects.RequestSpec' ): if not (CONF.quota.count_usage_from_placement or limit_utils.use_unified_limits()): return # TODO(melwitt): This is ugly but we have to do it this way because # instances quota is currently counted from the API database but cores # and ram are counted from placement. That means while an instance is # SHELVED_OFFLOADED, it will still consume instances quota but it will # not consume cores and ram. So we need an instances delta of # 0 but cores and ram deltas from the flavor. # Once instances usage is also being counted from placement, we can # replace this method with a normal check_num_instances_quota() call. vcpus = instance.flavor.vcpus memory_mb = instance.flavor.memory_mb # We are not looking to create a new server, we are unshelving an # existing one. deltas = {'instances': 0, 'cores': vcpus, 'ram': memory_mb} objects.Quotas.check_deltas( context, deltas, context.project_id, user_id=context.user_id, check_project_id=instance.project_id, check_user_id=instance.user_id, ) # Do the same for unified limits. placement_limits.enforce_num_instances_and_flavor( context, context.project_id, instance.flavor, request_spec.is_bfv, 0, 0, delta_updates={'servers': 0}) # Check quota before we save any changes to the database, but only if # we are counting quota usage from placement. When an instance is # SHELVED_OFFLOADED, it will not consume cores or ram resources in # placement. This means it is possible that an unshelve would cause the # project/user to go over quota. if instance.vm_state == vm_states.SHELVED_OFFLOADED: self._check_quota_unshelve_offloaded( context, instance, request_spec) ",,265,28
openstack%2Ftripleo-common~stable%2Fwallaby~I1381b4f7f4f7808d29e7f1aa73e7c0cd06acc795,openstack/tripleo-common,stable/wallaby,I1381b4f7f4f7808d29e7f1aa73e7c0cd06acc795,Fix unassigned new_manifest_type variable,MERGED,2023-07-10 08:56:35.000000000,2023-07-11 02:37:15.000000000,2023-07-11 02:35:45.000000000,"[{'_account_id': 7144}, {'_account_id': 9816}, {'_account_id': 11166}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-10 08:56:35.000000000', 'files': ['tripleo_common/image/image_uploader.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/91f875acb7acc4ed3a94ee515dc685e40f36a943', 'message': 'Fix unassigned new_manifest_type variable\n\nThis fixes the regression caused by the recent change[1], and ensures\nthe new_manifest_type variable is defined in any code paths.\n\n[1] I04f6ac171b10af7a294819d6248eac641090cc49\n\nCloses-Bug: #2026711\nChange-Id: I1381b4f7f4f7808d29e7f1aa73e7c0cd06acc795\n'}]",0,888025,91f875acb7acc4ed3a94ee515dc685e40f36a943,9,4,1,9816,,,0,"Fix unassigned new_manifest_type variable

This fixes the regression caused by the recent change[1], and ensures
the new_manifest_type variable is defined in any code paths.

[1] I04f6ac171b10af7a294819d6248eac641090cc49

Closes-Bug: #2026711
Change-Id: I1381b4f7f4f7808d29e7f1aa73e7c0cd06acc795
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/25/888025/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_common/image/image_uploader.py'],1,91f875acb7acc4ed3a94ee515dc685e40f36a943,bug/2026711, new_manifest_type = manifest_type,,1,0
openstack%2Fironic~master~Iefc044c31ef029e400a7dad294504175a4462638,openstack/ironic,master,Iefc044c31ef029e400a7dad294504175a4462638,Unit tests: Isolate mysql test migrations,MERGED,2023-06-29 16:16:17.000000000,2023-07-11 01:15:08.000000000,2023-07-11 01:13:06.000000000,"[{'_account_id': 10342}, {'_account_id': 15519}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 16:16:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/cd23265fa8ea4f94e121df7a6e08d0507d20ae4c', 'message': ""Unit tests: Isolate mysql test migrations\n\nAll database migration testing in opestack is done through\nan opportunistic worker model, where if the database is available\nand correctly configured for testing, i.e. openstack-citest user\nand access appropriately granted, then the tests will create and\ntest migrations.\n\nHowever, this has been problematic with mysql as of recent, as we\nhave seen a long standing migration issue boil to the surface often\nwith tests.\n\nAs a result, we're isolating that test down to it's own job so we\ncan limit the blast damage. This also helps us isolate is it all\nof the tests, or is it just soley isolated down to the mysql test\nrun class, which is an additional data point.\n\nBy default, we continue to run Postgres migration tests in the\nmain jobs, as they haven't been impacted by this issue.\n\nChange-Id: Iefc044c31ef029e400a7dad294504175a4462638\n""}, {'number': 2, 'created': '2023-06-29 16:23:40.000000000', 'files': ['zuul.d/ironic-jobs.yaml', 'zuul.d/project.yaml', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/ironic/commit/5cad8ac7736618a875d9e6d6dd29d8db5f81fcd8', 'message': ""Unit tests: Isolate mysql test migrations\n\nAll database migration testing in opestack is done through\nan opportunistic worker model, where if the database is available\nand correctly configured for testing, i.e. openstack-citest user\nand access appropriately granted, then the tests will create and\ntest migrations.\n\nHowever, this has been problematic with mysql as of recent, as we\nhave seen a long standing migration issue boil to the surface often\nwith tests.\n\nAs a result, we're isolating that test down to it's own job so we\ncan limit the blast damage. This also helps us isolate is it all\nof the tests, or is it just soley isolated down to the mysql test\nrun class, which is an additional data point.\n\nBy default, we continue to run Postgres migration tests in the\nmain jobs, as they haven't been impacted by this issue.\n\nChange-Id: Iefc044c31ef029e400a7dad294504175a4462638\n""}]",8,887297,5cad8ac7736618a875d9e6d6dd29d8db5f81fcd8,29,3,2,11655,,,0,"Unit tests: Isolate mysql test migrations

All database migration testing in opestack is done through
an opportunistic worker model, where if the database is available
and correctly configured for testing, i.e. openstack-citest user
and access appropriately granted, then the tests will create and
test migrations.

However, this has been problematic with mysql as of recent, as we
have seen a long standing migration issue boil to the surface often
with tests.

As a result, we're isolating that test down to it's own job so we
can limit the blast damage. This also helps us isolate is it all
of the tests, or is it just soley isolated down to the mysql test
run class, which is an additional data point.

By default, we continue to run Postgres migration tests in the
main jobs, as they haven't been impacted by this issue.

Change-Id: Iefc044c31ef029e400a7dad294504175a4462638
",git fetch https://review.opendev.org/openstack/ironic refs/changes/97/887297/1 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/ironic-jobs.yaml', 'zuul.d/project.yaml', 'tox.ini']",3,cd23265fa8ea4f94e121df7a6e08d0507d20ae4c,, stestr run --slowest --parallel-class --exclude-regex TestMigrationsMySQL {posargs}[testenv:mysql-migrations] sitepackages = False commands = stestr run --slowest --parallel-class TestMigrationsMySQL {posargs} , stestr run --slowest --parallel-class {posargs},16,1
openstack%2Fnova~stable%2Fyoga~Icc9b6366aebba2f8468e2127da7b7e099098513a,openstack/nova,stable/yoga,Icc9b6366aebba2f8468e2127da7b7e099098513a,Reproducer for bug 2003991 unshelving offloaded instance,NEW,2023-07-11 00:38:06.000000000,2023-07-11 01:07:10.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-11 00:38:06.000000000', 'files': ['nova/tests/functional/test_servers.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/291c02a7d51585551266fe5b7e7afd9726317671', 'message': 'Reproducer for bug 2003991 unshelving offloaded instance\n\nThis adds test coverage for:\n\n  * Shelve/unshelve offloaded with legacy quota usage\n  * Shelve/unshelve offloaded with quota usage from placement\n  * Shelve/unshelve offloaded with unified limits\n  * Shelve/unshelve with legacy quota usage\n  * Shelve/unshelve with quota usage from placement\n  * Shelve/unshelve with unified limits\n\nRelated-Bug: #2003991\n\nChange-Id: Icc9b6366aebba2f8468e2127da7b7e099098513a\n(cherry picked from commit 427b2cb4d61cdfaf18b2467eb50b3772dffd3def)\n(cherry picked from commit 004a773a3a286f39889519bb5b2009fb9bf44fb1)\n(cherry picked from commit 710116f4beadb1553c9ad7991ea69b0e286657d6)\n'}]",0,888095,291c02a7d51585551266fe5b7e7afd9726317671,3,1,1,4690,,,0,"Reproducer for bug 2003991 unshelving offloaded instance

This adds test coverage for:

  * Shelve/unshelve offloaded with legacy quota usage
  * Shelve/unshelve offloaded with quota usage from placement
  * Shelve/unshelve offloaded with unified limits
  * Shelve/unshelve with legacy quota usage
  * Shelve/unshelve with quota usage from placement
  * Shelve/unshelve with unified limits

Related-Bug: #2003991

Change-Id: Icc9b6366aebba2f8468e2127da7b7e099098513a
(cherry picked from commit 427b2cb4d61cdfaf18b2467eb50b3772dffd3def)
(cherry picked from commit 004a773a3a286f39889519bb5b2009fb9bf44fb1)
(cherry picked from commit 710116f4beadb1553c9ad7991ea69b0e286657d6)
",git fetch https://review.opendev.org/openstack/nova refs/changes/95/888095/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/functional/test_servers.py'],1,291c02a7d51585551266fe5b7e7afd9726317671,bug/2003991,"from oslo_limit import fixture as limit_fixture def test_unshelve_offloaded_overquota(self): # Use a quota limit of 3 vcpus. self.flags(cores=3, group='quota') # Use flavor that has vcpus = 1. for i in range(0, 3): server = self._create_server(flavor_id=1) # We should be at the quota limit now. Shelve an instance and wait for # it to become SHELVED_OFFLOADED. self._shelve_server(server, expected_state='SHELVED_OFFLOADED') # Try to boot another instance. It should fail because shelved # offloaded instances still consume quota. ex = self.assertRaises(client.OpenStackApiException, self._create_server, flavor_id=1) self.assertEqual(403, ex.response.status_code) # Unshelving the instance should also succeed. self._unshelve_server(server) def _test_unshelve_offloaded_overquota_placement(self): # Use flavor that has vcpus = 1. for i in range(0, 3): server = self._create_server(flavor_id=1) # We should be at the quota limit now. Shelve an instance and wait for # it to become SHELVED_OFFLOADED. self._shelve_server(server, expected_state='SHELVED_OFFLOADED') # Try to boot another instance. It should succeed because with # placement, shelved offloaded instances do not consume cores/ram # quota. self._create_server(flavor_id=1) # FIXME(melwitt): This is bug #2003991, the unshelve is supposed to # fail if we would be over quota after unshelving. # Now try to unshelve the earlier instance. It should fail because it # would put us over quota to have 4 running instances. # ex = self.assertRaises(client.OpenStackApiException, # self._unshelve_server, # server) # self.assertEqual(403, ex.response.status_code) self._unshelve_server(server) def test_unshelve_offloaded_overquota_placement(self): # Count quota usage from placement. self.flags(count_usage_from_placement=True, group='quota') # Use a quota limit of 3 vcpus. self.flags(cores=3, group='quota') self._test_unshelve_offloaded_overquota_placement() def test_unshelve_offloaded_overquota_ul(self): self.flags(driver='nova.quota.UnifiedLimitsDriver', group='quota') limits = { 'servers': 5, 'class:VCPU': 3, 'class:MEMORY_MB': 2048, 'class:DISK_GB': 5 } self.useFixture(limit_fixture.LimitFixture(limits, {})) self._test_unshelve_offloaded_overquota_placement() def test_unshelve_overquota(self): # Test for behavior where the shelved instance is not offloaded. self.flags(shelved_offload_time=3600) # Use a quota limit of 3 vcpus. self.flags(cores=3, group='quota') # Use flavor that has vcpus = 1. for i in range(0, 3): server = self._create_server(flavor_id=1) # We should be at the quota limit now. Shelve an instance. self._shelve_server(server, expected_state='SHELVED') # Try to boot another instance. It should fail because shelved # instances still consume quota. ex = self.assertRaises(client.OpenStackApiException, self._create_server, flavor_id=1) self.assertEqual(403, ex.response.status_code) # Verify that it's still SHELVED. self._wait_for_state_change(server, 'SHELVED') # Unshelving the instance should also succeed. self._unshelve_server(server) def test_unshelve_overquota_placement(self): # Count quota usage from placement, should behave the same as legacy. self.flags(count_usage_from_placement=True, group='quota') self.test_unshelve_overquota() def test_unshelve_overquota_ul(self): self.flags(driver='nova.quota.UnifiedLimitsDriver', group='quota') limits = { 'servers': 5, 'class:VCPU': 3, 'class:MEMORY_MB': 2048, 'class:DISK_GB': 5 } self.useFixture(limit_fixture.LimitFixture(limits, {})) self.test_unshelve_overquota_placement() ",,107,0
openstack%2Fswift-bench~master~Ie56dc0cdcc56577570e13e48732d7d72c63820e4,openstack/swift-bench,master,Ie56dc0cdcc56577570e13e48732d7d72c63820e4,Switch from optparse to argparse,NEW,2023-02-17 23:46:49.000000000,2023-07-10 22:51:19.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-02-17 23:46:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift-bench/commit/6bce89964bbb6362009aabb76bf1831152c87cca', 'message': 'Switch from optparse to argparse\n\nChange-Id: Ie56dc0cdcc56577570e13e48732d7d72c63820e4\n'}, {'number': 2, 'created': '2023-07-10 22:42:47.000000000', 'files': ['swiftbench/cli/__init__.py', 'tests/test_utils.py', 'tests/test_cli.py', 'swiftbench/utils.py'], 'web_link': 'https://opendev.org/openstack/swift-bench/commit/a395f374897dd5af72a83fe1f5c92900331d67fb', 'message': 'Switch from optparse to argparse\n\nChange-Id: Ie56dc0cdcc56577570e13e48732d7d72c63820e4\n'}]",0,874341,a395f374897dd5af72a83fe1f5c92900331d67fb,4,1,2,15343,,,0,"Switch from optparse to argparse

Change-Id: Ie56dc0cdcc56577570e13e48732d7d72c63820e4
",git fetch https://review.opendev.org/openstack/swift-bench refs/changes/41/874341/2 && git format-patch -1 --stdout FETCH_HEAD,"['swiftbench/cli/__init__.py', 'tests/test_utils.py', 'tests/test_cli.py', 'swiftbench/utils.py']",4,6bce89964bbb6362009aabb76bf1831152c87cca,," if isinstance(value, int): return value",,119,113
openstack%2Fswift-bench~master~Iedfb1bc5fd444109f9239eb7aeb57983883acf0f,openstack/swift-bench,master,Iedfb1bc5fd444109f9239eb7aeb57983883acf0f,Fix SyntaxWarning,MERGED,2023-07-10 21:35:46.000000000,2023-07-10 22:22:00.000000000,2023-07-10 22:22:00.000000000,"[{'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-10 21:35:46.000000000', 'files': ['bin/swift-bench'], 'web_link': 'https://opendev.org/openstack/swift-bench/commit/427a16cb085a1b2a5437e0c4a202e0e6070564a6', 'message': 'Fix SyntaxWarning\n\nSyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?\nChange-Id: Iedfb1bc5fd444109f9239eb7aeb57983883acf0f\n'}]",0,888069,427a16cb085a1b2a5437e0c4a202e0e6070564a6,6,2,1,15343,,,0,"Fix SyntaxWarning

SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?
Change-Id: Iedfb1bc5fd444109f9239eb7aeb57983883acf0f
",git fetch https://review.opendev.org/openstack/swift-bench refs/changes/69/888069/1 && git format-patch -1 --stdout FETCH_HEAD,['bin/swift-bench'],1,427a16cb085a1b2a5437e0c4a202e0e6070564a6,, if options.concurrency != '':, if options.concurrency is not '':,1,1
openstack%2Fopenstacksdk~master~Idedce3c0ae6f4239ff3c1df35dc1b9af0add0a58,openstack/openstacksdk,master,Idedce3c0ae6f4239ff3c1df35dc1b9af0add0a58,Support the API for managing external gateways,NEW,2023-07-07 12:40:46.000000000,2023-07-10 21:48:45.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 12:40:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/779e10c301bc8437b1714229a09043d739ead161', 'message': 'Support the API for managing external gateways\n\nRelevant Neutron core change:\nhttps://review.opendev.org/c/openstack/neutron/+/873593\n\nPartial-Bug: #2002687\nChange-Id: Idedce3c0ae6f4239ff3c1df35dc1b9af0add0a58\n'}, {'number': 2, 'created': '2023-07-10 20:10:59.000000000', 'files': ['openstack/network/v2/router.py', 'openstack/tests/unit/network/v2/test_proxy.py', 'openstack/network/v2/_proxy.py', 'doc/source/user/proxies/network.rst'], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/90c206fab40db76ad0016451be0775fcbce43ecd', 'message': 'Support the API for managing external gateways\n\nRelevant Neutron core change:\nhttps://review.opendev.org/c/openstack/neutron/+/873593\n\nPartial-Bug: #2002687\nChange-Id: Idedce3c0ae6f4239ff3c1df35dc1b9af0add0a58\n'}]",0,887952,90c206fab40db76ad0016451be0775fcbce43ecd,4,1,2,24824,,,0,"Support the API for managing external gateways

Relevant Neutron core change:
https://review.opendev.org/c/openstack/neutron/+/873593

Partial-Bug: #2002687
Change-Id: Idedce3c0ae6f4239ff3c1df35dc1b9af0add0a58
",git fetch https://review.opendev.org/openstack/openstacksdk refs/changes/52/887952/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack/network/v2/router.py', 'openstack/tests/unit/network/v2/test_proxy.py', 'openstack/network/v2/_proxy.py', 'doc/source/user/proxies/network.rst']",4,779e10c301bc8437b1714229a09043d739ead161,2023-aa-l3-gw-multihoming," add_external_gateways, update_external_gateways, remove_external_gatweays,",,129,0
openstack%2Fswift~master~I557bd01643375d7ad68c3031430899b85908a54f,openstack/swift,master,I557bd01643375d7ad68c3031430899b85908a54f,Object-server: keep SLO manifest files in page cache.,MERGED,2023-06-05 18:10:24.000000000,2023-07-10 21:45:05.000000000,2023-07-10 19:50:47.000000000,"[{'_account_id': 1179}, {'_account_id': 7847}, {'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-05 18:10:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/dc28ed29a94b99a6834e8381abf4efcae6527967', 'message': 'Object-server: keep SLO manifest files in page cache.\n\nChange-Id: I557bd01643375d7ad68c3031430899b85908a54f\n'}, {'number': 2, 'created': '2023-06-05 23:00:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/fd830fa39492067d8bf3daf7351568e919ff0396', 'message': 'Object-server: keep SLO manifest files in page cache.\n\nChange-Id: I557bd01643375d7ad68c3031430899b85908a54f\n'}, {'number': 3, 'created': '2023-06-06 03:46:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f3a0efbede8b45d94e8d31a5e535d2855a2cf1c1', 'message': 'Object-server: keep SLO manifest files in page cache.\n\nCurrently, SLO manifest files will be evicted from page cache\nafter reading it, which cause hard drives very busy when user\nrequests a lot of parallel byte range GETs for a particular\nSLO object.\n\nThis patch will try keeping the manifest files in page cache\nby not evicting them after reading. Also CC team members who\nhelped on this issue.\n\nCo-Authored-By: Tim Burke <tim.burke@gmail.com>\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\nCo-Authored-By: Alistair Coles <alistairncoles@gmail.com>\nChange-Id: I557bd01643375d7ad68c3031430899b85908a54f\n'}, {'number': 4, 'created': '2023-06-07 05:22:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0c661b4fe81932a9320a72a9c1519a155e04d712', 'message': 'Object-server: keep SLO manifest files in page cache.\n\nCurrently, SLO manifest files will be evicted from page cache\nafter reading it, which cause hard drives very busy when user\nrequests a lot of parallel byte range GETs for a particular\nSLO object.\n\nThis patch will try keeping the manifest files in page cache\nby not evicting them after reading.\n\nCo-Authored-By: Tim Burke <tim.burke@gmail.com>\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\nCo-Authored-By: Alistair Coles <alistairncoles@gmail.com>\nChange-Id: I557bd01643375d7ad68c3031430899b85908a54f\n'}, {'number': 5, 'created': '2023-06-10 21:52:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/8e8a64a653d83e3cb86ab9ec0b5593342ee6dd58', 'message': ""Object-server: keep SLO manifest files in page cache.\n\nCurrently, SLO manifest files will be evicted from page cache\nafter reading it, which cause hard drives very busy when user\nrequests a lot of parallel byte range GETs for a particular\nSLO object.\n\nThis patch will add a new config 'keep_cache_slo_manifest', and\ntry keeping the manifest files in page cache by not evicting them\nafter reading if config settings allow so.\n\nCo-Authored-By: Tim Burke <tim.burke@gmail.com>\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\nCo-Authored-By: Alistair Coles <alistairncoles@gmail.com>\nChange-Id: I557bd01643375d7ad68c3031430899b85908a54f\n""}, {'number': 6, 'created': '2023-07-07 19:55:21.000000000', 'files': ['swift/obj/server.py', 'test/unit/obj/test_server.py', 'doc/source/config/object_server_config.rst', 'test/unit/common/test_utils.py', 'etc/object-server.conf-sample'], 'web_link': 'https://opendev.org/openstack/swift/commit/cb1e584e6495b4e35b9b2833f250ac38b76ab0b3', 'message': ""Object-server: keep SLO manifest files in page cache.\n\nCurrently, SLO manifest files will be evicted from page cache\nafter reading it, which cause hard drives very busy when user\nrequests a lot of parallel byte range GETs for a particular\nSLO object.\n\nThis patch will add a new config 'keep_cache_slo_manifest', and\ntry keeping the manifest files in page cache by not evicting them\nafter reading if config settings allow so.\n\nCo-Authored-By: Tim Burke <tim.burke@gmail.com>\nCo-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>\nCo-Authored-By: Alistair Coles <alistairncoles@gmail.com>\nChange-Id: I557bd01643375d7ad68c3031430899b85908a54f\n""}]",40,885302,cb1e584e6495b4e35b9b2833f250ac38b76ab0b3,44,4,6,34930,,,0,"Object-server: keep SLO manifest files in page cache.

Currently, SLO manifest files will be evicted from page cache
after reading it, which cause hard drives very busy when user
requests a lot of parallel byte range GETs for a particular
SLO object.

This patch will add a new config 'keep_cache_slo_manifest', and
try keeping the manifest files in page cache by not evicting them
after reading if config settings allow so.

Co-Authored-By: Tim Burke <tim.burke@gmail.com>
Co-Authored-By: Clay Gerrard <clay.gerrard@gmail.com>
Co-Authored-By: Alistair Coles <alistairncoles@gmail.com>
Change-Id: I557bd01643375d7ad68c3031430899b85908a54f
",git fetch https://review.opendev.org/openstack/swift refs/changes/02/885302/6 && git format-patch -1 --stdout FETCH_HEAD,['swift/obj/server.py'],1,dc28ed29a94b99a6834e8381abf4efcae6527967,slo_manifest_pagecache," keep_cache = ( self.keep_cache_private or ( ""X-Auth-Token"" not in request.headers and ""X-Storage-Token"" not in request.headers ) or ( ""X-Static-Large-Object"" in request.headers and request.headers[""X-Static-Large-Object""].lower() == ""true"" ) )", keep_cache = (self.keep_cache_private or ('X-Auth-Token' not in request.headers and 'X-Storage-Token' not in request.headers)),12,3
openstack%2Fos-vif~stable%2Ftrain~I984ec62730276f8ee60d71a02a98fbfc4c37f7d8,openstack/os-vif,stable/train,I984ec62730276f8ee60d71a02a98fbfc4c37f7d8,Use TCP keepalives for ovsdb connections,NEW,2022-05-13 16:14:20.000000000,2023-07-10 21:08:29.000000000,,"[{'_account_id': 11604}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-05-13 16:14:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-vif/commit/ea99a05cd26461ca0bb95156eca1a54eda671c08', 'message': ""Use TCP keepalives for ovsdb connections\n\nUltimately, this is something that should be fixed in python-ovs,\nbut setting the SO_KEEPALIVE socket option benefits the client by\nremoving the need to send 'echo' requests, which can time out on\nan overloaded ovsdb-server, which causes a disconnection which then#\nadds even more load on the ovsdb-server as it has to send the entire\ndb contents over the wire after the connection is restored.\n\nThis patch ports the optimisation form neutron to reduce the likelyhood\nof a reconnection which can cause the nova compute agent to hang\ntemporarily while the connection is reestablished.\n\nChange-Id: I984ec62730276f8ee60d71a02a98fbfc4c37f7d8\nRelated-Bug: #1930926\nPartial-Bug: #1929446\n(cherry picked from commit 09c0629bb728ad342a41d844143d8e7437c925c4)\n(cherry picked from commit 26f073f0969cfd9719375a4edfc16ce4ec139c4f)\n(cherry picked from commit 700b8cc377c6caa79e8d98cd55b34d77f23ff780)\n(cherry picked from commit 9d1279e3289662ba2dffea7db7427ac7d228bb2e)\n(cherry picked from commit c2d6bf427f014e319fe9ef6f336adc0ce36815ad)\n""}, {'number': 2, 'created': '2023-07-10 18:40:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-vif/commit/74888b2956ce97c9881452879fcce6529754c13a', 'message': ""Use TCP keepalives for ovsdb connections\n\nUltimately, this is something that should be fixed in python-ovs,\nbut setting the SO_KEEPALIVE socket option benefits the client by\nremoving the need to send 'echo' requests, which can time out on\nan overloaded ovsdb-server, which causes a disconnection which then#\nadds even more load on the ovsdb-server as it has to send the entire\ndb contents over the wire after the connection is restored.\n\nThis patch ports the optimisation form neutron to reduce the likelyhood\nof a reconnection which can cause the nova compute agent to hang\ntemporarily while the connection is reestablished.\n\nNote: explict use of new style classes is reintoduced for py27 pep8\nchecks in vif_plug_ovs/ovsdb/impl_idl.py\nNoProbesMixin: -> NoProbesMixin(object):\n\nChange-Id: I984ec62730276f8ee60d71a02a98fbfc4c37f7d8\nRelated-Bug: #1930926\nPartial-Bug: #1929446\n(cherry picked from commit 09c0629bb728ad342a41d844143d8e7437c925c4)\n(cherry picked from commit 26f073f0969cfd9719375a4edfc16ce4ec139c4f)\n(cherry picked from commit 700b8cc377c6caa79e8d98cd55b34d77f23ff780)\n(cherry picked from commit 9d1279e3289662ba2dffea7db7427ac7d228bb2e)\n(cherry picked from commit c2d6bf427f014e319fe9ef6f336adc0ce36815ad)\n""}, {'number': 3, 'created': '2023-07-10 19:08:13.000000000', 'files': ['vif_plug_ovs/ovsdb/impl_idl.py'], 'web_link': 'https://opendev.org/openstack/os-vif/commit/ef5019e0e959e450069947738897b680ee3040c8', 'message': ""Use TCP keepalives for ovsdb connections\n\nUltimately, this is something that should be fixed in python-ovs,\nbut setting the SO_KEEPALIVE socket option benefits the client by\nremoving the need to send 'echo' requests, which can time out on\nan overloaded ovsdb-server, which causes a disconnection which then#\nadds even more load on the ovsdb-server as it has to send the entire\ndb contents over the wire after the connection is restored.\n\nThis patch ports the optimisation form neutron to reduce the likelyhood\nof a reconnection which can cause the nova compute agent to hang\ntemporarily while the connection is reestablished.\n\nNote: explict use of new style classes is reintoduced for py27 pep8\nchecks in vif_plug_ovs/ovsdb/impl_idl.py\nNoProbesMixin: -> NoProbesMixin(object):\n\nChange-Id: I984ec62730276f8ee60d71a02a98fbfc4c37f7d8\nRelated-Bug: #1930926\nPartial-Bug: #1929446\n(cherry picked from commit 09c0629bb728ad342a41d844143d8e7437c925c4)\n(cherry picked from commit 26f073f0969cfd9719375a4edfc16ce4ec139c4f)\n(cherry picked from commit 700b8cc377c6caa79e8d98cd55b34d77f23ff780)\n(cherry picked from commit 9d1279e3289662ba2dffea7db7427ac7d228bb2e)\n(cherry picked from commit 3c46ca71bfc1048c6021b2977fe9eadcd509443d)\n""}]",2,841779,ef5019e0e959e450069947738897b680ee3040c8,8,2,3,11604,,,0,"Use TCP keepalives for ovsdb connections

Ultimately, this is something that should be fixed in python-ovs,
but setting the SO_KEEPALIVE socket option benefits the client by
removing the need to send 'echo' requests, which can time out on
an overloaded ovsdb-server, which causes a disconnection which then#
adds even more load on the ovsdb-server as it has to send the entire
db contents over the wire after the connection is restored.

This patch ports the optimisation form neutron to reduce the likelyhood
of a reconnection which can cause the nova compute agent to hang
temporarily while the connection is reestablished.

Note: explict use of new style classes is reintoduced for py27 pep8
checks in vif_plug_ovs/ovsdb/impl_idl.py
NoProbesMixin: -> NoProbesMixin(object):

Change-Id: I984ec62730276f8ee60d71a02a98fbfc4c37f7d8
Related-Bug: #1930926
Partial-Bug: #1929446
(cherry picked from commit 09c0629bb728ad342a41d844143d8e7437c925c4)
(cherry picked from commit 26f073f0969cfd9719375a4edfc16ce4ec139c4f)
(cherry picked from commit 700b8cc377c6caa79e8d98cd55b34d77f23ff780)
(cherry picked from commit 9d1279e3289662ba2dffea7db7427ac7d228bb2e)
(cherry picked from commit 3c46ca71bfc1048c6021b2977fe9eadcd509443d)
",git fetch https://review.opendev.org/openstack/os-vif refs/changes/79/841779/3 && git format-patch -1 --stdout FETCH_HEAD,['vif_plug_ovs/ovsdb/impl_idl.py'],1,ea99a05cd26461ca0bb95156eca1a54eda671c08,bug/1929446,"import functools import socket from ovs import socket_util from ovs import stream # this is derived form https://review.opendev.org/c/openstack/neutron/+/794892 def add_keepalives(fn): @functools.wraps(fn) def _open(*args, **kwargs): error, sock = fn(*args, **kwargs) try: sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1) except socket.error as e: sock.close() return socket_util.get_exception_errno(e), None return error, sock return _open class NoProbesMixin: @staticmethod def needs_probes(): # If we are using keepalives, we can force probe_interval=0 return False class TCPStream(stream.TCPStream, NoProbesMixin): @classmethod @add_keepalives def _open(cls, suffix, dscp): return super()._open(suffix, dscp) class SSLStream(stream.SSLStream, NoProbesMixin): @classmethod @add_keepalives def _open(cls, suffix, dscp): return super()._open(suffix, dscp) # Overwriting globals in a library is clearly a good idea stream.Stream.register_method(""tcp"", TCPStream) stream.Stream.register_method(""ssl"", SSLStream)",,45,0
openstack%2Fos-vif~stable%2Ftrain~I635dff2b4fcff905ca8f431eb7e928265200f92a,openstack/os-vif,stable/train,I635dff2b4fcff905ca8f431eb7e928265200f92a,only register tables used by os-vif,NEW,2022-05-13 16:14:20.000000000,2023-07-10 21:04:29.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2022-05-13 16:14:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-vif/commit/08df3491a14e25e1b2b6a891d89a5031ac3d5f82', 'message': 'only register tables used by os-vif\n\nThis change limits the tables registered in the native driver\nto the set actully used by os-vif. This will shorten the inital\nstartup time and reconnection time if the ovs db connection is dropped.\nas a result this will help mitigate bug #1929446 where on reconnection\nthe nova compute agent can stall until reconnection is completed.\n\nChange-Id: I635dff2b4fcff905ca8f431eb7e928265200f92a\nPartial-Bug: #1929446\n(cherry picked from commit e4dc8b5664ccee8bde9e90fc9e618d6b705a0b68)\n(cherry picked from commit e6e791bd0af560ac501e383814fa7a5bce004536)\n(cherry picked from commit bac7f7d9d4337c60bf684d5b518c79f9aff95771)\n(cherry picked from commit 037e83f1b593153547e5c26a0ecb2dc918b90152)\n(cherry picked from commit ce81fb99c0e86792cde7c4e532d8abcbddb4aab8)\n'}, {'number': 2, 'created': '2023-07-10 18:40:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-vif/commit/ae71e848238837667effdca867853afa5194f0d7', 'message': 'only register tables used by os-vif\n\nThis change limits the tables registered in the native driver\nto the set actully used by os-vif. This will shorten the inital\nstartup time and reconnection time if the ovs db connection is dropped.\nas a result this will help mitigate bug #1929446 where on reconnection\nthe nova compute agent can stall until reconnection is completed.\n\nChange-Id: I635dff2b4fcff905ca8f431eb7e928265200f92a\nPartial-Bug: #1929446\n(cherry picked from commit e4dc8b5664ccee8bde9e90fc9e618d6b705a0b68)\n(cherry picked from commit e6e791bd0af560ac501e383814fa7a5bce004536)\n(cherry picked from commit bac7f7d9d4337c60bf684d5b518c79f9aff95771)\n(cherry picked from commit 037e83f1b593153547e5c26a0ecb2dc918b90152)\n'}, {'number': 3, 'created': '2023-07-10 19:08:13.000000000', 'files': ['vif_plug_ovs/ovsdb/impl_idl.py'], 'web_link': 'https://opendev.org/openstack/os-vif/commit/a8b8766a3e56b3b5d999a2c75b4a3dcc4b6ae0c4', 'message': 'only register tables used by os-vif\n\nThis change limits the tables registered in the native driver\nto the set actully used by os-vif. This will shorten the inital\nstartup time and reconnection time if the ovs db connection is dropped.\nas a result this will help mitigate bug #1929446 where on reconnection\nthe nova compute agent can stall until reconnection is completed.\n\nChange-Id: I635dff2b4fcff905ca8f431eb7e928265200f92a\nPartial-Bug: #1929446\n(cherry picked from commit e4dc8b5664ccee8bde9e90fc9e618d6b705a0b68)\n(cherry picked from commit e6e791bd0af560ac501e383814fa7a5bce004536)\n(cherry picked from commit bac7f7d9d4337c60bf684d5b518c79f9aff95771)\n(cherry picked from commit 037e83f1b593153547e5c26a0ecb2dc918b90152)\n(cherry picked from commit a2b91e8b42f47e805b8199b3ae9d73a284f72f1f)\n'}]",1,841780,a8b8766a3e56b3b5d999a2c75b4a3dcc4b6ae0c4,5,1,3,11604,,,0,"only register tables used by os-vif

This change limits the tables registered in the native driver
to the set actully used by os-vif. This will shorten the inital
startup time and reconnection time if the ovs db connection is dropped.
as a result this will help mitigate bug #1929446 where on reconnection
the nova compute agent can stall until reconnection is completed.

Change-Id: I635dff2b4fcff905ca8f431eb7e928265200f92a
Partial-Bug: #1929446
(cherry picked from commit e4dc8b5664ccee8bde9e90fc9e618d6b705a0b68)
(cherry picked from commit e6e791bd0af560ac501e383814fa7a5bce004536)
(cherry picked from commit bac7f7d9d4337c60bf684d5b518c79f9aff95771)
(cherry picked from commit 037e83f1b593153547e5c26a0ecb2dc918b90152)
(cherry picked from commit a2b91e8b42f47e805b8199b3ae9d73a284f72f1f)
",git fetch https://review.opendev.org/openstack/os-vif refs/changes/80/841780/1 && git format-patch -1 --stdout FETCH_HEAD,['vif_plug_ovs/ovsdb/impl_idl.py'],1,08df3491a14e25e1b2b6a891d89a5031ac3d5f82,bug/1929446,"REQUIRED_TABLES = ('Interface', 'Port', 'Bridge', 'Open_vSwitch') for table in REQUIRED_TABLES: helper.register_table(table)", helper.register_all(),4,1
openstack%2Fos-vif~stable%2Fussuri~I984ec62730276f8ee60d71a02a98fbfc4c37f7d8,openstack/os-vif,stable/ussuri,I984ec62730276f8ee60d71a02a98fbfc4c37f7d8,Use TCP keepalives for ovsdb connections,NEW,2022-05-13 16:13:46.000000000,2023-07-10 20:46:04.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2022-05-13 16:13:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-vif/commit/c2d6bf427f014e319fe9ef6f336adc0ce36815ad', 'message': ""Use TCP keepalives for ovsdb connections\n\nUltimately, this is something that should be fixed in python-ovs,\nbut setting the SO_KEEPALIVE socket option benefits the client by\nremoving the need to send 'echo' requests, which can time out on\nan overloaded ovsdb-server, which causes a disconnection which then#\nadds even more load on the ovsdb-server as it has to send the entire\ndb contents over the wire after the connection is restored.\n\nThis patch ports the optimisation form neutron to reduce the likelyhood\nof a reconnection which can cause the nova compute agent to hang\ntemporarily while the connection is reestablished.\n\nChange-Id: I984ec62730276f8ee60d71a02a98fbfc4c37f7d8\nRelated-Bug: #1930926\nPartial-Bug: #1929446\n(cherry picked from commit 09c0629bb728ad342a41d844143d8e7437c925c4)\n(cherry picked from commit 26f073f0969cfd9719375a4edfc16ce4ec139c4f)\n(cherry picked from commit 700b8cc377c6caa79e8d98cd55b34d77f23ff780)\n(cherry picked from commit 9d1279e3289662ba2dffea7db7427ac7d228bb2e)\n""}, {'number': 2, 'created': '2023-07-10 18:53:23.000000000', 'files': ['vif_plug_ovs/ovsdb/impl_idl.py'], 'web_link': 'https://opendev.org/openstack/os-vif/commit/3c46ca71bfc1048c6021b2977fe9eadcd509443d', 'message': ""Use TCP keepalives for ovsdb connections\n\nUltimately, this is something that should be fixed in python-ovs,\nbut setting the SO_KEEPALIVE socket option benefits the client by\nremoving the need to send 'echo' requests, which can time out on\nan overloaded ovsdb-server, which causes a disconnection which then#\nadds even more load on the ovsdb-server as it has to send the entire\ndb contents over the wire after the connection is restored.\n\nThis patch ports the optimisation form neutron to reduce the likelyhood\nof a reconnection which can cause the nova compute agent to hang\ntemporarily while the connection is reestablished.\n\nNote: explict use of new style classes is reintoduced for py27 pep8\nchecks in vif_plug_ovs/ovsdb/impl_idl.py\nNoProbesMixin: -> NoProbesMixin(object):\n\nChange-Id: I984ec62730276f8ee60d71a02a98fbfc4c37f7d8\nRelated-Bug: #1930926\nPartial-Bug: #1929446\n(cherry picked from commit 09c0629bb728ad342a41d844143d8e7437c925c4)\n(cherry picked from commit 26f073f0969cfd9719375a4edfc16ce4ec139c4f)\n(cherry picked from commit 700b8cc377c6caa79e8d98cd55b34d77f23ff780)\n(cherry picked from commit 9d1279e3289662ba2dffea7db7427ac7d228bb2e)\n""}]",1,841777,3c46ca71bfc1048c6021b2977fe9eadcd509443d,4,1,2,11604,,,0,"Use TCP keepalives for ovsdb connections

Ultimately, this is something that should be fixed in python-ovs,
but setting the SO_KEEPALIVE socket option benefits the client by
removing the need to send 'echo' requests, which can time out on
an overloaded ovsdb-server, which causes a disconnection which then#
adds even more load on the ovsdb-server as it has to send the entire
db contents over the wire after the connection is restored.

This patch ports the optimisation form neutron to reduce the likelyhood
of a reconnection which can cause the nova compute agent to hang
temporarily while the connection is reestablished.

Note: explict use of new style classes is reintoduced for py27 pep8
checks in vif_plug_ovs/ovsdb/impl_idl.py
NoProbesMixin: -> NoProbesMixin(object):

Change-Id: I984ec62730276f8ee60d71a02a98fbfc4c37f7d8
Related-Bug: #1930926
Partial-Bug: #1929446
(cherry picked from commit 09c0629bb728ad342a41d844143d8e7437c925c4)
(cherry picked from commit 26f073f0969cfd9719375a4edfc16ce4ec139c4f)
(cherry picked from commit 700b8cc377c6caa79e8d98cd55b34d77f23ff780)
(cherry picked from commit 9d1279e3289662ba2dffea7db7427ac7d228bb2e)
",git fetch https://review.opendev.org/openstack/os-vif refs/changes/77/841777/2 && git format-patch -1 --stdout FETCH_HEAD,['vif_plug_ovs/ovsdb/impl_idl.py'],1,c2d6bf427f014e319fe9ef6f336adc0ce36815ad,bug/1929446,"import functools import socket from ovs import socket_util from ovs import stream # this is derived form https://review.opendev.org/c/openstack/neutron/+/794892 def add_keepalives(fn): @functools.wraps(fn) def _open(*args, **kwargs): error, sock = fn(*args, **kwargs) try: sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1) except socket.error as e: sock.close() return socket_util.get_exception_errno(e), None return error, sock return _open class NoProbesMixin: @staticmethod def needs_probes(): # If we are using keepalives, we can force probe_interval=0 return False class TCPStream(stream.TCPStream, NoProbesMixin): @classmethod @add_keepalives def _open(cls, suffix, dscp): return super()._open(suffix, dscp) class SSLStream(stream.SSLStream, NoProbesMixin): @classmethod @add_keepalives def _open(cls, suffix, dscp): return super()._open(suffix, dscp) # Overwriting globals in a library is clearly a good idea stream.Stream.register_method(""tcp"", TCPStream) stream.Stream.register_method(""ssl"", SSLStream)",,45,0
openstack%2Fos-vif~stable%2Fussuri~I635dff2b4fcff905ca8f431eb7e928265200f92a,openstack/os-vif,stable/ussuri,I635dff2b4fcff905ca8f431eb7e928265200f92a,only register tables used by os-vif,NEW,2022-05-13 16:13:46.000000000,2023-07-10 20:29:23.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2022-05-13 16:13:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/os-vif/commit/ce81fb99c0e86792cde7c4e532d8abcbddb4aab8', 'message': 'only register tables used by os-vif\n\nThis change limits the tables registered in the native driver\nto the set actully used by os-vif. This will shorten the inital\nstartup time and reconnection time if the ovs db connection is dropped.\nas a result this will help mitigate bug #1929446 where on reconnection\nthe nova compute agent can stall until reconnection is completed.\n\nChange-Id: I635dff2b4fcff905ca8f431eb7e928265200f92a\nPartial-Bug: #1929446\n(cherry picked from commit e4dc8b5664ccee8bde9e90fc9e618d6b705a0b68)\n(cherry picked from commit e6e791bd0af560ac501e383814fa7a5bce004536)\n(cherry picked from commit bac7f7d9d4337c60bf684d5b518c79f9aff95771)\n(cherry picked from commit 037e83f1b593153547e5c26a0ecb2dc918b90152)\n'}, {'number': 2, 'created': '2023-07-10 18:53:23.000000000', 'files': ['vif_plug_ovs/ovsdb/impl_idl.py'], 'web_link': 'https://opendev.org/openstack/os-vif/commit/a2b91e8b42f47e805b8199b3ae9d73a284f72f1f', 'message': 'only register tables used by os-vif\n\nThis change limits the tables registered in the native driver\nto the set actully used by os-vif. This will shorten the inital\nstartup time and reconnection time if the ovs db connection is dropped.\nas a result this will help mitigate bug #1929446 where on reconnection\nthe nova compute agent can stall until reconnection is completed.\n\nChange-Id: I635dff2b4fcff905ca8f431eb7e928265200f92a\nPartial-Bug: #1929446\n(cherry picked from commit e4dc8b5664ccee8bde9e90fc9e618d6b705a0b68)\n(cherry picked from commit e6e791bd0af560ac501e383814fa7a5bce004536)\n(cherry picked from commit bac7f7d9d4337c60bf684d5b518c79f9aff95771)\n(cherry picked from commit 037e83f1b593153547e5c26a0ecb2dc918b90152)\n'}]",1,841778,a2b91e8b42f47e805b8199b3ae9d73a284f72f1f,4,1,2,11604,,,0,"only register tables used by os-vif

This change limits the tables registered in the native driver
to the set actully used by os-vif. This will shorten the inital
startup time and reconnection time if the ovs db connection is dropped.
as a result this will help mitigate bug #1929446 where on reconnection
the nova compute agent can stall until reconnection is completed.

Change-Id: I635dff2b4fcff905ca8f431eb7e928265200f92a
Partial-Bug: #1929446
(cherry picked from commit e4dc8b5664ccee8bde9e90fc9e618d6b705a0b68)
(cherry picked from commit e6e791bd0af560ac501e383814fa7a5bce004536)
(cherry picked from commit bac7f7d9d4337c60bf684d5b518c79f9aff95771)
(cherry picked from commit 037e83f1b593153547e5c26a0ecb2dc918b90152)
",git fetch https://review.opendev.org/openstack/os-vif refs/changes/78/841778/1 && git format-patch -1 --stdout FETCH_HEAD,['vif_plug_ovs/ovsdb/impl_idl.py'],1,ce81fb99c0e86792cde7c4e532d8abcbddb4aab8,bug/1929446,"REQUIRED_TABLES = ('Interface', 'Port', 'Bridge', 'Open_vSwitch') for table in REQUIRED_TABLES: helper.register_table(table)", helper.register_all(),4,1
openstack%2Fmanila~master~I58dcd9716cf95d0d696c13a4c831df787726bcda,openstack/manila,master,I58dcd9716cf95d0d696c13a4c831df787726bcda,Fix duplicate entries in share_server_backend_details,MERGED,2023-06-22 13:01:36.000000000,2023-07-10 20:27:21.000000000,2023-07-10 19:06:46.000000000,"[{'_account_id': 16643}, {'_account_id': 18816}, {'_account_id': 22348}, {'_account_id': 29632}, {'_account_id': 31721}, {'_account_id': 33038}]","[{'number': 1, 'created': '2023-06-22 13:01:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/22b10b023019fc1b4b7ecdb77cdc85b438a4b88b', 'message': 'Fix duplicate entries of share_server_backend_details\n\nshare_server_backend_details_set() add entries in table without checking\nexisting entries with given combinaton of share_server_id and key. This\ncauses duplicate records. Fix it by validating presence of key.\n\nCloses-bug: #2024658\nChange-Id: I58dcd9716cf95d0d696c13a4c831df787726bcda\n'}, {'number': 2, 'created': '2023-06-22 15:02:45.000000000', 'files': ['manila/tests/db/sqlalchemy/test_api.py', 'manila/db/sqlalchemy/api.py', 'manila/exception.py', 'releasenotes/notes/bug-2024658-fix-duplicate-entries-of-share-server-backend-details-adf45b417d45b437.yaml'], 'web_link': 'https://opendev.org/openstack/manila/commit/37278df338ab4d2753ca004e556a197f847be0ce', 'message': 'Fix duplicate entries in share_server_backend_details\n\nshare_server_backend_details_set() add entries in db table without\nchecking existing entries with given combinaton of share_server_id\nand key. This causes duplicate records. Fix it by validating presence\nof share server id and key.\n\nCloses-bug: #2024658\nChange-Id: I58dcd9716cf95d0d696c13a4c831df787726bcda\n'}]",3,886748,37278df338ab4d2753ca004e556a197f847be0ce,17,6,2,32919,,,0,"Fix duplicate entries in share_server_backend_details

share_server_backend_details_set() add entries in db table without
checking existing entries with given combinaton of share_server_id
and key. This causes duplicate records. Fix it by validating presence
of share server id and key.

Closes-bug: #2024658
Change-Id: I58dcd9716cf95d0d696c13a4c831df787726bcda
",git fetch https://review.opendev.org/openstack/manila refs/changes/48/886748/1 && git format-patch -1 --stdout FETCH_HEAD,"['manila/tests/db/sqlalchemy/test_api.py', 'manila/db/sqlalchemy/api.py', 'manila/exception.py', 'releasenotes/notes/bug-2024658-fix-duplicate-entries-of-share-server-backend-details-adf45b417d45b437.yaml']",4,22b10b023019fc1b4b7ecdb77cdc85b438a4b88b,bug/2024658,--- fixes: - | Share server backend details sets function adds db records without checking existing entries. This results in duplicate records for the combination of given share server id and key. Fixed it by updating records if already exist else creating new. For more details check `Launchpad bug #2024658<https://bugs.launchpad.net/manila/+bug/2024658>`_ ,,50,6
openstack%2Frequirements~master~Iccd7166f0a8091bdf4613186f9c8048864603f29,openstack/requirements,master,Iccd7166f0a8091bdf4613186f9c8048864603f29,Raise cap for XStatic-JQuery in ``global-requirements.txt``,MERGED,2023-07-10 15:56:25.000000000,2023-07-10 20:22:07.000000000,2023-07-10 20:21:09.000000000,"[{'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-10 15:56:25.000000000', 'files': ['global-requirements.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/df5efbd42bd4bf89f38bee2527b61415927c5823', 'message': 'Raise cap for XStatic-JQuery in ``global-requirements.txt``\n\nThis patch raise the version of XStatic-JQuery <3.6\nin ``global-requirements.txt`` file because horizon\nis already using XStatic-JQuery-Migrate version 3.3.2.1 [1]\nbut XStatic-jQuery old version is not compatiable with\nlatest version of XStatic-JQuery-Migrate version.\nSo horizon is updating XStatic-JQuery to <3.6 and for that\nthis patch needs to be merge first.\n\nNote: ``upper-constraints.txt`` is updated in a seprate patch\n[3].\n\n[1] https://review.opendev.org/c/openstack/requirements/+/883402\n[2] https://review.opendev.org/c/openstack/horizon/+/887548\n[3] https://review.opendev.org/c/openstack/requirements/+/887933\n\nChange-Id: Iccd7166f0a8091bdf4613186f9c8048864603f29\n'}]",0,888055,df5efbd42bd4bf89f38bee2527b61415927c5823,7,2,1,29313,,,0,"Raise cap for XStatic-JQuery in ``global-requirements.txt``

This patch raise the version of XStatic-JQuery <3.6
in ``global-requirements.txt`` file because horizon
is already using XStatic-JQuery-Migrate version 3.3.2.1 [1]
but XStatic-jQuery old version is not compatiable with
latest version of XStatic-JQuery-Migrate version.
So horizon is updating XStatic-JQuery to <3.6 and for that
this patch needs to be merge first.

Note: ``upper-constraints.txt`` is updated in a seprate patch
[3].

[1] https://review.opendev.org/c/openstack/requirements/+/883402
[2] https://review.opendev.org/c/openstack/horizon/+/887548
[3] https://review.opendev.org/c/openstack/requirements/+/887933

Change-Id: Iccd7166f0a8091bdf4613186f9c8048864603f29
",git fetch https://review.opendev.org/openstack/requirements refs/changes/55/888055/1 && git format-patch -1 --stdout FETCH_HEAD,['global-requirements.txt'],1,df5efbd42bd4bf89f38bee2527b61415927c5823,,XStatic-jQuery<3.6 # MIT License,XStatic-jQuery<3 # MIT License,1,1
openstack%2Fopenstack-ansible-haproxy_server~master~I184021b65d6f3f28526c9fa09bea90a2baef77b2,openstack/openstack-ansible-haproxy_server,master,I184021b65d6f3f28526c9fa09bea90a2baef77b2,Fix `regen pem` with `extra_lb_tls_vip_addresses`,MERGED,2023-07-04 08:24:04.000000000,2023-07-10 20:05:07.000000000,2023-07-10 20:04:11.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 28619}]","[{'number': 1, 'created': '2023-07-04 08:24:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-haproxy_server/commit/b2229d8cc6382ecccc9d75d96c1c6aa5bcf37911', 'message': 'Fix `regen pem` when using `extra_lb_tls_vip_addresses`\n\n`extra_lb_tls_vip_addresses` is list of additional internal VIP\naddresses, which gets parsed into `haproxy_tls_vip_binds` without\n`interface` attribute.\n\nChange-Id: I184021b65d6f3f28526c9fa09bea90a2baef77b2\n'}, {'number': 2, 'created': '2023-07-04 08:25:21.000000000', 'files': ['handlers/main.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-haproxy_server/commit/848e316ef5066cc0937ec9a91e215382373ae243', 'message': 'Fix `regen pem` with `extra_lb_tls_vip_addresses`\n\n`extra_lb_tls_vip_addresses` is list of additional internal VIP\naddresses, which gets parsed into `haproxy_tls_vip_binds` without\n`interface` attribute.\n\nChange-Id: I184021b65d6f3f28526c9fa09bea90a2baef77b2\n'}]",5,887573,848e316ef5066cc0937ec9a91e215382373ae243,16,3,2,34653,,,0,"Fix `regen pem` with `extra_lb_tls_vip_addresses`

`extra_lb_tls_vip_addresses` is list of additional internal VIP
addresses, which gets parsed into `haproxy_tls_vip_binds` without
`interface` attribute.

Change-Id: I184021b65d6f3f28526c9fa09bea90a2baef77b2
",git fetch https://review.opendev.org/openstack/openstack-ansible-haproxy_server refs/changes/73/887573/2 && git format-patch -1 --stdout FETCH_HEAD,['handlers/main.yml'],1,b2229d8cc6382ecccc9d75d96c1c6aa5bcf37911,fix_regen_pem_with_extra_lb_tls_vip_addresses," item_interface: ""{{ item['interface'] | default('') }}"""," item_interface: ""{{ item['interface'] }}""",1,1
openstack%2Fcharms.ceph~stable%2Fpacific~I07f0b34e80978189ca03ab7bad956e5ce5da4201,openstack/charms.ceph,stable/pacific,I07f0b34e80978189ca03ab7bad956e5ce5da4201,Add missing octopus -> pacific upgrade path,MERGED,2023-06-13 04:53:01.000000000,2023-07-10 19:24:48.000000000,2023-07-10 19:24:48.000000000,"[{'_account_id': 8992}, {'_account_id': 15382}, {'_account_id': 21107}, {'_account_id': 22348}, {'_account_id': 33717}, {'_account_id': 34952}]","[{'number': 1, 'created': '2023-06-13 04:53:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charms.ceph/commit/bfcbc3f6fea9c0eec447ef5885f0c91797b2b930', 'message': ""Add missing octopus -> pacific upgrade path\n\nThe octopus -> pacific upgrade path is missing from the stable/pacific\nbranch, which results in an error and failure to upgrade.\n\nInvalid upgrade path from octopus to octopus. Valid paths are: ['firefly\n-> hammer', 'hammer -> jewel', 'jewel -> luminous', 'luminous -> mimic',\n'mimic -> nautilus', 'nautilus -> octopus']\n\nChange-Id: I07f0b34e80978189ca03ab7bad956e5ce5da4201\n""}, {'number': 2, 'created': '2023-06-13 06:59:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charms.ceph/commit/e3c81383c2c06ab991dedb87ad7798ef0476c529', 'message': ""Add missing octopus -> pacific upgrade path\n\nThe octopus -> pacific upgrade path is missing from the stable/pacific\nbranch, which results in an error and failure to upgrade.\n\nInvalid upgrade path from octopus to octopus. Valid paths are: ['firefly\n-> hammer', 'hammer -> jewel', 'jewel -> luminous', 'luminous -> mimic',\n'mimic -> nautilus', 'nautilus -> octopus']\n\nChange-Id: I07f0b34e80978189ca03ab7bad956e5ce5da4201\n""}, {'number': 3, 'created': '2023-06-14 01:58:39.000000000', 'files': ['charms_ceph/utils.py', 'unit_tests/test_utils.py'], 'web_link': 'https://opendev.org/openstack/charms.ceph/commit/2e600bdf71ef302de5d91bb5f9b9b989478e3b1c', 'message': ""Add missing octopus -> pacific upgrade path\n\nThe octopus -> pacific upgrade path is missing from the stable/pacific\nbranch, which results in an error and failure to upgrade.\n\nInvalid upgrade path from octopus to octopus. Valid paths are: ['firefly\n-> hammer', 'hammer -> jewel', 'jewel -> luminous', 'luminous -> mimic',\n'mimic -> nautilus', 'nautilus -> octopus']\n\nChange-Id: I07f0b34e80978189ca03ab7bad956e5ce5da4201\n""}]",17,885933,2e600bdf71ef302de5d91bb5f9b9b989478e3b1c,19,6,3,21107,,,0,"Add missing octopus -> pacific upgrade path

The octopus -> pacific upgrade path is missing from the stable/pacific
branch, which results in an error and failure to upgrade.

Invalid upgrade path from octopus to octopus. Valid paths are: ['firefly
-> hammer', 'hammer -> jewel', 'jewel -> luminous', 'luminous -> mimic',
'mimic -> nautilus', 'nautilus -> octopus']

Change-Id: I07f0b34e80978189ca03ab7bad956e5ce5da4201
",git fetch https://review.opendev.org/openstack/charms.ceph refs/changes/33/885933/2 && git format-patch -1 --stdout FETCH_HEAD,"['charms_ceph/utils.py', 'unit_tests/test_utils.py']",2,bfcbc3f6fea9c0eec447ef5885f0c91797b2b930,," 'octopus -> pacific',",,5,0
openstack%2Fgovernance~master~I6707ea066b1bafabd4bbb4b83852605bfa06e816,openstack/governance,master,I6707ea066b1bafabd4bbb4b83852605bfa06e816,Apply New Extended Maintenance Requirements,NEW,2023-07-07 15:01:28.000000000,2023-07-10 19:21:00.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 15:01:28.000000000', 'files': ['resolutions/20230707-apply-em-requirements.rst'], 'web_link': 'https://opendev.org/openstack/governance/commit/d8f4cb1bd5c53210649d0ee279f3750cc2167657', 'message': 'Apply New Extended Maintenance Requirements\n\nThis proposal aims to offer an alternative for what to do with\ncurrent EM branches and when to transition them to the new policy.\n\nChange-Id: I6707ea066b1bafabd4bbb4b83852605bfa06e816\n'}]",2,887968,d8f4cb1bd5c53210649d0ee279f3750cc2167657,4,1,1,16465,,,0,"Apply New Extended Maintenance Requirements

This proposal aims to offer an alternative for what to do with
current EM branches and when to transition them to the new policy.

Change-Id: I6707ea066b1bafabd4bbb4b83852605bfa06e816
",git fetch https://review.opendev.org/openstack/governance refs/changes/68/887968/1 && git format-patch -1 --stdout FETCH_HEAD,['resolutions/20230707-apply-em-requirements.rst'],1,d8f4cb1bd5c53210649d0ee279f3750cc2167657,formal-vote,"======================================================================== 2023-07-07 Extend New Extended Maintenance Requirements to All Branches ======================================================================== This resolution identifies that the current branches under extended maintenance are not receiving the expected level of maintenance for most project teams and repositories and extends requirements defined in resolution `2023-07-07 New Requirements for Extended Maintenance`[0] to apply to all extended maintenance branches, effective 2023-09-01. [0]. https://governance.openstack.org/tc/resolutions/20230707-extended-maintenance-new-requirements.html ",,12,0
openstack%2Fcharms.ceph~stable%2Fpacific~I16d01e15438e3d61033863b4c03087d1a0b7a008,openstack/charms.ceph,stable/pacific,I16d01e15438e3d61033863b4c03087d1a0b7a008,Fix linting errors,MERGED,2023-06-14 01:53:22.000000000,2023-07-10 19:13:23.000000000,2023-07-10 19:13:23.000000000,"[{'_account_id': 1131}, {'_account_id': 8992}, {'_account_id': 15382}, {'_account_id': 22348}, {'_account_id': 34952}]","[{'number': 1, 'created': '2023-06-14 01:53:22.000000000', 'files': ['charms_ceph/utils.py', 'unit_tests/test_utils.py'], 'web_link': 'https://opendev.org/openstack/charms.ceph/commit/763495aeb39f1312011b17eedbb84edda351b649', 'message': 'Fix linting errors\n\nChange-Id: I16d01e15438e3d61033863b4c03087d1a0b7a008\n(cherry picked from commit 78bd3fd21ae81f7ae9180d9bd8a271d1d27d7537)\n'}]",1,886038,763495aeb39f1312011b17eedbb84edda351b649,9,5,1,21107,,,0,"Fix linting errors

Change-Id: I16d01e15438e3d61033863b4c03087d1a0b7a008
(cherry picked from commit 78bd3fd21ae81f7ae9180d9bd8a271d1d27d7537)
",git fetch https://review.opendev.org/openstack/charms.ceph refs/changes/38/886038/1 && git format-patch -1 --stdout FETCH_HEAD,"['charms_ceph/utils.py', 'unit_tests/test_utils.py']",2,763495aeb39f1312011b17eedbb84edda351b649,, assert (utils.use_bluestore()), assert(utils.use_bluestore()),3,3
openstack%2Fneutron~stable%2Fwallaby~If06c372a9d5cb1dc1ec1af768abb61f52c2c5abd,openstack/neutron,stable/wallaby,If06c372a9d5cb1dc1ec1af768abb61f52c2c5abd,[OVN] Improve Hash Ring logs,MERGED,2023-06-29 12:57:48.000000000,2023-07-10 19:06:56.000000000,2023-07-10 19:05:56.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 12:57:48.000000000', 'files': ['neutron/tests/unit/db/test_ovn_hash_ring_db.py', 'neutron/common/ovn/exceptions.py', 'neutron/db/ovn_hash_ring_db.py', 'neutron/common/ovn/hash_ring_manager.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/e28fd4b4a5820f8c9aeaee3b43608e6260883069', 'message': '[OVN] Improve Hash Ring logs\n\nDebugging Hash Ring problems can be difficult challenge given that prior\nto this patch the logs were very limited.\n\nThis patch improves the logging for this feature as follow:\n\n1. Log when a node is added to the ring\n2. Log when nodes are removed from the ring\n3. Keep track the number of offline nodes and log it upon loading the\n   ring\n4. Improve the ""Hash Ring is empty"" exception with the number of offline\n   nodes found (if 0, means the ovn_hash_ring table has no entries)\n\nCloses-Bug: #2023670\nChange-Id: Ic90432b5ddea8cf176de159ec7eaafd5fd7bdd6e\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n(cherry picked from commit afa20faec3c37bd06346360cadbad0d69e9925f0)\n\n[OVN] The all() and count() methods should be inside a DB txn\n\nThe ``ovn_hash_ring_db`` methods ``get_active_nodes`` and\n``count_offline_nodes`` are sending SQL requests that should be issued\nfrom inside a READER context.\n\nCloses-Bug: #2024447\nChange-Id: If06c372a9d5cb1dc1ec1af768abb61f52c2c5abd\n(cherry picked from commit 0c66dfaed8e1ec00726c3e484e69174779678abd)\n'}]",7,887278,e28fd4b4a5820f8c9aeaee3b43608e6260883069,25,3,1,6773,,,0,"[OVN] Improve Hash Ring logs

Debugging Hash Ring problems can be difficult challenge given that prior
to this patch the logs were very limited.

This patch improves the logging for this feature as follow:

1. Log when a node is added to the ring
2. Log when nodes are removed from the ring
3. Keep track the number of offline nodes and log it upon loading the
   ring
4. Improve the ""Hash Ring is empty"" exception with the number of offline
   nodes found (if 0, means the ovn_hash_ring table has no entries)

Closes-Bug: #2023670
Change-Id: Ic90432b5ddea8cf176de159ec7eaafd5fd7bdd6e
Signed-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>
(cherry picked from commit afa20faec3c37bd06346360cadbad0d69e9925f0)

[OVN] The all() and count() methods should be inside a DB txn

The ``ovn_hash_ring_db`` methods ``get_active_nodes`` and
``count_offline_nodes`` are sending SQL requests that should be issued
from inside a READER context.

Closes-Bug: #2024447
Change-Id: If06c372a9d5cb1dc1ec1af768abb61f52c2c5abd
(cherry picked from commit 0c66dfaed8e1ec00726c3e484e69174779678abd)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/78/887278/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/db/test_ovn_hash_ring_db.py', 'neutron/common/ovn/exceptions.py', 'neutron/db/ovn_hash_ring_db.py', 'neutron/common/ovn/hash_ring_manager.py']",4,e28fd4b4a5820f8c9aeaee3b43608e6260883069,," self._offline_node_count = 0 self._offline_node_count = db_hash_ring.count_offline_nodes( self.admin_ctx, constants.HASH_RING_NODES_TIMEOUT, self._group) LOG.debug(""Hash Ring loaded. %d active nodes. %d offline nodes"", len(nodes), self._offline_node_count) raise exceptions.HashRingIsEmpty( key=key, node_count=self._offline_node_count)", raise exceptions.HashRingIsEmpty(key=key),72,12
openstack%2Ftripleo-common~stable%2Ftrain~I1381b4f7f4f7808d29e7f1aa73e7c0cd06acc795,openstack/tripleo-common,stable/train,I1381b4f7f4f7808d29e7f1aa73e7c0cd06acc795,Fix unassigned new_manifest_type variable,MERGED,2023-07-10 08:57:01.000000000,2023-07-10 18:45:57.000000000,2023-07-10 18:45:01.000000000,"[{'_account_id': 7144}, {'_account_id': 8449}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-10 08:57:01.000000000', 'files': ['tripleo_common/image/image_uploader.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/ae83d4b24a6cbbfa711d0bf67f69c717308f1b68', 'message': 'Fix unassigned new_manifest_type variable\n\nThis fixes the regression caused by the recent change[1], and ensures\nthe new_manifest_type variable is defined in any code paths.\n\n[1] I04f6ac171b10af7a294819d6248eac641090cc49\n\nCloses-Bug: #2026711\nChange-Id: I1381b4f7f4f7808d29e7f1aa73e7c0cd06acc795\n'}]",1,888026,ae83d4b24a6cbbfa711d0bf67f69c717308f1b68,9,3,1,9816,,,0,"Fix unassigned new_manifest_type variable

This fixes the regression caused by the recent change[1], and ensures
the new_manifest_type variable is defined in any code paths.

[1] I04f6ac171b10af7a294819d6248eac641090cc49

Closes-Bug: #2026711
Change-Id: I1381b4f7f4f7808d29e7f1aa73e7c0cd06acc795
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/26/888026/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_common/image/image_uploader.py'],1,ae83d4b24a6cbbfa711d0bf67f69c717308f1b68,bug/2026711, new_manifest_type = manifest_type,,1,0
openstack%2Fneutron~stable%2Fwallaby~I15d22e0e2b4bf3e4f882b521bcd6a13f84e4feed,openstack/neutron,stable/wallaby,I15d22e0e2b4bf3e4f882b521bcd6a13f84e4feed,[qos] _validate_create_network_callback return in no network,MERGED,2023-06-27 12:56:50.000000000,2023-07-10 18:22:24.000000000,2023-07-10 18:21:10.000000000,"[{'_account_id': 11975}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-27 12:56:50.000000000', 'files': ['neutron/services/qos/qos_plugin.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/18ab40fcade6a5a0ed8e5bfa6c42b48567157782', 'message': '[qos] _validate_create_network_callback return in no network\n\nIt seems that _validate_create_network_callback notified without\nnetwork_id in payload, to avoid issues in such case return.\n\nChange-Id: I15d22e0e2b4bf3e4f882b521bcd6a13f84e4feed\nCloses-Bug: #2008912\n(cherry picked from commit ec4bfb91f0d3ae4aa4cbe8ba05c20cb00515b00d)\n'}]",8,887044,18ab40fcade6a5a0ed8e5bfa6c42b48567157782,41,3,1,16688,,,0,"[qos] _validate_create_network_callback return in no network

It seems that _validate_create_network_callback notified without
network_id in payload, to avoid issues in such case return.

Change-Id: I15d22e0e2b4bf3e4f882b521bcd6a13f84e4feed
Closes-Bug: #2008912
(cherry picked from commit ec4bfb91f0d3ae4aa4cbe8ba05c20cb00515b00d)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/44/887044/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/services/qos/qos_plugin.py'],1,18ab40fcade6a5a0ed8e5bfa6c42b48567157782,bug/2008912," if not network or not getattr(network, 'qos_policy_id', None): policy_id = network.qos_policy_id", policy_id = network.qos_policy_id if policy_id is None:,2,2
openstack%2Fhorizon~master~I48a0da02fdec55efbeb1cce6815a68f01d8f12df,openstack/horizon,master,I48a0da02fdec55efbeb1cce6815a68f01d8f12df,Centralize the export credentials in users menu,NEW,2023-03-31 14:41:58.000000000,2023-07-10 18:19:12.000000000,,"[{'_account_id': 22348}, {'_account_id': 28356}]","[{'number': 1, 'created': '2023-03-31 14:41:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/a7ac878cfe8cdf008c0e1b413d70ab496ee857aa', 'message': 'Centralize the export credentials in users menu\n\nImplements: blueprint action-to-export-credentials-in-users-menu\nChange-Id: I48a0da02fdec55efbeb1cce6815a68f01d8f12df\n'}, {'number': 2, 'created': '2023-03-31 15:40:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/dc4a465e689aeb2c07f6c0c2d77c01aa2faab8a7', 'message': 'Centralize the export credentials in users menu\n\nImplements: blueprint action-to-export-credentials-in-users-menu\nChange-Id: I48a0da02fdec55efbeb1cce6815a68f01d8f12df\n'}, {'number': 3, 'created': '2023-03-31 16:45:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/bdce271226a3621e8fc15912b49c534f246d054d', 'message': 'Centralize the export credentials in users menu\n\nImplements: blueprint action-to-export-credentials-in-users-menu\nChange-Id: I48a0da02fdec55efbeb1cce6815a68f01d8f12df\n'}, {'number': 4, 'created': '2023-07-10 12:39:32.000000000', 'files': ['doc/source/configuration/settings.rst', 'openstack_dashboard/dashboards/project/api_access/templates/api_access/local_user.yaml.template', 'openstack_dashboard/templates/horizon/_scripts.html', 'openstack_dashboard/dashboards/project/api_access/views.py', 'openstack_dashboard/dashboards/project/api_access/templates/api_access/federated_user.yaml.template', 'openstack_dashboard/dashboards/project/api_access/templates/api_access/federated_user.rc.template', 'horizon/static/horizon/js/horizon.forms.clipboard.js', 'openstack_dashboard/dashboards/project/api_access/templates/api_access/session_token.rc.template', 'openstack_dashboard/dashboards/project/api_access/templates/api_access/session_token.yaml.template', 'openstack_dashboard/templates/header/_user_menu.html', 'openstack_dashboard/dashboards/project/api_access/urls.py', 'openstack_dashboard/dashboards/project/api_access/templates/api_access/local_user.rc.template', 'releasenotes/notes/centralize-export-credentials-in-popup-modal-0e2f70415a8e92a4.yaml', 'openstack_dashboard/dashboards/project/api_access/tests.py', 'openstack_dashboard/dashboards/project/api_access/workflows.py', 'horizon/templates/horizon/common/fields/_clipboard.html', 'horizon/static/horizon/js/horizon.modals.js', 'openstack_dashboard/defaults.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/2072921104ab8680ca42705a36cdc3bc4e5339c8', 'message': 'Centralize the export credentials in users menu\n\nImplements: blueprint action-to-export-credentials-in-users-menu\nChange-Id: I48a0da02fdec55efbeb1cce6815a68f01d8f12df\n'}]",3,879163,2072921104ab8680ca42705a36cdc3bc4e5339c8,13,2,4,30695,,,0,"Centralize the export credentials in users menu

Implements: blueprint action-to-export-credentials-in-users-menu
Change-Id: I48a0da02fdec55efbeb1cce6815a68f01d8f12df
",git fetch https://review.opendev.org/openstack/horizon refs/changes/63/879163/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/configuration/settings.rst', 'openstack_dashboard/dashboards/project/api_access/templates/api_access/local_user.yaml.template', 'openstack_dashboard/templates/horizon/_scripts.html', 'openstack_dashboard/dashboards/project/api_access/views.py', 'openstack_dashboard/dashboards/project/api_access/templates/api_access/federated_user.yaml.template', 'openstack_dashboard/dashboards/project/api_access/templates/api_access/federated_user.rc.template', 'horizon/static/horizon/js/horizon.forms.clipboard.js', 'openstack_dashboard/settings.py', 'openstack_dashboard/dashboards/project/api_access/templates/api_access/session_token.rc.template', 'openstack_dashboard/dashboards/project/api_access/templates/api_access/session_token.yaml.template', 'openstack_dashboard/templates/header/_user_menu.html', 'openstack_dashboard/dashboards/project/api_access/urls.py', 'openstack_dashboard/dashboards/project/api_access/templates/api_access/local_user.rc.template', 'releasenotes/notes/centralize-export-credentials-in-popup-modal-0e2f70415a8e92a4.yaml', 'openstack_dashboard/dashboards/project/api_access/tests.py', 'openstack_dashboard/dashboards/project/api_access/workflows.py', 'horizon/templates/horizon/common/fields/_clipboard.html', 'horizon/static/horizon/js/horizon.modals.js', 'openstack_dashboard/defaults.py']",19,a7ac878cfe8cdf008c0e1b413d70ab496ee857aa,centralize-credentials-exportation-on-users-menu, # The Identity Federation attributes to be set in the export credentials # response. WEBSSO_EXPORT_CREDENTIALS = {},,683,13
openstack%2Frequirements~master~I6c67cd897adb0cf8a84748aea2c92a998964f990,openstack/requirements,master,I6c67cd897adb0cf8a84748aea2c92a998964f990,update constraint for python-glanceclient to new release 4.4.0,MERGED,2023-07-07 14:35:43.000000000,2023-07-10 18:16:38.000000000,2023-07-10 18:15:46.000000000,"[{'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-07 14:35:43.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/e835b6370d575025c5fdc559b61ff61eac06230e', 'message': 'update constraint for python-glanceclient to new release 4.4.0\n\nmeta: version: 4.4.0\nmeta: diff-start: -\nmeta: series: bobcat\nmeta: branch: master\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: yes\nmeta: release:Author: Elod Illes <elod.illes@est.tech>\nmeta: release:Commit: Pranali Deore <pdeore@redhat.com>\nmeta: release:Change-Id: I327a1884a1c93ece4fe5d7531d9fe4a6db3bd742\nmeta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>\nmeta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>\nmeta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>\nChange-Id: I6c67cd897adb0cf8a84748aea2c92a998964f990\n'}]",1,887961,e835b6370d575025c5fdc559b61ff61eac06230e,9,3,1,11131,,,0,"update constraint for python-glanceclient to new release 4.4.0

meta: version: 4.4.0
meta: diff-start: -
meta: series: bobcat
meta: branch: master
meta: release-type: release
meta: pypi: yes
meta: first: yes
meta: release:Author: Elod Illes <elod.illes@est.tech>
meta: release:Commit: Pranali Deore <pdeore@redhat.com>
meta: release:Change-Id: I327a1884a1c93ece4fe5d7531d9fe4a6db3bd742
meta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>
meta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>
meta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>
Change-Id: I6c67cd897adb0cf8a84748aea2c92a998964f990
",git fetch https://review.opendev.org/openstack/requirements refs/changes/61/887961/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,e835b6370d575025c5fdc559b61ff61eac06230e,new-release,python-glanceclient===4.4.0,python-glanceclient===4.3.0,1,1
openstack%2Fswift~master~I0ea464bcda16678997865667287aa11ea89cdcde,openstack/swift,master,I0ea464bcda16678997865667287aa11ea89cdcde,Encode header in latin-1 with wsgi_to_bytes,MERGED,2023-05-24 15:59:59.000000000,2023-07-10 17:45:39.000000000,2023-07-10 17:44:30.000000000,"[{'_account_id': 597}, {'_account_id': 7847}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-05-24 15:59:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/52af7ea6857649742c1292d8bc9767043d183cab', 'message': ""Encode header in latin-1 with wsgi_to_bytes\n\nPrevent encoding corruption in client's metadata during ssync\n\nCloses-Bug: #2020667\nChange-Id: I0ea464bcda16678997865667287aa11ea89cdcde\n""}, {'number': 2, 'created': '2023-05-31 11:06:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/cd6e71e62b7b56ff1a2770adfb3f9f4441d874e4', 'message': ""Encode header in latin-1 with wsgi_to_bytes\n\nPrevent encoding corruption in client's metadata during ssync\n\nCloses-Bug: #2020667\nChange-Id: I0ea464bcda16678997865667287aa11ea89cdcde\n""}, {'number': 3, 'created': '2023-05-31 23:07:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1e167471ea4c4d7e28e6fd1378d7c7d3d058c96f', 'message': ""Encode header in latin-1 with wsgi_to_bytes\n\nPrevent encoding corruption in client's metadata during ssync\n\nCloses-Bug: #2020667\nChange-Id: I0ea464bcda16678997865667287aa11ea89cdcde\n""}, {'number': 4, 'created': '2023-06-01 16:19:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e6179e98211a9488a5581f50427df9bdd647c783', 'message': ""Encode header in latin-1 with wsgi_to_bytes\n\nPrevent encoding corruption in client's metadata during ssync\n\nCloses-Bug: #2020667\nChange-Id: I0ea464bcda16678997865667287aa11ea89cdcde\n""}, {'number': 5, 'created': '2023-06-23 21:23:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/28172879f193afb96017ebd44a41d1f43cb6e2c2', 'message': ""Encode header in latin-1 with wsgi_to_bytes\n\nPrevent encoding corruption in client's metadata during ssync\n\nCloses-Bug: #2020667\nChange-Id: I0ea464bcda16678997865667287aa11ea89cdcde\n""}, {'number': 6, 'created': '2023-07-10 14:20:39.000000000', 'files': ['swift/obj/server.py', 'test/unit/obj/test_ssync_sender.py', 'test/unit/obj/test_server.py', 'swift/obj/ssync_sender.py', 'swift/obj/ssync_receiver.py', 'test/unit/obj/test_ssync_receiver.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/365c0ef005ca14691f8ba21f1e81d37ae7c0bfc0', 'message': ""Encode header in latin-1 with wsgi_to_bytes\n\nPrevent encoding corruption in client's metadata during ssync\n\nCloses-Bug: #2020667\nChange-Id: I0ea464bcda16678997865667287aa11ea89cdcde\n""}]",25,884240,365c0ef005ca14691f8ba21f1e81d37ae7c0bfc0,34,3,6,28499,,,0,"Encode header in latin-1 with wsgi_to_bytes

Prevent encoding corruption in client's metadata during ssync

Closes-Bug: #2020667
Change-Id: I0ea464bcda16678997865667287aa11ea89cdcde
",git fetch https://review.opendev.org/openstack/swift refs/changes/40/884240/1 && git format-patch -1 --stdout FETCH_HEAD,['swift/obj/ssync_sender.py'],1,52af7ea6857649742c1292d8bc9767043d183cab,bug2020667,"from swift.common.swob import wsgi_to_bytes msg.append(wsgi_to_bytes('%s: %s' % (key, value)))"," if six.PY2: msg.append(b'%s: %s' % (key, value)) else: msg.append(b'%s: %s' % ( key.encode('utf8', 'surrogateescape'), str(value).encode('utf8', 'surrogateescape')))",2,6
openstack%2Ftacker-specs~master~I71571607804b5ac1d2cd7a77cf34e89b1ec4af03,openstack/tacker-specs,master,I71571607804b5ac1d2cd7a77cf34e89b1ec4af03,"Update Spec of ""Enhance Tacker API Access Control""",MERGED,2023-06-26 04:29:54.000000000,2023-07-10 17:28:54.000000000,2023-07-10 17:28:54.000000000,"[{'_account_id': 17255}, {'_account_id': 22348}, {'_account_id': 25701}, {'_account_id': 31668}, {'_account_id': 31730}, {'_account_id': 31857}, {'_account_id': 32707}]","[{'number': 1, 'created': '2023-06-26 04:29:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/97227437a0201306d4a93117ca1a73f65b01e28d', 'message': 'Update Spec of ""Enhance Tacker API Access Control""\n\nThe spec of ""Enhancement of Tacker API Resource Access Control""\nwill be updated for the following items.\n* The attribute of ""namespace"" and the special roles of ""NAMESPACE"" are\n  changed to ""tenant"" and ""TENANT"" in Antelope\n* Add tenant control for VNF will be supported in Bobcat\n  (Remove the expression of ""CNF only"".)\n\nThis patch fixes these changes in the current spec.\n\nImplements: blueprint enhance-api-policy\nChange-Id: I71571607804b5ac1d2cd7a77cf34e89b1ec4af03\n'}, {'number': 2, 'created': '2023-07-04 23:51:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/f09d5c1a8e791bb3c8444c247fe13f7b8156a539', 'message': 'Update Spec of ""Enhance Tacker API Access Control""\n\nThe spec of ""Enhancement of Tacker API Resource Access Control""\nwill be updated for the following items.\n* The attribute of ""namespace"" and the special roles of ""NAMESPACE"" are\n  changed to ""tenant"" and ""TENANT"" in Antelope\n* Add tenant control for VNF will be supported in Bobcat\n  (Remove the expression of ""CNF only"".)\n\nThis patch fixes these changes in the current spec.\n\nImplements: blueprint enhance-api-policy\nChange-Id: I71571607804b5ac1d2cd7a77cf34e89b1ec4af03\n'}, {'number': 3, 'created': '2023-07-05 00:30:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/f2b80a3fe275b673e76018fd311055d6326aa055', 'message': 'Update Spec of ""Enhance Tacker API Access Control""\n\nThe spec of ""Enhancement of Tacker API Resource Access Control""\nwill be updated for the following items.\n* The attribute of ""namespace"" and the special roles of ""NAMESPACE"" are\n  changed to ""tenant"" and ""TENANT"" in Antelope\n* Add tenant control for VNF will be supported in Bobcat\n  (Remove the expression of ""CNF only"".)\n\nThis patch fixes these changes in the current spec.\n\nImplements: blueprint enhance-api-policy\nChange-Id: I71571607804b5ac1d2cd7a77cf34e89b1ec4af03\n'}, {'number': 4, 'created': '2023-07-10 07:26:28.000000000', 'files': ['specs/2023.1/enhance-tacker-policy.rst'], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/0b15b736da207c6381928ae0ab4f70dbfa44c89e', 'message': 'Update Spec of ""Enhance Tacker API Access Control""\n\nThe spec of ""Enhancement of Tacker API Resource Access Control""\nwill be updated for the following items.\n* The attribute of ""namespace"" and the special roles of ""NAMESPACE"" are\n  changed to ""tenant"" and ""TENANT"" in Antelope\n* Add tenant control for VNF will be supported in Bobcat\n  (Remove the expression of ""CNF only"".)\n\nThis patch fixes these changes in the current spec.\n\nImplements: blueprint enhance-api-policy\nChange-Id: I71571607804b5ac1d2cd7a77cf34e89b1ec4af03\n'}]",17,886944,0b15b736da207c6381928ae0ab4f70dbfa44c89e,27,7,4,34712,,,0,"Update Spec of ""Enhance Tacker API Access Control""

The spec of ""Enhancement of Tacker API Resource Access Control""
will be updated for the following items.
* The attribute of ""namespace"" and the special roles of ""NAMESPACE"" are
  changed to ""tenant"" and ""TENANT"" in Antelope
* Add tenant control for VNF will be supported in Bobcat
  (Remove the expression of ""CNF only"".)

This patch fixes these changes in the current spec.

Implements: blueprint enhance-api-policy
Change-Id: I71571607804b5ac1d2cd7a77cf34e89b1ec4af03
",git fetch https://review.opendev.org/openstack/tacker-specs refs/changes/44/886944/4 && git format-patch -1 --stdout FETCH_HEAD,['specs/2023.1/enhance-tacker-policy.rst'],1,97227437a0201306d4a93117ca1a73f65b01e28d,fix-enhance-policy," * - tenant - {""tenant"": ""default""} * - tenant - {""tenant"": ""default""} * - TENANT - tenant value - TENANT_default, TENANT_all * - TENANT - tenant value - TENANT_default -> {""tenant"": [""default""]} * - TENANT - tenant value - {""tenant"": ""default""} -> {""tenant"": [""default""]} as area, vendor and tenant. At this time, if the enhanced policy ""vnflcm_attrs_cmp"": ""area:%(area)s and vendor:%(vendor)s and tenant:%(tenant)s""* TENANT_default * TENANT_tenant_A * TENANT_all* TENANT_all* TENANT_all* TENANT_all* TENANT_all* TENANT_all* TENANT_all* TENANT_all"," * - namespace(CNF) - {""namespace"": ""default""} * - namespace(CNF) - {""namespace"": ""default""} * - NAMESPACE - namespace value - NAMESPACE_default, NAMESPACE_all * - NAMESPACE - namespace value - NAMESPACE_default -> {""namespace"": [""default""]} * - NAMESPACE - namespace value - {""namespace"": ""default""} -> {""namespace"": [""default""]} as area, vendor and namespace(CNF). At this time, if the enhanced policy ""vnflcm_attrs_cmp"": ""area:%(area)s and vendor:%(vendor)s and namespace:%(namespace)s""* NAMESPACE_default * NAMESPACE_namespace_A * NAMESPACE_all* NAMESPACE_all* NAMESPACE_all* NAMESPACE_all* NAMESPACE_all* NAMESPACE_all* NAMESPACE_all* NAMESPACE_all",25,26
openstack%2Ftacker-specs~master~I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1,openstack/tacker-specs,master,I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1,Add sphinx plantUML pluging,MERGED,2023-03-27 06:11:00.000000000,2023-07-10 17:25:47.000000000,2023-07-10 17:25:47.000000000,"[{'_account_id': 17255}, {'_account_id': 22348}, {'_account_id': 25701}, {'_account_id': 31857}, {'_account_id': 32102}]","[{'number': 1, 'created': '2023-03-27 06:11:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/c5306c6105a6a27fdaa06dec0a7a9f5d4e6fc590', 'message': '[DNM] Add plantUML\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 2, 'created': '2023-03-27 06:47:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/109132688ece06fac9f4e4c3636f83380f2fc2af', 'message': '[DNM] Add plantUML\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 3, 'created': '2023-03-27 07:04:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/f8718f3f553c751566f2fb97aa5ea0b3b0780117', 'message': '[DNM] Add plantUML\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 4, 'created': '2023-03-27 07:51:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/08b7dce36bf43b6b1441d717088df39301c7b7f8', 'message': '[DNM] Add plantUML\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 5, 'created': '2023-03-27 09:08:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/feebd8d7f751e4ca02e194e735ba739f8a80dba0', 'message': '[DNM] Add plantUML\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 6, 'created': '2023-03-27 09:30:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/a43812e25da309e024845e1297bdeda544517704', 'message': '[DNM] Add plantUML\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 7, 'created': '2023-03-27 10:18:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/8ff398ab5fb6232699b2305dec085313a01a221d', 'message': '[DNM] Add plantUML\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 8, 'created': '2023-03-27 10:28:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/5cbb5411972a21c28cde0901ba7f045c99edc979', 'message': '[DNM] Add plantUML\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 9, 'created': '2023-03-27 10:38:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/8b9f1e5421f1634aa637ea51c5c59bd0aeb2a547', 'message': '[DNM] Add plantUML\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 10, 'created': '2023-03-27 11:12:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/d452852375f5ffa59ace4a63311261eb8ea73ac9', 'message': '[DNM] Add plantUML\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 11, 'created': '2023-03-27 11:41:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/c1dafc582038b82b6ead15e5dedbfad08ae1b2e8', 'message': '[DNM] Add plantUML\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 12, 'created': '2023-03-27 11:59:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/cb31cd31d4f43daae5585ff28757a1e6c4ba6047', 'message': '[DNM] Add plantUML\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 13, 'created': '2023-03-28 03:05:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/fcd56911148c8d6a3645b486b853d5d0b3b31e68', 'message': 'Add sphinx plantUML pluging\n\nThis patch enables the sphinx PlantUML extension. As PlantUML is often\nused in Tacker documents, authors of specs can re-use diagrams when\nthey write user guides with small revising.\n\nTo build plantUML with a sphinx PlantUML extension:\n- Updated conf.py\n- Updated requirements\n- Added plantuml.jar\n\nTo use the sphinx PlantUML extension in Zuul jobs:\n- Added bindep [1][2] to install graphviz\n- Added pre.yaml to install java [3]\n- Updated setup.cfg (this is necessary to place more than two\n  directories at the project root)\n\nOthers:\n- Added examples to 2023.2/placeholder.rst and template.rst\n\n[1] https://github.com/openstack/cinder-specs/blob/master/bindep.txt\n[2] https://docs.opendev.org/opendev/bindep/latest/readme.html\n[3] https://opendev.org/zuul/zuul-jobs/src/branch/master/roles/ensure-java\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}, {'number': 14, 'created': '2023-07-09 13:15:05.000000000', 'files': ['tools/plantuml.jar', 'bindep.txt', 'specs/template.rst', 'specs/2023.2/placeholder.rst', 'playbooks/pre.yaml', 'README.rst', '.zuul.yaml', 'doc/source/conf.py', 'doc/requirements.txt', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/3779761ee4cfd59798d6ca6c800330d12d0dafa6', 'message': 'Add sphinx plantUML pluging\n\nThis patch enables the sphinx PlantUML extension. As PlantUML is often\nused in Tacker documents, authors of specs can re-use diagrams when\nthey write user guides with small revising.\n\nTo build plantUML with a sphinx PlantUML extension:\n- Updated conf.py\n- Updated requirements\n- Added plantuml.jar\n\nTo use the sphinx PlantUML extension in Zuul jobs:\n- Added bindep [1][2] to install graphviz\n- Added pre.yaml to install java [3]\n- Updated setup.cfg (this is necessary to place more than two\n  directories at the project root)\n\nOthers:\n- Added examples to 2023.2/placeholder.rst and template.rst\n\n[1] https://github.com/openstack/cinder-specs/blob/master/bindep.txt\n[2] https://docs.opendev.org/opendev/bindep/latest/readme.html\n[3] https://opendev.org/zuul/zuul-jobs/src/branch/master/roles/ensure-java\n\nChange-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1\n'}]",11,878615,3779761ee4cfd59798d6ca6c800330d12d0dafa6,41,5,14,33455,,,0,"Add sphinx plantUML pluging

This patch enables the sphinx PlantUML extension. As PlantUML is often
used in Tacker documents, authors of specs can re-use diagrams when
they write user guides with small revising.

To build plantUML with a sphinx PlantUML extension:
- Updated conf.py
- Updated requirements
- Added plantuml.jar

To use the sphinx PlantUML extension in Zuul jobs:
- Added bindep [1][2] to install graphviz
- Added pre.yaml to install java [3]
- Updated setup.cfg (this is necessary to place more than two
  directories at the project root)

Others:
- Added examples to 2023.2/placeholder.rst and template.rst

[1] https://github.com/openstack/cinder-specs/blob/master/bindep.txt
[2] https://docs.opendev.org/opendev/bindep/latest/readme.html
[3] https://opendev.org/zuul/zuul-jobs/src/branch/master/roles/ensure-java

Change-Id: I72c2c7887ceba4734b367f7e5bc78921eb0ab0a1
",git fetch https://review.opendev.org/openstack/tacker-specs refs/changes/15/878615/4 && git format-patch -1 --stdout FETCH_HEAD,"['specs/ocata/nsd-support.rst', 'specs/xena/test_sol_with_robot_api_tests.rst', 'specs/victoria/support-notification-api-based-on-etsi-nfv-sol.rst', 'specs/wallaby/mgmt-driver-for-k8s-cluster.rst', 'specs/train/tacker-studio.rst', 'specs/2023.1/support-threshold-pm-interface.rst', 'specs/mitaka/tosca-parser-integration.rst', 'specs/2023.1/vnfm-autoheal-and-autoscale.rst', 'specs/ussuri/enhance_vnf_package_support.rst', 'specs/queens/zabbix-plugin.rst', 'specs/victoria/support-vnf-update-api-based-on-etsi-nfv-sol.rst', 'specs/victoria/add-event-alarm-policy.rst', 'specs/zed/db-migration-tool.rst', 'specs/wallaby/support-cnf-scale.rst', 'specs/pike/vnf-cluster-management-by-senlin.rst', 'specs/xena/index.rst', 'specs/yoga/support-heal-scale-in-user_lcm.rst', 'specs/zed/centos-stream-zuul.rst', 'specs/2023.1/enhance_placement_process.rst', 'specs/pike/encryption-with-barbican.rst', 'specs/wallaby/support-fundamental-vnf-lcm-based-on-ETSI-NFV.rst', 'specs/rocky/vdu-affinity-policy.rst', 'specs/xena/support-nfv-solv3-get-information.rst', 'specs/pike/python-openstackclient.rst', 'specs/yoga/prometheus-plugin-heal.rst', 'specs/victoria/add-artifacts.rst', 'specs/wallaby/support-change-external-VNF-connectivity-operation.rst', 'specs/wallaby/support-cnf-heal.rst', 'specs/2023.1/support-multi-conductors-onboarding.rst', 'specs/pike/persistent-block-storage.rst', 'specs/xena/support-nfv-solv3-start-and-terminate-vnf.rst', 'specs/victoria/index.rst', 'specs/2023.2/placeholder.rst', 'specs/zed/enhance_change_current_vnf_package_API.rst', 'specs/mitaka/enhanced-placement.rst', 'specs/2023.1/improving-mgmt-driver-log.rst', 'specs/victoria/enhancement_enhance-vnf-lcm-api-support.rst', 'specs/newton/index.rst', 'specs/xena/k8s-mgmtdriver-kubespray.rst', 'specs/zed/enhancement-container-update.rst', 'specs/pike/vnffg-scaling.rst', 'specs/mitaka/multi-site-feature.rst', 'specs/victoria/use_robot_api_tests.rst', 'specs/zed/support-v2-cnf-rollback.rst', 'specs/ussuri/vnf_parameter_update.rst', 'specs/yoga/add-sample-ansible-mgmt-driver.rst', 'specs/yoga/support-nfv-solv3-error-handling.rst', 'specs/zed/prometheus-plugin-autoheal-and-autoscale.rst', 'specs/ocata/tacker-API-framework.rst', 'specs/liberty/monitor-framework.rst', 'specs/train/vnf-rolling-upgrade.rst', 'specs/yoga/vim-monitor-feature.rst', 'specs/zed/enhance-multi-tenant-policy.rst', 'specs/ocata/tacker-vnfc.rst', 'specs/2023.1/enhance-tacker-policy.rst', 'specs/yoga/enhance-nfv-solv3-lcm-operation.rst', 'specs/ussuri/lcm-operation-with-lcm-operation-user-data.rst', 'specs/zed/individual-vnfc-management.rst', '.zuul.yaml', 'specs/victoria/support-scale-api-based-on-etsi-nfv-sol.rst', 'specs/zed/code-refactoring.rst', 'specs/pike/vnffg-autohealing.rst', 'specs/newton/alarm-based-monitoring-driver.rst', 'doc/source/index.rst', 'specs/yoga/index.rst', 'specs/queens/update-vnffg.rst', 'specs/zed/database-synchronization.rst', 'specs/wallaby/support-error-handling-based-on-ETSI-NFV.rst', 'specs/yoga/container-update.rst', 'specs/wallaby/mgmt-driver-for-k8s-heal.rst', 'specs/zed/enhance-cnf-operations.rst', 'specs/zed/support-v2-cnf-scale.rst', 'specs/2023.1/support-tacker-db-manage-postgresql.rst', 'specs/newton/manual-and-auto-scaling.rst', 'specs/ocata/vnf-inline-template.rst', 'specs/yoga/k8s-namespace.rst', 'specs/zed/enhance-cli-for-paging.rst', 'specs/stein/reservation-vnfm.rst', 'specs/train/index.rst', 'specs/queens/Kubernetes-as-VIM.rst', 'specs/ocata/index.rst', 'specs/ussuri/index.rst', 'specs/stein/index.rst', 'specs/xena/multi-version-api.rst', 'specs/xena/helmchart-k8s-vim.rst', 'specs/zed/faultnotification-autoheal.rst', 'specs/newton/tacker-vnffg.rst', 'specs/wallaby/mgmt-driver-for-k8s-scale.rst', 'specs/newton/event_logging.rst', 'specs/xena/cir-k8s-cluster.rst', 'specs/rocky/shared_vim_for_policy_action.rst', 'specs/victoria/action_driver.rst', 'specs/liberty/index.rst', 'specs/yoga/add-vnf-package-sample-for-practical-use-cases.rst', 'specs/ussuri/etsi-nfv-sol-rest-api-for-VNF-deployment.rst', 'specs/yoga/upgrade-vnf-package.rst', 'specs/wallaby/hardware-aware-pod-affinity.rst', 'specs/train/vnf_package_support.rst', 'specs/zed/index.rst', 'specs/victoria/support-sol003-vnfm-operations.rst', 'specs/queens/kubernetes-type-for-containerized-VNF.rst', 'specs/yoga/multi-tenant-policy.rst', 'specs/wallaby/mgmt-driver-for-ha-k8s.rst', 'specs/mitaka/index.rst', 'specs/yoga/paging-query-result.rst', 'specs/train/multi-interface-container.rst', 'specs/2023.1/add-sample-coordination.rst', 'specs/template.rst', 'specs/pike/index.rst', 'specs/victoria/support-etsi-nfv-based-errorhandling.rst', 'specs/2023.1/srbac-implement-project-personas.rst', 'specs/zed/support_multi_artifact_of_ansible_driver.rst', 'specs/rocky/index.rst', 'specs/rocky/vnffg-ns.rst', 'specs/zed/support-v2-cnf-heal.rst', 'specs/pike/mistral_vim_monitor.rst', 'specs/mitaka/automatic-resource-creation.rst', 'specs/queens/index.rst', 'specs/newton/tacker-networking-sfc.rst', 'doc/source/conf.py', 'specs/stein/vdu-auto-healing.rst', 'doc/source/plantuml.jar', 'specs/pike/mistral_vnf_monitor_policies.rst', 'specs/xena/pv-k8s-cluster.rst', 'specs/wallaby/index.rst', 'specs/zed/support-openid-k8s-vim.rst', 'specs/stein/support-force-delete.rst', 'specs/victoria/container-network-function.rst', 'specs/xena/support-hot-according-to-nfv-sol014.rst', 'specs/2023.1/index.rst', 'doc/requirements.txt', 'specs/liberty/tacker-api-mano.rst']",132,c5306c6105a6a27fdaa06dec0a7a9f5d4e6fc590,plantuml,,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Implement Tacker API v1 based on NFV MANO ========================================== https://blueprints.launchpad.net/tacker/+spec/tacker-api-mano This spec describes the plan to introduce new Tacker REST API endpoints based on ETSI NFV MANO standards [1]. The current REST API endpoints based on 'servicevm' standards will be retained for backward compatibility and support. Problem description =================== Tacker service currently implements REST API endpoints based on 'servicevm' standards. However, Tacker is built on principles of an NFV orchestrator with in-built VNF Manager as described in the ETSI NFV MANO architecture [1]. Tacker should support and implement CRUD operations on VNF resources. Towards this, REST API endpoints based on VNF has to be introduced which can then be invoked by a user using an independent client or through the python-tackerclient itself. Proposed change =============== The actual task of moving Tacker from 'servicevm' to NFV MANO standards in entirety is complex and will be done in phases. As part of this spec, the task concerning REST API endpoints based on NFV MANO will be introduced and implemented. The proposed changes involve the following action items: * Tacker REST API extension will be moved from 'servicevm' to 'vnfm'. * Add two new REST API end points 'vnf' and 'vnfd' to describe VNF resources. * The exiting resources 'device' and 'device_template' will be moved under the new 'vnfm' extension. * The implementation of 'vnfm' REST API will be a wrapper around existing 'servicevm' implementation. * The 'vnfm' resource attributes will be the same as 'servicevm' resources except for 'services' attribute which is not being used in the project currently. The new 'vnfm' extension will be integrated to Tacker v1 REST API. The current implementation of 'servicevm' extension will be retained for backward compatibility. Alternatives ------------ Other solution is to integrate the Pecan framework in to Tacker. This involves re-factoring the entire project in one phase to update to NFV MANO standards. Currently, tacker is based on stable Kilo release branch and does not support Pecan framework. This solution can only be implemented when Tacker moves to master release for OpenStack services. The high-level tasks include: * Modify plugin and database backend implementation to move from servicevm to NFV MANO standards. * Move out of home grown REST framework and implement the Pecan framework to describe and implement CRUD operations on VNF resources. Data model impact ----------------- None REST API impact --------------- New extension 'vnfm' will be introduced in v1 which will implement REST API end points as described below: **/vnfd** :: +---------------------------------------------------------------------------+ |Attribute |Type |Access |Default |Validation/ |Description | |Name | | |Value |Conversion | | +---------------------------------------------------------------------------+ |id |string |RO, All |generated |N/A |identity | | |(UUID) | | | | | +---------------------------------------------------------------------------+ |name |string |RW, All |'' |string |human+readable | | | | | | |name | +---------------------------------------------------------------------------+ |description |string |RW, All |'' |string |description of | | | | | | |template | +---------------------------------------------------------------------------+ |attributes |dict |RW, All |None |dict |TOSCA YAML file | | | | | | | | +---------------------------------------------------------------------------+ |infra_driver |string |RW, All |heat |string |driver to provision| | | | | | |VNF | +---------------------------------------------------------------------------+ |mgmt_driver |string |RW All |noop |string |driver to configure| | | | | | |VNF | +---------------------------------------------------------------------------+ |service_types |list |RW, All |[] |service_type|NFV service type | | | | | |_list |(VNF, NSD) | +---------------------------------------------------------------------------+ |tenant_id |string |RO, All |N/A |string |project id to | | | | | | |launch VNF | +--------------+-------+--------+----------+--------------------------------+ **/vnf** :: +----------------------------------------------------------------------------+ |Attribute |Type |Access |Default |Validation/ |Description | |Name | | |Value |Conversion | | +----------------------------------------------------------------------------+ |id |string |RO, All |generated |N/A |identity | | |(UUID) | | | | | +----------------------------------------------------------------------------+ |name |string |RW, All |'' |string |human+readable | | | | | | |name | +----------------------------------------------------------------------------+ |description |string |RW, All |'' |string |description of | | | | | | |template | +----------------------------------------------------------------------------+ |attributes |dict |RW, All |None |dict |TOSCA YAML file | | | | | | | | +----------------------------------------------------------------------------+ |instance_id |string |RO, All |generated |string |identity of | | | | | | |VM instance | +----------------------------------------------------------------------------+ |mgmt_url |string |RO, All |None |string |IP address of | | | | | | |VNF management net. | +----------------------------------------------------------------------------+ |tenant_id |string |RW, All |generated |string |project id to | | | | | | |launch VNF | +----------------------------------------------------------------------------+ |template_id |string |RW, All |None |string |VNFD id | | | | | | | | +----------------------------------------------------------------------------+ |status |string |RO, All |generated |string |current state | | | | | | |of VNF | +--------------+-------+--------+----------+---------------------------------+ |service_ |list |RW, All |[] |service_ |VNF role for a given| |contexts | | | |context_list|network | +--------------+-------+--------+----------+---------------------------------+ Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- There will be no direct impact on python-tackerclient in the way the user will interact with the client. With the current implementation, VNF resource requests were internally forwarded to 'servicevm' resource requests. However, with the new implementation, python-tackerclient will directly invoke the 'vnfm' REST API for VNF resource requests. Performance Impact ------------------ None Other deployer impact --------------------- None Developer impact ---------------- None Implementation ============== Assignee(s) ----------- Primary assignee: sseetha Other contributors: None Work Items ---------- 1. Add new extension 'vnfm' to tacker v1 and deprecate the existing 'servicevm' extension. 2. The new extension should internally call servicevm plugin base. 3. Modify VNFM API requests from tackerclient to reflect VNF resources in request body and remove the current wrapper implementation around 'servicevm'. 4. Add unit tests for the new extension and contribute to existing API related test cases. 5. Add REST api doc file that will capture the 'vnfm' extension in detail. Dependencies ============ None Testing ======= As of now, there are no tempest tests added to Tacker and will be tracked as a separate activity. Documentation Impact ==================== A documentation page capturing the new REST API VNF v1 resources will be added in Tacker wiki link [2]. References ========== [1] http://www.ietf.org/proceedings/88/slides/slides-88-opsawg-6.pdf [2] https://wiki.openstack.org/wiki/Tacker/API ",22,60557
openstack%2Fneutron~master~Iad73856cb86b04f8d2a10b186befa4aa8c6a933d,openstack/neutron,master,Iad73856cb86b04f8d2a10b186befa4aa8c6a933d,Return back the test_dvr_router_interface_mtu_update test case,MERGED,2023-06-19 15:16:48.000000000,2023-07-10 17:18:13.000000000,2023-07-10 17:16:57.000000000,"[{'_account_id': 5948}, {'_account_id': 8313}, {'_account_id': 9845}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-19 15:16:48.000000000', 'files': ['neutron/tests/functional/agent/l3/test_dvr_router.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/57e860ca1996d4afa8ede87239b53395ee55ef03', 'message': ""Return back the test_dvr_router_interface_mtu_update test case\n\nThis patch is actually a partial cherry-pick of the commit\nb5dd6efdca0aa6e7405a55d66c7042a49ec72214 by Slawek Kaplonski.\nBy some chance the commit 3a9a17ad8216d039ca3dadce0e8fc160f3ec18ba\nremoves this test case, although it shoudldn't\n\nRelated-Bug: #1933273\nRelated-Bug: #2024381\n\n(partially cherry picked from commit\nb5dd6efdca0aa6e7405a55d66c7042a49ec72214)\n\nChange-Id: Iad73856cb86b04f8d2a10b186befa4aa8c6a933d\n""}]",2,886408,57e860ca1996d4afa8ede87239b53395ee55ef03,12,5,1,28722,,,0,"Return back the test_dvr_router_interface_mtu_update test case

This patch is actually a partial cherry-pick of the commit
b5dd6efdca0aa6e7405a55d66c7042a49ec72214 by Slawek Kaplonski.
By some chance the commit 3a9a17ad8216d039ca3dadce0e8fc160f3ec18ba
removes this test case, although it shoudldn't

Related-Bug: #1933273
Related-Bug: #2024381

(partially cherry picked from commit
b5dd6efdca0aa6e7405a55d66c7042a49ec72214)

Change-Id: Iad73856cb86b04f8d2a10b186befa4aa8c6a933d
",git fetch https://review.opendev.org/openstack/neutron refs/changes/08/886408/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/functional/agent/l3/test_dvr_router.py'],1,57e860ca1996d4afa8ede87239b53395ee55ef03,bug/2024381," def _test_router_interface_mtu_update(self, ha): original_mtu = 1450 router_info = self.generate_dvr_router_info( enable_ha=ha, enable_snat=True) router_info['_interfaces'][0]['mtu'] = original_mtu router_info['gw_port']['mtu'] = original_mtu router_info[lib_constants.SNAT_ROUTER_INTF_KEY][0]['mtu'] = ( original_mtu) router = self.manage_router(self.agent, router_info) if ha: utils.wait_until_true(lambda: router.ha_state == 'primary') # Keepalived notifies of a state transition when it starts, # not when it ends. Thus, we have to wait until keepalived finishes # configuring everything. We verify this by waiting until the last # device has an IP address. device = router.router[lib_constants.INTERFACE_KEY][-1] device_exists = functools.partial( self.device_exists_with_ips_and_mac, device, router.get_internal_device_name, router.ns_name) utils.wait_until_true(device_exists) interface_name = router.get_internal_device_name( router_info['_interfaces'][0]['id']) gw_interface_name = router.get_external_device_name( router_info['gw_port']['id']) snat_internal_port = router_info[lib_constants.SNAT_ROUTER_INTF_KEY] snat_interface_name = router._get_snat_int_device_name( snat_internal_port[0]['id']) snat_namespace = dvr_snat_ns.SnatNamespace.get_snat_ns_name( router_info['id']) self.assertEqual( original_mtu, ip_lib.IPDevice(interface_name, router.ns_name).link.mtu) self.assertEqual( original_mtu, ip_lib.IPDevice(gw_interface_name, snat_namespace).link.mtu) self.assertEqual( original_mtu, ip_lib.IPDevice(snat_interface_name, snat_namespace).link.mtu) updated_mtu = original_mtu + 1 router_info_copy = copy.deepcopy(router_info) router_info_copy['_interfaces'][0]['mtu'] = updated_mtu router_info_copy['gw_port']['mtu'] = updated_mtu router_info_copy[lib_constants.SNAT_ROUTER_INTF_KEY][0]['mtu'] = ( updated_mtu) self.agent._process_updated_router(router_info_copy) self.assertEqual( updated_mtu, ip_lib.IPDevice(interface_name, router.ns_name).link.mtu) self.assertEqual( updated_mtu, ip_lib.IPDevice(gw_interface_name, snat_namespace).link.mtu) self.assertEqual( updated_mtu, ip_lib.IPDevice(snat_interface_name, snat_namespace).link.mtu) def test_dvr_router_interface_mtu_update(self): self._test_router_interface_mtu_update(ha=False)",,66,0
openstack%2Foctavia-tempest-plugin~master~Ia8f73e9fa07cbfaea5024047c650dfe5ca747420,openstack/octavia-tempest-plugin,master,Ia8f73e9fa07cbfaea5024047c650dfe5ca747420,Add stable/2023.1 jobs on master gate,MERGED,2023-06-09 07:25:51.000000000,2023-07-10 16:34:35.000000000,2023-07-10 16:32:45.000000000,"[{'_account_id': 22348}, {'_account_id': 29244}, {'_account_id': 34429}]","[{'number': 1, 'created': '2023-06-09 07:25:51.000000000', 'files': ['zuul.d/projects.yaml', 'zuul.d/jobs.yaml'], 'web_link': 'https://opendev.org/openstack/octavia-tempest-plugin/commit/63b5c60964578d2214ab6bcaa6c9bca733239263', 'message': 'Add stable/2023.1 jobs on master gate\n\nAs 2023.1 is released, we should add its job on master\ngate to keep branchless tempest plugins compatible\nto stable branch.\n\nRef: Tempest plugins guide for stable branch testing:\n- https://docs.openstack.org/tempest/latest/stable_branch_testing_policy.html\n\nChange-Id: Ia8f73e9fa07cbfaea5024047c650dfe5ca747420\n'}]",2,885723,63b5c60964578d2214ab6bcaa6c9bca733239263,12,3,1,8556,,,0,"Add stable/2023.1 jobs on master gate

As 2023.1 is released, we should add its job on master
gate to keep branchless tempest plugins compatible
to stable branch.

Ref: Tempest plugins guide for stable branch testing:
- https://docs.openstack.org/tempest/latest/stable_branch_testing_policy.html

Change-Id: Ia8f73e9fa07cbfaea5024047c650dfe5ca747420
",git fetch https://review.opendev.org/openstack/octavia-tempest-plugin refs/changes/23/885723/1 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/projects.yaml', 'zuul.d/jobs.yaml']",2,63b5c60964578d2214ab6bcaa6c9bca733239263,2023-1-stable-job, name: octavia-v2-dsvm-noop-api-stable-2023-1 parent: octavia-v2-dsvm-noop-api nodeset: octavia-single-node-ubuntu-jammy override-checkout: stable/2023.1 - job: name: octavia-v2-dsvm-scenario-stable-2023-1 parent: octavia-v2-dsvm-scenario nodeset: octavia-single-node-ubuntu-jammy override-checkout: stable/2023.1 - job: name: octavia-v2-dsvm-scenario-traffic-ops-stable-2023-1 parent: octavia-v2-dsvm-scenario-stable-2023-1 vars: tempest_test_regex: ^octavia_tempest_plugin.tests.scenario.v2.*traffic_ops - job: name: octavia-v2-dsvm-scenario-non-traffic-ops-stable-2023-1 parent: octavia-v2-dsvm-scenario-stable-2023-1 vars: tempest_test_regex: ^octavia_tempest_plugin.tests.scenario.v2.(?!.*traffic_ops) - job: name: octavia-v2-dsvm-tls-barbican-stable-2023-1 parent: octavia-v2-dsvm-tls-barbican nodeset: octavia-single-node-ubuntu-jammy override-checkout: stable/2023-1 - job: name: octavia-v2-act-stdby-dsvm-scenario-stable-2023-1 parent: octavia-v2-act-stdby-dsvm-scenario nodeset: octavia-single-node-ubuntu-jammy override-checkout: stable/2023-1 - job:,,46,0
openstack%2Foctavia-tempest-plugin~master~Id851d5a5388e290770c617267daa9fdd0a50dae6,openstack/octavia-tempest-plugin,master,Id851d5a5388e290770c617267daa9fdd0a50dae6,Adding jobs for stable/zed,MERGED,2022-11-24 08:33:17.000000000,2023-07-10 16:25:50.000000000,2023-07-10 16:23:54.000000000,"[{'_account_id': 22348}, {'_account_id': 29244}, {'_account_id': 31664}, {'_account_id': 32761}, {'_account_id': 34429}]","[{'number': 1, 'created': '2022-11-24 08:33:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia-tempest-plugin/commit/e0be9a3cc3164cc5445ad4c75f554077339a9019', 'message': 'Adding jobs for stable/zed\n\nChange-Id: Id851d5a5388e290770c617267daa9fdd0a50dae6\n'}, {'number': 2, 'created': '2023-06-09 07:16:50.000000000', 'files': ['zuul.d/projects.yaml', 'zuul.d/jobs.yaml'], 'web_link': 'https://opendev.org/openstack/octavia-tempest-plugin/commit/c45b5546e42f590556d1abbff322def7c16c4f03', 'message': 'Adding jobs for stable/zed\n\nChange-Id: Id851d5a5388e290770c617267daa9fdd0a50dae6\n'}]",8,865513,c45b5546e42f590556d1abbff322def7c16c4f03,28,5,2,29244,,,0,"Adding jobs for stable/zed

Change-Id: Id851d5a5388e290770c617267daa9fdd0a50dae6
",git fetch https://review.opendev.org/openstack/octavia-tempest-plugin refs/changes/13/865513/1 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/projects.yaml', 'zuul.d/jobs.yaml']",2,e0be9a3cc3164cc5445ad4c75f554077339a9019,, name: octavia-v2-dsvm-noop-api-stable-zed parent: octavia-v2-dsvm-noop-api nodeset: octavia-single-node-ubuntu-focal override-checkout: stable/zed - job: name: octavia-v2-dsvm-scenario-stable-zed parent: octavia-v2-dsvm-scenario nodeset: octavia-single-node-ubuntu-focal override-checkout: stable/zed - job: name: octavia-v2-dsvm-tls-barbican-stable-zed parent: octavia-v2-dsvm-tls-barbican nodeset: octavia-single-node-ubuntu-focal override-checkout: stable/zed - job: name: octavia-v2-act-stdby-dsvm-scenario-stable-zed parent: octavia-v2-act-stdby-dsvm-scenario nodeset: octavia-single-node-ubuntu-focal override-checkout: stable/zed - job:,,32,0
openstack%2Frequirements~master~Ida9dff8453016c7dacd7259fc663ae42282192c3,openstack/requirements,master,Ida9dff8453016c7dacd7259fc663ae42282192c3,update constraint for os_vif to new release 3.2.0,NEW,2023-07-07 14:38:12.000000000,2023-07-10 15:53:41.000000000,,"[{'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-07 14:38:12.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/22248b174fb1ebd02c62791469fd351085c9a181', 'message': 'update constraint for os_vif to new release 3.2.0\n\nmeta: version: 3.2.0\nmeta: diff-start: -\nmeta: series: bobcat\nmeta: branch: master\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: yes\nmeta: release:Author: Elod Illes <elod.illes@est.tech>\nmeta: release:Commit: Elod Illes <elod.illes@est.tech>\nmeta: release:Change-Id: I15a4e9c8d036d2942daf2d749bd430643d0d0dcc\nmeta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>\nmeta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>\nmeta: release:Code-Review+1: Amit Uniyal <auniyal@redhat.com>\nmeta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>\nmeta: release:Code-Review+1: sean mooney <smooney@redhat.com>\nChange-Id: Ida9dff8453016c7dacd7259fc663ae42282192c3\n'}]",1,887962,22248b174fb1ebd02c62791469fd351085c9a181,8,3,1,11131,,,0,"update constraint for os_vif to new release 3.2.0

meta: version: 3.2.0
meta: diff-start: -
meta: series: bobcat
meta: branch: master
meta: release-type: release
meta: pypi: yes
meta: first: yes
meta: release:Author: Elod Illes <elod.illes@est.tech>
meta: release:Commit: Elod Illes <elod.illes@est.tech>
meta: release:Change-Id: I15a4e9c8d036d2942daf2d749bd430643d0d0dcc
meta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>
meta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>
meta: release:Code-Review+1: Amit Uniyal <auniyal@redhat.com>
meta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>
meta: release:Code-Review+1: sean mooney <smooney@redhat.com>
Change-Id: Ida9dff8453016c7dacd7259fc663ae42282192c3
",git fetch https://review.opendev.org/openstack/requirements refs/changes/62/887962/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,22248b174fb1ebd02c62791469fd351085c9a181,new-release,os-vif===3.2.0,os-vif===3.1.1,1,1
openstack%2Fnetworking-generic-switch~master~Ic0ef1d3f917f3b1e666e5f1b87ae13457663330c,openstack/networking-generic-switch,master,Ic0ef1d3f917f3b1e666e5f1b87ae13457663330c,WIP: Add VLAN and Port whitelist,NEW,2023-07-10 14:16:24.000000000,2023-07-10 15:48:17.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-10 14:16:24.000000000', 'files': ['networking_generic_switch/tests/unit/test_devices.py', 'networking_generic_switch/devices/__init__.py', 'networking_generic_switch/generic_switch_mech.py'], 'web_link': 'https://opendev.org/openstack/networking-generic-switch/commit/ee25d9cd220fd2f440fd58a2ba7727f26c23390c', 'message': 'WIP: Add VLAN and Port whitelist\n\nAdds support for a VLAN and port whitelist for high security\nenvironments to be able to restrict Ironic activities only to ports\nwhich are attached to nodes (or any other restriction desired and\nconfigured).\n\nTODO: Documentation for this feature is needed\n\nCo-authored-by: John Garbutt <John.Garbutt@gresearch.co.uk>\nCo-authored-by: Andrew Ellard <andrew.ellard@gresearch.co.uk>\nCo-authored-by: Doug Szumski <doug@stackhpc.com>\nChange-Id: Ic0ef1d3f917f3b1e666e5f1b87ae13457663330c\n'}]",0,888047,ee25d9cd220fd2f440fd58a2ba7727f26c23390c,2,1,1,10342,,,0,"WIP: Add VLAN and Port whitelist

Adds support for a VLAN and port whitelist for high security
environments to be able to restrict Ironic activities only to ports
which are attached to nodes (or any other restriction desired and
configured).

TODO: Documentation for this feature is needed

Co-authored-by: John Garbutt <John.Garbutt@gresearch.co.uk>
Co-authored-by: Andrew Ellard <andrew.ellard@gresearch.co.uk>
Co-authored-by: Doug Szumski <doug@stackhpc.com>
Change-Id: Ic0ef1d3f917f3b1e666e5f1b87ae13457663330c
",git fetch https://review.opendev.org/openstack/networking-generic-switch refs/changes/47/888047/1 && git format-patch -1 --stdout FETCH_HEAD,"['networking_generic_switch/tests/unit/test_devices.py', 'networking_generic_switch/devices/__init__.py', 'networking_generic_switch/generic_switch_mech.py']",3,ee25d9cd220fd2f440fd58a2ba7727f26c23390c,," # Fail if port or vlan not in allow list if not switch.is_allowed(port_id, segmentation_id): LOG.warn(""Skipped binding port %(port_id)s, "" ""port %(port)s in segment "" ""%(segment_id)s on device %(device)s, as either "" ""the port or vlan is not on the allow list"", {'port_id': port['id'], 'port': port_id, 'device': switch_info, 'segment_id': segmentation_id}) return ",,98,1
openstack%2Fmanila-tempest-plugin~master~Ibca3fa93ac6ee382baa3fad256ea438d6596608b,openstack/manila-tempest-plugin,master,Ibca3fa93ac6ee382baa3fad256ea438d6596608b,Replication: verify access rules in detail,NEW,2022-12-23 10:01:59.000000000,2023-07-10 15:36:33.000000000,,"[{'_account_id': 16643}, {'_account_id': 22348}, {'_account_id': 29632}]","[{'number': 1, 'created': '2022-12-23 10:01:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-tempest-plugin/commit/d72fa7f0c8915865274991a016b7e3e070dc759a', 'message': 'Replication: verify access rules in detail\n\nRelated-bug: #2000253\nChange-Id: Ibca3fa93ac6ee382baa3fad256ea438d6596608b\n'}, {'number': 2, 'created': '2022-12-23 11:58:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-tempest-plugin/commit/99df094dea774c2b249e7ff63ade66ca8dc29efc', 'message': 'Replication: verify access rules in detail\n\nRelated-bug: #2000253\nDepends-On: Ie7ddd9f631510ba97e92a1eb0eb9a5d944ec1b3b\nChange-Id: Ibca3fa93ac6ee382baa3fad256ea438d6596608b\n'}, {'number': 3, 'created': '2023-01-05 09:42:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-tempest-plugin/commit/a8320a4972049a1b0da7cc878350d0e42df0510b', 'message': 'Replication: verify access rules in detail\n\nRelated-bug: #2000253\nDepends-On: Ie7ddd9f631510ba97e92a1eb0eb9a5d944ec1b3b\nChange-Id: Ibca3fa93ac6ee382baa3fad256ea438d6596608b\n'}, {'number': 4, 'created': '2023-04-13 18:36:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-tempest-plugin/commit/e64324fbfda18b6ce8b842d09fab42294275334e', 'message': 'Replication: verify access rules in detail\n\nRelated-bug: #2000253\nChange-Id: Ibca3fa93ac6ee382baa3fad256ea438d6596608b\n'}, {'number': 5, 'created': '2023-04-13 18:37:50.000000000', 'files': ['manila_tempest_tests/tests/api/test_replication.py'], 'web_link': 'https://opendev.org/openstack/manila-tempest-plugin/commit/0db023abcec7d20b2491d6d770dd6fff69813e6e', 'message': 'Replication: verify access rules in detail\n\nRelated-bug: #2000253\nDepends-On: Ie7ddd9f631510ba97e92a1eb0eb9a5d944ec1b3b\nChange-Id: Ibca3fa93ac6ee382baa3fad256ea438d6596608b\n'}]",9,868340,0db023abcec7d20b2491d6d770dd6fff69813e6e,27,3,5,18816,,,0,"Replication: verify access rules in detail

Related-bug: #2000253
Depends-On: Ie7ddd9f631510ba97e92a1eb0eb9a5d944ec1b3b
Change-Id: Ibca3fa93ac6ee382baa3fad256ea438d6596608b
",git fetch https://review.opendev.org/openstack/manila-tempest-plugin refs/changes/40/868340/4 && git format-patch -1 --stdout FETCH_HEAD,['manila_tempest_tests/tests/api/test_replication.py'],1,d72fa7f0c8915865274991a016b7e3e070dc759a,bug/2000253," access_level = 'ro' access_level=access_level) # verify rule's values rules_list = self.shares_v2_client.list_access_rules( self.shares[0][""id""])['access_list'] self.assertEqual(1, len(rules_list)) self.assertEqual(access_type, rules_list[0][""access_type""]) self.assertEqual(access_to, rules_list[0][""access_to""]) self.assertEqual(access_level, rules_list[0][""access_level""]) self.assertEqual(constants.RULE_STATE_ACTIVE, rules_list[0][""state""]) access_level = 'ro' access_level=access_level) # verify rule's values rules_list = self.shares_v2_client.list_access_rules( self.shares[0][""id""])['access_list'] self.assertEqual(1, len(rules_list)) self.assertEqual(access_type, rules_list[0][""access_type""]) self.assertEqual(access_to, rules_list[0][""access_to""]) self.assertEqual(access_level, rules_list[0][""access_level""]) self.assertEqual(constants.RULE_STATE_ACTIVE, rules_list[0][""state""]) access_level = 'ro' access_level=access_level) self.assertEqual(access_level, rules_list[0][""access_level""]) self.assertEqual(constants.RULE_STATE_ACTIVE, rules_list[0][""state""])"," access_level='ro') access_level='ro') access_level='ro') self.assertEqual('ro', rules_list[0][""access_level""])",26,4
openstack%2Frequirements~master~Ic6d777ee150a649eaee90465e60d7e8927ea4edf,openstack/requirements,master,Ic6d777ee150a649eaee90465e60d7e8927ea4edf,update constraint for keystonemiddleware to new release 10.4.0,NEW,2023-07-07 14:46:32.000000000,2023-07-10 15:02:39.000000000,,"[{'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-07 14:46:32.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/76a64b4b0d1a8745b81c45df714f0c0c1a53279d', 'message': 'update constraint for keystonemiddleware to new release 10.4.0\n\nmeta: version: 10.4.0\nmeta: diff-start: -\nmeta: series: bobcat\nmeta: branch: master\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: no\nmeta: release:Author: Elod Illes <elod.illes@est.tech>\nmeta: release:Commit: Elod Illes <elod.illes@est.tech>\nmeta: release:Change-Id: I0bee8a1e7285d971dc7c6ca5d09f25271848d993\nmeta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>\nmeta: release:Workflow+1: Elod Illes <elod.illes@est.tech>\nmeta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>\nChange-Id: Ic6d777ee150a649eaee90465e60d7e8927ea4edf\n'}]",1,887964,76a64b4b0d1a8745b81c45df714f0c0c1a53279d,8,3,1,11131,,,0,"update constraint for keystonemiddleware to new release 10.4.0

meta: version: 10.4.0
meta: diff-start: -
meta: series: bobcat
meta: branch: master
meta: release-type: release
meta: pypi: yes
meta: first: no
meta: release:Author: Elod Illes <elod.illes@est.tech>
meta: release:Commit: Elod Illes <elod.illes@est.tech>
meta: release:Change-Id: I0bee8a1e7285d971dc7c6ca5d09f25271848d993
meta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>
meta: release:Workflow+1: Elod Illes <elod.illes@est.tech>
meta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>
Change-Id: Ic6d777ee150a649eaee90465e60d7e8927ea4edf
",git fetch https://review.opendev.org/openstack/requirements refs/changes/64/887964/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,76a64b4b0d1a8745b81c45df714f0c0c1a53279d,new-release,keystonemiddleware===10.4.0,keystonemiddleware===10.3.0,1,1
openstack%2Fcloudkitty~master~If217a639f9af1e2693e6a132e46033df6bf96415,openstack/cloudkitty,master,If217a639f9af1e2693e6a132e46033df6bf96415,Fix random unit test failures,MERGED,2023-07-06 06:49:56.000000000,2023-07-10 15:01:56.000000000,2023-07-10 15:00:48.000000000,"[{'_account_id': 22348}, {'_account_id': 25277}, {'_account_id': 28356}]","[{'number': 1, 'created': '2023-07-06 06:49:56.000000000', 'files': ['cloudkitty/tests/storage/v2/test_storage_unit.py'], 'web_link': 'https://opendev.org/openstack/cloudkitty/commit/b460fd937fd2c8cc83979efa1abb7fa82d40a3f7', 'message': ""Fix random unit test failures\n\nAs per https://bugs.debian.org/1029646, Cloudkitty often fails to build\nas it fails its unit tests during the package build. This error happens\nrandomly. Sometimes it fails, sometimes it does not fail, but it's\nclearly a false positive, because we don't really want the test to fail\nin such case.\n\nThis patch makes it a lot less likely (10 times less) to happen by\nincreasing the tolerance.\n\nChange-Id: If217a639f9af1e2693e6a132e46033df6bf96415\n""}]",0,887752,b460fd937fd2c8cc83979efa1abb7fa82d40a3f7,8,3,1,6476,,,0,"Fix random unit test failures

As per https://bugs.debian.org/1029646, Cloudkitty often fails to build
as it fails its unit tests during the package build. This error happens
randomly. Sometimes it fails, sometimes it does not fail, but it's
clearly a false positive, because we don't really want the test to fail
in such case.

This patch makes it a lot less likely (10 times less) to happen by
increasing the tolerance.

Change-Id: If217a639f9af1e2693e6a132e46033df6bf96415
",git fetch https://review.opendev.org/openstack/cloudkitty refs/changes/52/887752/1 && git format-patch -1 --stdout FETCH_HEAD,['cloudkitty/tests/storage/v2/test_storage_unit.py'],1,b460fd937fd2c8cc83979efa1abb7fa82d40a3f7,," abs(expected_total - float(returned_total)), 0.0001) abs(expected_qty - float(returned_qty)), 0.0001) 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,"," abs(expected_total - float(returned_total)), 0.00001) abs(expected_qty - float(returned_qty)), 0.00001) 0.00001, 0.00001, 0.00001, 0.00001, 0.00001, 0.00001, 0.00001, 0.00001,",10,10
openstack%2Fkeystone~master~Ifd4487c4566853244c4b2c90a178b1067c17fbc6,openstack/keystone,master,Ifd4487c4566853244c4b2c90a178b1067c17fbc6,Remove unnecessary removal of pyc files,MERGED,2023-02-28 17:28:53.000000000,2023-07-10 14:43:15.000000000,2023-07-10 14:41:55.000000000,"[{'_account_id': 597}, {'_account_id': 7414}, {'_account_id': 14250}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-02-28 17:28:53.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/keystone/commit/cbe2f7f6f22fbc33cfc3e5a3ffc011258a83dc40', 'message': ""Remove unnecessary removal of pyc files\n\nIn change I8fcd9370a6adbfe8bbb2ce441a6f2efad45d089a, we started setting\nthe 'PYTHONDONTWRITEBYTECODE=1' flag. With this set, Python won't\ngenerate pyc files. As these files aren't generated, there's no need to\nremove them. Remove the 'find' calls that were doing this.\n\nChange-Id: Ifd4487c4566853244c4b2c90a178b1067c17fbc6\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n""}]",0,875766,cbe2f7f6f22fbc33cfc3e5a3ffc011258a83dc40,10,4,1,15334,,,0,"Remove unnecessary removal of pyc files

In change I8fcd9370a6adbfe8bbb2ce441a6f2efad45d089a, we started setting
the 'PYTHONDONTWRITEBYTECODE=1' flag. With this set, Python won't
generate pyc files. As these files aren't generated, there's no need to
remove them. Remove the 'find' calls that were doing this.

Change-Id: Ifd4487c4566853244c4b2c90a178b1067c17fbc6
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/keystone refs/changes/66/875766/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,cbe2f7f6f22fbc33cfc3e5a3ffc011258a83dc40,trivial,," find keystone -type f -name ""*.pyc"" -delete find find keystone -type f -name ""*.pyc"" -delete find keystone -type f -name ""*.pyc"" -delete find keystone -type f -name ""*.pyc"" -delete find keystone -type f -name ""*.pyc"" -delete",0,6
openstack%2Fdevstack~stable%2Fwallaby~I83fc9250ae5b7c1686938a0dd25d66b40fc6c6aa,openstack/devstack,stable/wallaby,I83fc9250ae5b7c1686938a0dd25d66b40fc6c6aa,Fix installation of OVS/OVN from sources,NEW,2023-07-06 10:41:04.000000000,2023-07-10 14:28:09.000000000,,"[{'_account_id': 4393}, {'_account_id': 6773}, {'_account_id': 8556}, {'_account_id': 13252}, {'_account_id': 16688}, {'_account_id': 22348}, {'_account_id': 22873}]","[{'number': 1, 'created': '2023-07-06 10:41:04.000000000', 'files': ['lib/neutron_plugins/ovn_agent'], 'web_link': 'https://opendev.org/openstack/devstack/commit/70b25d7e515048d4fac659ce3494a8b72563d84e', 'message': 'Fix installation of OVS/OVN from sources\n\nThis patch changes user who runs ovsdb-server and ovn-nortd services\nto root.\nIt also adds installation of the libssl dev package before compilation\nof the openvswitch if TLS service is enabled.\n\nCo-Authored-By: Fernando Royo <froyo@redhat.com>\n\nCloses-Bug: #1987832\nChange-Id: I83fc9250ae5b7c1686938a0dd25d66b40fc6c6aa\n(cherry picked from commit 3de92db6634a6d1455b7211ec869aed35508c58c)\n(cherry picked from commit 9794f48abb7d21c8a7bd24ad398caf083bbca2c5)\nConflicts:\n\tlib/neutron_plugins/ovs_source\n'}]",6,887790,70b25d7e515048d4fac659ce3494a8b72563d84e,10,7,1,13861,,,0,"Fix installation of OVS/OVN from sources

This patch changes user who runs ovsdb-server and ovn-nortd services
to root.
It also adds installation of the libssl dev package before compilation
of the openvswitch if TLS service is enabled.

Co-Authored-By: Fernando Royo <froyo@redhat.com>

Closes-Bug: #1987832
Change-Id: I83fc9250ae5b7c1686938a0dd25d66b40fc6c6aa
(cherry picked from commit 3de92db6634a6d1455b7211ec869aed35508c58c)
(cherry picked from commit 9794f48abb7d21c8a7bd24ad398caf083bbca2c5)
Conflicts:
	lib/neutron_plugins/ovs_source
",git fetch https://review.opendev.org/openstack/devstack refs/changes/90/887790/1 && git format-patch -1 --stdout FETCH_HEAD,['lib/neutron_plugins/ovn_agent'],1,70b25d7e515048d4fac659ce3494a8b72563d84e,bug/1987832," _run_process ovsdb-server ""$dbcmd"" """" ""$STACK_GROUP"" ""root"" _run_process ovn-northd ""$cmd"" ""$stop_cmd"" ""$STACK_GROUP"" ""root"""," _run_process ovsdb-server ""$dbcmd"" _run_process ovn-northd ""$cmd"" ""$stop_cmd""",2,2
openstack%2Ftripleo-upgrade~stable%2Fwallaby~Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6,openstack/tripleo-upgrade,stable/wallaby,Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6,workload : generate traffic to the vm.,MERGED,2023-02-09 12:40:43.000000000,2023-07-10 14:23:53.000000000,2023-07-10 14:23:53.000000000,"[{'_account_id': 8297}, {'_account_id': 11166}, {'_account_id': 22348}, {'_account_id': 33080}]","[{'number': 1, 'created': '2023-02-09 12:40:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/88486281226956ac3a843f99e56152f84e62d813', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when some\ntraffic have happen on the vm. Furthermore it match more closely what\ncustomer would have (running vm with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nWe only do that for the ""workload"" option.\n\nWe also add:\n - -o pipefail: to make sure errors are caught, note that -e would\n   require major refactoring and thus in not enabled in this review;\n - prepopulate ~/vm_ip.sh so that it doesn\'t have to be re-calculated later.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}, {'number': 2, 'created': '2023-02-09 15:42:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/6bce5a10bc8a42d166865538c75cc126100098f2', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when some\ntraffic have happen on the vm. Furthermore it match more closely what\ncustomer would have (running vm with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nWe only do that for the ""workload"" option.\n\nWe also add:\n - -o pipefail: to make sure errors are caught, note that -e would\n   require major refactoring and thus in not enabled in this review;\n - prepopulate ~/vm_ip.sh so that it doesn\'t have to be re-calculated later.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}, {'number': 3, 'created': '2023-02-09 17:12:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/51d23f5cb899e95d95d76fb7986de0e0b0b80af5', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when some\ntraffic have happen on the vm. Furthermore it match more closely what\ncustomer would have (running vm with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nWe only do that for the ""workload"" option.\n\nWe also add:\n - -o pipefail: to make sure errors are caught, note that -e would\n   require major refactoring and thus in not enabled in this review;\n - prepopulate ~/vm_ip.sh so that it doesn\'t have to be re-calculated later.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}, {'number': 4, 'created': '2023-02-09 17:26:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/9bb794153fd5c1ab475372b0ef33a6dd820ea394', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when some\ntraffic have happen on the vm. Furthermore it match more closely what\ncustomer would have (running vm with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nWe only do that for the ""workload"" option.\n\nWe also add:\n - -o pipefail: to make sure errors are caught, note that -e would\n   require major refactoring and thus in not enabled in this review;\n - prepopulate ~/vm_ip.sh so that it doesn\'t have to be re-calculated later.\n\nNote that a weird templating issue force the ""}"" to be before the else\nthe endif templating clauses in set_vm_ip or else the ""}"" is rendered\nlike this:\n\n    VM_IP=""${INSTANCE_FIP}""}\n\nwhich is a shell syntax error.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}, {'number': 5, 'created': '2023-02-15 14:23:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/a108d1d536e1052896d87b7ec5dcd556e8ac06df', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when some\ntraffic have happen on the vm. Furthermore it match more closely what\ncustomer would have (running vm with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nWe only do that for the ""workload"" option.\n\nWe also add:\n - -o pipefail: to make sure errors are caught, note that -e would\n   require major refactoring and thus in not enabled in this review;\n - prepopulate ~/vm_ip.sh so that it doesn\'t have to be re-calculated later.\n\nNote that a weird templating issue force the ""}"" to be before the else\nthe endif templating clauses in set_vm_ip or else the ""}"" is rendered\nlike this:\n\n    VM_IP=""${INSTANCE_FIP}""}\n\nwhich is a shell syntax error.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}, {'number': 6, 'created': '2023-02-20 13:08:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/998bfd8eeb3baeed159274a248c54b173526edb2', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when some\ntraffic have happen on the vm. Furthermore it match more closely what\ncustomer would have (running vm with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nWe only do that for the ""workload"" option.\n\nWe also add:\n - -o pipefail: to make sure errors are caught, note that -e would\n   require major refactoring and thus in not enabled in this review;\n - prepopulate ~/vm_ip.sh so that it doesn\'t have to be re-calculated later.\n\nNote that a weird templating issue force the ""}"" to be before the else\nthe endif templating clauses in set_vm_ip or else the ""}"" is rendered\nlike this:\n\n    VM_IP=""${INSTANCE_FIP}""}\n\nwhich is a shell syntax error.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}, {'number': 7, 'created': '2023-02-20 14:51:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/bbb0587720716789bdeda09c12ddb42127aa3301', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when some\ntraffic have happen on the vm. Furthermore it match more closely what\ncustomer would have (running vm with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nWe only do that for the ""workload"" option.\n\nWe also add:\n - -o pipefail: to make sure errors are caught, note that -e would\n   require major refactoring and thus in not enabled in this review;\n - prepopulate ~/vm_ip.sh so that it doesn\'t have to be re-calculated later.\n\nNote that a weird templating issue force the ""}"" to be before the else\nthe endif templating clauses in set_vm_ip or else the ""}"" is rendered\nlike this:\n\n    VM_IP=""${INSTANCE_FIP}""}\n\nwhich is a shell syntax error.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}, {'number': 8, 'created': '2023-02-21 17:12:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/456c088c92d0cc9edaa69d8115a4b867050a2d5d', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when some\ntraffic have happen on the vm. Furthermore it match more closely what\ncustomer would have (running vm with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nWe only do that for the ""workload"" option.\n\nWe also add:\n - -o pipefail: to make sure errors are caught, note that -e would\n   require major refactoring and thus in not enabled in this review;\n - prepopulate ~/vm_ip.sh so that it doesn\'t have to be re-calculated later.\n\nNote that a weird templating issue force the ""}"" to be before the else\nthe endif templating clauses in set_vm_ip or else the ""}"" is rendered\nlike this:\n\n    VM_IP=""${INSTANCE_FIP}""}\n\nwhich is a shell syntax error.\n\nEventually we take into account vm with misconfigured name resolution,\nby temporarily taking the resolv.conf from the undercloud (which\nshould be always working) and use that for getting iperf into the vm.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}, {'number': 9, 'created': '2023-06-19 17:29:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/4caad16428bf03d5823c5994917af0a5f1f8938e', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when there have\nbeen some traffic to the vm. Furthermore it match more closely what\ncustomer would have (running vms with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nThis is opt-in as the mode needs to be changed to workload_traffic. To\naccess it add `--install-upgrade-workload-traffic` to the infrared\ncommand or set `workload_launch_traffic` to true in other\nenvironments.\n\nWe also take into account vm with misconfigured name resolution, by\ntemporarily taking the resolv.conf from the undercloud (which should\nbe always working) and use that for getting iperf into the vm.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}, {'number': 10, 'created': '2023-06-21 16:50:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/ea596f9adab6ee87f827e79e8b235220c0c9d8aa', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when there have\nbeen some traffic to the vm. Furthermore it match more closely what\ncustomer would have (running vms with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nThis is opt-in as the mode needs to be changed to workload_traffic. To\naccess it add `--install-upgrade-workload-traffic` to the infrared\ncommand or set `workload_launch_traffic` to true in other\nenvironments.\n\nWe also take into account vm with misconfigured name resolution, by\ntemporarily taking the resolv.conf from the undercloud (which should\nbe always working) and use that for getting iperf into the vm.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}, {'number': 11, 'created': '2023-06-26 09:59:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/89473540edec342f03aafc787c3e7f80fc33db2c', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when there have\nbeen some traffic to the vm. Furthermore it match more closely what\ncustomer would have (running vms with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nThis is opt-in as the mode needs to be changed to workload_traffic. To\naccess it add `--install-upgrade-workload-traffic` to the infrared\ncommand or set `workload_launch_traffic` to true in other\nenvironments.\n\nWe also take into account vm with misconfigured name resolution, by\ntemporarily taking the resolv.conf from the undercloud (which should\nbe always working) and use that for getting iperf into the vm.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}, {'number': 12, 'created': '2023-07-03 08:35:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/ce510a8ba6418e9d8ee3993070fcacd03c837dc1', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when there have\nbeen some traffic to the vm. Furthermore it match more closely what\ncustomer would have (running vms with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nThis is opt-in as the mode needs to be changed to workload_traffic. To\naccess it add `--install-upgrade-workload-traffic` to the infrared\ncommand or set `workload_launch_traffic` to true in other\nenvironments.\n\nWe also take into account vm with misconfigured name resolution, by\ntemporarily taking the resolv.conf from the undercloud (which should\nbe always working) and use that for getting iperf into the vm.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}, {'number': 13, 'created': '2023-07-03 12:18:52.000000000', 'files': ['tasks/main.yml', 'README.rst', 'infrared_plugin/plugin.spec', 'templates/workload_launch.sh.j2', 'infrared_plugin/main.yml', 'defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/8f7c90f474678b71e1ce8702052ec811eac17ba8', 'message': 'workload : generate traffic to the vm.\n\nSome issues on the dataplane are more likely to happen when there have\nbeen some traffic to the vm. Furthermore it match more closely what\ncustomer would have (running vms with existing traffic)\n\nThis patch add a run of iperf during 1 minute to simulate that traffic\njust after the vm has been created.\n\nThis is opt-in as the mode needs to be changed to workload_traffic. To\naccess it add `--install-upgrade-workload-traffic` to the infrared\ncommand or set `workload_launch_traffic` to true in other\nenvironments.\n\nWe also take into account vm with misconfigured name resolution, by\ntemporarily taking the resolv.conf from the undercloud (which should\nbe always working) and use that for getting iperf into the vm.\n\nChange-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6\n'}]",21,873251,8f7c90f474678b71e1ce8702052ec811eac17ba8,40,4,13,8297,,,0,"workload : generate traffic to the vm.

Some issues on the dataplane are more likely to happen when there have
been some traffic to the vm. Furthermore it match more closely what
customer would have (running vms with existing traffic)

This patch add a run of iperf during 1 minute to simulate that traffic
just after the vm has been created.

This is opt-in as the mode needs to be changed to workload_traffic. To
access it add `--install-upgrade-workload-traffic` to the infrared
command or set `workload_launch_traffic` to true in other
environments.

We also take into account vm with misconfigured name resolution, by
temporarily taking the resolv.conf from the undercloud (which should
be always working) and use that for getting iperf into the vm.

Change-Id: Ide4d840ba8e42ef57b7bf748a65307d4ed5cc9a6
",git fetch https://review.opendev.org/openstack/tripleo-upgrade refs/changes/51/873251/1 && git format-patch -1 --stdout FETCH_HEAD,['templates/workload_launch.sh.j2'],1,88486281226956ac3a843f99e56152f84e62d813,873251-873251-873255-873255-update-ctl-plane-test,"set -o pipefail IPERF_STATIC_URL=""https://github.com/userdocs/iperf3-static/releases/download/3.12%2B/iperf3-amd64"" SSH=""ssh -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null""function set_vm_ip { ## assign floating ip or external ip {% if workload_sriov | bool -%} EXTERNAL_IP=$(openstack port show ${SRIOV_PORT} -f json -c fixed_ips | jq -r -c '.fixed_ips[0][""ip_address""]') VM_IP=${EXTERNAL_IP} {% else -%} INSTANCE_FIP=$(openstack floating ip create ${EXTERNAL_NET_NAME} -f json | jq -r -c '.floating_ip_address' ) echo ""Assign FIP[${INSTANCE_FIP}] to server ${INSTANCE_NAME}"" openstack server add floating ip ${INSTANCE_NAME} ${INSTANCE_FIP} if [ $? -ne 0 ]; then echo ""Network related error detected while attaching FIP to VM. Exiting with non-zero code"" if [[ ""${MODE}"" == ""sanity"" ]]; then sanity_teardown fi exit 1 fi VM_IP=${INSTANCE_FIP} # Newline workaround for endif behaving like endif -%, ie I get VM_IP=...FIP}}"" which fails. {%- endif %} } function generate_traffic { if [ -z ""${VM_IP}"" ]; then set_vm_ip fi if ! ${SSH} cirros@${VM_IP} test -e iperf3-amd64; then ${SSH} cirros@${VM_IP} curl -L -k -O ""${IPERF_STATIC_URL}"" ${SSH} cirros@${VM_IP} chmod +x iperf3-amd64 fi if ! ${SSH} cirros@${VM_IP} ps fauxw | grep -q iperf3-amd64; then ${SSH} -T cirros@${VM_IP} ./iperf3-amd64 -D -s fi if ! openstack security group show ${SECGROUP_NAME} | grep -q 'port_range.*=.5201'; then openstack security group rule create --proto tcp --dst-port 5201 ${SECGROUP_NAME} fi if ! rpm -qa | grep iperf3; then sudo dnf -y install iperf3 fi if [ ! -e iperf3.log ]; then echo -n ""Generating traffic ...."" iperf3 -c ${VM_IP} -t 60 --connect-timeout 3000 --logfile iperf3.log rc=$? if [ ""${rc}"" -ne 0 ]; then echo ""!!"" echo ""Problem generating traffic using iperf: ${rc}"" cat iperf3.log exit 1 fi echo "" done"" fi } set_vm_ip ${SSH} \ echo ""Write VM_IP ${VM_IP} to file ~/${INSTANCE_NAME}"" echo ""export VM_IP=${VM_IP}"" > ~/vm_ip.sh generate_traffic"," ## assign floating ip or external ip {% if workload_sriov | bool -%} EXTERNAL_IP=$(openstack port show ${SRIOV_PORT} -f json -c fixed_ips | jq -r -c '.fixed_ips[0][""ip_address""]') VM_IP=${EXTERNAL_IP} {% else -%} INSTANCE_FIP=$(openstack floating ip create ${EXTERNAL_NET_NAME} -f json | jq -r -c '.floating_ip_address' ) echo ""Assign FIP[${INSTANCE_FIP}] to server ${INSTANCE_NAME}"" openstack server add floating ip ${INSTANCE_NAME} ${INSTANCE_FIP} if [ $? -ne 0 ]; then echo ""Network related error detected while attaching FIP to VM. Exiting with non-zero code"" if [[ ""${MODE}"" == ""sanity"" ]]; then sanity_teardown fi exit 1 fi VM_IP=${INSTANCE_FIP} {%- endif %} ssh -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \ echo ""Write VM_IP ${VM_IP} to file""",63,21
openstack%2Fneutron~stable%2Fvictoria~Ib52cc6b9c99535b2461a42832c5c8f7603ca7df9,openstack/neutron,stable/victoria,Ib52cc6b9c99535b2461a42832c5c8f7603ca7df9,[DNM] Test patch neutron-tempest-plugin/+/888029,ABANDONED,2023-07-10 11:14:34.000000000,2023-07-10 14:12:03.000000000,,[],"[{'number': 1, 'created': '2023-07-10 11:14:34.000000000', 'files': ['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/ea68d3bba09e0ce546f55c17947b4d005a913dd4', 'message': '[DNM] Test patch neutron-tempest-plugin/+/888029\n\nDepends-On: https://review.opendev.org/c/openstack/neutron-tempest-plugin/+/888029\nChange-Id: Ib52cc6b9c99535b2461a42832c5c8f7603ca7df9\n'}]",0,888031,ea68d3bba09e0ce546f55c17947b4d005a913dd4,2,0,1,16688,,,0,"[DNM] Test patch neutron-tempest-plugin/+/888029

Depends-On: https://review.opendev.org/c/openstack/neutron-tempest-plugin/+/888029
Change-Id: Ib52cc6b9c99535b2461a42832c5c8f7603ca7df9
",git fetch https://review.opendev.org/openstack/neutron refs/changes/31/888031/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py'],1,ea68d3bba09e0ce546f55c17947b4d005a913dd4,wallaby_vitoria_ovn_21.06,TEST = 'test',,1,0
openstack%2Fneutron~stable%2Fwallaby~Ib52cc6b9c99535b2461a42832c5c8f7603ca7df9,openstack/neutron,stable/wallaby,Ib52cc6b9c99535b2461a42832c5c8f7603ca7df9,[DNM] Test patch neutron-tempest-plugin/+/888029,ABANDONED,2023-07-10 10:33:29.000000000,2023-07-10 14:08:36.000000000,,[{'_account_id': 6773}],"[{'number': 1, 'created': '2023-07-10 10:33:29.000000000', 'files': ['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/c661752a5cefcb8f44909d7800b60a5cb9f84cb2', 'message': '[DNM] Test patch neutron-tempest-plugin/+/888029\n\nDepends-On: https://review.opendev.org/c/openstack/neutron-tempest-plugin/+/888029\nChange-Id: Ib52cc6b9c99535b2461a42832c5c8f7603ca7df9\n'}]",0,888030,c661752a5cefcb8f44909d7800b60a5cb9f84cb2,4,1,1,16688,,,0,"[DNM] Test patch neutron-tempest-plugin/+/888029

Depends-On: https://review.opendev.org/c/openstack/neutron-tempest-plugin/+/888029
Change-Id: Ib52cc6b9c99535b2461a42832c5c8f7603ca7df9
",git fetch https://review.opendev.org/openstack/neutron refs/changes/30/888030/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py'],1,c661752a5cefcb8f44909d7800b60a5cb9f84cb2,wallaby_vitoria_ovn_21.06,TEST = 'test',,1,0
openstack%2Fneutron-lib~master~I85d8abf76712b069402cce69bf2c049ac51496c1,openstack/neutron-lib,master,I85d8abf76712b069402cce69bf2c049ac51496c1,Remove the dependency of allowedaddresspairs on atomic extensions,NEW,2023-07-05 08:42:07.000000000,2023-07-10 14:05:56.000000000,,"[{'_account_id': 4694}, {'_account_id': 5948}, {'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 22348}, {'_account_id': 28056}, {'_account_id': 34125}]","[{'number': 1, 'created': '2023-07-05 08:42:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-lib/commit/990b92faaebc9d7533349adbc321dc42ceb9994e', 'message': 'Remove the dependency of allowedaddresspairs on atomic extensions\n\nThe dependency of allowedaddresspairs.ALIAS on atomic extensions\nwill cause the unit tests of neutron to fail.\n\nPartial-Bug: #2012332\nChange-Id: I85d8abf76712b069402cce69bf2c049ac51496c1\n'}, {'number': 2, 'created': '2023-07-05 08:46:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-lib/commit/b00e9e4a3bc62cd627b9a07e588adae26ce9be6b', 'message': 'Remove the dependency of allowedaddresspairs on atomic extensions\n\nThe dependency of allowedaddresspairs.ALIAS on atomic extensions\nwill cause the unit tests of neutron [1] to fail.\n\n[1]https://review.opendev.org/c/openstack/neutron/+/880922\n\nPartial-Bug: #2012332\nChange-Id: I85d8abf76712b069402cce69bf2c049ac51496c1\n'}, {'number': 3, 'created': '2023-07-05 08:49:23.000000000', 'files': ['neutron_lib/api/definitions/allowedaddresspairs_atomic.py'], 'web_link': 'https://opendev.org/openstack/neutron-lib/commit/b8c383e7fb43093850d36cae46336142a247cc1a', 'message': 'Remove the dependency of allowedaddresspairs on atomic extensions\n\nThe dependency of allowedaddresspairs.ALIAS on atomic extensions\nwill cause the unit tests of neutron [1] to fail.\n\n[1]https://review.opendev.org/c/openstack/neutron/+/880922\n\nPartial-Bug: #2012332\nChange-Id: I85d8abf76712b069402cce69bf2c049ac51496c1\n'}]",4,887659,b8c383e7fb43093850d36cae46336142a247cc1a,9,8,3,34125,,,0,"Remove the dependency of allowedaddresspairs on atomic extensions

The dependency of allowedaddresspairs.ALIAS on atomic extensions
will cause the unit tests of neutron [1] to fail.

[1]https://review.opendev.org/c/openstack/neutron/+/880922

Partial-Bug: #2012332
Change-Id: I85d8abf76712b069402cce69bf2c049ac51496c1
",git fetch https://review.opendev.org/openstack/neutron-lib refs/changes/59/887659/3 && git format-patch -1 --stdout FETCH_HEAD,['neutron_lib/api/definitions/allowedaddresspairs_atomic.py'],1,990b92faaebc9d7533349adbc321dc42ceb9994e,bug/2012332,REQUIRED_EXTENSIONS = [],REQUIRED_EXTENSIONS = [allowedaddresspairs.ALIAS],1,1
openstack%2Fkeystone~master~Ie5dd375c4922cee895e4dfe25defb70a0b5e21a7,openstack/keystone,master,Ie5dd375c4922cee895e4dfe25defb70a0b5e21a7,api-ref: Correct app credentials auth response,MERGED,2023-02-03 18:18:55.000000000,2023-07-10 14:01:13.000000000,2023-07-10 13:59:54.000000000,"[{'_account_id': 7414}, {'_account_id': 14250}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-02-03 18:18:55.000000000', 'files': ['api-ref/source/v3/application-credentials.inc', 'api-ref/source/v3/samples/admin/auth-application-credential-response.json', 'api-ref/source/v3/parameters.yaml'], 'web_link': 'https://opendev.org/openstack/keystone/commit/434dbe1e73ba8cfeed9d43d34ce5d4fdff572975', 'message': ""api-ref: Correct app credentials auth response\n\nIn change I322a40404d8287748fe8c3a8d6dc1256d935d84a we switched from an\n'auth_credential_required' field in the response to an 'auth_credential'\nobject. Correct the api-ref.\n\nWhile we're alphabetize the response parameters.\n\nChange-Id: Ie5dd375c4922cee895e4dfe25defb70a0b5e21a7\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n""}]",0,872667,434dbe1e73ba8cfeed9d43d34ce5d4fdff572975,8,3,1,15334,,,0,"api-ref: Correct app credentials auth response

In change I322a40404d8287748fe8c3a8d6dc1256d935d84a we switched from an
'auth_credential_required' field in the response to an 'auth_credential'
object. Correct the api-ref.

While we're alphabetize the response parameters.

Change-Id: Ie5dd375c4922cee895e4dfe25defb70a0b5e21a7
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/keystone refs/changes/67/872667/1 && git format-patch -1 --stdout FETCH_HEAD,"['api-ref/source/v3/application-credentials.inc', 'api-ref/source/v3/samples/admin/auth-application-credential-response.json', 'api-ref/source/v3/parameters.yaml']",3,434dbe1e73ba8cfeed9d43d34ce5d4fdff572975,api-ref,auth_application_credential_body:,auth_application_credential_restricted_body:,41,35
openstack%2Fcharm-neutron-api-plugin-ovn~master~Icc05980b4bc9184ba4e52722c6c11abfcfc0d58c,openstack/charm-neutron-api-plugin-ovn,master,Icc05980b4bc9184ba4e52722c6c11abfcfc0d58c,Enable ovn_emit_need_to_frag,MERGED,2023-02-23 13:42:57.000000000,2023-07-10 13:58:54.000000000,2023-07-10 13:58:54.000000000,"[{'_account_id': 13686}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-02-23 13:42:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/4068345b48f483935336b9a21c25a6f9da377d42', 'message': 'bug#1947391 Added ovn_emit_need_to_frag flag\n\nChange-Id: Icc05980b4bc9184ba4e52722c6c11abfcfc0d58c\n'}, {'number': 2, 'created': '2023-02-24 10:38:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/5046e0b097faf2690f2e7479ee693b75bdb11312', 'message': 'Enable ovn_emit_need_to_frag for supported releases\n\nPassing ovn_emit_need_to_frag flag to automatically enable fragmentation\nsupport in OVN. Patching master branch because all recent releases will\nhave a kernel version >= 5.2. This change will be backported to Ussuri,\nwith a kernel version check added only to Ussuri.\n\nCloses-Bug: #1947391\nChange-Id: Icc05980b4bc9184ba4e52722c6c11abfcfc0d58c\n'}, {'number': 3, 'created': '2023-02-24 10:40:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/8ba09e3ea35642d75a660aa081407c92cd1f459c', 'message': 'Enable ovn_emit_need_to_frag for supported releases\n\nThe flag requires recent kernel versions (=> 5.2) to be functional.\n\nWe should backport this change only for the following releases:\n\n- kinetic-zed                               5.19\n- jammy-{yoga,zed}                          5.15\n- focal-{ussuri,victoria,wallaby,xena,yoga} 5.4\n\nCloses-Bug: #1947391\nChange-Id: Icc05980b4bc9184ba4e52722c6c11abfcfc0d58c\n'}, {'number': 4, 'created': '2023-02-25 13:15:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/0ac9b8ae3008cfafb83309ff3072d1b080ed34fc', 'message': 'Enable ovn_emit_need_to_frag for supported releases\n\nThe flag requires recent kernel versions (=> 5.2) to be functional.\n\nWe should backport this change only for the following releases:\n\n- kinetic-zed                               5.19\n- jammy-{yoga,zed}                          5.15\n- focal-{ussuri,victoria,wallaby,xena,yoga} 5.4\n\nCloses-Bug: #1947391\nChange-Id: Icc05980b4bc9184ba4e52722c6c11abfcfc0d58c\n'}, {'number': 5, 'created': '2023-02-27 11:12:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/bdccde40e115d163250cce79b17b9993cabb2997', 'message': 'Enable ovn_emit_need_to_frag for supported releases\n\nThe flag requires recent kernel versions (=> 5.2) to be functional.\n\nWe should backport this change only for the following releases:\n\n- kinetic-zed                               5.19\n- jammy-{yoga,zed}                          5.15\n- focal-{yoga}                              5.4\n\nCloses-Bug: #1947391\nChange-Id: Icc05980b4bc9184ba4e52722c6c11abfcfc0d58c\n'}, {'number': 6, 'created': '2023-02-27 11:16:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/777169eb57cb43d8773d255a05bc3976c4578e6a', 'message': 'Enable ovn_emit_need_to_frag\n\nThe flag requires recent kernel versions (=> 5.2) to be functional.\n\nCloses-Bug: #1947391\nChange-Id: Icc05980b4bc9184ba4e52722c6c11abfcfc0d58c\n'}, {'number': 7, 'created': '2023-02-27 11:46:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/8f8e46bf1d2408ee6f555473f48f33be5929cc54', 'message': 'Enable ovn_emit_need_to_frag\n\nEnabled by default since this branch only supports jammy where the\nkernel and ovn are new enough to support this flag. This will eliminate\nthe need for a more complex change or the use of a dedicated opt-in\nconfig option.\n\nFor more details, please refer to\nI089f95b40803a6cd5e01990acacd599ced3bbd91\n\nCloses-Bug: #1947391\nChange-Id: Icc05980b4bc9184ba4e52722c6c11abfcfc0d58c\n'}, {'number': 8, 'created': '2023-05-15 07:35:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/7e3e494ed28a5e344fa7f9d8ff59469dd84fd90c', 'message': 'Enable ovn_emit_need_to_frag\n\nEnabled by default since this branch only supports jammy where the\nkernel and ovn are new enough to support this flag. This will eliminate\nthe need for a more complex change or the use of a dedicated opt-in\nconfig option.\n\nFor more details, please refer to\nI089f95b40803a6cd5e01990acacd599ced3bbd91\n\nCloses-Bug: #1947391\nChange-Id: Icc05980b4bc9184ba4e52722c6c11abfcfc0d58c\n'}, {'number': 9, 'created': '2023-07-04 10:15:19.000000000', 'files': ['src/reactive/neutron_api_plugin_ovn_handlers.py', 'unit_tests/test_reactive_neutron_api_plugin_ovn_handlers.py'], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/c26114ddd8fa859f418e2e3a6ab3db8680e41708', 'message': 'Enable ovn_emit_need_to_frag\n\nEnabled by default since this branch only supports jammy where the\nkernel and ovn are new enough to support this flag. This will eliminate\nthe need for a more complex change or the use of a dedicated opt-in\nconfig option.\n\nFor more details, please refer to\nI089f95b40803a6cd5e01990acacd599ced3bbd91\n\nCloses-Bug: #1947391\nChange-Id: Icc05980b4bc9184ba4e52722c6c11abfcfc0d58c\n'}]",26,874922,c26114ddd8fa859f418e2e3a6ab3db8680e41708,50,3,9,35788,,,0,"Enable ovn_emit_need_to_frag

Enabled by default since this branch only supports jammy where the
kernel and ovn are new enough to support this flag. This will eliminate
the need for a more complex change or the use of a dedicated opt-in
config option.

For more details, please refer to
I089f95b40803a6cd5e01990acacd599ced3bbd91

Closes-Bug: #1947391
Change-Id: Icc05980b4bc9184ba4e52722c6c11abfcfc0d58c
",git fetch https://review.opendev.org/openstack/charm-neutron-api-plugin-ovn refs/changes/22/874922/8 && git format-patch -1 --stdout FETCH_HEAD,"['src/reactive/neutron_api_plugin_ovn_handlers.py', 'unit_tests/test_reactive_neutron_api_plugin_ovn_handlers.py']",2,4068345b48f483935336b9a21c25a6f9da377d42,," ('ovn_emit_need_to_frag', True),",,2,0
openstack%2Fcharm-neutron-api-plugin-ovn~master~I20789f637c9443bd274df5f91522f9e2ce973164,openstack/charm-neutron-api-plugin-ovn,master,I20789f637c9443bd274df5f91522f9e2ce973164,Add 'ovn-source' config option.,MERGED,2023-06-30 08:47:03.000000000,2023-07-10 13:46:49.000000000,2023-07-10 13:46:49.000000000,"[{'_account_id': 13686}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-30 08:47:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/47f8642b837592efbb6f5aa90a04582772fb453c', 'message': 'Add \'ovn-source\' config option.\n\nThis option enables configuration of overlay package\nrepository for installation of OVN packages that are\nnot available in default distribution repository.\n\nExpected behavior:\n* New deployments will use default overlay for\n  their series.\n* Setting this option to ""distro"" allows new\n  deployment that does not use overlay repository\n* Existing deployments that are upgraded to this\n  version of the charm won\'t automatically apply\n  repository overlay and will keep using their\n  current defaults.\n\nChange-Id: I20789f637c9443bd274df5f91522f9e2ce973164\n'}, {'number': 2, 'created': '2023-07-03 16:49:48.000000000', 'files': ['src/config.yaml', 'unit_tests/test_lib_charm_openstack_ovn.py', 'src/reactive/neutron_api_plugin_ovn_handlers.py', 'src/lib/charm/openstack/neutron_api_plugin_ovn.py', 'unit_tests/test_reactive_neutron_api_plugin_ovn_handlers.py'], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/412885acc9aeb94de910757d1fee5658a8de9a10', 'message': 'Add \'ovn-source\' config option.\n\nThis option enables configuration of overlay package\nrepository for installation of OVN packages that are\nnot available in default distribution repository.\n\nExpected behavior:\n* New deployments will use default overlay for\n  their series.\n* Setting this option to ""distro"" allows new\n  deployment that does not use overlay repository\n* Existing deployments that are upgraded to this\n  version of the charm won\'t automatically apply\n  repository overlay and will keep using their\n  current defaults.\n\nCloses-Bug: #1992770\nChange-Id: I20789f637c9443bd274df5f91522f9e2ce973164\n'}]",59,887362,412885acc9aeb94de910757d1fee5658a8de9a10,17,3,2,32288,,,0,"Add 'ovn-source' config option.

This option enables configuration of overlay package
repository for installation of OVN packages that are
not available in default distribution repository.

Expected behavior:
* New deployments will use default overlay for
  their series.
* Setting this option to ""distro"" allows new
  deployment that does not use overlay repository
* Existing deployments that are upgraded to this
  version of the charm won't automatically apply
  repository overlay and will keep using their
  current defaults.

Closes-Bug: #1992770
Change-Id: I20789f637c9443bd274df5f91522f9e2ce973164
",git fetch https://review.opendev.org/openstack/charm-neutron-api-plugin-ovn refs/changes/62/887362/1 && git format-patch -1 --stdout FETCH_HEAD,"['src/config.yaml', 'unit_tests/test_lib_charm_openstack_ovn.py', 'src/reactive/neutron_api_plugin_ovn_handlers.py', 'src/lib/charm/openstack/neutron_api_plugin_ovn.py', 'unit_tests/test_reactive_neutron_api_plugin_ovn_handlers.py']",5,47f8642b837592efbb6f5aa90a04582772fb453c,," 'ovn_source_changed': ('config.changed.ovn-source',), def test_ovn_source_config_changed(self): """"""Test that changing 'ovn-source' config triggers package upgrade."""""" config = {'ovn-source': 'cloud:focal-ovn-22.03'} handlers.ch_core.hookenv.config.return_value = config handlers.ovn_source_changed() self.charm.upgrade_ovn.assert_called_once_with() def test_ovn_source_config_changed_no_trigger(self): """"""Test no package upgrade is triggered if 'ovn-source' is default. Not triggering OVN package upgrade if value of 'ovn-source' is default empty string ensures that packages are not automatically upgraded on charm upgrade. """""" config = {'ovn-source': ''} handlers.ch_core.hookenv.config.return_value = config handlers.ovn_source_changed() self.charm.upgrade_ovn.assert_not_called()",,316,2
openstack%2Fcharm-magpie~master~I0f4b2bc5427533b990f8b216a955565c1faaefd2,openstack/charm-magpie,master,I0f4b2bc5427533b990f8b216a955565c1faaefd2,Migrate charm to binary,MERGED,2023-06-28 06:13:25.000000000,2023-07-10 13:41:39.000000000,2023-07-10 13:41:39.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 22348}, {'_account_id': 34352}]","[{'number': 1, 'created': '2023-06-28 06:13:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-magpie/commit/d7f6dab8a5e2a67bb991159174717157dec07dd7', 'message': 'Migrate charm to binary\n\nThis greatly improve the deployment of Magpie where\nit is actually deployed many times for different\nspaces on an identical underlaying machine\n\nChange-Id: I0f4b2bc5427533b990f8b216a955565c1faaefd2\n'}, {'number': 2, 'created': '2023-06-28 09:45:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-magpie/commit/97c4f87cb6de87d20a5d6a6267909206ddd28277', 'message': 'Migrate charm to binary\n\nThis greatly improve the installation phase of Magpie\nwhere it is common to have the charm deployed\nmultiple times on a same underlaying machine mainly\nfor each network spaces.\nFrom a setup from 3 nodes and 5 network spaces the\ninstallation time required was reduced from 12 to\n1 minute\n\nChange-Id: I0f4b2bc5427533b990f8b216a955565c1faaefd2\n'}, {'number': 3, 'created': '2023-06-28 09:56:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-magpie/commit/4112f94ee55d5fbf21c9a8d1f462f4cd0a0a9596', 'message': 'Migrate charm to binary\n\nThis greatly improve the installation phase of Magpie\nwhere it is common to have the charm deployed\nmultiple times on a same underlaying machine mainly\nfor each network spaces.\nFrom a setup from 3 nodes and 5 network spaces the\ninstallation time required was reduced from 12 to\n1 minute\n\nDrop Kinetic support\n\nChange-Id: I0f4b2bc5427533b990f8b216a955565c1faaefd2\n'}, {'number': 4, 'created': '2023-06-30 08:41:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-magpie/commit/0f3d9947161635c4147cf609450a6c7192e9cec4', 'message': 'Migrate charm to binary\n\nThis greatly improve the installation phase of Magpie\nwhere it is common to have the charm deployed\nmultiple times on a same underlaying machine mainly\nfor each network spaces.\nFrom a setup from 3 nodes and 5 network spaces the\ninstallation time required was reduced from 12 to\n1 minute\n\nDrop Impish and Kinetic support\n\nChange-Id: I0f4b2bc5427533b990f8b216a955565c1faaefd2\n'}, {'number': 5, 'created': '2023-06-30 09:18:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-magpie/commit/490c8012fff88bf2416670473f40c85617abcce4', 'message': 'Migrate charm to binary\n\nThis greatly improve the installation phase of Magpie\nwhere it is common to have the charm deployed\nmultiple times on a same underlaying machine mainly\nfor each network spaces.\nFrom a setup from 3 nodes and 5 network spaces the\ninstallation time required was reduced from 12 to\n1 minute\n\nDrop Impish and Kinetic support\n\nChange-Id: I0f4b2bc5427533b990f8b216a955565c1faaefd2\n'}, {'number': 6, 'created': '2023-06-30 09:39:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-magpie/commit/578bf66c0405f0f63e1f80a416848c9ed6652898', 'message': 'Migrate charm to binary\n\nThis greatly improve the installation phase of Magpie\nwhere it is common to have the charm deployed\nmultiple times on a same underlaying machine mainly\nfor each network spaces.\nFrom a setup from 3 nodes and 5 network spaces the\ninstallation time required was reduced from 12 to\n1 minute\n\nDrop Impish and Kinetic support\n\nChange-Id: I0f4b2bc5427533b990f8b216a955565c1faaefd2\n'}, {'number': 7, 'created': '2023-06-30 09:55:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-magpie/commit/cb45a3daab3175bf456228e58259756fd0958659', 'message': 'Migrate charm to binary\n\nThis greatly improve the installation phase of Magpie\nwhere it is common to have the charm deployed\nmultiple times on a same underlaying machine mainly\nfor each network spaces.\nFrom a setup from 3 nodes and 5 network spaces the\ninstallation time required was reduced from 12 to\n1 minute\n\nDrop Impish and Kinetic support\n\nChange-Id: I0f4b2bc5427533b990f8b216a955565c1faaefd2\n'}, {'number': 8, 'created': '2023-07-04 09:08:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-magpie/commit/116ca15ad04b1877ea203cfe0f8c6a0d3a49ad36', 'message': 'Migrate charm to binary\n\nThis greatly improve the installation phase of Magpie\nwhere it is common to have the charm deployed\nmultiple times on a same underlaying machine mainly\nfor each network spaces.\nFrom a setup from 3 nodes and 5 network spaces the\ninstallation time required was reduced from 12 to\n1 minute\n\nDrop Impish and Kinetic support\n\nChange-Id: I0f4b2bc5427533b990f8b216a955565c1faaefd2\n'}, {'number': 9, 'created': '2023-07-04 10:28:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-magpie/commit/cf76f7725d63846025aec56b7e1a9154402b74bc', 'message': 'Migrate charm to binary\n\nThis greatly improve the installation phase of Magpie\nwhere it is common to have the charm deployed\nmultiple times on a same underlaying machine mainly\nfor each network spaces.\nFrom a setup from 3 nodes and 5 network spaces the\ninstallation time required was reduced from 12 to\n1 minute\n\nDrop Impish and Kinetic support\n\nChange-Id: I0f4b2bc5427533b990f8b216a955565c1faaefd2\n'}, {'number': 10, 'created': '2023-07-04 12:37:48.000000000', 'files': ['src/tests/bundles/kinetic.yaml', 'osci.yaml', 'src/tests/bundles/focal.yaml', 'src/tests/bundles/lunar.yaml', 'rename.sh', 'charmcraft.yaml', 'src/metadata.yaml', 'src/tests/bundles/jammy.yaml', 'src/tests/tests.yaml', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/charm-magpie/commit/cb60a11f572c201268fffc293bf40a530aadc3d7', 'message': 'Migrate charm to binary\n\nThis greatly improve the installation phase of Magpie\nwhere it is common to have the charm deployed\nmultiple times on a same underlaying machine mainly\nfor each network spaces.\nFrom a setup from 3 nodes and 5 network spaces the\ninstallation time required was reduced from 12 to\n1 minute\n\nDrop Impish and Kinetic support\n\nChange-Id: I0f4b2bc5427533b990f8b216a955565c1faaefd2\n'}]",6,887136,cb60a11f572c201268fffc293bf40a530aadc3d7,41,4,10,34232,,,0,"Migrate charm to binary

This greatly improve the installation phase of Magpie
where it is common to have the charm deployed
multiple times on a same underlaying machine mainly
for each network spaces.
From a setup from 3 nodes and 5 network spaces the
installation time required was reduced from 12 to
1 minute

Drop Impish and Kinetic support

Change-Id: I0f4b2bc5427533b990f8b216a955565c1faaefd2
",git fetch https://review.opendev.org/openstack/charm-magpie refs/changes/36/887136/5 && git format-patch -1 --stdout FETCH_HEAD,"['charmcraft.yaml', 'src/metadata.yaml']",2,d7f6dab8a5e2a67bb991159174717157dec07dd7,binary-reactive,,- kinetic,87,10
openstack%2Fcharm-magpie~master~I595b39a2ca9853ef0989dd0124c6e02f03dc51ff,openstack/charm-magpie,master,I595b39a2ca9853ef0989dd0124c6e02f03dc51ff,Fix iperf errors and results on Focal,MERGED,2023-06-28 06:10:57.000000000,2023-07-10 13:35:10.000000000,2023-07-10 13:35:10.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 22348}, {'_account_id': 34352}]","[{'number': 1, 'created': '2023-06-28 06:10:57.000000000', 'files': ['src/lib/charms/layer/magpie_tools.py', 'unit_tests/test_magpie_tools.py'], 'web_link': 'https://opendev.org/openstack/charm-magpie/commit/5bda1a00495e9a22b0f7d8c7b9d644b62924fae6', 'message': 'Fix iperf errors and results on Focal\n\nOn Focal, the value of bandwidth would be doubled\nthan in reality\n\nIt fixes also a silent error when the action fails\nto reach a single node\nIt will return at least the source ip, interface\nand destination used for the test\n\nCloses-Bug: #2025212\nChange-Id: I595b39a2ca9853ef0989dd0124c6e02f03dc51ff\n'}]",1,887135,5bda1a00495e9a22b0f7d8c7b9d644b62924fae6,8,4,1,34232,,,0,"Fix iperf errors and results on Focal

On Focal, the value of bandwidth would be doubled
than in reality

It fixes also a silent error when the action fails
to reach a single node
It will return at least the source ip, interface
and destination used for the test

Closes-Bug: #2025212
Change-Id: I595b39a2ca9853ef0989dd0124c6e02f03dc51ff
",git fetch https://review.opendev.org/openstack/charm-magpie refs/changes/35/887135/1 && git format-patch -1 --stdout FETCH_HEAD,"['src/lib/charms/layer/magpie_tools.py', 'unit_tests/test_magpie_tools.py']",2,5bda1a00495e9a22b0f7d8c7b9d644b62924fae6,focal_error_output," self.maxDiff = None @patch( ""lib.charms.layer.magpie_tools.get_src_ip_from_dest"", lambda _: ""192.168.2.2"" ) ""mynode"", ""192.168.2.1"", ""10"", ""2"" ""iperf -t10 -c 192.168.2.1 --port 5001 -P2 --reportstyle c"" @patch('subprocess.PIPE', None) @patch('subprocess.run') def test_get_src_ip_from_dest(self, mock_subprocess): mock_stdout = MagicMock() mock_stdout.configure_mock( **{ 'stdout.decode.return_value': '[{""dst"":""192.168.12.1"",' '""dev"":""enp5s0"",""prefsrc"":""192.168.12.15"",""flags"":[],' '""uid"":1000,""cache"":[]}]' } ) mock_subprocess.return_value = mock_stdout self.assertEqual( magpie_tools.get_src_ip_from_dest(""192.168.12.1""), '192.168.12.15', )"," ""mynode"", ""192.168.2.2"", ""10"", ""2"" ""iperf -t10 -c 192.168.2.2 --port 5001 -P2 --reportstyle c""",92,32
openstack%2Fneutron-tempest-plugin~master~I961bfb5b58d34bf6400117c6ea788231db40a5fe,openstack/neutron-tempest-plugin,master,I961bfb5b58d34bf6400117c6ea788231db40a5fe,Use OVN v21.06 in Wallaby and Victoria,MERGED,2023-07-10 10:31:21.000000000,2023-07-10 13:20:19.000000000,2023-07-10 13:20:19.000000000,"[{'_account_id': 1131}, {'_account_id': 6773}, {'_account_id': 8313}, {'_account_id': 13861}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-10 10:31:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/14e157454f81a7e335c233d76450408511cb986a', 'message': 'Use OVN v21.06 in Wallaby and Victoria\n\nThis patch is a follow-up of [1]. In order to keep using the same OVS\nand OVN versions used before in the stable branched Wallaby and\nVictoria, this patch enforces the OVN version to v21.06\n\n[1]https://review.opendev.org/c/openstack/neutron-tempest-plugin/+/887666\n\nChange-Id: I961bfb5b58d34bf6400117c6ea788231db40a5fe\n'}, {'number': 2, 'created': '2023-07-10 12:42:38.000000000', 'files': ['zuul.d/wallaby_jobs.yaml', 'zuul.d/victoria_jobs.yaml'], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/d5023e1b9914fe93d909ea92b38baba76f2d7869', 'message': 'Use OVN v21.06 in Wallaby and Victoria\n\nThis patch is a follow-up of [1]. In order to keep using the same OVS\nand OVN versions used before in the stable branched Wallaby and\nVictoria, this patch enforces the OVN version to v21.06\n\n[1]https://review.opendev.org/c/openstack/neutron-tempest-plugin/+/887666\n\nChange-Id: I961bfb5b58d34bf6400117c6ea788231db40a5fe\n'}]",6,888029,d5023e1b9914fe93d909ea92b38baba76f2d7869,14,6,2,16688,,,0,"Use OVN v21.06 in Wallaby and Victoria

This patch is a follow-up of [1]. In order to keep using the same OVS
and OVN versions used before in the stable branched Wallaby and
Victoria, this patch enforces the OVN version to v21.06

[1]https://review.opendev.org/c/openstack/neutron-tempest-plugin/+/887666

Change-Id: I961bfb5b58d34bf6400117c6ea788231db40a5fe
",git fetch https://review.opendev.org/openstack/neutron-tempest-plugin refs/changes/29/888029/1 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/wallaby_jobs.yaml', 'zuul.d/victoria_jobs.yaml']",2,14e157454f81a7e335c233d76450408511cb986a,wallaby_vitoria_ovn_21.06," OVN_BRANCH: ""v21.06.0"" OVS_BRANCH: ""a4b04276ab5934d087669ff2d191a23931335c87"""," OVN_BRANCH: ""branch-22.03"" OVS_BRANCH: ""branch-3.0""",4,4
openstack%2Fpython-manilaclient~master~I204cd0aeb66dfb7f0b4da29413b80d76a6034aa3,openstack/python-manilaclient,master,I204cd0aeb66dfb7f0b4da29413b80d76a6034aa3,Metadata for Share Network Subnets,MERGED,2023-01-12 11:41:31.000000000,2023-07-10 13:15:40.000000000,2023-07-10 13:14:39.000000000,"[{'_account_id': 16643}, {'_account_id': 22348}, {'_account_id': 29632}, {'_account_id': 32594}, {'_account_id': 33756}, {'_account_id': 34489}]","[{'number': 1, 'created': '2023-01-12 11:41:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/7be34e31a5e77a8fa4943d70ef7ad95b44c7934f', 'message': 'Metadata for Share Network Subnets\n\nExtend these into OSC capabilities where appropriate.\nBumps max microversion to 2.74.\n\nDepends-On: I8d5a03eb127941a84eea5e6e9bdf76b3489f17a8\nImplements: bp/metadata-for-share-resources\nChange-Id: I204cd0aeb66dfb7f0b4da29413b80d76a6034aa3\n'}, {'number': 2, 'created': '2023-03-08 20:54:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/4aa858e8bc9a66d2b94ddc681649a64b16209d0b', 'message': 'Metadata for Share Network Subnets\n\nExtend these into OSC capabilities where appropriate.\nBumps max microversion to 2.78.\n\nDepends-On: I8d5a03eb127941a84eea5e6e9bdf76b3489f17a8\nImplements: bp/metadata-for-share-resources\nChange-Id: I204cd0aeb66dfb7f0b4da29413b80d76a6034aa3\n'}, {'number': 3, 'created': '2023-05-09 01:49:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/a52a4f63e1ec8cef68f3637c7a56b1ddd854e2cd', 'message': 'Metadata for Share Network Subnets\n\nExtend these into OSC capabilities where appropriate.\nBumps max microversion to 2.78.\n\nDepends-On: I8d5a03eb127941a84eea5e6e9bdf76b3489f17a8\nImplements: bp/metadata-for-share-resources\nChange-Id: I204cd0aeb66dfb7f0b4da29413b80d76a6034aa3\n'}, {'number': 4, 'created': '2023-05-10 12:54:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/afe7fa9c62bd71d7f4d87fe2961744ff53182d4c', 'message': 'Metadata for Share Network Subnets\n\nExtend these into OSC capabilities where appropriate.\nBumps max microversion to 2.78.\n\nDepends-On: I8d5a03eb127941a84eea5e6e9bdf76b3489f17a8\nImplements: bp/metadata-for-share-resources\nChange-Id: I204cd0aeb66dfb7f0b4da29413b80d76a6034aa3\n'}, {'number': 5, 'created': '2023-07-07 19:26:09.000000000', 'files': ['manilaclient/tests/unit/v2/test_share_network_subnets.py', 'manilaclient/tests/unit/osc/v2/fakes.py', 'manilaclient/tests/unit/osc/v2/test_share_network_subnets.py', 'manilaclient/osc/v2/share_network_subnets.py', 'manilaclient/osc/v2/share_networks.py', 'releasenotes/notes/add-subnet-metadata-82426986431b0179.yaml', 'setup.cfg', 'manilaclient/v2/share_network_subnets.py', 'manilaclient/api_versions.py'], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/e3f01e5c2a134cd0b228bf35c70d6d110187ee36', 'message': 'Metadata for Share Network Subnets\n\nExtend these into OSC capabilities where appropriate.\nBumps max microversion to 2.78.\n\nDepends-On: I8d5a03eb127941a84eea5e6e9bdf76b3489f17a8\nImplements: bp/metadata-for-share-resources\nChange-Id: I204cd0aeb66dfb7f0b4da29413b80d76a6034aa3\n'}]",65,869709,e3f01e5c2a134cd0b228bf35c70d6d110187ee36,35,6,5,31721,,,0,"Metadata for Share Network Subnets

Extend these into OSC capabilities where appropriate.
Bumps max microversion to 2.78.

Depends-On: I8d5a03eb127941a84eea5e6e9bdf76b3489f17a8
Implements: bp/metadata-for-share-resources
Change-Id: I204cd0aeb66dfb7f0b4da29413b80d76a6034aa3
",git fetch https://review.opendev.org/openstack/python-manilaclient refs/changes/09/869709/4 && git format-patch -1 --stdout FETCH_HEAD,"['manilaclient/tests/unit/v2/test_share_network_subnets.py', 'releasenotes/notes/add_subnet_metadata-82426986431b0179.yaml', 'manilaclient/tests/unit/osc/v2/fakes.py', 'manilaclient/v2/shell.py', 'manilaclient/tests/unit/osc/v2/test_share_network_subnets.py', 'manilaclient/osc/v2/share_network_subnets.py', 'setup.cfg', 'manilaclient/v2/share_network_subnets.py', 'manilaclient/api_versions.py']",9,7be34e31a5e77a8fa4943d70ef7ad95b44c7934f,manual_vlan,MAX_VERSION = '2.74',MAX_VERSION = '2.73',350,7
openstack%2Fkeystoneauth~master~I7775ec31ffa79160c696805d4ae607067b1463c7,openstack/keystoneauth,master,I7775ec31ffa79160c696805d4ae607067b1463c7,Replace deprecated datetime.utcnow method,NEW,2023-07-10 11:58:58.000000000,2023-07-10 13:04:35.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-10 11:58:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystoneauth/commit/42b94097440c11d98597b1a05fdc3af6f072e594', 'message': 'Replace deprecated datetime.utcnow method\n\nWhile rebuilding openstacksdk on Fedora/Python3.12 [1], we hit\nthe deprecation warning on datetime.utcnow method [2].\nThis patch updates it with the recommended way.\n\n[1] https://bugzilla.redhat.com/show_bug.cgi?id=2220377\n[2] https://docs.python.org/3.12/library/datetime.html#datetime.datetime.utcnow\n\nChange-Id: I7775ec31ffa79160c696805d4ae607067b1463c7\n'}, {'number': 2, 'created': '2023-07-10 12:50:01.000000000', 'files': ['keystoneauth1/_utils.py', 'keystoneauth1/extras/_saml2/v3/adfs.py'], 'web_link': 'https://opendev.org/openstack/keystoneauth/commit/23b5967f1641d06e3dbb46b95215bd5817dd7c11', 'message': 'Replace deprecated datetime.utcnow method\n\nWhile rebuilding openstacksdk on Fedora/Python3.12 [1], we hit\nthe deprecation warning on datetime.utcnow method [2].\nThis patch updates it with the recommended way.\n\n[1] https://bugzilla.redhat.com/show_bug.cgi?id=2220377\n[2] https://docs.python.org/3.12/library/datetime.html#datetime.datetime.utcnow\n\nChange-Id: I7775ec31ffa79160c696805d4ae607067b1463c7\n'}]",0,888033,23b5967f1641d06e3dbb46b95215bd5817dd7c11,5,1,2,31068,,,0,"Replace deprecated datetime.utcnow method

While rebuilding openstacksdk on Fedora/Python3.12 [1], we hit
the deprecation warning on datetime.utcnow method [2].
This patch updates it with the recommended way.

[1] https://bugzilla.redhat.com/show_bug.cgi?id=2220377
[2] https://docs.python.org/3.12/library/datetime.html#datetime.datetime.utcnow

Change-Id: I7775ec31ffa79160c696805d4ae607067b1463c7
",git fetch https://review.opendev.org/openstack/keystoneauth refs/changes/33/888033/2 && git format-patch -1 --stdout FETCH_HEAD,"['keystoneauth1/_utils.py', 'keystoneauth1/extras/_saml2/v3/adfs.py']",2,42b94097440c11d98597b1a05fdc3af6f072e594,, date_created = datetime.datetime.now(datetime.UTC), date_created = datetime.datetime.utcnow(),3,3
openstack%2Fcharm-rabbitmq-server~stable%2Ffocal~I88942dd0b246c498d0ab40b00d586d4349b0f100,openstack/charm-rabbitmq-server,stable/focal,I88942dd0b246c498d0ab40b00d586d4349b0f100,Rabbitmq metrics and splitbrain detection,NEW,2023-02-23 15:05:53.000000000,2023-07-10 12:54:48.000000000,,"[{'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-02-23 15:05:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-rabbitmq-server/commit/b999a31a54b6246333c77d7dc9b46e3e13b94c00', 'message': 'Rabbitmq metrics and splitbrain detection\n\nEnabled rabbitmq_prometheus plugin for prometheus to scrape\nthe metrics of rabbitmq and alert if rabbitmq splitbrain is\ndetected.\n\nIntegrated rabbitmq dashboards in grafana via dashboards\nrelations\n\nAdded new unit test cases\n\nCloses-Bug: 1899183\nChange-Id: I88942dd0b246c498d0ab40b00d586d4349b0f100\n(cherry picked from commit 0653c186cecf720c522353da6169a2ecf05d3284)\n'}, {'number': 2, 'created': '2023-02-23 15:08:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-rabbitmq-server/commit/e9290f93cbf9f5575d202bb020a2403ed8afde63', 'message': 'Rabbitmq metrics and splitbrain detection\n\nEnabled rabbitmq_prometheus plugin for prometheus to scrape\nthe metrics of rabbitmq and alert if rabbitmq splitbrain is\ndetected.\n\nIntegrated rabbitmq dashboards in grafana via dashboards\nrelations\n\nAdded new unit test cases\n\nCloses-Bug: 1899183\nChange-Id: I88942dd0b246c498d0ab40b00d586d4349b0f100\n(cherry picked from commit 0653c186cecf720c522353da6169a2ecf05d3284)\n'}, {'number': 3, 'created': '2023-03-13 14:44:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-rabbitmq-server/commit/e8fb81d2c90bfac089ca61a98030f0f33846cbd0', 'message': 'Rabbitmq metrics and splitbrain detection\n\nEnabled rabbitmq_prometheus plugin for prometheus to scrape\nthe metrics of rabbitmq and alert if rabbitmq splitbrain is\ndetected.\n\nIntegrated rabbitmq dashboards in grafana via dashboards\nrelations\n\nAdded new unit test cases\n\nCloses-Bug: 1899183\nChange-Id: I88942dd0b246c498d0ab40b00d586d4349b0f100\n(cherry picked from commit 0653c186cecf720c522353da6169a2ecf05d3284)\n'}, {'number': 4, 'created': '2023-05-19 00:57:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-rabbitmq-server/commit/ba41d3855911024acc23f148f294d65241a0b3d7', 'message': 'Rabbitmq metrics and splitbrain detection\n\nEnabled rabbitmq_prometheus plugin for prometheus to scrape\nthe metrics of rabbitmq and alert if rabbitmq splitbrain is\ndetected.\n\nIntegrated rabbitmq dashboards in grafana via dashboards\nrelations\n\nAdded new unit test cases\n\nCloses-Bug: 1899183\nChange-Id: I88942dd0b246c498d0ab40b00d586d4349b0f100\n(cherry picked from commit 0653c186cecf720c522353da6169a2ecf05d3284)\n'}, {'number': 5, 'created': '2023-06-05 09:57:45.000000000', 'files': ['hooks/prometheus-rules-relation-joined', 'hooks/scrape-relation-joined', 'unit_tests/test_rabbit_utils.py', 'hooks/scrape-relation-broken', 'hooks/prometheus-rules-relation-created', 'hooks/rabbitmq_server_relations.py', 'files/grafana-dashboard.json', 'hooks/dashboards-relation-joined', 'README.md', 'hooks/scrape-relation-created', 'files/prom_rule_rmq_splitbrain.yaml', 'metadata.yaml', 'hooks/rabbit_utils.py'], 'web_link': 'https://opendev.org/openstack/charm-rabbitmq-server/commit/e965ff7e85951207914a4c92af8e319ec6de339a', 'message': 'Rabbitmq metrics and splitbrain detection\n\nEnabled rabbitmq_prometheus plugin for prometheus to scrape\nthe metrics of rabbitmq and alert if rabbitmq splitbrain is\ndetected.\n\nIntegrated rabbitmq dashboards in grafana via dashboards\nrelations\n\nAdded new unit test cases\n\nCloses-Bug: 1899183\nChange-Id: I88942dd0b246c498d0ab40b00d586d4349b0f100\n(cherry picked from commit 0653c186cecf720c522353da6169a2ecf05d3284)\n'}]",7,874835,e965ff7e85951207914a4c92af8e319ec6de339a,23,2,5,9247,,,0,"Rabbitmq metrics and splitbrain detection

Enabled rabbitmq_prometheus plugin for prometheus to scrape
the metrics of rabbitmq and alert if rabbitmq splitbrain is
detected.

Integrated rabbitmq dashboards in grafana via dashboards
relations

Added new unit test cases

Closes-Bug: 1899183
Change-Id: I88942dd0b246c498d0ab40b00d586d4349b0f100
(cherry picked from commit 0653c186cecf720c522353da6169a2ecf05d3284)
",git fetch https://review.opendev.org/openstack/charm-rabbitmq-server refs/changes/35/874835/3 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/prometheus-rules-relation-joined', 'hooks/scrape-relation-joined', 'unit_tests/test_rabbit_utils.py', 'hooks/scrape-relation-broken', 'hooks/prometheus-rules-relation-created', 'hooks/rabbitmq_server_relations.py', 'files/grafana-dashboard.json', 'hooks/dashboards-relation-joined', 'README.md', 'hooks/scrape-relation-created', 'files/prom_rule_rmq_splitbrain.yaml', 'metadata.yaml', 'hooks/rabbit_utils.py']",13,b999a31a54b6246333c77d7dc9b46e3e13b94c00,bug/1899183-stable/focal,"<<<<<<< HEAD (6aed1d Ensure that charmcraft builds on 18.04 for bionic and focal) ======= def get_plugin_manager(): """"""Find the path to the executable for managing plugins. :returns: Path to rabbitmq-plugins executable :rtype: str """""" # At version 3.8.2, only /sbin/rabbitmq-plugins can enable plugin correctly if os.path.exists(""/sbin/rabbitmq-plugins""): return '/sbin/rabbitmq-plugins' else: return glob.glob( '/usr/lib/rabbitmq/lib/rabbitmq_server-*/sbin/rabbitmq-plugins')[0] >>>>>>> CHANGE (0653c1 Rabbitmq metrics and splitbrain detection)",,5972,0
openstack%2Fcharm-ceph-mon~master~I9428e93ba6107ba5e2ebcc667995b3d88eb03d27,openstack/charm-ceph-mon,master,I9428e93ba6107ba5e2ebcc667995b3d88eb03d27,Set consistent source,MERGED,2023-07-10 07:41:28.000000000,2023-07-10 12:54:25.000000000,2023-07-10 12:54:25.000000000,"[{'_account_id': 20648}, {'_account_id': 22348}, {'_account_id': 33717}, {'_account_id': 34952}]","[{'number': 1, 'created': '2023-07-10 07:41:28.000000000', 'files': ['config.yaml'], 'web_link': 'https://opendev.org/openstack/charm-ceph-mon/commit/ab84214805e130848db3c45884223619904b3734', 'message': 'Set consistent source\n\nAvoid the unintuitive situation where users are deploying from\nchannel=quincy but get an older ceph due to deploying series=focal by\nexplicitly setting source=quincy which is what most users want anyway;\nthose that do not can still explicitly set source.\n\nChange-Id: I9428e93ba6107ba5e2ebcc667995b3d88eb03d27\n'}]",1,888024,ab84214805e130848db3c45884223619904b3734,8,4,1,15382,,,0,"Set consistent source

Avoid the unintuitive situation where users are deploying from
channel=quincy but get an older ceph due to deploying series=focal by
explicitly setting source=quincy which is what most users want anyway;
those that do not can still explicitly set source.

Change-Id: I9428e93ba6107ba5e2ebcc667995b3d88eb03d27
",git fetch https://review.opendev.org/openstack/charm-ceph-mon refs/changes/24/888024/1 && git format-patch -1 --stdout FETCH_HEAD,['config.yaml'],1,ab84214805e130848db3c45884223619904b3734,set-source-default, default: quincy, default: distro,1,1
openstack%2Fcharm-ceph-mon~stable%2Fquincy.2~I1ca4316aaf4f0b855a12aa582a8188c88e926fa6,openstack/charm-ceph-mon,stable/quincy.2,I1ca4316aaf4f0b855a12aa582a8188c88e926fa6,Fix ceph-mon upgrade path,MERGED,2023-07-10 07:53:50.000000000,2023-07-10 12:41:26.000000000,2023-07-10 12:41:26.000000000,"[{'_account_id': 20648}, {'_account_id': 22348}, {'_account_id': 33717}]","[{'number': 1, 'created': '2023-07-10 07:53:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ceph-mon/commit/356a44a62cc43c9bd92f5c5844fb245ff7b3d132', 'message': 'Fix ceph-mon upgrade path\n\nThis PR makes some small changes in the upgrade path logic by\nproviding a fallback method of fetching the current ceph-mon\nversion and adding additional checks to see if the upgrade can\nbe done in a sane way.\n\nCloses-Bug: #2024253\nChange-Id: I1ca4316aaf4f0b855a12aa582a8188c88e926fa6\n(cherry picked from commit 1a41aa24ce82c411da936ef3f21c61a57c059155)\n'}, {'number': 2, 'created': '2023-07-10 10:20:05.000000000', 'files': ['src/ceph_hooks.py', 'test-requirements.txt', 'unit_tests/test_upgrade.py'], 'web_link': 'https://opendev.org/openstack/charm-ceph-mon/commit/9db193ef9b27074a0a4469dc85b9fa49137370d4', 'message': 'Fix ceph-mon upgrade path\n\nThis PR makes some small changes in the upgrade path logic by\nproviding a fallback method of fetching the current ceph-mon\nversion and adding additional checks to see if the upgrade can\nbe done in a sane way.\n\nCloses-Bug: #2024253\nChange-Id: I1ca4316aaf4f0b855a12aa582a8188c88e926fa6\n(cherry picked from commit 1a41aa24ce82c411da936ef3f21c61a57c059155)\n'}]",1,887881,9db193ef9b27074a0a4469dc85b9fa49137370d4,10,3,2,15382,,,0,"Fix ceph-mon upgrade path

This PR makes some small changes in the upgrade path logic by
providing a fallback method of fetching the current ceph-mon
version and adding additional checks to see if the upgrade can
be done in a sane way.

Closes-Bug: #2024253
Change-Id: I1ca4316aaf4f0b855a12aa582a8188c88e926fa6
(cherry picked from commit 1a41aa24ce82c411da936ef3f21c61a57c059155)
",git fetch https://review.opendev.org/openstack/charm-ceph-mon refs/changes/81/887881/2 && git format-patch -1 --stdout FETCH_HEAD,"['src/ceph_hooks.py', 'unit_tests/test_upgrade.py']",2,356a44a62cc43c9bd92f5c5844fb245ff7b3d132,fix-upgrade-stable/quincy.2,"from charms_ceph.utils import resolve_ceph_version as resolve_ceph_version_orig @patch('ceph_hooks.ceph.resolve_ceph_version') @patch('ceph_hooks.subprocess.check_output') @patch('ceph_hooks.add_source') @patch('ceph_hooks.ceph.is_bootstrapped') @patch('ceph_hooks.hookenv') @patch('ceph_hooks.ceph.roll_monitor_cluster') def test_check_for_upgrade_no_current_version(self, roll_monitor_cluster, hookenv, is_bootstrapped, add_source, check_output, resolve_ceph_version): _resolve_first = True def _resolve_version(arg): nonlocal _resolve_first if _resolve_first: _resolve_first = False return None return resolve_ceph_version_orig(arg) resolve_ceph_version.side_effect = _resolve_version check_output.return_value = b"""""" ceph version 16.2.13 (123) pacific (stable)"""""" is_bootstrapped.return_value = True hookenv.config.side_effect = self.test_config self.test_config.set('source', 'cloud:focal-yoga') check_for_upgrade() roll_monitor_cluster.assert_called() add_source.assert_not_called() @patch('ceph_hooks.ceph.resolve_ceph_version') @patch('ceph_hooks.subprocess.check_output') @patch('ceph_hooks.add_source') @patch('ceph_hooks.ceph.is_bootstrapped') @patch('ceph_hooks.hookenv') @patch('ceph_hooks.ceph.roll_monitor_cluster') def test_check_for_upgrade_no_versions(self, roll_monitor_cluster, hookenv, is_bootstrapped, add_source, check_output, resolve_ceph_version): resolve_ceph_version.return_value = None check_output.return_value = b"""""" ceph version 17.2.5 (456) quincy (stable)"""""" is_bootstrapped.return_value = True hookenv.config.side_effect = self.test_config check_for_upgrade() roll_monitor_cluster.assert_not_called() add_source.assert_not_called()",,77,1
openstack%2Fironic~master~I1a85c0c9285359ee92fb676ec56c817cbe350367,openstack/ironic,master,I1a85c0c9285359ee92fb676ec56c817cbe350367,Move standalone jobs to focal,MERGED,2023-07-07 15:31:44.000000000,2023-07-10 12:34:00.000000000,2023-07-10 12:31:51.000000000,"[{'_account_id': 10239}, {'_account_id': 22348}, {'_account_id': 23851}]","[{'number': 1, 'created': '2023-07-07 15:31:44.000000000', 'files': ['zuul.d/ironic-jobs.yaml'], 'web_link': 'https://opendev.org/openstack/ironic/commit/6c35a44424ea03a8a147d64cad17cee79777d210', 'message': ""Move standalone jobs to focal\n\nWe are seeing a lot of failures in our standalone jobs\nafter we switched to jammy, see[1].\nLet's pin the jobs to focal and to isolate the problem and\nfix in a separate patch.\n\n[1] https://zuul.opendev.org/t/openstack/builds?job_name=ironic-standalone-redfish&project=openstack%2Fironic&branch=master&skip=0\n\nChange-Id: I1a85c0c9285359ee92fb676ec56c817cbe350367\n""}]",0,887971,6c35a44424ea03a8a147d64cad17cee79777d210,11,3,1,15519,,,0,"Move standalone jobs to focal

We are seeing a lot of failures in our standalone jobs
after we switched to jammy, see[1].
Let's pin the jobs to focal and to isolate the problem and
fix in a separate patch.

[1] https://zuul.opendev.org/t/openstack/builds?job_name=ironic-standalone-redfish&project=openstack%2Fironic&branch=master&skip=0

Change-Id: I1a85c0c9285359ee92fb676ec56c817cbe350367
",git fetch https://review.opendev.org/openstack/ironic refs/changes/71/887971/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/ironic-jobs.yaml'],1,6c35a44424ea03a8a147d64cad17cee79777d210,pin_standalone, nodeset: openstack-single-node-focal nodeset: openstack-single-node-focal,,2,0
openstack%2Ftripleo-common~stable%2Fwallaby~Ifacd7b81e2f9e8509ab287d603f38f61fae22902,openstack/tripleo-common,stable/wallaby,Ifacd7b81e2f9e8509ab287d603f38f61fae22902,Use new_manifest_type var,ABANDONED,2023-07-10 12:25:53.000000000,2023-07-10 12:31:15.000000000,,[],"[{'number': 1, 'created': '2023-07-10 12:25:53.000000000', 'files': ['tripleo_common/image/image_uploader.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/3767578974d52592d56e3e2b0516dd19bf7373e5', 'message': ""Use new_manifest_type var\n\nThis a followup fix for b0962d2ba09fbb4da33daa328e6a50cac5e3ba05.\nThe new_manifest_type var needs to be set since we assume it's has the\nvalue of the manifest type later in the function.\n\nChange-Id: Ifacd7b81e2f9e8509ab287d603f38f61fae22902\nSigned-off-by: James Slagle <jslagle@redhat.com>\n""}]",0,888035,3767578974d52592d56e3e2b0516dd19bf7373e5,2,0,1,7144,,,0,"Use new_manifest_type var

This a followup fix for b0962d2ba09fbb4da33daa328e6a50cac5e3ba05.
The new_manifest_type var needs to be set since we assume it's has the
value of the manifest type later in the function.

Change-Id: Ifacd7b81e2f9e8509ab287d603f38f61fae22902
Signed-off-by: James Slagle <jslagle@redhat.com>
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/35/888035/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_common/image/image_uploader.py'],1,3767578974d52592d56e3e2b0516dd19bf7373e5,, new_manifest_type = MEDIA_MANIFEST_V1_SIGNED else: new_manifest_type = MEDIA_MANIFEST_V1, manifest_type = MEDIA_MANIFEST_V1_SIGNED else: manifest_type = MEDIA_MANIFEST_V1,2,2
openstack%2Ftripleo-common~stable%2Ftrain~Ifacd7b81e2f9e8509ab287d603f38f61fae22902,openstack/tripleo-common,stable/train,Ifacd7b81e2f9e8509ab287d603f38f61fae22902,Use new_manifest_type var,ABANDONED,2023-07-10 12:26:34.000000000,2023-07-10 12:31:05.000000000,,[],"[{'number': 1, 'created': '2023-07-10 12:26:34.000000000', 'files': ['tripleo_common/image/image_uploader.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/61ef3793af7bd4fbccf977e8c0ae829e4267822d', 'message': ""Use new_manifest_type var\n\nThis a followup fix for b0962d2ba09fbb4da33daa328e6a50cac5e3ba05.\nThe new_manifest_type var needs to be set since we assume it's has the\nvalue of the manifest type later in the function.\n\nChange-Id: Ifacd7b81e2f9e8509ab287d603f38f61fae22902\nSigned-off-by: James Slagle <jslagle@redhat.com>\n(cherry picked from commit 3767578974d52592d56e3e2b0516dd19bf7373e5)\n""}]",0,888036,61ef3793af7bd4fbccf977e8c0ae829e4267822d,2,0,1,7144,,,0,"Use new_manifest_type var

This a followup fix for b0962d2ba09fbb4da33daa328e6a50cac5e3ba05.
The new_manifest_type var needs to be set since we assume it's has the
value of the manifest type later in the function.

Change-Id: Ifacd7b81e2f9e8509ab287d603f38f61fae22902
Signed-off-by: James Slagle <jslagle@redhat.com>
(cherry picked from commit 3767578974d52592d56e3e2b0516dd19bf7373e5)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/36/888036/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_common/image/image_uploader.py'],1,61ef3793af7bd4fbccf977e8c0ae829e4267822d,, new_manifest_type = MEDIA_MANIFEST_V1_SIGNED else: new_manifest_type = MEDIA_MANIFEST_V1, manifest_type = MEDIA_MANIFEST_V1_SIGNED else: manifest_type = MEDIA_MANIFEST_V1,2,2
openstack%2Fhorizon~master~I2887bbea530e5e8c5873aea4dc9c6e01ca641df8,openstack/horizon,master,I2887bbea530e5e8c5873aea4dc9c6e01ca641df8,Add support for default IdP login for multiple IdPs,NEW,2023-02-03 18:02:35.000000000,2023-07-10 11:53:24.000000000,,"[{'_account_id': 22348}, {'_account_id': 28356}]","[{'number': 1, 'created': '2023-02-03 18:02:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/horizon/commit/cc147fdb8283bfe533b47ee49be9dbad3972400c', 'message': 'Add support for default IdP login for multiple IdPs\n\nProblem description\n===================\n\nIn an OpenStack environment with multiple organizations, each one\nwith your own identity provider and Horizon instance with specific\nthemes. In these cases, when we enable the default websso redirect\nwe get a list of available identity providers, which in most cases\nthe user not even knows which one to select.\n\nProposal\n========\n\nFor the scenarios where there are multiple identity providers\nand is desired to select a default one for the Horizon instance,\nwe propose to extend the WEBSSO_DEFAULT_REDIRECT flow to allow\noperators to also configure the desired identity provider that\nthe user will be automatically redirected to.\n\nChange-Id: I2887bbea530e5e8c5873aea4dc9c6e01ca641df8\n'}, {'number': 2, 'created': '2023-02-03 21:04:20.000000000', 'files': ['doc/source/configuration/settings.rst', 'openstack_auth/defaults.py', 'releasenotes/notes/add-support-to-set-default-idp-when-multiples-available-815c083660de3480.yaml', 'openstack_auth/tests/unit/test_auth.py', 'openstack_auth/views.py', 'openstack_auth/utils.py'], 'web_link': 'https://opendev.org/openstack/horizon/commit/9467b43c14ef8825544e5ce8fc58860131d18b10', 'message': 'Add support for default IdP login for multiple IdPs\n\nProblem description\n===================\n\nIn an OpenStack environment with multiple organizations, each one\nwith your own identity provider and Horizon instance with specific\nthemes. In these cases, when we enable the default websso redirect\nwe get a list of available identity providers, which in most cases\nthe user not even knows which one to select.\n\nProposal\n========\n\nFor the scenarios where there are multiple identity providers\nand is desired to select a default one for the Horizon instance,\nwe propose to extend the WEBSSO_DEFAULT_REDIRECT flow to allow\noperators to also configure the desired identity provider that\nthe user will be automatically redirected to.\n\nChange-Id: I2887bbea530e5e8c5873aea4dc9c6e01ca641df8\n'}]",5,872666,9467b43c14ef8825544e5ce8fc58860131d18b10,5,2,2,30695,,,0,"Add support for default IdP login for multiple IdPs

Problem description
===================

In an OpenStack environment with multiple organizations, each one
with your own identity provider and Horizon instance with specific
themes. In these cases, when we enable the default websso redirect
we get a list of available identity providers, which in most cases
the user not even knows which one to select.

Proposal
========

For the scenarios where there are multiple identity providers
and is desired to select a default one for the Horizon instance,
we propose to extend the WEBSSO_DEFAULT_REDIRECT flow to allow
operators to also configure the desired identity provider that
the user will be automatically redirected to.

Change-Id: I2887bbea530e5e8c5873aea4dc9c6e01ca641df8
",git fetch https://review.opendev.org/openstack/horizon refs/changes/66/872666/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/configuration/settings.rst', 'openstack_auth/defaults.py', 'openstack_auth/tests/unit/test_auth.py', 'openstack_auth/views.py', 'openstack_auth/utils.py']",5,cc147fdb8283bfe533b47ee49be9dbad3972400c,feature-add-default-idp-login-for-multiple-registered-idps,import randomimport stringdef get_random_csrf(): random_csrf = ''.join( [random.choice(string.ascii_letters + string.digits) for _ in range(11)]) ,,85,0
openstack%2Fopenstack-ansible~stable%2F2023.1~I87a5c85d42aa757f9789a81974403284c2d32051,openstack/openstack-ansible,stable/2023.1,I87a5c85d42aa757f9789a81974403284c2d32051,Bump SHAs for stable/2023.1,MERGED,2023-07-03 12:59:13.000000000,2023-07-10 11:53:08.000000000,2023-07-10 11:51:51.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 32666}]","[{'number': 1, 'created': '2023-07-03 12:59:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/88a602bde722d38d662ffb4a615d3b1616194661', 'message': 'Bump SHAs for stable/2023.1\n\nCloses-Bug: #2025513\nCloses-Bug: #2024407\nChange-Id: I87a5c85d42aa757f9789a81974403284c2d32051\n'}, {'number': 2, 'created': '2023-07-06 20:22:47.000000000', 'files': ['inventory/group_vars/trove_all/source_git.yml', 'ansible-collection-requirements.yml', 'inventory/group_vars/cinder_all/source_git.yml', 'inventory/group_vars/ironic_all/source_git.yml', 'inventory/group_vars/all/source_git.yml', 'inventory/group_vars/magnum_all/source_git.yml', 'inventory/group_vars/horizon_all/source_git.yml', 'ansible-role-requirements.yml', 'inventory/group_vars/gnocchi_all/source_git.yml', 'inventory/group_vars/designate_all/source_git.yml', 'inventory/group_vars/swift_all/source_git.yml', 'inventory/group_vars/neutron_all/source_git.yml', 'inventory/group_vars/nova_all/source_git.yml', 'inventory/group_vars/octavia_all/source_git.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/9921c23ea463c8cd16f709198e7602157848e41a', 'message': 'Bump SHAs for stable/2023.1\n\nCloses-Bug: #2025513\nCloses-Bug: #2024407\nChange-Id: I87a5c85d42aa757f9789a81974403284c2d32051\n'}]",1,887513,9921c23ea463c8cd16f709198e7602157848e41a,12,3,2,28619,,,0,"Bump SHAs for stable/2023.1

Closes-Bug: #2025513
Closes-Bug: #2024407
Change-Id: I87a5c85d42aa757f9789a81974403284c2d32051
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/13/887513/1 && git format-patch -1 --stdout FETCH_HEAD,"['inventory/group_vars/trove_all/source_git.yml', 'ansible-collection-requirements.yml', 'inventory/group_vars/cinder_all/source_git.yml', 'inventory/group_vars/ironic_all/source_git.yml', 'inventory/group_vars/all/source_git.yml', 'inventory/group_vars/magnum_all/source_git.yml', 'inventory/group_vars/horizon_all/source_git.yml', 'ansible-role-requirements.yml', 'inventory/group_vars/designate_all/source_git.yml', 'inventory/group_vars/swift_all/source_git.yml', 'inventory/group_vars/neutron_all/source_git.yml', 'inventory/group_vars/nova_all/source_git.yml', 'inventory/group_vars/octavia_all/source_git.yml']",13,88a602bde722d38d662ffb4a615d3b1616194661,bump_osa,### HEAD as of 03.07.2023 ###octavia_git_install_branch: 34d95db1332dea340ec1ca492cca71cdda53d7c2octavia_ovn_octavia_provider_git_install_branch: 217d1ab4317b886ef3544c029819121c91f0709f,### HEAD as of 30.05.2023 ###octavia_git_install_branch: 920dbfa5ad7d529ba0ee6ae2162fb73ddfc8ec13octavia_ovn_octavia_provider_git_install_branch: f6537b1d90a10d4e9bd03e93a2018d312df656bc,43,43
openstack%2Fbifrost~master~Id6279d681faf0c9a1893c00953b0b59d9319e08b,openstack/bifrost,master,Id6279d681faf0c9a1893c00953b0b59d9319e08b,Fix key-order[task] linter warnings,MERGED,2023-02-23 09:19:51.000000000,2023-07-10 11:44:08.000000000,2023-07-10 11:43:10.000000000,"[{'_account_id': 4571}, {'_account_id': 10342}, {'_account_id': 22348}, {'_account_id': 25600}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-02-23 09:19:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/efe3f6de22c43998d1bddeb4b363163fba5dcf41', 'message': 'Fix key-order[task] linter warnings\n\nAlso make indentation consistent, and remove traces of Suse.\n\nChange-Id: Id6279d681faf0c9a1893c00953b0b59d9319e08b\n'}, {'number': 2, 'created': '2023-03-12 22:57:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/796e3c8a34da0cefe044dfa2ef011a3fddafe239', 'message': 'Fix key-order[task] linter warnings\n\nIt looks a little jarring at first so see keys ordered this way,\nbut once you get used to it, it reads so much better!\n\nAlso make indentation consistent, clean up epel install\nand remove traces of Suse.\n\nChange-Id: Id6279d681faf0c9a1893c00953b0b59d9319e08b\n'}, {'number': 3, 'created': '2023-05-30 18:01:22.000000000', 'files': ['playbooks/roles/bifrost-ironic-install/ssh_public_key_path.yaml', 'playbooks/roles/bifrost-pip-install/tasks/main.yml', 'playbooks/roles/bifrost-create-vm-nodes/tasks/prepare_libvirt.yml', 'playbooks/test-bifrost.yaml', 'playbooks/roles/bifrost-create-vm-nodes/tasks/create_vm.yml', 'playbooks/roles/bifrost-keystone-install/tasks/bootstrap.yml', 'playbooks/roles/bifrost-keystone-client-config/tasks/validate.yml', 'playbooks/roles/bifrost-download-packages/tasks/main.yml', 'playbooks/roles/bifrost-ironic-install/tasks/create_tftpboot.yml', '.ansible-lint', 'playbooks/roles/bifrost-ironic-install/tasks/bootstrap.yml', 'playbooks/roles/bifrost-ironic-install/tasks/install.yml', 'playbooks/roles/bifrost-deploy-nodes-dynamic/tasks/main.yml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/db24a0c721477d759bde03a7bbed4f48f6c30c74', 'message': 'Fix key-order[task] linter warnings\n\nIt looks a little jarring at first so see keys ordered this way,\nbut once you get used to it, it reads so much better!\n\nAlso make indentation consistent, clean up epel install\nand remove traces of Suse.\n\nChange-Id: Id6279d681faf0c9a1893c00953b0b59d9319e08b\n'}]",2,874854,db24a0c721477d759bde03a7bbed4f48f6c30c74,23,5,3,25600,,,0,"Fix key-order[task] linter warnings

It looks a little jarring at first so see keys ordered this way,
but once you get used to it, it reads so much better!

Also make indentation consistent, clean up epel install
and remove traces of Suse.

Change-Id: Id6279d681faf0c9a1893c00953b0b59d9319e08b
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/54/874854/2 && git format-patch -1 --stdout FETCH_HEAD,"['playbooks/roles/bifrost-ironic-install/ssh_public_key_path.yaml', 'playbooks/roles/bifrost-pip-install/tasks/main.yml', 'playbooks/roles/bifrost-create-vm-nodes/tasks/prepare_libvirt.yml', 'playbooks/test-bifrost.yaml', 'playbooks/roles/bifrost-create-vm-nodes/tasks/create_vm.yml', 'playbooks/roles/bifrost-keystone-install/tasks/bootstrap.yml', 'playbooks/roles/bifrost-keystone-client-config/tasks/validate.yml', 'playbooks/roles/bifrost-download-packages/tasks/main.yml', 'playbooks/roles/bifrost-ironic-install/tasks/create_tftpboot.yml', 'playbooks/roles/bifrost-ironic-install/tasks/bootstrap.yml', 'playbooks/roles/bifrost-configdrives-dynamic/tasks/ssh_public_key_path.yaml', 'playbooks/roles/bifrost-ironic-install/tasks/install.yml', 'playbooks/roles/bifrost-deploy-nodes-dynamic/tasks/main.yml']",13,efe3f6de22c43998d1bddeb4b363163fba5dcf41,linters, when: instance_info is not defined or instance_info == {} block: when: - deploy_image_checksum is not defined - not deploy_image_source.startswith('file://') block:, block: block: when: - deploy_image_checksum is not defined - not deploy_image_source.startswith('file://') when: instance_info is not defined or instance_info == {} ,193,200
openstack%2Fopenstack-ansible~master~I44cb4e6bb41976c9e5f87958400a7b5c0816553b,openstack/openstack-ansible,master,I44cb4e6bb41976c9e5f87958400a7b5c0816553b,Reduce memory consumption in CI,ABANDONED,2022-06-16 21:58:28.000000000,2023-07-10 11:24:15.000000000,,"[{'_account_id': 22348}, {'_account_id': 28619}]","[{'number': 1, 'created': '2022-06-16 21:58:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/5a423850ff617f91b7586b74e339c741f739bc56', 'message': 'Reduce memory consumption in CI\n\nThis patch reduces memory usage of the different OpenStack Python\nservices by tuning glibc.\n\nThe specific tuning consist on disabling the per thread arenas and\ndisabling dynamic thresholds.\n\nRelated-To: https://review.opendev.org/c/openstack/tripleo-common/+/845807\nRelated-To: https://review.opendev.org/c/openstack/devstack/+/845805\nChange-Id: I44cb4e6bb41976c9e5f87958400a7b5c0816553b\n'}, {'number': 2, 'created': '2022-06-17 08:14:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/bd2d25e90f4b8c794cf91910e4f70cbe6fc1b94e', 'message': 'Reduce memory consumption in CI\n\nThis patch reduces memory usage of the different OpenStack Python\nservices by tuning glibc.\n\nThe specific tuning consist on disabling the per thread arenas and\ndisabling dynamic thresholds.\n\nRelated-To: https://review.opendev.org/c/openstack/tripleo-common/+/845807\nRelated-To: https://review.opendev.org/c/openstack/devstack/+/845805\nChange-Id: I44cb4e6bb41976c9e5f87958400a7b5c0816553b\n'}, {'number': 3, 'created': '2022-06-17 08:16:42.000000000', 'files': ['tests/roles/bootstrap-host/templates/user_variables.aio.yml.j2'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/0e468455e5dcbff2ec58f30340c289ee8dd646af', 'message': 'Reduce memory consumption in CI\n\nThis patch reduces memory usage of the different OpenStack Python\nservices by tuning glibc.\n\nThe specific tuning consist on disabling the per thread arenas and\ndisabling dynamic thresholds.\n\nRelated-To: https://review.opendev.org/c/openstack/tripleo-common/+/845807\nRelated-To: https://review.opendev.org/c/openstack/devstack/+/845805\nChange-Id: I44cb4e6bb41976c9e5f87958400a7b5c0816553b\n'}]",4,846228,0e468455e5dcbff2ec58f30340c289ee8dd646af,10,2,3,28619,,,0,"Reduce memory consumption in CI

This patch reduces memory usage of the different OpenStack Python
services by tuning glibc.

The specific tuning consist on disabling the per thread arenas and
disabling dynamic thresholds.

Related-To: https://review.opendev.org/c/openstack/tripleo-common/+/845807
Related-To: https://review.opendev.org/c/openstack/devstack/+/845805
Change-Id: I44cb4e6bb41976c9e5f87958400a7b5c0816553b
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/28/846228/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/roles/bootstrap-host/files/user_variables_proxy.yml', 'tests/roles/bootstrap-host/templates/user_variables.aio.yml.j2']",2,5a423850ff617f91b7586b74e339c741f739bc56,846228,global_environment_variables: MALLOC_ARENA_MAX: 2 MALLOC_MMAP_THRESHOLD_: 131072 MALLOC_TRIM_THRESHOLD_: 262144 ,,8,0
openstack%2Fgovernance~master~I81902f38079a344a8ba3ed33938da5a3b13c0907,openstack/governance,master,I81902f38079a344a8ba3ed33938da5a3b13c0907,Phased EOL of EM branches,NEW,2023-07-07 15:02:59.000000000,2023-07-10 09:50:54.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 15:02:59.000000000', 'files': ['resolutions/20230707-phased-eol-of-em.rst'], 'web_link': 'https://opendev.org/openstack/governance/commit/b20070002ec55013c876cfbe6b392342901255e6', 'message': 'Phased EOL of EM branches\n\nThis proposal aims for a phased EOL of EM branches as opposed to\nan immediate EOL.\n\nChange-Id: I81902f38079a344a8ba3ed33938da5a3b13c0907\n'}]",0,887969,b20070002ec55013c876cfbe6b392342901255e6,2,1,1,16465,,,0,"Phased EOL of EM branches

This proposal aims for a phased EOL of EM branches as opposed to
an immediate EOL.

Change-Id: I81902f38079a344a8ba3ed33938da5a3b13c0907
",git fetch https://review.opendev.org/openstack/governance refs/changes/69/887969/1 && git format-patch -1 --stdout FETCH_HEAD,['resolutions/20230707-phased-eol-of-em.rst'],1,b20070002ec55013c876cfbe6b392342901255e6,formal-vote,"=================================================================== 2023-07-07 Phased Application of New Extended Maintenance Branches =================================================================== This resolution aims to write a policy for moving extended maintenance branches for which resolution `2018-03-01 Extended maintenance for stable branches`[0] applies, to end-of-life. - For every release that exits the maintained phase, the last 3 branches under extended maintenance are moved to the new extended maintenance policy and requirements as defined in resolution `2023-07-07 New Requirements for Extended Maintenance`[1] [0]. https://governance.openstack.org/tc/resolutions/20180301-stable-branch-eol.html [1]. https://governance.openstack.org/tc/resolutions/20230707-extended-maintenance-new-requirements.html ",,14,0
openstack%2Fglance~master~Ic7b93b387ea93a9eec94a92920770e0e361c579d,openstack/glance,master,Ic7b93b387ea93a9eec94a92920770e0e361c579d,Fix parameter verification in import_image(),NEW,2023-04-04 06:42:49.000000000,2023-07-10 09:42:12.000000000,,"[{'_account_id': 9303}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-04-04 06:42:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/glance/commit/5acde251f316649f8ec1fc25a83f697bebe697f0', 'message': 'Fix parameter verification in import_image()\n\nA logic problem occurred in import_image() function,\nfix it.\n\nChange-Id: Ic7b93b387ea93a9eec94a92920770e0e361c579d\n'}, {'number': 2, 'created': '2023-07-10 07:39:08.000000000', 'files': ['glance/api/v2/images.py', 'api-ref/source/v2/images-import.inc', 'glance/tests/unit/v2/test_images_resource.py'], 'web_link': 'https://opendev.org/openstack/glance/commit/ee44dfd571309919063659c6ccbe58c632132ea7', 'message': 'Fix parameter verification in import_image()\n\n1. A logic problem occurred in import_image() function,\nfix it.\n2. Setting all_stores_must_succeed default to False.\n\nChange-Id: Ic7b93b387ea93a9eec94a92920770e0e361c579d\n'}]",5,879304,ee44dfd571309919063659c6ccbe58c632132ea7,14,2,2,30858,,,0,"Fix parameter verification in import_image()

1. A logic problem occurred in import_image() function,
fix it.
2. Setting all_stores_must_succeed default to False.

Change-Id: Ic7b93b387ea93a9eec94a92920770e0e361c579d
",git fetch https://review.opendev.org/openstack/glance refs/changes/04/879304/2 && git format-patch -1 --stdout FETCH_HEAD,['glance/api/v2/images.py'],1,5acde251f316649f8ec1fc25a83f697bebe697f0,, if all_stores_must_succeed and (not CONF.enabled_backends):, if (not all_stores_must_succeed) and (not CONF.enabled_backends):,1,1
openstack%2Fopenstacksdk~master~I44335a22846f1a11ba60e8bb758b10c39e728897,openstack/openstacksdk,master,I44335a22846f1a11ba60e8bb758b10c39e728897,Fix list of server migrations,NEW,2023-07-06 13:35:46.000000000,2023-07-10 08:54:25.000000000,,"[{'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 13:35:46.000000000', 'files': ['openstack/compute/v2/_proxy.py'], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/d7c448ecb60a1a3b4f8b654c49b1b268c66d2e10', 'message': 'Fix list of server migrations\n\nthe _list method goes straight through to formatting base_path with attrs,\nand for server migrations the attr in the base path is server_uuid,\nnot server_id.\n\nChange-Id: I44335a22846f1a11ba60e8bb758b10c39e728897\nStory: 2010633\nTask: 47591\n'}]",4,887847,d7c448ecb60a1a3b4f8b654c49b1b268c66d2e10,5,2,1,9542,,,0,"Fix list of server migrations

the _list method goes straight through to formatting base_path with attrs,
and for server migrations the attr in the base path is server_uuid,
not server_id.

Change-Id: I44335a22846f1a11ba60e8bb758b10c39e728897
Story: 2010633
Task: 47591
",git fetch https://review.opendev.org/openstack/openstacksdk refs/changes/47/887847/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/compute/v2/_proxy.py'],1,d7c448ecb60a1a3b4f8b654c49b1b268c66d2e10,story/2010633," server_uuid=server_id,"," server_id=server_id,",1,1
openstack%2Foctavia~master~Iae9bc44f39ccea406b482f5008d3cb569c41e631,openstack/octavia,master,Iae9bc44f39ccea406b482f5008d3cb569c41e631,update requirements,ABANDONED,2020-10-28 07:46:31.000000000,2023-07-10 08:37:06.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2020-10-28 07:46:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a63554e8b2d97c6502021ffe17076e8c77d442ed', 'message': 'Bump hacking min version to 3.0.1\n\nhacking 3.0.1 fix the pinning of flake8 to avoid bringing in a new\nversion with new checks.\n\nbumping the min version for hacking so that any older hacking versions\nwhich auto adopt the new checks are not used.\n\nChange-Id: Iae9bc44f39ccea406b482f5008d3cb569c41e631\n'}, {'number': 2, 'created': '2020-10-29 02:08:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a151fcc144f5d0ce5fee748f9fbdf06b62f87bb0', 'message': 'update requirements\n\nWe also need to change the lower-constraint requirements to make them\npy3.8 compatible. See https://bugs.launchpad.net/nova/+bug/1886298\n\nMarkupSafe==1.1.1\nparamiko==2.7.1\n\nChange-Id: Iae9bc44f39ccea406b482f5008d3cb569c41e631\n'}, {'number': 3, 'created': '2020-12-27 03:41:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/85d2667617233ca7a4e837d027b615a4430410a5', 'message': 'update requirements\n\nWe also need to change the lower-constraint requirements to make them\npy3.8 compatible. See https://bugs.launchpad.net/nova/+bug/1886298\n\nMarkupSafe==1.1.1\nparamiko==2.7.1\n\nChange-Id: Iae9bc44f39ccea406b482f5008d3cb569c41e631\n'}, {'number': 4, 'created': '2020-12-27 08:19:24.000000000', 'files': ['test-requirements.txt', 'lower-constraints.txt'], 'web_link': 'https://opendev.org/openstack/octavia/commit/a3ab813a11d91bc74909868d4a8fe8a8e5b070f1', 'message': 'update requirements\n\nWe also need to change the lower-constraint requirements to make them\npy3.8 compatible. See https://bugs.launchpad.net/nova/+bug/1886298\n\nMarkupSafe==1.1.1\nparamiko==2.7.1\n\nChange-Id: Iae9bc44f39ccea406b482f5008d3cb569c41e631\n'}]",0,760065,a3ab813a11d91bc74909868d4a8fe8a8e5b070f1,12,1,4,32029,,,0,"update requirements

We also need to change the lower-constraint requirements to make them
py3.8 compatible. See https://bugs.launchpad.net/nova/+bug/1886298

MarkupSafe==1.1.1
paramiko==2.7.1

Change-Id: Iae9bc44f39ccea406b482f5008d3cb569c41e631
",git fetch https://review.opendev.org/openstack/octavia refs/changes/65/760065/1 && git format-patch -1 --stdout FETCH_HEAD,['test-requirements.txt'],1,a63554e8b2d97c6502021ffe17076e8c77d442ed,hacking-fix,hacking>=3.0.1 # Apache-2.0,hacking>=3.0 # Apache-2.0,1,1
openstack%2Fos-brick~master~I819e32f807bcba4c9d6f928f2ad04c4dd27f22c2,openstack/os-brick,master,I819e32f807bcba4c9d6f928f2ad04c4dd27f22c2,linuxrbd: Remove rados_connect_timeout parameter,MERGED,2023-02-06 20:59:45.000000000,2023-07-10 08:36:56.000000000,2023-07-10 08:35:56.000000000,"[{'_account_id': 5314}, {'_account_id': 20813}, {'_account_id': 22348}, {'_account_id': 27615}, {'_account_id': 35075}]","[{'number': 1, 'created': '2023-02-06 20:59:45.000000000', 'files': ['os_brick/initiator/linuxrbd.py'], 'web_link': 'https://opendev.org/openstack/os-brick/commit/acd0265f8d806341672ede5597441967edd5c5d2', 'message': ""linuxrbd: Remove rados_connect_timeout parameter\n\nThis parameter doesn't do anything, we should skip it here.\n\nhttps://docs.ceph.com/en/latest/rados/api/python/#rados.Rados.connect\nhttps://github.com/ceph/ceph/blob/974339d1f/src/pybind/rados/rados.pyx#L674\n\nChange-Id: I819e32f807bcba4c9d6f928f2ad04c4dd27f22c2\n""}]",4,872536,acd0265f8d806341672ede5597441967edd5c5d2,22,5,1,4523,,,0,"linuxrbd: Remove rados_connect_timeout parameter

This parameter doesn't do anything, we should skip it here.

https://docs.ceph.com/en/latest/rados/api/python/#rados.Rados.connect
https://github.com/ceph/ceph/blob/974339d1f/src/pybind/rados/rados.pyx#L674

Change-Id: I819e32f807bcba4c9d6f928f2ad04c4dd27f22c2
",git fetch https://review.opendev.org/openstack/os-brick refs/changes/36/872536/1 && git format-patch -1 --stdout FETCH_HEAD,['os_brick/initiator/linuxrbd.py'],1,acd0265f8d806341672ede5597441967edd5c5d2,," LOG.debug(""opening connection to ceph cluster"") client.connect()"," self.rados_connect_timeout: int = kwargs.get('rados_connect_timeout', -1) LOG.debug(""opening connection to ceph cluster (timeout=%s)."", self.rados_connect_timeout) if self.rados_connect_timeout >= 0: client.connect( timeout=self.rados_connect_timeout) else: client.connect()",2,9
openstack%2Fpython-zaqarclient~master~I7ad69220a938d27529b794830c9da3221dd150c7,openstack/python-zaqarclient,master,I7ad69220a938d27529b794830c9da3221dd150c7,bump py37 to py38 in tox.ini,ABANDONED,2020-10-21 09:43:34.000000000,2023-07-10 08:36:55.000000000,,"[{'_account_id': 22348}, {'_account_id': 32029}]","[{'number': 1, 'created': '2020-10-21 09:43:34.000000000', 'files': ['setup.cfg', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/python-zaqarclient/commit/09e6cc9261894709b2532d7536a6f2a6c50b9b65', 'message': ""bump py37 to py38 in tox.ini\n\nfrom 'victoria' cycle, we should test py38 by default.\n\n[1] https://governance.openstack.org/tc/reference/runtimes/victoria.html\n\nChange-Id: I7ad69220a938d27529b794830c9da3221dd150c7\n""}]",0,758982,09e6cc9261894709b2532d7536a6f2a6c50b9b65,5,2,1,32029,,,0,"bump py37 to py38 in tox.ini

from 'victoria' cycle, we should test py38 by default.

[1] https://governance.openstack.org/tc/reference/runtimes/victoria.html

Change-Id: I7ad69220a938d27529b794830c9da3221dd150c7
",git fetch https://review.opendev.org/openstack/python-zaqarclient refs/changes/82/758982/1 && git format-patch -1 --stdout FETCH_HEAD,"['setup.cfg', 'tox.ini']",2,09e6cc9261894709b2532d7536a6f2a6c50b9b65,py38,"envlist = py38,pep8","envlist = py37,pep8",2,1
openstack%2Fpython-barbicanclient~master~I11d75990bcfddd54c4d0346823e972554a216c2c,openstack/python-barbicanclient,master,I11d75990bcfddd54c4d0346823e972554a216c2c,Drop python3.6/3.7 support in testing runtime,ABANDONED,2022-06-01 08:26:30.000000000,2023-07-10 08:36:07.000000000,,"[{'_account_id': 7973}, {'_account_id': 14250}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-06-01 08:26:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/a14579a78dd7b2c4a95dc7e9d120009429c111fb', 'message': 'Drop python3.6/3.7 support in testing runtime\n\nIn Zed cycle testing runtime, we are targetting to drop the\npython 3.6/3.7 support, project started adding python 3.8 as minimum\n\n[1] https://governance.openstack.org/tc/reference/runtimes/zed.html\n\nChange-Id: I11d75990bcfddd54c4d0346823e972554a216c2c\n'}, {'number': 2, 'created': '2023-02-10 11:46:22.000000000', 'files': ['setup.cfg'], 'web_link': 'https://opendev.org/openstack/python-barbicanclient/commit/db45c116c0e11214976816a753fb3556bdff8eeb', 'message': 'Drop python3.6/3.7 support in testing runtime\n\nIn Zed cycle testing runtime, we are targetting to drop the\npython 3.6/3.7 support, project started adding python 3.8 as minimum\n\n[1] https://governance.openstack.org/tc/reference/runtimes/zed.html\n\nChange-Id: I11d75990bcfddd54c4d0346823e972554a216c2c\n'}]",2,844258,db45c116c0e11214976816a753fb3556bdff8eeb,7,3,2,32029,,,0,"Drop python3.6/3.7 support in testing runtime

In Zed cycle testing runtime, we are targetting to drop the
python 3.6/3.7 support, project started adding python 3.8 as minimum

[1] https://governance.openstack.org/tc/reference/runtimes/zed.html

Change-Id: I11d75990bcfddd54c4d0346823e972554a216c2c
",git fetch https://review.opendev.org/openstack/python-barbicanclient refs/changes/58/844258/2 && git format-patch -1 --stdout FETCH_HEAD,['setup.cfg'],1,a14579a78dd7b2c4a95dc7e9d120009429c111fb,setup,, Programming Language :: Python :: 3.6 Programming Language :: Python :: 3.7,0,2
openstack%2Fglance~master~Ide428268f52527972165ab59ac6e3da0688968f4,openstack/glance,master,Ide428268f52527972165ab59ac6e3da0688968f4,Remove the last occurrence of six.add_metaclass,MERGED,2023-06-22 13:50:25.000000000,2023-07-10 08:21:51.000000000,2023-07-10 08:20:27.000000000,"[{'_account_id': 4393}, {'_account_id': 9303}, {'_account_id': 22348}, {'_account_id': 32238}]","[{'number': 1, 'created': '2023-06-22 13:50:25.000000000', 'files': ['glance/async_/flows/_internal_plugins/base_download.py'], 'web_link': 'https://opendev.org/openstack/glance/commit/11061d5b5a11fec0fb2ca6d55b0f116b8c716928', 'message': 'Remove the last occurrence of six.add_metaclass\n\nChange-Id: Ide428268f52527972165ab59ac6e3da0688968f4\n'}]",7,886754,11061d5b5a11fec0fb2ca6d55b0f116b8c716928,55,4,1,8122,,,0,"Remove the last occurrence of six.add_metaclass

Change-Id: Ide428268f52527972165ab59ac6e3da0688968f4
",git fetch https://review.opendev.org/openstack/glance refs/changes/54/886754/1 && git format-patch -1 --stdout FETCH_HEAD,['glance/async_/flows/_internal_plugins/base_download.py'],1,11061d5b5a11fec0fb2ca6d55b0f116b8c716928,remove-six-metaclass,"class BaseDownload(task.Task, metaclass=abc.ABCMeta):",import six@six.add_metaclass(abc.ABCMeta) class BaseDownload(task.Task):,1,3
openstack%2Fdesignate~master~I7c30db493111107aba79de6c5361ab0726046704,openstack/designate,master,I7c30db493111107aba79de6c5361ab0726046704,Imported Translations from Zanata,MERGED,2023-07-08 02:53:33.000000000,2023-07-10 08:19:10.000000000,2023-07-10 08:17:33.000000000,"[{'_account_id': 22348}, {'_account_id': 22623}]","[{'number': 1, 'created': '2023-07-08 02:53:33.000000000', 'files': ['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'], 'web_link': 'https://opendev.org/openstack/designate/commit/963751f2b403c3e9a73008ac62854ed3d3758247', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I7c30db493111107aba79de6c5361ab0726046704\n'}]",0,888005,963751f2b403c3e9a73008ac62854ed3d3758247,7,2,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: I7c30db493111107aba79de6c5361ab0726046704
",git fetch https://review.opendev.org/openstack/designate refs/changes/05/888005/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'],1,963751f2b403c3e9a73008ac62854ed3d3758247,zanata/translations,"""POT-Creation-Date: 2023-07-05 16:39+0000\n""""PO-Revision-Date: 2023-07-07 09:08+0000\n""msgid ""15.0.0-7"" msgstr ""15.0.0-7""msgid ""16.0.0-45"" msgstr ""16.0.0-45""msgid """" ""Fixed issues with list zones and recordsets when a zone is shared with more "" ""than one project."" msgstr """" ""Fixed issues with list zones and recordsets when a zone is shared with more "" ""than one project."" ","""POT-Creation-Date: 2023-06-08 16:18+0000\n""""PO-Revision-Date: 2023-06-17 02:38+0000\n""msgid ""15.0.0-6"" msgstr ""15.0.0-6""msgid ""16.0.0-41"" msgstr ""16.0.0-41""",13,6
openstack%2Fcharm-ceph-mon~master~I07314133118939a9fe24603342282e24057d5b9f,openstack/charm-ceph-mon,master,I07314133118939a9fe24603342282e24057d5b9f,DNM: test merge request,ABANDONED,2023-07-08 19:43:19.000000000,2023-07-10 07:04:47.000000000,,"[{'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-08 19:43:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ceph-mon/commit/28abe435367428556b4ddf798752b2c797368c60', 'message': 'DNM: test merge request\n\nChange-Id: I07314133118939a9fe24603342282e24057d5b9f\n'}]",0,888009,28abe435367428556b4ddf798752b2c797368c60,4,2,1,15382,,,0,"DNM: test merge request

Change-Id: I07314133118939a9fe24603342282e24057d5b9f
",git fetch https://review.opendev.org/openstack/charm-ceph-mon refs/changes/09/888009/1 && git format-patch -1 --stdout FETCH_HEAD,[],0,28abe435367428556b4ddf798752b2c797368c60,,,,0,0
openstack%2Foctavia~master~I6bebc1bb0c8e2165fd6f26a341ff3f36ec669eda,openstack/octavia,master,I6bebc1bb0c8e2165fd6f26a341ff3f36ec669eda,New spec for Active-Active L3 Distributor,NEW,2023-07-10 06:19:37.000000000,2023-07-10 06:28:54.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-10 06:19:37.000000000', 'files': ['doc/source/contributor/index.rst', 'specs/version1.2/active-active-l3-distributor.rst'], 'web_link': 'https://opendev.org/openstack/octavia/commit/796199ff17f92797d8e444709628599b35eb8084', 'message': 'New spec for Active-Active L3 Distributor\n\nChange-Id: I6bebc1bb0c8e2165fd6f26a341ff3f36ec669eda\n'}]",0,888020,796199ff17f92797d8e444709628599b35eb8084,2,1,1,29244,,,0,"New spec for Active-Active L3 Distributor

Change-Id: I6bebc1bb0c8e2165fd6f26a341ff3f36ec669eda
",git fetch https://review.opendev.org/openstack/octavia refs/changes/20/888020/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/contributor/index.rst', 'specs/version1.2/active-active-l3-distributor.rst']",2,796199ff17f92797d8e444709628599b35eb8084,active-active-l3-distributor-spec,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode =================================================== Distributor for L3 Active-Active, N+1 Amphora Setup =================================================== .. attention:: Please review the active-active topology blueprint first ( :doc:`../version0.9/active-active-topology` ). This spec is based on a previous spec :doc:`../version1.1/active-active-l3-distributor`. Problem description =================== This blueprint describes a *L3 active-active* distributor implementation to support the Octavia *active-active-topology*. The *L3 active-active* distributor will leverage the capabilities of a layer 3 Clos network fabric in order to distribute traffic to an *Amphora Cluster* of 1 or more amphoras. Specifically, the *L3 active-active* distributor design will leverage Equal Cost Multipath Load Sharing (ECMP) with anycast routing to achieve traffic distribution across the *Amphora Cluster*. In this reference implementation, the BGP routing protocol will be used to inject anycast routes into the L3 fabric. In order to scale a single VIP address across multiple active amphoras it is required to have a *distributor* to balance the traffic. By leveraging the existing capabilities of a modern L3 network, we can use the network itself as the *distributor*. This approach has several advantages, which include: * Traffic will be routed via the best path to the destination amphora. There is no need to add an additional hop (*distributor*) between the network and the amphora. * The *distributor* is not in the data path and simply becomes a function of the L3 network. * The performance and scale of the *distributor* is the same as the L3 network. * Native support for both IPv4 and IPv6, without customized logic for each address family. .. _P2: **Note:** Items marked with [`P2`_] refer to lower priority features to be designed / implemented only after initial release. Proposed change =============== * Octavia shall implement the *L3 active-active* distributor as an alternative driver which could replace VRRP in the Amphora driver. * The distributor control plane function (*bgp speaker*) will run inside the amphora and leverage the existing amphora lifecycle manager. * Each amphora will run a *bgp speaker* in the default namespace in order to announce the anycast VIP into the L3 fabric. BGP peering and announcements will occur over a dedicated network, the distributor network. The anycast VIP will get advertised as a /32 or /128 route with a next-hop of the front-end IP assigned to the amphora instance. The front-end network IPs must be directly routable from the L3 fabric, such as in the provider networking model. **Note:** [gthiemonge] if the distributor subnet is also the vip network, the LB uses 3 IP addresses allocated from the vip subnet for each amphorae (1 for the distributor port in the default namespace, 1 for the amphora IP, 1 for the VIP - shared with other amphorae) * Octavia shall implement the ability to specify a distributor subnet and VIP subnet (front-end subnet) when creating a new load balancer. The amphora will have ports on three networks (distributor, vip, management). The anycast VIP will get configured on the loopback interface inside the *amphora-haproxy* network namespace. **Note:** [gthiemonge] the distributor subnet can be specified via the Distributor BGP Speaker API * The operator shall be able to define a *bgp peer profile* for a tenant, which includes the required metadata for the amphora to establish a bgp peering session with the L3 fabric. The bgp peering information will be passed into the amphora-agent configuration file via the Octavia Amphora API. * The operator shall be able to define a *bgp speaker profile* associated to a *bgp peer profile*, which includes additional metadata for amphora to establish the bgp peering session with the L3 fabric. The bgp peering information will be passed into the amphora-agent configuration file via the Octavia Amphora API. The amphora will use the bgp peering information and the bgp speaker information to establish a BGP peer and announce its anycast VIP. * [`P2_`] A load balancer will handle multiple *bgp speaker* configuration. * [`P2`_] Add the option to allow the *bgp speaker* to run on a dedicated amphora instance that is not running the software load balancer (HAProxy). In this model a dedicated *bgp speaker* could advertise anycast VIPs for one or more amphoras. Each BGP speaker (peer) can only announce a single next-hop route for an anycast VIP. In order to perform ECMP load sharing, multiple dedicated amphoras running bgp speakers will be required, each of them would then announce a different next-hop address for the anycast VIP. Each next-hop address is the front-end (provider network) IP of an amphora instance running the software load balancer. * [`P2`_] The *Amphora Cluster* will provide resilient flow handling in order to handle ECMP group flow remapping events and support amphora connection draining. * [`P2`_] Support Floating IPs (FIPs). In order to support FIPs the existing Neutron *floatingips* API would need to be extended. Architecture ------------ High-level Topology Description ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The below diagram shows the interaction between 2 .. n amphora instances from each tenant and how they interact with the L3 network distributor. :: Management Front-End Internet Network Networks (World) ║ (provider) ║ ║ ┌─────────────────────────────┐ ║ ║ ║ │ Amphora of Tenant A │ ║ ┌──╨──────────┐ ║ ┌────┬┴──────────┬──────────────────┴┬───╨┐ │ │ ╠══════╡MGMT│ns: default│ns: amphora-haproxy│f.e.│ │ │ ║ │ IP ├-----------┼-------------------┤ IP │ │ │ ║ └────┤ BGP │ Anycast VIP ├───╥┘ │ │ ║ │ Speaker │ (loopback) │ ║ │ │ ║ └───────────┴──────────────╥────┘ ║ │ │ ║ | ║ ║ │ │ ║ | ║ ║ │ │ Peering Session 1..* | ║ ║ │ │---------------------------+ ║ ║ │ │ {anycast VIP}/32 next-hop {f.e. IP} ║ ║ │ │ ║ ║ ║ │ Distributor │ ║ ┌─────────────────────────╨───┐ ║ │ BGP Peer │ ║ │ Amphora of Tenant A │ ║ │ (L3 Network)│ ║ ┌────┬┴──────────┬──────────────────┴┬───╨┐ │ ╞════════╬══════╡MGMT│ns: default│ns: amphora-haproxy│f.e.│ │ │ ║ │ IP ├-----------┼-------------------┤ IP │ │ │ ║ └────┤ BGP │ Anycast VIP ├───╥┘ │ │ ║ │ Speaker │ (loopback) │ ║ │ │ ║ └───────────┴──────────────╥────┘ ║ │ │ ║ | ║ ║ │ │ ║ | ║ ║ │ │ Peering Session 1..* | ║ ║ │ │---------------------------+ ║ ║ │ │ {anycast VIP}/32 next-hop {f.e. IP} ║ ║ └──╥──────────┘ ║ ║ ║ ║ ║ ┌─────────────────────────╨───┐ ║ ┌──╨──────────┐ ║ │ Amphora of Tenant C │ ║ │ │ ║ ┌────┬┴──────────┬──────────────────┴┬───╨┐ │ │ ╚══════╡MGMT│ns: default│ns: amphora-haproxy│f.e.│ │ │ │ IP ├-----------┼-------------------┤ IP │ │ │ └────┤ BGP │ Anycast VIP ├────┘ │ │ │ Speaker │ (loopback) │ │ Distributor │ └───────────┴──────────────╥────┘ │ BGP Peer │ | ║ │ (L3 Network)│ | ║ │ │ Peering Session 1..* | ║ │ │---------------------------+ ║ │ │ {anycast VIP}/32 next-hop {f.e. IP} ║ │ │ ║ │ ╞═══════════════════════════════════════════════Anycast └─────────────┘ 1..* Network * Whenever a new active-active amphora is instantiated it will create BGP peering session(s) over the distributor network to the L3 fabric. The BGP peer will need to have a neighbor definition in order to allow the peering sessions from the amphoras. In order to ease configuration, a neighbor statement allowing peers from the entire distributor IP prefix range can be defined: ``neighbor 10.10.10.0/24`` * The BGP peer IP can either be a route reflector (RR) or any other network device that will redistribute routes learned from the amphora BGP speaker. In order to help scaling, it is possible to peer with the ToR switch based on the rack the amphora instance is provisioned in. The configuration can be simplified by creating an ``anycast loopback interface`` on each ToR switch, which will provide a consistent BGP peer IP regardless of which rack or hypervisor is hosting the amphora instance. * Once a peering session is established between an amphora and the L3 fabric, the amphora will need to announce its anycast VIP with a next-hop address of its front-end network IP. The front-end network IP (provider) must be routable and reachable from the L3 network in order to be used. * In order to leverage ECMP for distributing traffic across multiple amphoras, multiple equal-cost routes must be installed into the network for the anycast VIP. This requires the L3 network to have ``Multipath BGP`` enabled, so BGP installs multiple paths and does not select a single best path. * After the amphoras in a cluster are initialized there will be an ECMP group with multiple equal-cost routes for the anycast VIP. The data flow for traffic is highlighted below: 1. Traffic will ingress into the L3 network fabric with a destination IP address of the anycast VIP. 2. If this is a new flow, the flow will get hashed to one of the next-hop addresses in the ECMP group. 3. The packet will get sent to the front-end IP address of the amphora instance that was selected from the above step. 4. The amphora will accept the packet and send it to the back-end server over the front-end network or a directly attached back-end (tenant) network attached to the amphora. 5. The amphora will receive the response from the back-end server and forward it on to the next-hop gateway of front-end (provider) network using the anycast VIP as the source IP address. 6. All subsequent packets belonging to the same flow will get routed through the same path. * Adding or removing members to a L3 active-active amphora cluster will result in flow remapping, as different paths will be selected due to rehashing. It is recommended to enable the ``resilient hashing`` feature on ECMP groups in order to minimize flow remapping. Distributor (BGP Speaker) Lifecycle ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The below diagram shows the interaction between an amphora instance that is serving as a distributor and the L3 network. In this example we are peering with the ToR switch in order to disseminate anycast VIP routes into the L3 network. :: +------------------------------------------------+ | Initialize Distributor on Amphora | +------------------------------------------------+ | | | +---------------+ +---------------+ | | |1 | |4 | | | | Amphora | | Ready to | | | | (boot) | | announce | | | | | | VIP(s) | | | +-------+-------+ +-------+-------+ | | | ^ | | | | | | | | | | | | | | | | | | v | | | +-------+-------+ +-------+-------+ | | |2 | |3 Establish | | | | Read Config | | BGP connection| | | | Drive +----------->+ to ToR(s) | | | | (BGP Config) | | (BGP Speaker) | | | +---------------+ +---------------+ | | | +------------------------------------------------+ +------------------------------------------------+ | Register AMP to Distributor or Listener Start | +------------------------------------------------+ | | | +---------------+ +---------------+ | | |5 | |8 | | | | Amphora | | Amphora | | | | BGP Speaker | | (Receives VIP | | | |(Announce VIP) | | Traffic) | | | +-------+-------+ +-------+-------+ | | | ^ | | | | | | |BGP Peering | | | |Session(s) | | | | | | | v | | | +-------+-------+ +-------+-------+ | | |6 | |7 | | | | ToR(s) | | L3 Fabric | | | |(Injects Route +----------->+ Accepts Route | | | | into Fabric) | | (ECMP) | | | +---------------+ +---------------+ | | | +------------------------------------------------+ +------------------------------------------------+ | Unregister AMP to Distributor or Listener Stop | +------------------------------------------------+ | | | +---------------+ +---------------+ | | |9 | |12 | | | | Amphora | | Amphora | | | | BGP Speaker | |(No longer sent| | | |(Withdraw VIP) | | VIP traffic) | | | +-------+-------+ +-------+-------+ | | | ^ | | | | | | |BGP Peering | | | |Session(s) | | | | | | | v | | | +-------+-------+ +-------+-------+ | | |10 | |11 | | | | ToR(s) | | L3 Fabric | | | |(Removes Route +----------->+ Removes Route | | | | from Fabric) | | (ECMP) | | | +---------------+ +---------------+ | | | +------------------------------------------------+ 1. The amphora gets created and is booted. In this example, the amphora will perform both the load balancing (HAProxy) and L3 Distributor function (BGP Speaker). 2. The Controller Worker will send the BGP configuration information to the Amphora through the API and the Amphora agent will configure the BGP Speaker to peer with the ToR switch. 3. The BGP Speaker process will start and establish a BGP peering session with the ToR switch. 4. Once the BGP peering session is active, the amphora is ready to advertise its anycast VIP into the network with a next-hop of its front-end IP address. 5. The BGP speaker will communicate using the BGP protocol and send a BGP ""announce"" message to the ToR switch in order to announce a VIP route. If the amphora is serving as both a load balancer and distributor the announcement will happen on listener start. Otherwise the announce will happen on a register amphora request to the distributor. 6. The ToR switch will learn this new route and advertise it into the L3 fabric. At this point the L3 fabric will know of the new VIP route and how to reach it (via the ToR that just announced it). 7. The L3 fabric will create an ECMP group if it has received multiple route advertisements for the same anycast VIP. This will result in a single VIP address with multiple next-hop addresses. 8. Once the route is accepted by the L3 fabric, traffic will get distributed to the recently registered amphora (HAProxy). 9. The BGP speaker will communicate using the BGP protocol and send a BGP ""withdraw"" message to the ToR switch in order to withdraw a VIP route. If the amphora is serving as both a load balancer and distributor the withdrawal will happen on listener stop. Otherwise the withdraw will happen on an unregister amphora request to the distributor. 10. The ToR switch will tell the L3 fabric over BGP that the anycast VIP route for the amphora being unregistered is no longer valid. 11. The L3 fabric will remove the VIP address with the next-hop address to the amphora (HAProxy) being unregistered. It will keep all other existing VIP routes to other amphora (HAProxy) instances until they are explicitly unregistered. 12. Once the route is removed the amphora (HAProxy) will no longer receive any traffic for the VIP. Alternatives ------------ TBD Data model impact ----------------- Add the following columns to the existing ``vip`` table: * distributor_id ``(String(36) , nullable=True)`` ID of the distributor responsible for distributing traffic for the corresponding VIP. Add table ``distributor`` with the following columns: * id ``(String(36) , nullable=False)`` ID of Distributor instance. * name ``(String(255) , nullable=True)`` Name of Distributor instance. * description ``(String(255) , nullable=True)`` Description of Distributor instance. * project_id ``(String(36) , nullable=False)`` Project of the Distributor instance. * enabled ``(bool , nullable=False)`` Admin status of Distributor instance. * distributor_type ``(String(16) , nullable=False)`` Type of distributor (``BGP-PEER`` or ``BGP-SPEAKER``). * provisioning_status ``(String(16) , nullable=False)`` Provisioning status. * operating_status ``(String(16) , nullable=False)`` Operating status. Update existing table ``amphora``. The vrrp_* tables will be renamed to frontend_* in order to make the purpose of this interface more apparent and to better represent other use cases besides active/standy. * distributor_port_id ``(String(36) , nullable=True)`` New field that represents the port ID that is attached to the distributor network (can be null if the management network is used). * distributor_ip ``(String(36) , nullable=True)`` New field that represents the IP address of the distributor port (is null if the distributor port is not used). * frontend_ip ``(String(64) , nullable=True)`` New name for former vrrp_ip field. This is the primary IP address inside the amphora-haproxy namespace used for L3 communication to back-end members. * frontend_subnet_id ``(String(36) , nullable=True)`` New field added to the amphora table, which is the neutron subnet id of the front-end network connected to the amphora. * frontend_port_id ``(String(36) , nullable=True)`` New name for former vrrp_port_id field. This represents the neutron port ID of a port attached to the front-end network. It should no longer be assumed that the front-end subnet is the same as the VIP subnet. * frontend_interface ``(String(16) , nullable=True)`` New name for former vrrp_interface field. * frontend_id ``(Integer , nullable=True)`` New name for former vrrp_id field. * frontend_priority ``(Integer , nullable=True)`` New name for former vrrp_priority field. Add table ``distributor_l3_bgp_peer`` with the following columns: * id ``(String(36) , nullable=False)`` ID of the BGP peer. Each ``distributor_l3_bgp_peer`` entry is associated with a ``distributor`` entry of type ``BGP-PEER``, they share the same ID. **Note:** [gthiemonge] need to validate this approach * name ``(String(255) , nullable=True)`` Name of BGP peer. * description ``(String(255) , nullable=True)`` Description of BGP peer. * project_id ``(String(36) , nullable=False)`` Project of the BGP peer. * enabled ``(bool , nullable=False)`` Admin status of BGP peer. * peer_ip ``(String(64) , nullable=False)`` The IP address of the BGP neighbor. * remote_as ``(Integer , nullable=False)`` Remote AS of the BGP peer. * auth_type ``(String(16) , nullable=True)`` Authentication type (``md5`` or null). * auth_pass ``(String(128) , nullable=True)`` Authentication password (only if ``auth_type`` is not null). **Note:** [gthiemonge] do we need another object to hide the password from the users? * ttl_hops ``(Integer , nullable=True)`` Number of hops between speaker and peer for ttl security ``1-254``. * hold_time ``(Integer , nullable=True)`` Amount of time in seconds that can elapse between messages from peer. * keepalive_interval ``(Integer , nullable=True)`` How often to send keep alive packets in seconds. Add table ``distributor_l3_bgp_speaker`` with the following columns: * id ``(String(36) , nullable=False)`` ID of the BGP speaker. Each ``distributor_l3_bgp_speaker`` entry is associated with a ``distributor`` entry of type ``BGP-PEER``, they share the same ID. * name ``(String(255) , nullable=True)`` Name of BGP speaker. * description ``(String(255) , nullable=True)`` Description of BGP speaker. * project_id ``(String(36) , nullable=False)`` Project of the BGP speaker. * ip_version ``(Integer , nullable=False)`` Protocol version of the BGP speaker. IP version ``4`` or ``6``. * local_as ``(Integer , nullable=False)`` Local AS number for the BGP speaker. * subnet_id ``(String(64) , nullable=False)`` The subnet used by the Amphora to send the BGP announcements. **Note:** [gthiemonge] the previous spec added this field to the load balancer API/DB, but a user may not know which subnet they could use for BGP peers. Add table ``distributor_l3_bgp_peer_speaker_binding`` with the following columns: * peer_id ``(String(36), nullable=False, primary_key=True)`` The ID of the L3 BGP Peer. * speaker_id ``(String(36), nullable=False, primary_key=True)`` The ID of the L3 BGP Speaker. **Note:** This table maintains a n:n relationship between L3 Peers and L3 Speakers. REST API impact --------------- LBaaSv2 API ^^^^^^^^^^^ Add a new constant ``MULTI_ACTIVE``. It represents the topology of the Load Balancer that implements this spec. Distributor API: * GET /lbaas/distributors List all the distributors (it includes BGP Peers and BGP Speakers). rule: load-balancer:read * GET /lbaas/distributors/<uuid> Get a specific distributor. rule: load-balancer:read Distributor BGP Peer API: * GET /lbaas/distributors/bgp/peers List the Distributor BGP Peers. rule: load-balancer:admin * GET /lbaas/distributors/bgp/peers/<uuid> Get a specific Distributor BGP Peers. rule: load-balancer:admin * POST /lbaas/distributors/bgp/peers/<uuid> Create a new Distributor BGP Peer. rule: load-balancer:admin The POST request may include: * ``name`` (optional) * ``description`` (optional) * ``admin_state_up`` (optional, default True) * ``project_id`` (optional, a valid project_id, default current project) * ``peer_ip`` (required) * ``remote_as`` (required) * ``auth_type`` (optional, default null) * ``auth_pass`` (required if ``auth_type`` is not null) Distributor BGP Peers are created per project. * PUT /lbaas/distributors/bgp/peers/<uuid> Update a BGP Peer. rule: load-balancer:admin * ``name`` (optional) * ``description`` (optional) * ``admin_state_up`` (optional) [`P2`_] Handle the update of the other fields (would require to update all the load balancers that use this BGP Peer) * DELETE /lbaas/distributors/bgp/peers/<uuid> Delete a BGP Peer rule: load-balancer:admin Deleting a BGP Peer that is in use by a Load Balancer would result in a conflict error (409). Deleting a BGP Peer that is associated with a BGP speaker would delete both objects. Distributor BGP Speaker API: * GET /lbaas/distributors/bgp/speakers List the Distributor BGP Speakers. rule: load-balancer:admin * GET /lbaas/distributors/bgp/speakers/<uuid> Get a specific Distributor BGP Speaker. rule: load-balancer:admin * POST /lbaas/distributors/bgp/speakers/<uuid> Create a new Distributor BGP Speaker. rule: load-balancer:admin The POST request may include: * ``name`` (optional) * ``description`` (optional) * ``admin_state_up`` (optional, default True) * ``local_as`` (required) * ``subnet_id`` (optional, a valid subnet, default null) Distributor BGP Speakers s are created per project. * PUT /lbaas/distributors/bgp/speakers/<uuid> Update a BGP Speaker. rule: load-balancer:admin * ``name`` (optional) * ``description`` (optional) * ``admin_state_up`` (optional) [`P2`_] Handle the update of the other fields (would require to update all the load balancers that use this BGP Speaker) * DELETE /lbaas/distributors/bgp/speakers/<uuid> Delete a BGP Speaker. rule: load-balancer:admin Changes to the current API: Loadbalancer API: * POST Add a new parameter ``distributor_id``. In the current implementation ``distributor_id`` must be the UUID of a valid BGP Speaker. ``distributor_id`` is required and must only be used with MULTI_ACTIVE topology. ``distributor_id`` cannot be updated after the creation of a load balancer. [`P2`_] Support for multiple distributors per load balancer. Extended Amphora API ^^^^^^^^^^^^^^^^^^^^ The L3 BGP distributor driver will call the extended amphora API in order to implement the control plane (BGP) and advertise new anycast VIP routes into the network. The below extended amphora API calls will be implemented for amphoras running as a dedicated distributor: 1. ``Register Amphora`` This call will result in the BGP speaker announcing the anycast VIP into the L3 network with a next-hop of the front-end IP of the amphora being registered. Prior to this call, the load balancing amphora will have to configure the anycast VIP on the loopback interface inside the amphora-haproxy namespace. - amphora_id ID of the amphora running the load balancer to register. - vip_ip The VIP IP address. - nexthop_ip The amphora's front-end network IP address used to handle anycast VIP traffic. - peer_id ID of the peer that will be used to announce the anycast VIP. If not specified, VIP will be announced across all peers. 2. ``Unregister Amphora`` The BGP speaker will withdraw the anycast VIP route for the specified amphora from the L3 network. After the route is withdrawn, the anycast VIP IP will be removed from the loopback interface on the load balancing amphora. - amphora_id ID of the amphora running the load balancer to unregister. - vip_ip The VIP IP address. - nexthop_ip The amphora's front-end network IP Address used to handle anycast VIP traffic. - peer_id ID of the peer that will be used to withdraw the anycast VIP. If not specified, route will be withdrawn from all peers. 3. ``List Amphora`` Will return a list of all amphora IDs and their anycast VIP routes currently being advertised by the BGP speaker. 4. [`P2`_] ``Drain Amphora`` All new flows will get redirected to other members of the cluster and existing flows will be drained. Once the active flows have been drained, the BGP speaker will withdraw the anycast VIP route from the L3 network and unconfigure the VIP from the lo interface. 5. [`P2`_] ``Register VIP`` This call will be used for registering anycast routes for non-amphora endpoints, such as for UDP load balancing. - vip_ip The VIP IP address. - nexthop_ip The nexthop network IP Address used to handle anycast VIP traffic. - peer_id ID of the peer that will be used to announce the anycast VIP. If not specified, route will be announced from all peers. 6. [`P2`_] ``Unregister VIP`` This call will be used for unregistering anycast routes for non-amphora endpoints, such as for UDP load balancing. - vip_ip The VIP IP address. - nexthop_ip The nexthop network IP Address used to handle anycast VIP traffic. - peer_id ID of the peer that will be used to withdraw the anycast VIP. If not specified, route will be withdrawn from all peers. 6. [`P2`_] ``List VIP`` Will return a list of all non-amphora anycast VIP routes currently being advertised by the BGP speaker. Security impact --------------- The distributor inherently supports multi-tenancy, as it is simply providing traffic distribution across multiple amphoras. Network isolation on a per tenant basis is handled by the amphoras themselves, as they service only a single tenant. Further isolation can be provided by defining separate anycast network(s) on a per tenant basis. Firewall or ACL policies can then be built around these prefixes. To further enhance BGP security, route-maps, prefix-lists, and communities to control what routes are allowed to be advertised in the L3 network from a particular BGP peer can be used. MD5 password and GTSM can provide additional security to limit unauthorized BGP peers to the L3 network. Notifications impact -------------------- Other end user impact --------------------- Performance Impact ------------------ Other deployer impact --------------------- Developer impact ---------------- Implementation ============== Assignee(s) ----------- Work Items ---------- Dependencies ============ Testing ======= * Unit tests with tox. * Function tests with tox. Documentation Impact ==================== The API-Ref documentation will need to be updated for load balancer create. An additional optional parameter frontend_network_id will be added. If set, this parameter will result in the primary interface inside the amphora-haproxy namespace getting created on the specified network. Default behavior is to provision this interface on the VIP subnet. References ========== * `Active-Active Topology <https://blueprints.launchpad.net/octavia/+spec/active-active-topology/>`_ ",,803,0
openstack%2Ftacker-specs~master~I20a70c12c565ca3a86ff34f4b9c9007207edf8aa,openstack/tacker-specs,master,I20a70c12c565ca3a86ff34f4b9c9007207edf8aa,Fix Pillow version to 9.5.0,MERGED,2023-07-06 08:54:00.000000000,2023-07-10 05:47:28.000000000,2023-07-10 05:46:30.000000000,"[{'_account_id': 17255}, {'_account_id': 22348}, {'_account_id': 25701}, {'_account_id': 31668}, {'_account_id': 31857}, {'_account_id': 32029}, {'_account_id': 33455}, {'_account_id': 34712}, {'_account_id': 35978}]","[{'number': 1, 'created': '2023-07-06 08:54:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/8dcc46659a108ea43463697205ad1ae4c31b337c', 'message': 'Downgrade Pillows version\n\nThis patch is for downgrading the\nPillow version from 10.0 to 9.5.\n\nChange-Id: I20a70c12c565ca3a86ff34f4b9c9007207edf8aa\n'}, {'number': 2, 'created': '2023-07-06 08:55:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/9d44a167e113307c903e05bd193d917d17f37ea9', 'message': 'Downgrade Pillows version\n\nThis patch is for downgrading the\nPillow version from 10.0 to 9.5.\n\nChange-Id: I20a70c12c565ca3a86ff34f4b9c9007207edf8aa\n'}, {'number': 3, 'created': '2023-07-07 00:03:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/9247a42998c0b7e06ff2d54be4290ff65486f1f8', 'message': ""Downgrade Pillows version\n\nThis patch is for downgrading the\nPillow version from 10.0 to 9.5.\n\nIamgeDrew.textsize was removed in version 10 of the Pillows.\nHowever, Sphinx uses ImageDrew.textsize.\nwe need to downgrade until Sphinx supports ImageDrew.\n\nIn addition, the current tacker-spec's tox does not have Pillow version\ncontrol.\nFix tox to use openstack requirements[1].\n\n[1]\nhttps://opendev.org/openstack/requirements/src/branch/master/upper-constraints.txt\n\nChange-Id: I20a70c12c565ca3a86ff34f4b9c9007207edf8aa\n""}, {'number': 4, 'created': '2023-07-07 00:06:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/da8f0582bdcc801113487a8ef818898fd2143c09', 'message': ""Fix tox to upper-constraints\n\nthe current tacker-spec's tox does not have version\ncontrol.\nFix tox to use openstack requirements[1].\n\n[1]\nhttps://opendev.org/openstack/requirements/src/branch/master/upper-constraints.txt\n\nChange-Id: I20a70c12c565ca3a86ff34f4b9c9007207edf8aa\n""}, {'number': 5, 'created': '2023-07-07 00:14:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/1c07cce8e10e9ee12f48ac5498caaf56f37d0f0e', 'message': ""Fix tox to use upper-constraints\n\nthe current tacker-spec's tox does not have version\ncontrol.\nFix tox to use openstack requirements[1].\n\n[1]\nhttps://opendev.org/openstack/requirements/src/branch/master/upper-constraints.txt\n\nChange-Id: I20a70c12c565ca3a86ff34f4b9c9007207edf8aa\n""}, {'number': 6, 'created': '2023-07-07 07:34:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/094c56e84d55f3b714e41c6cb70722a52096b427', 'message': ""Fix tox to use upper-constraints\n\nThe current tacker-spec's tox does not have version\ncontrol.\nFix tox to use openstack requirements[1].\n\nUpper-constraints requirements pillow version 9.5.0.\nThis patch fixed tacker-spec's pillow and shpinx bug[2].\n\n[1]\nhttps://opendev.org/openstack/requirements/src/branch/master/upper-constraints.txt\n[2]\nhttps://bugs.launchpad.net/tacker/+bug/2026345\n\nChange-Id: I20a70c12c565ca3a86ff34f4b9c9007207edf8aa\n""}, {'number': 7, 'created': '2023-07-07 08:16:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/85163078759ad1106504ff0e701540dd43eddeaa', 'message': ""Fix tox to use upper-constraints\n\nThe current tacker-spec's tox does not have version\ncontrol.\nFix tox to use openstack requirements[1].\n\nUpper-constraints requirements pillow version 9.5.0.\nThis patch fixed tacker-spec's pillow and shpinx problem.\n\n[1]\nhttps://opendev.org/openstack/requirements/src/branch/master/upper-constraints.txt\n\nChange-Id: I20a70c12c565ca3a86ff34f4b9c9007207edf8aa\nCloses-Bug: #2026345\n""}, {'number': 8, 'created': '2023-07-09 23:47:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/0f0d3ac9a601be6f639effd147e997818d3a5c49', 'message': ""Fix tox to use upper-constraints\n\nThe current tacker-spec's tox does not have version\ncontrol.\nFix tox to use openstack requirements[1].\n\nUpper-constraints requirements pillow version 9.5.0.\nThis patch fixed tacker-spec's pillow and sphinx's problem.\n\n[1]\nhttps://opendev.org/openstack/requirements/src/branch/master/upper-constraints.txt\n\nChange-Id: I20a70c12c565ca3a86ff34f4b9c9007207edf8aa\nCloses-Bug: #2026345\n""}, {'number': 9, 'created': '2023-07-09 23:56:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/5ca00db94f879c0573df24c67b9f7f637c5e478b', 'message': ""Fix Pillow version to 9.5.0\n\nThe current tacker-spec's tox does not have version\ncontrol.\n\nThis patch fixes the problem of pillow and sphinx's by\nselecting version 9.5.0.\n\n[1]\nhttps://opendev.org/openstack/requirements/src/branch/master/upper-constraints.txt\n\nChange-Id: I20a70c12c565ca3a86ff34f4b9c9007207edf8aa\nCloses-Bug: #2026345\n""}, {'number': 10, 'created': '2023-07-10 02:45:28.000000000', 'files': ['doc/requirements.txt'], 'web_link': 'https://opendev.org/openstack/tacker-specs/commit/66c3a8b7d8b2cde832a0bea2c0d5bc8b8fb158c2', 'message': ""Fix Pillow version to 9.5.0\n\nThis patch fixes the problem of pillow and sphinx's by\nselecting version 9.5.0.\n\n[1]\nhttps://opendev.org/openstack/requirements/src/branch/master/upper-constraints.txt\n\nChange-Id: I20a70c12c565ca3a86ff34f4b9c9007207edf8aa\nCloses-Bug: #2026345\n""}]",26,887772,66c3a8b7d8b2cde832a0bea2c0d5bc8b8fb158c2,40,9,10,32707,,,0,"Fix Pillow version to 9.5.0

This patch fixes the problem of pillow and sphinx's by
selecting version 9.5.0.

[1]
https://opendev.org/openstack/requirements/src/branch/master/upper-constraints.txt

Change-Id: I20a70c12c565ca3a86ff34f4b9c9007207edf8aa
Closes-Bug: #2026345
",git fetch https://review.opendev.org/openstack/tacker-specs refs/changes/72/887772/10 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,8dcc46659a108ea43463697205ad1ae4c31b337c,bug/2026345,deps = -c{env:TOX_CONSTRAINTS_FILE:https://releases.openstack.org/constraints/upper/master} -r{toxinidir}/doc/requirements.txt ,deps = -r{toxinidir}/doc/requirements.txt,4,1
openstack%2Fmanila-specs~master~Ib184995f6fce2a9aaa60f8251513d58c5b663112,openstack/manila-specs,master,Ib184995f6fce2a9aaa60f8251513d58c5b663112,Access rule visibility and deletion restrictions,MERGED,2023-05-01 23:10:47.000000000,2023-07-10 02:53:22.000000000,2023-07-10 02:52:13.000000000,"[{'_account_id': 4393}, {'_account_id': 11604}, {'_account_id': 16207}, {'_account_id': 22348}, {'_account_id': 29632}, {'_account_id': 30407}]","[{'number': 1, 'created': '2023-05-01 23:10:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/68bdcb2e0c52b4ab057aab04448fcf1e4a9179a8', 'message': 'Access rule visibility and deletion restrictions\n\nDesign of better visibility and manipulation protections\nto access rules of a share.\n\nAPIImpact\nPartially-Implements: bp protect-access-rules\n\nChange-Id: Ib184995f6fce2a9aaa60f8251513d58c5b663112\nSigned-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>\n'}, {'number': 2, 'created': '2023-06-14 05:50:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/211db932384e197503b1029807a4836cb44aa63c', 'message': 'Access rule visibility and deletion restrictions\n\nDesign of better visibility and manipulation protections\nto access rules of a share.\n\nAPIImpact\nPartially-Implements: bp protect-access-rules\n\nChange-Id: Ib184995f6fce2a9aaa60f8251513d58c5b663112\nSigned-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>\n'}, {'number': 3, 'created': '2023-07-06 17:23:59.000000000', 'files': ['specs/bobcat/protect-access-rules.rst', 'specs/bobcat/allow-locking-shares-against-deletion.rst', 'doc/source/index.rst'], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/10e4c609fea5838f4b7dd128626568e1f0a5a2f1', 'message': 'Access rule visibility and deletion restrictions\n\nDesign of better visibility and manipulation protections\nto access rules of a share.\n\nAPIImpact\nPartially-Implements: bp protect-access-rules\n\nChange-Id: Ib184995f6fce2a9aaa60f8251513d58c5b663112\nSigned-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>\n'}]",81,881934,10e4c609fea5838f4b7dd128626568e1f0a5a2f1,28,6,3,16643,,,0,"Access rule visibility and deletion restrictions

Design of better visibility and manipulation protections
to access rules of a share.

APIImpact
Partially-Implements: bp protect-access-rules

Change-Id: Ib184995f6fce2a9aaa60f8251513d58c5b663112
Signed-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>
",git fetch https://review.opendev.org/openstack/manila-specs refs/changes/34/881934/3 && git format-patch -1 --stdout FETCH_HEAD,"['specs/bobcat/protect-access-rules.rst', 'doc/source/index.rst']",2,68bdcb2e0c52b4ab057aab04448fcf1e4a9179a8,bp/protect-access-rules,2023.2 Bobcat approved specs ============================ .. toctree:: :glob: :maxdepth: 1 specs/bobcat/* 2023.1 Antelope approved specs ==============================,Antelope approved specs =======================,378,2
openstack%2Fneutron~master~I758c376f55b71d7159fa3f5d83e47d2b05da3218,openstack/neutron,master,I758c376f55b71d7159fa3f5d83e47d2b05da3218,Refactor for ovs qos driver meter limit features,MERGED,2022-10-09 01:50:35.000000000,2023-07-10 02:49:31.000000000,2023-07-10 02:48:02.000000000,"[{'_account_id': 8313}, {'_account_id': 9531}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-10-09 01:50:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/2a36d111e057daff514a944f8b881dc0265a7003', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 2, 'created': '2022-10-09 06:57:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/0dc07ac0570ed57a6d5ae3636b6d4e1ff4cae658', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 3, 'created': '2022-10-18 03:05:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3b0a5e493aef282de8af924a9746fca2769fc406', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 4, 'created': '2022-10-19 01:44:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/d66cd08f372ea03f568fa1a2a7edf467cb723844', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 5, 'created': '2022-11-14 01:40:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/129d08ecbad5697731897cd723d326c3cd1f21b4', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 6, 'created': '2022-11-14 03:40:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/1c3c447cfa8dfc85b9a16359478c4e3d4d31ec5d', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 7, 'created': '2022-11-14 06:47:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/5ee6434540da04132bc2568036a84bb406281472', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 8, 'created': '2022-11-15 02:21:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/beff37583ba5129f334c9989b4dbc5e1289a3d56', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 9, 'created': '2022-11-16 01:33:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/15f67175aa97f32bb9925c74183b739f264b98f0', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 10, 'created': '2022-12-05 04:18:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/874b3e03f61fcd49070ab05cb2fa205a9503231b', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 11, 'created': '2022-12-05 04:20:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/dc3d1dfa29cac770de925549b6094add9ef95e9e', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 12, 'created': '2022-12-14 03:07:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/11295e130f135ebc6d65818dc0b7754b02a1b900', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 13, 'created': '2022-12-16 00:55:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/334016e0141862c8c409251649578c9a0f9aee54', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 14, 'created': '2022-12-21 02:05:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/20b99eb63d0402139bb965bb863e3dc60b68610a', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 15, 'created': '2023-03-09 01:34:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/dd886cf8b049a7f62db78cf75095561989f21744', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 16, 'created': '2023-04-11 01:52:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e39145073e3d782f8a02b9771e9c512ed7389577', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 17, 'created': '2023-06-01 07:14:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e839702fea6c29865ac35c7188e61e64161fe7a4', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 18, 'created': '2023-07-05 08:52:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/acdaf3c07ce18ce31cf85126df53cff0e60a28b4', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}, {'number': 19, 'created': '2023-07-05 09:51:06.000000000', 'files': ['neutron/tests/common/agents/l2_extensions.py', 'neutron/plugins/ml2/drivers/openvswitch/agent/extension_drivers/qos_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/extension_drivers/test_qos_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/openflow/native/test_br_int.py', 'neutron/plugins/ml2/drivers/openvswitch/agent/openflow/native/br_int.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/02b12b09175c8608ac7d2032baa5d6caf01c660b', 'message': 'Refactor for ovs qos driver meter limit features\n\nMove common functions create/update/delete_packet_rate_limit\nto the QosOVSAgentDriver, and keep special driver methods in\ntheir own classes.\n\nCloses-Bug: #1964342\nChange-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218\n'}]",32,860766,02b12b09175c8608ac7d2032baa5d6caf01c660b,124,5,19,9531,,,0,"Refactor for ovs qos driver meter limit features

Move common functions create/update/delete_packet_rate_limit
to the QosOVSAgentDriver, and keep special driver methods in
their own classes.

Closes-Bug: #1964342
Change-Id: I758c376f55b71d7159fa3f5d83e47d2b05da3218
",git fetch https://review.opendev.org/openstack/neutron refs/changes/66/860766/16 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ml2/drivers/openvswitch/agent/extension_drivers/qos_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/openvswitch/agent/extension_drivers/test_qos_driver.py']",2,2a36d111e057daff514a944f8b881dc0265a7003,bug/1964342," self.qos_driver.meter_cache_pps.br_int = self.qos_driver.br_int self.qos_driver.meter_cache_pps.max_meter = 65535 burst=self.rules[2].max_burst_kpps * 1000, type_=comm_consts.METER_FLAG_PPS), burst=self.rules[3].max_burst_kpps * 1000, type_=comm_consts.METER_FLAG_PPS)]) in_port=111, type_=comm_consts.METER_FLAG_PPS), local_vlan=1, type_=comm_consts.METER_FLAG_PPS)]) in_port=111, type_=comm_consts.METER_FLAG_PPS), local_vlan=1, type_=comm_consts.METER_FLAG_PPS)]) burst=self.rules[2].max_burst_kpps * 1000, type_=comm_consts.METER_FLAG_PPS), burst=self.rules[3].max_burst_kpps * 1000, type_=comm_consts.METER_FLAG_PPS)]) in_port=111, type_=comm_consts.METER_FLAG_PPS), local_vlan=1, type_=comm_consts.METER_FLAG_PPS)])"," self.qos_driver.meter_cache.br_int = self.qos_driver.br_int self.qos_driver.meter_cache.max_meter = 65535 burst=self.rules[2].max_burst_kpps * 1000), burst=self.rules[3].max_burst_kpps * 1000)]) in_port=111), local_vlan=1)]) in_port=111), local_vlan=1)]) burst=self.rules[2].max_burst_kpps * 1000), burst=self.rules[3].max_burst_kpps * 1000)]) in_port=111), local_vlan=1)])",132,98
openstack%2Fkeystonemiddleware~master~I40bc0b3b024d38aef5cfd79bda5bd6603230fa6e,openstack/keystonemiddleware,master,I40bc0b3b024d38aef5cfd79bda5bd6603230fa6e,Fix Max retries exceeded with v3 ec2tokens,ABANDONED,2023-06-20 16:31:18.000000000,2023-07-10 02:04:34.000000000,,"[{'_account_id': 597}, {'_account_id': 7414}, {'_account_id': 7973}, {'_account_id': 16465}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-20 16:31:18.000000000', 'files': ['keystonemiddleware/tests/unit/test_ec2_token_middleware.py'], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/3282df1a8f50368f1c91159d0bacd22f0f451c5b', 'message': ""Fix Max retries exceeded with v3 ec2tokens\n\nFixed mocks by cherry-picking [1] to fit recently introduced the timeout\narg and the change of method from request to post. This error is caused\nby the gaps between codes and tests. The ec2_token is recently updated\n[2], but test codes on the master branch haven't been updated.\n\n[1] https://review.opendev.org/c/openstack/keystonemiddleware/+/878544\n[2] https://review.opendev.org/c/openstack/keystonemiddleware/+/877808\n\nCloses-Bug: #2023689\nChange-Id: I40bc0b3b024d38aef5cfd79bda5bd6603230fa6e\n""}]",3,886521,3282df1a8f50368f1c91159d0bacd22f0f451c5b,7,5,1,33455,,,0,"Fix Max retries exceeded with v3 ec2tokens

Fixed mocks by cherry-picking [1] to fit recently introduced the timeout
arg and the change of method from request to post. This error is caused
by the gaps between codes and tests. The ec2_token is recently updated
[2], but test codes on the master branch haven't been updated.

[1] https://review.opendev.org/c/openstack/keystonemiddleware/+/878544
[2] https://review.opendev.org/c/openstack/keystonemiddleware/+/877808

Closes-Bug: #2023689
Change-Id: I40bc0b3b024d38aef5cfd79bda5bd6603230fa6e
",git fetch https://review.opendev.org/openstack/keystonemiddleware refs/changes/21/886521/1 && git format-patch -1 --stdout FETCH_HEAD,['keystonemiddleware/tests/unit/test_ec2_token_middleware.py'],1,3282df1a8f50368f1c91159d0bacd22f0f451c5b,bug/2023689," requests, 'post', 'http://localhost:5000/v3/ec2tokens', verify=True, cert=None, timeout=mock.ANY) requests, 'post', 'http://localhost:5000/v3/ec2tokens', verify=True, cert=None, timeout=mock.ANY) 'post', mock_request.assert_called_with(mock.ANY, verify=mock.ANY, cert=mock.ANY, timeout=mock.ANY) 'post', mock_request.assert_called_with(mock.ANY, verify=mock.ANY, cert=mock.ANY, timeout=mock.ANY)"," requests, 'request', 'POST', 'http://localhost:5000/v3/ec2tokens', verify=True, cert=None) requests, 'request', 'POST', 'http://localhost:5000/v3/ec2tokens', verify=True, cert=None) 'request', mock_request.assert_called_with('POST', mock.ANY, verify=mock.ANY, cert=mock.ANY) 'request', mock_request.assert_called_with('POST', mock.ANY, verify=mock.ANY, cert=mock.ANY)",14,12
openstack%2Ftempest~master~I2047e24118be5d0abf4f79ff9a6d79a06ac807e6,openstack/tempest,master,I2047e24118be5d0abf4f79ff9a6d79a06ac807e6,Enable file injection tests,NEW,2023-04-04 19:59:21.000000000,2023-07-10 00:40:57.000000000,,"[{'_account_id': 8556}, {'_account_id': 22348}, {'_account_id': 33983}]","[{'number': 1, 'created': '2023-04-04 19:59:21.000000000', 'files': ['zuul.d/integrated-gate.yaml'], 'web_link': 'https://opendev.org/openstack/tempest/commit/7c7349c337a97ff547e2bc75ba8cabc0a5d0eaf0', 'message': ""Enable file injection tests\n\nLet's enable file injection tests as the bug, which was the reason\nbehind disabling them, has been resolved.\n\nRelated-Bug: #1882421\nChange-Id: I2047e24118be5d0abf4f79ff9a6d79a06ac807e6\n""}]",10,879510,7c7349c337a97ff547e2bc75ba8cabc0a5d0eaf0,15,3,1,22873,,,0,"Enable file injection tests

Let's enable file injection tests as the bug, which was the reason
behind disabling them, has been resolved.

Related-Bug: #1882421
Change-Id: I2047e24118be5d0abf4f79ff9a6d79a06ac807e6
",git fetch https://review.opendev.org/openstack/tempest refs/changes/10/879510/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/integrated-gate.yaml'],1,7c7349c337a97ff547e2bc75ba8cabc0a5d0eaf0,, ENABLE_FILE_INJECTION: true ENABLE_FILE_INJECTION: true, # TODO(gmann): Enable File injection tests once nova bug is fixed # https://bugs.launchpad.net/nova/+bug/1882421 # ENABLE_FILE_INJECTION: true # TODO(gmann): Enable File injection tests once nova bug is fixed # https://bugs.launchpad.net/nova/+bug/1882421 # ENABLE_FILE_INJECTION: true,2,6
openstack%2Fmetalsmith~stable%2Fwallaby~Ied2d16ec33fe71522c3461d3df6e70fbfdd976b2,openstack/metalsmith,stable/wallaby,Ied2d16ec33fe71522c3461d3df6e70fbfdd976b2,Allow both 'network' and 'subnet' in NIC,MERGED,2023-07-06 18:06:06.000000000,2023-07-09 23:03:11.000000000,2023-07-09 23:02:17.000000000,"[{'_account_id': 4571}, {'_account_id': 11655}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 18:06:06.000000000', 'files': ['releasenotes/notes/allow-both-network-and-subnet-in-nic-info-af8b40a26d55828e.yaml', 'metalsmith/_nics.py'], 'web_link': 'https://opendev.org/openstack/metalsmith/commit/554df57f34352f5e6e7daeca510f049d218874e4', 'message': ""Allow both 'network' and 'subnet' in NIC\n\nFixes and issue where a port cannot be created on\na specific subnet if there are multiple subnets\nwith the same name on different networks.\n\nAllows both 'network' and 'subnet' in NIC information,\nwhen looking up the subnet filter on the network_id when\nboth 'network' and 'subnet' is provided.\n\nConflicts:\n\tmetalsmith/test/test_nics.py\n\n  Unit test for nics was added after wallaby.\n\nStory: 2009732\nTask: 44152\nChange-Id: Ied2d16ec33fe71522c3461d3df6e70fbfdd976b2\n(cherry picked from commit 264836d59ac741424c3fad4d47e51073722c848f)\n""}]",0,887867,554df57f34352f5e6e7daeca510f049d218874e4,8,3,1,24245,,,0,"Allow both 'network' and 'subnet' in NIC

Fixes and issue where a port cannot be created on
a specific subnet if there are multiple subnets
with the same name on different networks.

Allows both 'network' and 'subnet' in NIC information,
when looking up the subnet filter on the network_id when
both 'network' and 'subnet' is provided.

Conflicts:
	metalsmith/test/test_nics.py

  Unit test for nics was added after wallaby.

Story: 2009732
Task: 44152
Change-Id: Ied2d16ec33fe71522c3461d3df6e70fbfdd976b2
(cherry picked from commit 264836d59ac741424c3fad4d47e51073722c848f)
",git fetch https://review.opendev.org/openstack/metalsmith refs/changes/67/887867/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/allow-both-network-and-subnet-in-nic-info-af8b40a26d55828e.yaml', 'metalsmith/_nics.py']",2,554df57f34352f5e6e7daeca510f049d218874e4,story/2009732," unexpected = set(nic) - {'network', 'fixed_ip', 'subnet'} fixed_ip = {} if nic.get('fixed_ip'): fixed_ip['ip_address'] = nic['fixed_ip'] if nic.get('subnet'): try: subnet = self._connection.network.find_subnet( nic['subnet'], network_id=network.id, ignore_missing=False) except sdk_exc.SDKException as exc: raise exceptions.InvalidNIC( 'Cannot find subnet %(subnet)s on network %(net)s: ' '%(error)s' % {'net': nic['network'], 'subnet': nic['subnet'], 'error': exc}) fixed_ip['subnet_id'] = subnet.id port_args = {'network_id': network.id} if fixed_ip: port_args['fixed_ips'] = [fixed_ip]"," unexpected = set(nic) - {'network', 'fixed_ip'} port_args = {'network_id': network.id} if nic.get('fixed_ip'): port_args['fixed_ips'] = [{'ip_address': nic['fixed_ip']}]",26,3
openstack%2Fglance_store~master~If5c27deb39ce62c5791b8157efeb42cfc728fc67,openstack/glance_store,master,If5c27deb39ce62c5791b8157efeb42cfc728fc67,Imported Translations from Zanata,MERGED,2023-07-08 02:08:40.000000000,2023-07-09 16:06:26.000000000,2023-07-09 16:05:22.000000000,"[{'_account_id': 5314}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-08 02:08:40.000000000', 'files': ['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po', 'glance_store/locale/en_GB/LC_MESSAGES/glance_store.po'], 'web_link': 'https://opendev.org/openstack/glance_store/commit/0c60291637d1c941dcd8d2e022acb22ba0bed440', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: If5c27deb39ce62c5791b8157efeb42cfc728fc67\n'}]",1,888004,0c60291637d1c941dcd8d2e022acb22ba0bed440,9,2,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: If5c27deb39ce62c5791b8157efeb42cfc728fc67
",git fetch https://review.opendev.org/openstack/glance_store refs/changes/04/888004/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po', 'glance_store/locale/en_GB/LC_MESSAGES/glance_store.po']",2,0c60291637d1c941dcd8d2e022acb22ba0bed440,zanata/translations,"# Andi Chandler <andi@gowling.com>, 2023. #zanata""POT-Creation-Date: 2023-06-23 19:07+0000\n""""PO-Revision-Date: 2023-07-07 09:08+0000\n""""This option is used to define a relative weight for this store over\n"" ""any others that are configured. The actual value of the weight is "" ""meaningless\n"" ""and only serves to provide a \""sort order\"" compared to others. Any stores\n"" ""with the same weight will be treated as equivalent.\n"" msgstr """" ""\n"" ""This option is used to define a relative weight for this store over\n"" ""any others that are configured. The actual value of the weight is "" ""meaningless\n"" ""and only serves to provide a \""sort order\"" compared to others. Any stores\n"" ""with the same weight will be treated as equivalent.\n"" msgid """" ""\n""","""POT-Creation-Date: 2020-05-04 16:18+0000\n""""PO-Revision-Date: 2020-05-04 08:05+0000\n""",32,4
openstack%2Fopenstack-helm~master~If224d1162a531759fe6ad2d636406be60721f16b,openstack/openstack-helm,master,If224d1162a531759fe6ad2d636406be60721f16b,Add ubuntu-jammy based images,MERGED,2023-07-04 13:08:20.000000000,2023-07-09 15:56:32.000000000,2023-07-09 15:54:35.000000000,"[{'_account_id': 3009}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 13:08:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/dde50115db04ef4397b535570e0f5662e17c9863', 'message': 'Add ubuntu-jammy based images\n\nChange-Id: If224d1162a531759fe6ad2d636406be60721f16b\n'}, {'number': 2, 'created': '2023-07-04 16:07:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/21508269046027203865d44c7efb3da2bf0f9ae7', 'message': 'Add ubuntu-jammy based images\n\nChange-Id: If224d1162a531759fe6ad2d636406be60721f16b\n'}, {'number': 3, 'created': '2023-07-05 00:55:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/7ebf9623416a18ad3a821ec8c79b15b8a951f4ec', 'message': 'Add ubuntu-jammy based images\n\nChange-Id: If224d1162a531759fe6ad2d636406be60721f16b\n'}, {'number': 4, 'created': '2023-07-08 14:25:54.000000000', 'files': ['zuul.d/jobs-openstack-helm.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/66f2affb8d15a86dd6cc83cb496c4f89ac68a812', 'message': 'Add ubuntu-jammy based images\n\nChange-Id: If224d1162a531759fe6ad2d636406be60721f16b\n'}]",8,887599,66f2affb8d15a86dd6cc83cb496c4f89ac68a812,28,2,4,35691,,,0,"Add ubuntu-jammy based images

Change-Id: If224d1162a531759fe6ad2d636406be60721f16b
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/99/887599/3 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/jobs-openstack-helm.yaml'],1,dde50115db04ef4397b535570e0f5662e17c9863,jammy_jobs, name: openstack-helm-cinder-zed-ubuntu_jammy parent: openstack-helm-cinder vars: osh_params: openstack_release: zed container_distro_name: ubuntu container_distro_version: jammy - job: name: openstack-helm-compute-kit-zed-ubuntu_jammy parent: openstack-helm-compute-kit nodeset: openstack-helm-single-32GB-focal-tmp # TODO: Use jammy nodeset vars: osh_params: openstack_release: zed container_distro_name: ubuntu container_distro_version: jammy - job:,,19,0
openstack%2Fglance~master~I4fd052503d1980c650234b90cfed71053e22e3b5,openstack/glance,master,I4fd052503d1980c650234b90cfed71053e22e3b5,Release notes for Bobcat Milestone 2,MERGED,2023-07-03 06:50:58.000000000,2023-07-09 12:13:36.000000000,2023-07-09 12:12:25.000000000,"[{'_account_id': 8122}, {'_account_id': 9303}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 06:50:58.000000000', 'files': ['releasenotes/notes/bobcat-milestone-2-releasenotes-085084b03f66d671.yaml'], 'web_link': 'https://opendev.org/openstack/glance/commit/735db034851e7b9c5612435a1ae822e5bc0dbcae', 'message': 'Release notes for Bobcat Milestone 2\n\nChange-Id: I4fd052503d1980c650234b90cfed71053e22e3b5\n'}]",1,887476,735db034851e7b9c5612435a1ae822e5bc0dbcae,12,3,1,19138,,,0,"Release notes for Bobcat Milestone 2

Change-Id: I4fd052503d1980c650234b90cfed71053e22e3b5
",git fetch https://review.opendev.org/openstack/glance refs/changes/76/887476/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/notes/bobcat-milestone-2-releasenotes-085084b03f66d671.yaml'],1,735db034851e7b9c5612435a1ae822e5bc0dbcae,bobcat-milestone-2,--- fixes: - | Bug 1937901_: healthcheck middleware should be deployed as app instead of filter - | Bug 1889664_: Image Import 'web-download' is broken with py37+ .. _1937901: https://code.launchpad.net/bugs/1937901 .. _1889664: https://code.launchpad.net/bugs/1889664 ,,10,0
openstack%2Fneutron-tempest-plugin~master~I5d41e317fadd8dc9935fc639c67f4eb3f27c7622,openstack/neutron-tempest-plugin,master,I5d41e317fadd8dc9935fc639c67f4eb3f27c7622,add a comment,ABANDONED,2023-07-09 11:17:33.000000000,2023-07-09 11:22:36.000000000,,[],"[{'number': 1, 'created': '2023-07-09 11:17:33.000000000', 'files': ['neutron_tempest_plugin/scenario/test_basic.py'], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/f0bbb9efdefbe003f5d2ce2ba79fdbd62ddf6992', 'message': 'add a comment\n\nChange-Id: I5d41e317fadd8dc9935fc639c67f4eb3f27c7622\n'}]",0,888011,f0bbb9efdefbe003f5d2ce2ba79fdbd62ddf6992,2,0,1,36141,,,0,"add a comment

Change-Id: I5d41e317fadd8dc9935fc639c67f4eb3f27c7622
",git fetch https://review.opendev.org/openstack/neutron-tempest-plugin refs/changes/11/888011/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron_tempest_plugin/scenario/test_basic.py'],1,f0bbb9efdefbe003f5d2ce2ba79fdbd62ddf6992,, # TEST,,2,0
openstack%2Fglance~master~If22e04649fb9a5c237ca59e3844974756303d127,openstack/glance,master,If22e04649fb9a5c237ca59e3844974756303d127,Refresh Glance example configs for bobcat milestone 2,MERGED,2023-07-03 06:50:58.000000000,2023-07-09 11:11:39.000000000,2023-07-09 11:10:27.000000000,"[{'_account_id': 8122}, {'_account_id': 9303}, {'_account_id': 22348}, {'_account_id': 32238}]","[{'number': 1, 'created': '2023-07-03 06:50:58.000000000', 'files': ['etc/glance-api.conf'], 'web_link': 'https://opendev.org/openstack/glance/commit/d68f99a7384be677836a1309cb3b0f5d9f58277d', 'message': 'Refresh Glance example configs for bobcat milestone 2\n\nChange-Id: If22e04649fb9a5c237ca59e3844974756303d127\n'}]",14,887475,d68f99a7384be677836a1309cb3b0f5d9f58277d,56,4,1,19138,,,0,"Refresh Glance example configs for bobcat milestone 2

Change-Id: If22e04649fb9a5c237ca59e3844974756303d127
",git fetch https://review.opendev.org/openstack/glance refs/changes/75/887475/1 && git format-patch -1 --stdout FETCH_HEAD,['etc/glance-api.conf'],1,d68f99a7384be677836a1309cb3b0f5d9f58277d,bobcat-milestone-2,# How long to wait (in seconds) before reconnecting in response to an AMQP # consumer cancel notification. (floating point value) # Minimum value: 0.0 # Maximum value: 4.5# Deprecated group/name - [oslo_messaging_rabbit]/rabbit_quroum_max_memory_length #rabbit_quorum_max_memory_length = 0# Deprecated group/name - [oslo_messaging_rabbit]/rabbit_quroum_max_memory_bytes #rabbit_quorum_max_memory_bytes = 0,# How long to wait before reconnecting in response to an AMQP consumer cancel # notification. (floating point value)#rabbit_quroum_max_memory_length = 0#rabbit_quroum_max_memory_bytes = 0,8,4
openstack%2Fansible-collections-openstack~master~I3c3b6f59393928d098e9b80c55b87fc6ee1e9912,openstack/ansible-collections-openstack,master,I3c3b6f59393928d098e9b80c55b87fc6ee1e9912,fix(inventory): bug when using clouds_yaml_path,MERGED,2023-06-01 17:41:36.000000000,2023-07-09 08:28:17.000000000,2023-07-09 08:28:17.000000000,"[{'_account_id': 22348}, {'_account_id': 32962}]","[{'number': 1, 'created': '2023-06-01 17:41:36.000000000', 'files': ['plugins/inventory/openstack.py'], 'web_link': 'https://opendev.org/openstack/ansible-collections-openstack/commit/2808d1c15500b4a5f2d977a0131f58d74abe7679', 'message': 'fix(inventory): bug when using clouds_yaml_path\n\nBefore this fix the current implementation in combination with the most\nrecent openstacksdk (1.2.0) resulted in a list containing the default\nvalues and another list inside this list containing the value of\nclouds_yaml_path. The clouds_yaml_path value gets now added directly to\nthe list only if it was set.\n\nChange-Id: I3c3b6f59393928d098e9b80c55b87fc6ee1e9912\n'}]",1,885085,2808d1c15500b4a5f2d977a0131f58d74abe7679,7,2,1,36071,,,0,"fix(inventory): bug when using clouds_yaml_path

Before this fix the current implementation in combination with the most
recent openstacksdk (1.2.0) resulted in a list containing the default
values and another list inside this list containing the value of
clouds_yaml_path. The clouds_yaml_path value gets now added directly to
the list only if it was set.

Change-Id: I3c3b6f59393928d098e9b80c55b87fc6ee1e9912
",git fetch https://review.opendev.org/openstack/ansible-collections-openstack refs/changes/85/885085/1 && git format-patch -1 --stdout FETCH_HEAD,['plugins/inventory/openstack.py'],1,2808d1c15500b4a5f2d977a0131f58d74abe7679,patch-clouds_yaml_path, config_files = openstack.config.loader.CONFIG_FILES if clouds_yaml_path: config_files += clouds_yaml_path, config_files = ( openstack.config.loader.CONFIG_FILES + ([clouds_yaml_path] if clouds_yaml_path else [])),3,3
openstack%2Fcinder~master~Iab75b90df887dc08d24cd704f632e9cdbf5b6bde,openstack/cinder,master,Iab75b90df887dc08d24cd704f632e9cdbf5b6bde,DNM: Create for temporary test,NEW,2023-06-07 12:28:18.000000000,2023-07-09 07:28:22.000000000,,"[{'_account_id': 22348}, {'_account_id': 35822}]","[{'number': 1, 'created': '2023-06-07 12:28:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/354034876f1bd53c6d2cdafb061e534454ed1533', 'message': 'DNM: Create for temporary test\n\ndo not merge.\nPlease ignore this patch.\n\nChange-Id: Iab75b90df887dc08d24cd704f632e9cdbf5b6bde\n'}, {'number': 2, 'created': '2023-07-03 09:07:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/5043d1b2d8a98c039f973eed37a1bb7366a3a3a9', 'message': 'DNM: Create for temporary test\n\ndo not merge.\nPlease ignore this patch.\n\nChange-Id: Iab75b90df887dc08d24cd704f632e9cdbf5b6bde\n'}, {'number': 3, 'created': '2023-07-07 04:29:20.000000000', 'files': ['cinder/volume/drivers/hpe/hpe_3par_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/627192442a67f718c5af61c4bbb164abb8f99db7', 'message': 'DNM: Create for temporary test\n\ndo not merge.\nPlease ignore this patch.\n\nChange-Id: Iab75b90df887dc08d24cd704f632e9cdbf5b6bde\n'}]",20,885435,627192442a67f718c5af61c4bbb164abb8f99db7,72,2,3,29122,,,0,"DNM: Create for temporary test

do not merge.
Please ignore this patch.

Change-Id: Iab75b90df887dc08d24cd704f632e9cdbf5b6bde
",git fetch https://review.opendev.org/openstack/cinder refs/changes/35/885435/3 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/drivers/hpe/hpe_3par_common.py'],1,354034876f1bd53c6d2cdafb061e534454ed1533,Iab75b90df887dc08d24cd704f632e9cdbf5b6bde,# 2023_06_07 ,,2,0
openstack%2Fopenstack-helm-infra~master~If4ac19942642ae2fa6193d9f3db9e602929cfcb5,openstack/openstack-helm-infra,master,If4ac19942642ae2fa6193d9f3db9e602929cfcb5,Add metrics port,ABANDONED,2023-07-03 16:44:18.000000000,2023-07-09 05:29:02.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-03 16:44:18.000000000', 'files': ['rabbitmq/templates/service.yaml', 'rabbitmq/templates/statefulset.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/a4387cb3af2cf1f70e46407a3268a7ac26867ebd', 'message': 'Add metrics port\n\nChange-Id: If4ac19942642ae2fa6193d9f3db9e602929cfcb5\n'}]",2,887526,a4387cb3af2cf1f70e46407a3268a7ac26867ebd,5,1,1,35691,,,0,"Add metrics port

Change-Id: If4ac19942642ae2fa6193d9f3db9e602929cfcb5
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/26/887526/1 && git format-patch -1 --stdout FETCH_HEAD,"['rabbitmq/templates/service.yaml', 'rabbitmq/templates/statefulset.yaml']",2,a4387cb3af2cf1f70e46407a3268a7ac26867ebd,rabbitmq-metrics, - containerPort: 15692 name: metrics protocol: TCP,,7,0
openstack%2Fopenstack-helm~master~I4301a6cbea0688369c735d4751c741106b3fe7ab,openstack/openstack-helm,master,I4301a6cbea0688369c735d4751c741106b3fe7ab,Ensure that the script handles cases where the PID file exists but is empty or does not contain the expected data structure.,MERGED,2023-07-06 14:52:00.000000000,2023-07-09 01:13:57.000000000,2023-07-09 01:12:30.000000000,"[{'_account_id': 1004}, {'_account_id': 3009}, {'_account_id': 22348}, {'_account_id': 34821}]","[{'number': 1, 'created': '2023-07-06 14:52:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/941d54451932ff7918dcf6dfef806b15c6648a25', 'message': 'Ensure that the script handles cases where the PID file exists but is empty or does not contain the expected data structure.\n\nChange-Id: I4301a6cbea0688369c735d4751c741106b3fe7ab\n'}, {'number': 2, 'created': '2023-07-06 16:50:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/d9b4ca2eee38b7d6abcbfde8edd886509a535917', 'message': 'Ensure that the script handles cases where the PID file exists but is empty or does not contain the expected data structure.\n\nChange-Id: I4301a6cbea0688369c735d4751c741106b3fe7ab\n'}, {'number': 3, 'created': '2023-07-06 19:21:11.000000000', 'files': ['nova/templates/bin/_health-probe.py.tpl', 'releasenotes/notes/nova.yaml', 'nova/Chart.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/5ef1d54607809c6eaab0c64ce6e179823213abf1', 'message': 'Ensure that the script handles cases where the PID file exists but is empty or does not contain the expected data structure.\n\nChange-Id: I4301a6cbea0688369c735d4751c741106b3fe7ab\n'}]",5,887856,5ef1d54607809c6eaab0c64ce6e179823213abf1,21,4,3,35943,,,0,"Ensure that the script handles cases where the PID file exists but is empty or does not contain the expected data structure.

Change-Id: I4301a6cbea0688369c735d4751c741106b3fe7ab
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/56/887856/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/templates/bin/_health-probe.py.tpl'],1,941d54451932ff7918dcf6dfef806b15c6648a25,," file_content = f.read().strip() if file_content: data = json.loads(file_content) if 'pid' in data and check_pid_running(data['pid']): if 'exit_count' in data and data['exit_count'] > 1: # Third time in, kill the previous process os.kill(int(data['pid']), signal.SIGTERM) else: data['exit_count'] = data.get('exit_count', 0) + 1 with open(pidfile, 'w') as f: json.dump(data, f) sys.exit(0)"," data = json.load(f) if check_pid_running(data['pid']): if data['exit_count'] > 1: # Third time in, kill the previous process os.kill(int(data['pid']), signal.SIGTERM) else: data['exit_count'] = data['exit_count'] + 1 with open(pidfile, 'w') as f: json.dump(data, f) sys.exit(0)",13,10
openstack%2Fopenstack-helm-infra~master~I680edbc03167dac3b4656ee7f88bfac02a390aa1,openstack/openstack-helm-infra,master,I680edbc03167dac3b4656ee7f88bfac02a390aa1,Fix rabbitmq in ipv6 disabled env,MERGED,2023-07-03 16:40:56.000000000,2023-07-09 01:05:29.000000000,2023-07-09 01:04:32.000000000,"[{'_account_id': 1004}, {'_account_id': 3009}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 16:40:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/8997f798d568e5adaf05a0d8c7c07ba8d8298a89', 'message': 'Fix rabbitmq in ipv6 disabled env\n\nChange-Id: I680edbc03167dac3b4656ee7f88bfac02a390aa1\n'}, {'number': 2, 'created': '2023-07-04 10:12:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/cf82578d1fe90da8e80c00d54262ae096fcd095a', 'message': 'Fix rabbitmq in ipv6 disabled env\n\nChange-Id: I680edbc03167dac3b4656ee7f88bfac02a390aa1\n'}, {'number': 3, 'created': '2023-07-05 18:04:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/bc712dee79fc68c21587cd8a1d5256e1c7f9756f', 'message': 'Fix rabbitmq in ipv6 disabled env\n\nChange-Id: I680edbc03167dac3b4656ee7f88bfac02a390aa1\n'}, {'number': 4, 'created': '2023-07-05 19:48:14.000000000', 'files': ['releasenotes/notes/rabbitmq.yaml', 'rabbitmq/templates/configmap-etc.yaml', 'rabbitmq/Chart.yaml', 'rabbitmq/values.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/1dd1989fff3dede1e20123f3de5c6e82b25de401', 'message': 'Fix rabbitmq in ipv6 disabled env\n\nChange-Id: I680edbc03167dac3b4656ee7f88bfac02a390aa1\n'}]",3,887525,1dd1989fff3dede1e20123f3de5c6e82b25de401,19,3,4,35691,,,0,"Fix rabbitmq in ipv6 disabled env

Change-Id: I680edbc03167dac3b4656ee7f88bfac02a390aa1
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/25/887525/2 && git format-patch -1 --stdout FETCH_HEAD,"['rabbitmq/templates/configmap-etc.yaml', 'rabbitmq/values.yaml']",2,8997f798d568e5adaf05a0d8c7c07ba8d8298a89,rabbitmq_ipv6," bind_address: ""::""",,2,1
openstack%2Fopenstack-helm-infra~master~I9c22bb692385dbb7bc2816233c83c7472e071dd4,openstack/openstack-helm-infra,master,I9c22bb692385dbb7bc2816233c83c7472e071dd4,[ceph-osd] Extend the ceph-osd post-apply job PG wait,MERGED,2023-07-07 14:40:54.000000000,2023-07-08 20:41:29.000000000,2023-07-08 20:40:31.000000000,"[{'_account_id': 3009}, {'_account_id': 22348}, {'_account_id': 28372}, {'_account_id': 32433}, {'_account_id': 33330}, {'_account_id': 34821}]","[{'number': 1, 'created': '2023-07-07 14:40:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/9fbf40be9b17484a7547e14fcc008cb705f52b53', 'message': ""[ceph-osd] Extend the ceph-osd post-apply job PG wait\n\nIn some cases, especially for disruptive OSD restarts on upgrade,\nPGs can take longer than the allowed ~30 seconds to get into a\npeering state. In these cases, the post-apply job fails prematurely\ninstead of allowing time for the OSDs and PGs to recover. This\nchange extends that timeout to ~10 minutes instead to allow the PGs\nplenty of recovery time.\n\nThe only negative effect of this change is that a legitimate\nfailure where the PGs can't recover will take 10 minutes to fail\nthe post-apply job instead of 30 seconds.\n\nChange-Id: I9c22bb692385dbb7bc2816233c83c7472e071dd4\n""}, {'number': 2, 'created': '2023-07-07 14:42:36.000000000', 'files': ['ceph-osd/templates/bin/_post-apply.sh.tpl', 'ceph-osd/Chart.yaml', 'releasenotes/notes/ceph-osd.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/8d6cc364b7d227013df29d87874bea9ad9cf0b17', 'message': ""[ceph-osd] Extend the ceph-osd post-apply job PG wait\n\nIn some cases, especially for disruptive OSD restarts on upgrade,\nPGs can take longer than the allowed ~30 seconds to get into a\npeering state. In these cases, the post-apply job fails prematurely\ninstead of allowing time for the OSDs and PGs to recover. This\nchange extends that timeout to ~10 minutes instead to allow the PGs\nplenty of recovery time.\n\nThe only negative effect of this change is that a legitimate\nfailure where the PGs can't recover will take 10 minutes to fail\nthe post-apply job instead of 30 seconds.\n\nChange-Id: I9c22bb692385dbb7bc2816233c83c7472e071dd4\n""}]",2,887963,8d6cc364b7d227013df29d87874bea9ad9cf0b17,14,6,2,29974,,,0,"[ceph-osd] Extend the ceph-osd post-apply job PG wait

In some cases, especially for disruptive OSD restarts on upgrade,
PGs can take longer than the allowed ~30 seconds to get into a
peering state. In these cases, the post-apply job fails prematurely
instead of allowing time for the OSDs and PGs to recover. This
change extends that timeout to ~10 minutes instead to allow the PGs
plenty of recovery time.

The only negative effect of this change is that a legitimate
failure where the PGs can't recover will take 10 minutes to fail
the post-apply job instead of 30 seconds.

Change-Id: I9c22bb692385dbb7bc2816233c83c7472e071dd4
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/63/887963/1 && git format-patch -1 --stdout FETCH_HEAD,['ceph-osd/templates/bin/_post-apply.sh.tpl'],1,9fbf40be9b17484a7547e14fcc008cb705f52b53,," if [[ $pgs_inactive -gt 200 ]]; then # If inactive PGs aren't peering after ~10 minutes, fail"," if [[ $pgs_inactive -gt 10 ]]; then # If inactive PGs aren't peering, fail",2,2
openstack%2Fironic-python-agent-builder~master~I5e2d454fb84b76810f3c5ed26a0caeef8ea06675,openstack/ironic-python-agent-builder,master,I5e2d454fb84b76810f3c5ed26a0caeef8ea06675,Extend the DIB_CHECKSUM variable usage,MERGED,2023-04-23 21:47:19.000000000,2023-07-08 20:11:15.000000000,2023-07-08 20:10:22.000000000,"[{'_account_id': 4571}, {'_account_id': 7118}, {'_account_id': 11655}, {'_account_id': 22348}, {'_account_id': 32761}]","[{'number': 1, 'created': '2023-04-23 21:47:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent-builder/commit/ac2c388136e6ae57485bec6679e9255730511bbb', 'message': 'Extend the DIB_CHECKSUM variable usage\n\nFollowup on I2dd1c60e3bfd9c823a7382b1390b1d40c52a5c97\n\nDepends-On: I2dd1c60e3bfd9c823a7382b1390b1d40c52a5c97\nChange-Id: I5e2d454fb84b76810f3c5ed26a0caeef8ea06675\nSigned-off-by: Maksim Malchuk <maksim.malchuk@gmail.com>\n'}, {'number': 2, 'created': '2023-04-23 22:01:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent-builder/commit/b3cf8d2b3a550580fc27857ed5b40e3e62e5bdd8', 'message': 'Extend the DIB_CHECKSUM variable usage\n\nFollowup on I2dd1c60e3bfd9c823a7382b1390b1d40c52a5c97\n\nDepends-On: I2dd1c60e3bfd9c823a7382b1390b1d40c52a5c97\nChange-Id: I5e2d454fb84b76810f3c5ed26a0caeef8ea06675\nSigned-off-by: Maksim Malchuk <maksim.malchuk@gmail.com>\n'}, {'number': 3, 'created': '2023-04-23 22:19:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent-builder/commit/0680f70aff4a40f2ec3c5d8d95e802c93ce81e81', 'message': 'Extend the DIB_CHECKSUM variable usage\n\nFollowup on I2dd1c60e3bfd9c823a7382b1390b1d40c52a5c97\n\nDepends-On: I2dd1c60e3bfd9c823a7382b1390b1d40c52a5c97\nChange-Id: I5e2d454fb84b76810f3c5ed26a0caeef8ea06675\nSigned-off-by: Maksim Malchuk <maksim.malchuk@gmail.com>\n'}, {'number': 4, 'created': '2023-04-23 22:31:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent-builder/commit/af7937ec87e982826c150d5c67f9f14928f37992', 'message': 'Extend the DIB_CHECKSUM variable usage\n\nFollowup on I2dd1c60e3bfd9c823a7382b1390b1d40c52a5c97\n\nChange-Id: I5e2d454fb84b76810f3c5ed26a0caeef8ea06675\nSigned-off-by: Maksim Malchuk <maksim.malchuk@gmail.com>\n'}, {'number': 5, 'created': '2023-04-23 22:36:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent-builder/commit/380a393d9c517452d3e0a3fd1ef3afc3abb06043', 'message': 'Extend the DIB_CHECKSUM variable usage\n\nFollowup on I2dd1c60e3bfd9c823a7382b1390b1d40c52a5c97\n\nChange-Id: I5e2d454fb84b76810f3c5ed26a0caeef8ea06675\nSigned-off-by: Maksim Malchuk <maksim.malchuk@gmail.com>\n'}, {'number': 6, 'created': '2023-04-25 17:27:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent-builder/commit/a51af60b1a3075b2585039caf9ef9fb0b6b4548f', 'message': 'Extend the DIB_CHECKSUM variable usage\n\nFollowup on I2dd1c60e3bfd9c823a7382b1390b1d40c52a5c97\n\nChange-Id: I5e2d454fb84b76810f3c5ed26a0caeef8ea06675\nSigned-off-by: Maksim Malchuk <maksim.malchuk@gmail.com>\n'}, {'number': 7, 'created': '2023-04-26 21:27:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent-builder/commit/3b982245582a69bc90225bca6dad36dd7ec40427', 'message': 'Extend the DIB_CHECKSUM variable usage\n\nFollowup on I2dd1c60e3bfd9c823a7382b1390b1d40c52a5c97\n\nChange-Id: I5e2d454fb84b76810f3c5ed26a0caeef8ea06675\nSigned-off-by: Maksim Malchuk <maksim.malchuk@gmail.com>\n'}, {'number': 8, 'created': '2023-05-24 20:44:14.000000000', 'files': ['dib/ironic-ramdisk-base/cleanup.d/99-ramdisk-create'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent-builder/commit/e118104f9838875c56c51cc40ec106994baa72b3', 'message': 'Extend the DIB_CHECKSUM variable usage\n\nFollowup on I2dd1c60e3bfd9c823a7382b1390b1d40c52a5c97\n\nChange-Id: I5e2d454fb84b76810f3c5ed26a0caeef8ea06675\nSigned-off-by: Maksim Malchuk <maksim.malchuk@gmail.com>\n'}]",11,881299,e118104f9838875c56c51cc40ec106994baa72b3,44,5,8,14200,,,0,"Extend the DIB_CHECKSUM variable usage

Followup on I2dd1c60e3bfd9c823a7382b1390b1d40c52a5c97

Change-Id: I5e2d454fb84b76810f3c5ed26a0caeef8ea06675
Signed-off-by: Maksim Malchuk <maksim.malchuk@gmail.com>
",git fetch https://review.opendev.org/openstack/ironic-python-agent-builder refs/changes/99/881299/2 && git format-patch -1 --stdout FETCH_HEAD,['dib/ironic-ramdisk-base/cleanup.d/99-ramdisk-create'],1,ac2c388136e6ae57485bec6679e9255730511bbb,extend-dib-checksum,"if [ -n ""$DIB_CHECKSUM"" ]; then [ ""$DIB_CHECKSUM"" == ""sha256"" ] || md5sum ${IMAGE_NAME}.initramfs ${IMAGE_NAME}.kernel > ${IMAGE_NAME}.md5 [ ""$DIB_CHECKSUM"" == ""md5""] || sha256sum ${IMAGE_NAME}.initramfs ${IMAGE_NAME}.kernel > ${IMAGE_NAME}.sha256","if [ ""$DIB_CHECKSUM"" == ""1"" ]; then md5sum ${IMAGE_NAME}.initramfs ${IMAGE_NAME}.kernel > ${IMAGE_NAME}.md5 sha256sum ${IMAGE_NAME}.initramfs ${IMAGE_NAME}.kernel > ${IMAGE_NAME}.sha256",3,3
openstack%2Fcinder~master~I5b41f761b0957d508eedd9f003860b833b14d56f,openstack/cinder,master,I5b41f761b0957d508eedd9f003860b833b14d56f,Imported Translations from Zanata,MERGED,2023-07-08 03:22:18.000000000,2023-07-08 19:54:58.000000000,2023-07-08 19:53:53.000000000,"[{'_account_id': 5314}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-08 03:22:18.000000000', 'files': ['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'], 'web_link': 'https://opendev.org/openstack/cinder/commit/7fa5561eb5ac9100f26be5a9c75ed070ecb728ac', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I5b41f761b0957d508eedd9f003860b833b14d56f\n'}]",0,888008,7fa5561eb5ac9100f26be5a9c75ed070ecb728ac,19,2,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: I5b41f761b0957d508eedd9f003860b833b14d56f
",git fetch https://review.opendev.org/openstack/cinder refs/changes/08/888008/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'],1,7fa5561eb5ac9100f26be5a9c75ed070ecb728ac,zanata/translations,"""POT-Creation-Date: 2023-07-03 19:36+0000\n""""PO-Revision-Date: 2023-07-07 09:07+0000\n""msgid ""16.4.2-17"" msgstr ""16.4.2-17""msgid ""20.3.0-2"" msgstr ""20.3.0-2"" msgid ""21.3.0-3"" msgstr ""21.3.0-3""msgid ""22.0.0.0rc1-74"" msgstr ""22.0.0.0rc1-74"" ","""POT-Creation-Date: 2023-06-27 22:25+0000\n""""PO-Revision-Date: 2023-06-20 11:18+0000\n""msgid ""16.4.2-16"" msgstr ""16.4.2-16""msgid ""21.3.0-2"" msgstr ""21.3.0-2""",12,6
openstack%2Fcinder~master~I22b767e848529cf98befe07a4a2983479570d0b1,openstack/cinder,master,I22b767e848529cf98befe07a4a2983479570d0b1,WIP: Add option to disable discard for encrypted volumes,NEW,2021-09-09 21:01:38.000000000,2023-07-08 18:53:33.000000000,,"[{'_account_id': 4523}, {'_account_id': 22348}]","[{'number': 1, 'created': '2021-09-09 21:01:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/ecf93a03eb13fbdd6afb3e948f4efacd9d7357aa', 'message': 'WIP: Add option to disable discard for encrypted volumes\n\nAdd a new option, ""allow_discard_on_encrypted_volumes"".\n\nWhen set to false, Cinder will no longer report that discard\nshould be enabled when a volume is encrypted.  This defaults\nto true to maintain current behavior.\n\nThis is beneficial for deployers/users with particularly strong\nsecurity concerns, because discard/TRIM patterns can reveal some\ninformation about the contents of a volume.\n\nTODO: add releasenote\n\nChange-Id: I22b767e848529cf98befe07a4a2983479570d0b1\n'}, {'number': 2, 'created': '2023-07-06 14:38:06.000000000', 'files': ['cinder/volume/manager.py', 'cinder/tests/unit/volume/test_volume_manager.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/0c60f998a07b7967b2761561f085f8d654eb94a6', 'message': 'WIP: Add option to disable discard for encrypted volumes\n\nAdd a new option, ""allow_discard_on_encrypted_volumes"".\n\nWhen set to false, Cinder will no longer report that discard\nshould be enabled when a volume is encrypted.  This defaults\nto true to maintain current behavior.\n\nThis is beneficial for deployers/users with particularly strong\nsecurity concerns, because discard/TRIM patterns can reveal some\ninformation about the contents of a volume.\n\nTODO: add releasenote\n\nChange-Id: I22b767e848529cf98befe07a4a2983479570d0b1\n'}]",0,808137,0c60f998a07b7967b2761561f085f8d654eb94a6,37,2,2,4523,,,0,"WIP: Add option to disable discard for encrypted volumes

Add a new option, ""allow_discard_on_encrypted_volumes"".

When set to false, Cinder will no longer report that discard
should be enabled when a volume is encrypted.  This defaults
to true to maintain current behavior.

This is beneficial for deployers/users with particularly strong
security concerns, because discard/TRIM patterns can reveal some
information about the contents of a volume.

TODO: add releasenote

Change-Id: I22b767e848529cf98befe07a4a2983479570d0b1
",git fetch https://review.opendev.org/openstack/cinder refs/changes/37/808137/2 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/manager.py', 'cinder/tests/unit/volume/test_volume_manager.py']",2,ecf93a03eb13fbdd6afb3e948f4efacd9d7357aa,," @mock.patch('cinder.volume.volume_types.get_volume_type_extra_specs') @mock.patch('cinder.volume.volume_types.get_volume_type_qos_specs', return_value={'qos_specs': None}) def test_parse_connection_options_discard(self, mock_get_qos, mock_get_extra_specs): ctxt = mock.Mock() manager = vol_manager.VolumeManager() vol = fake_volume.fake_volume_obj(ctxt) vol.volume_type_id = fake.VOLUME_TYPE_ID # no 'discard' set by driver, should not be reported conn_info = {""data"": {}} manager._parse_connection_options(ctxt, vol, conn_info) self.assertNotIn('discard', conn_info['data']) # driver sets 'discard' False conn_info = {""data"": {""discard"": False}} manager._parse_connection_options(ctxt, vol, conn_info) self.assertIn('discard', conn_info['data']) self.assertIs(conn_info['data']['discard'], False) # driver sets 'discard' True for encrypted vol w/ option on self.override_config('allow_discard_on_encrypted_volumes', True, group='backend_defaults') vol.encryption_key_id = fake.ENCRYPTION_KEY_ID conn_info = {""data"": {""discard"": True}} manager._parse_connection_options(ctxt, vol, conn_info) self.assertIs(conn_info['data']['discard'], True) # driver sets 'discard' True for encrypted vol w/ option off self.override_config('allow_discard_on_encrypted_volumes', False, group='backend_defaults') conn_info = {""data"": {""discard"": True}} manager._parse_connection_options(ctxt, vol, conn_info) self.assertIs(conn_info['data']['discard'], False) # driver sets 'discard' True for unencrypted vol w/ option on self.override_config('allow_discard_on_encrypted_volumes', True, group='backend_defaults') vol.encryption_key_id = None conn_info = {""data"": {""discard"": True}} manager._parse_connection_options(ctxt, vol, conn_info) self.assertIs(conn_info['data']['discard'], True) # driver sets 'discard' True for unencrypted vol w/ option off self.override_config('allow_discard_on_encrypted_volumes', False, group='backend_defaults') vol.encryption_key_id = None conn_info = {""data"": {""discard"": True}} manager._parse_connection_options(ctxt, vol, conn_info) self.assertIs(conn_info['data']['discard'], True)",,65,1
openstack%2Fopenstack-helm-infra~master~Icdcfa5684c2a5e610805f6dec9391a4947b213d4,openstack/openstack-helm-infra,master,Icdcfa5684c2a5e610805f6dec9391a4947b213d4,Make sure ovs ctl file exist before chown,MERGED,2023-07-07 18:36:41.000000000,2023-07-08 18:41:05.000000000,2023-07-08 18:40:09.000000000,"[{'_account_id': 1004}, {'_account_id': 3009}, {'_account_id': 22348}, {'_account_id': 28372}, {'_account_id': 29974}, {'_account_id': 32433}, {'_account_id': 34821}]","[{'number': 1, 'created': '2023-07-07 18:36:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/ba2d74aa0a3039a31017387dd6936613d3a45e5b', 'message': 'Make sure ovs ctl file exist before chown\n\nThis propose to make sure the exist of\n`/run/openvswitch/ovs-vswitchd.${PID}.ctl`\nbefore we do chown command with it.\n\nChange-Id: Icdcfa5684c2a5e610805f6dec9391a4947b213d4\n'}, {'number': 2, 'created': '2023-07-08 15:54:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/26376ccea786c959c94978a8e013251563d71295', 'message': 'Make sure ovs ctl file exist before chown\n\nThis propose to make sure the exist of\n`/run/openvswitch/ovs-vswitchd.${PID}.ctl`\nbefore we do chown command with it.\n\nChange-Id: Icdcfa5684c2a5e610805f6dec9391a4947b213d4\n'}, {'number': 3, 'created': '2023-07-08 16:56:03.000000000', 'files': ['releasenotes/notes/openvswitch.yaml', 'openvswitch/Chart.yaml', 'openvswitch/templates/bin/_openvswitch-vswitchd.sh.tpl'], 'web_link': 'https://opendev.org/openstack/openstack-helm-infra/commit/ee4d3ac71ce9fca76eceacfd569e44f019064c1e', 'message': 'Make sure ovs ctl file exist before chown\n\nThis propose to make sure the exist of\n`/run/openvswitch/ovs-vswitchd.${PID}.ctl`\nbefore we do chown command with it.\n\nChange-Id: Icdcfa5684c2a5e610805f6dec9391a4947b213d4\n'}]",1,887986,ee4d3ac71ce9fca76eceacfd569e44f019064c1e,12,7,3,12404,,,0,"Make sure ovs ctl file exist before chown

This propose to make sure the exist of
`/run/openvswitch/ovs-vswitchd.${PID}.ctl`
before we do chown command with it.

Change-Id: Icdcfa5684c2a5e610805f6dec9391a4947b213d4
",git fetch https://review.opendev.org/openstack/openstack-helm-infra refs/changes/86/887986/1 && git format-patch -1 --stdout FETCH_HEAD,['openvswitch/templates/bin/_openvswitch-vswitchd.sh.tpl'],1,ba2d74aa0a3039a31017387dd6936613d3a45e5b,fix-ovs," until [ -f $OVS_CTL ] do echo ""Waiting for file $OVS_CTL"" sleep 1 done",,6,0
openstack%2Fcinder~master~Ied74cd807891a8b0e257686529eb8a453e66c36f,openstack/cinder,master,Ied74cd807891a8b0e257686529eb8a453e66c36f,Fujitsu Driver: Change the function of attach/detach,NEW,2022-10-13 03:40:30.000000000,2023-07-08 18:20:51.000000000,,"[{'_account_id': 22348}, {'_account_id': 30615}]","[{'number': 1, 'created': '2022-10-13 03:40:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/0584f1546e627afc6f54ba786d5c22fc824edc37', 'message': 'Fujitsu Driver: Change the function of attach/detach\n\nModified the logic in attach volume and detach volume.\n\nNow the functions use CLI command to attach or detach volume.\n\nChange-Id: Ied74cd807891a8b0e257686529eb8a453e66c36f\n'}, {'number': 2, 'created': '2022-11-08 09:47:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/fd18ac9a89349cc5402bae17cdda8b42a8004974', 'message': 'Fujitsu Driver: Change the function of attach/detach\n\nModified the logic in attach volume and detach volume.\n\nNow the functions use CLI command to attach or detach volume.\n\nChange-Id: Ied74cd807891a8b0e257686529eb8a453e66c36f\n'}, {'number': 3, 'created': '2022-11-09 08:15:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/3f030d164ef118dd2df0ac7d33af46fb545af39a', 'message': 'Fujitsu Driver: Change the function of attach/detach\n\nModified the logic in attach volume and detach volume.\n\nNow the functions use CLI command to attach or detach volume.\n\nChange-Id: Ied74cd807891a8b0e257686529eb8a453e66c36f\n'}, {'number': 4, 'created': '2022-11-11 07:58:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9beda040ed48a7a5da8ce183b7e15d73d5c867e2', 'message': 'Fujitsu Driver: Change the function of attach/detach\n\nModified the logic in attach volume and detach volume.\nNow the functions use CLI command to attach or detach volume.\n\nFix error when detaching fujistu multiattached volume on same host.\nWhen detaching multiattached volume from one instance, connection is\ndeleted on the host so that other instances with same volume can\nnot use volume.\nWhen large than 1, do not terminate connection.\n\nChange-Id: Ied74cd807891a8b0e257686529eb8a453e66c36f\n'}, {'number': 5, 'created': '2022-11-15 07:51:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/e5522149149e20e8559cab71d17c8fdbf45b99e7', 'message': 'Fujitsu Driver: Change the function of attach/detach\n\nModified the logic in attach volume and detach volume.\nNow the functions use CLI command to attach or detach volume.\n\nFix error when detaching fujistu multiattached volume on same host.\nWhen detaching multiattached volume from one instance, connection is\ndeleted on the host so that other instances with same volume can\nnot use volume.\nWhen large than 1, do not terminate connection.\n\nChange-Id: Ied74cd807891a8b0e257686529eb8a453e66c36f\n'}, {'number': 6, 'created': '2022-11-18 09:45:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/022768415869bda792df0ead5d8030a881c52a6e', 'message': 'Fujitsu Driver: Change the function of attach/detach\n\nModified the logic in attach volume and detach volume.\nNow the functions use CLI command to attach or detach volume.\n\nFix error when detaching fujistu multiattached volume on same host.\nWhen detaching multiattached volume from one instance, connection is\ndeleted on the host so that other instances with same volume can\nnot use volume.\nWhen large than 1, do not terminate connection.\n\nChange-Id: Ied74cd807891a8b0e257686529eb8a453e66c36f\n'}, {'number': 7, 'created': '2022-11-21 07:34:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/e28c77f2ebedee499fcaeaf464c6b20405e31e1b', 'message': 'Fujitsu Driver: Change the function of attach/detach\n\nModified the logic in attach volume and detach volume.\nNow the functions use CLI command to attach or detach volume.\n\nFix error when detaching fujistu multiattached volume on same host.\nWhen detaching multiattached volume from one instance, connection is\ndeleted on the host so that other instances with same volume can\nnot use volume.\nWhen large than 1, do not terminate connection.\n\nChange-Id: Ied74cd807891a8b0e257686529eb8a453e66c36f\n'}, {'number': 8, 'created': '2023-04-03 03:49:33.000000000', 'files': ['cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_cli.py', 'releasenotes/notes/fujitsu-improve-function-of-attach-volume-e8b8a4ee371b87d8.yaml', 'cinder/tests/unit/volume/drivers/test_fujitsu_dx.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_fc.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_iscsi.py', 'cinder/volume/drivers/fujitsu/eternus_dx/constants.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/c8b4fcc39c63c6ea259fd7c4b31da0ff460d0c1e', 'message': 'Fujitsu Driver: Change the function of attach/detach\n\nModified the logic in attach volume and detach volume.\nNow the functions use CLI command to attach or detach volume.\n\nFix error when detaching fujistu multiattached volume on same host.\nWhen detaching multiattached volume from one instance, connection is\ndeleted on the host so that other instances with same volume can\nnot use volume.\nWhen large than 1, do not terminate connection.\n\nChange-Id: Ied74cd807891a8b0e257686529eb8a453e66c36f\n'}]",50,860997,c8b4fcc39c63c6ea259fd7c4b31da0ff460d0c1e,212,2,8,33609,,,0,"Fujitsu Driver: Change the function of attach/detach

Modified the logic in attach volume and detach volume.
Now the functions use CLI command to attach or detach volume.

Fix error when detaching fujistu multiattached volume on same host.
When detaching multiattached volume from one instance, connection is
deleted on the host so that other instances with same volume can
not use volume.
When large than 1, do not terminate connection.

Change-Id: Ied74cd807891a8b0e257686529eb8a453e66c36f
",git fetch https://review.opendev.org/openstack/cinder refs/changes/97/860997/8 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_cli.py', 'cinder/tests/unit/volume/drivers/test_fujitsu_dx.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_fc.py', 'cinder/volume/drivers/fujitsu/eternus_dx/constants.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_common.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_iscsi.py']",6,0584f1546e627afc6f54ba786d5c22fc824edc37,fujitsu-driver-update," model_update = self.common.create_volume(volume) return model_update return {'provider_location': str(element_path), 'metadata': metadata} return {'provider_location': str(element_path), 'metadata': metadata} self.common.delete_volume(volume) return {'provider_location': str(element_path)} self.common.delete_snapshot(snapshot) initiator = connector.get('initiator') if connector else None LOG.debug('initialize_connection, volume id: %(vid)s, ' 'initiator: %(initiator)s, Enter method.', {'vid': volume['id'], 'initiator': initiator}) info = self.common.initialize_connection( volume, connector) LOG.debug('initialize_connection, ' 'info: %s, Exit method.', info) LOG.debug('terminate_connection, volume id: %(vid)s, ' 'initiator: %(initiator)s, Enter method.', {'vid': volume['id'], 'initiator': initiator}) map_info = self.common.terminate_connection( volume, connector) LOG.debug('terminate_connection, ' 'map_num: %s, Exit method.', map_info.get('map_num')) 'pool name: %s.', pool_name) self.common.extend_volume(volume, new_size)","import six LOG.info('create_volume, volume id: %s, Enter method.', volume['id']) element_path, metadata = self.common.create_volume(volume) v_metadata = volume.get('volume_metadata') if v_metadata: for data in v_metadata: metadata[data['key']] = data['value'] else: v_metadata = volume.get('metadata', {}) metadata.update(v_metadata) LOG.info('create_volume, info: %s, Exit method.', metadata) return {'provider_location': six.text_type(element_path), 'metadata': metadata} LOG.info('create_volume_from_snapshot, ' 'volume id: %(vid)s, snap id: %(sid)s, Enter method.', {'vid': volume['id'], 'sid': snapshot['id']}) LOG.info('create_volume_from_snapshot, ' 'info: %s, Exit method.', metadata) return {'provider_location': six.text_type(element_path), 'metadata': metadata} LOG.info('create_cloned_volume, ' 'target volume id: %(tid)s, ' 'source volume id: %(sid)s, Enter method.', {'tid': volume['id'], 'sid': src_vref['id']}) LOG.info('create_cloned_volume, info: %s, Exit method.', metadata) return {'provider_location': six.text_type(element_path), 'metadata': metadata} LOG.info('delete_volume, volume id: %s, Enter method.', volume['id']) vol_exist = self.common.delete_volume(volume) LOG.info('delete_volume, delete: %s, Exit method.', vol_exist) return LOG.info('create_snapshot, snap id: %(sid)s, volume id: %(vid)s, ' 'Enter method.', {'sid': snapshot['id'], 'vid': snapshot['volume_id']}) LOG.info('create_snapshot, info: %s, Exit method.', metadata) return {'provider_location': six.text_type(element_path)} LOG.info('delete_snapshot, snap id: %(sid)s, volume id: %(vid)s, ' 'Enter method.', {'sid': snapshot['id'], 'vid': snapshot['volume_id']}) vol_exist = self.common.delete_snapshot(snapshot) LOG.info('delete_snapshot, delete: %s, Exit method.', vol_exist) return LOG.info('initialize_connection, volume id: %(vid)s, ' 'initiator: %(initiator)s, Enter method.', {'vid': volume['id'], 'initiator': connector['initiator']}) info = self.common.initialize_connection(volume, connector) LOG.info('initialize_connection, info: %s, Exit method.', info) LOG.info('terminate_connection, volume id: %(vid)s, ' 'initiator: %(initiator)s, Enter method.', {'vid': volume['id'], 'initiator': initiator}) map_exist = self.common.terminate_connection(volume, connector) LOG.info('terminate_connection, unmap: %s, Exit method.', map_exist) return LOG.debug('get_volume_stats, refresh: %s, Enter method.', refresh) 'pool name: %s, Exit method.', pool_name) LOG.info('extend_volume, volume id: %s, Enter method.', volume['id']) used_pool_name = self.common.extend_volume(volume, new_size) LOG.info('extend_volume, used pool name: %s, Exit method.', used_pool_name)",2361,834
openstack%2Fpython-swiftclient~master~I80af28b49415c5bea6399683922b6649f04ed011,openstack/python-swiftclient,master,I80af28b49415c5bea6399683922b6649f04ed011,tests: Skip keystoneauth tests if not available,NEW,2023-07-07 21:33:24.000000000,2023-07-08 18:10:54.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 21:33:24.000000000', 'files': ['test/unit/test_authv1.py', 'test/unit/test_swiftclient.py'], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/c52e404262e9a81afcdad1abed631699a14ec953', 'message': 'tests: Skip keystoneauth tests if not available\n\nChange-Id: I80af28b49415c5bea6399683922b6649f04ed011\n'}]",2,888000,c52e404262e9a81afcdad1abed631699a14ec953,6,1,1,15343,,,0,"tests: Skip keystoneauth tests if not available

Change-Id: I80af28b49415c5bea6399683922b6649f04ed011
",git fetch https://review.opendev.org/openstack/python-swiftclient refs/changes/00/888000/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/test_authv1.py', 'test/unit/test_swiftclient.py']",2,c52e404262e9a81afcdad1abed631699a14ec953,, try: from keystoneauth1 import exceptions as ksauthexceptions except ImportError: raise unittest.SkipTest('keystoneauth1 is not available'), from keystoneauth1 import exceptions as ksauthexceptions,17,5
openstack%2Fopenstack-ansible~master~Iaf81f778ffd9fcf6572e03e8ce73ffef46bd98aa,openstack/openstack-ansible,master,Iaf81f778ffd9fcf6572e03e8ce73ffef46bd98aa,Use include_role in task to avoid lack of access to vars,MERGED,2023-06-27 16:25:26.000000000,2023-07-08 17:44:03.000000000,2023-07-08 17:42:42.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 28619}]","[{'number': 1, 'created': '2023-06-27 16:25:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/f20904551ee7ad0cf18f1b8865fcf34c142be606', 'message': ""Use include_role in task to avoid lack of access to vars\n\nThis patch updates the security hardening playbook to use include_role\nwithin a task versus using 'roles' directly to fix cases where\napply_security_hardening is set to False. Some change to Ansible\nappears to limit access to vars when the role is skipped, resulting\nin failures. The side effect of this change is the role is skipped\nentirely (when applicable) versus the individual tasks being skipped,\nwhich speeds up deployment times.\n\nChange-Id: Iaf81f778ffd9fcf6572e03e8ce73ffef46bd98aa\n""}, {'number': 2, 'created': '2023-06-30 14:18:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/c5dba6dd5e9d2f36ae59c3ebb545f38f5b857e11', 'message': ""Use include_role in task to avoid lack of access to vars\n\nThis patch updates the security hardening playbook to use include_role\nwithin a task versus using 'roles' directly to fix cases where\napply_security_hardening is set to False. Some change to Ansible\nappears to limit access to vars when the role is skipped, resulting\nin failures. The side effect of this change is the role is skipped\nentirely (when applicable) versus the individual tasks being skipped,\nwhich speeds up deployment times.\n\nChange-Id: Iaf81f778ffd9fcf6572e03e8ce73ffef46bd98aa\n""}, {'number': 3, 'created': '2023-06-30 14:19:19.000000000', 'files': ['playbooks/infra-journal-remote.yml', 'playbooks/security-hardening.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/9690b34193430d99c201e3ac2e85258e6aba011e', 'message': ""Use include_role in task to avoid lack of access to vars\n\nThis patch updates the security hardening playbook to use include_role\nwithin a task versus using 'roles' directly to fix cases where\napply_security_hardening is set to False. Some change to Ansible\nappears to limit access to vars when the role is skipped, resulting\nin failures. The side effect of this change is the role is skipped\nentirely (when applicable) versus the individual tasks being skipped,\nwhich speeds up deployment times.\n\nChange-Id: Iaf81f778ffd9fcf6572e03e8ce73ffef46bd98aa\n""}]",5,887082,9690b34193430d99c201e3ac2e85258e6aba011e,27,3,3,16011,,,0,"Use include_role in task to avoid lack of access to vars

This patch updates the security hardening playbook to use include_role
within a task versus using 'roles' directly to fix cases where
apply_security_hardening is set to False. Some change to Ansible
appears to limit access to vars when the role is skipped, resulting
in failures. The side effect of this change is the role is skipped
entirely (when applicable) versus the individual tasks being skipped,
which speeds up deployment times.

Change-Id: Iaf81f778ffd9fcf6572e03e8ce73ffef46bd98aa
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/82/887082/3 && git format-patch -1 --stdout FETCH_HEAD,['playbooks/security-hardening.yml'],1,f20904551ee7ad0cf18f1b8865fcf34c142be606,osa/core-2.15," tasks: - name: Include security hardening role include_role: name: ""ansible-hardening"""," roles: - role: ""ansible-hardening""",4,2
openstack%2Fkolla-ansible~master~I6913756ff85b7cc626d50ce16d55085d3683db17,openstack/kolla-ansible,master,I6913756ff85b7cc626d50ce16d55085d3683db17,Performance: Split out prometheus node exporter and cadvisor,NEW,2020-07-08 18:32:35.000000000,2023-07-08 17:27:39.000000000,,"[{'_account_id': 167}, {'_account_id': 14200}, {'_account_id': 14826}, {'_account_id': 22348}, {'_account_id': 29543}, {'_account_id': 30523}, {'_account_id': 32067}, {'_account_id': 32553}, {'_account_id': 32657}]","[{'number': 1, 'created': '2020-07-08 18:32:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/9a1aaa7ac318dacd9ce84808c3de2e02c716afe4', 'message': 'Performance: Split out prometheus node exporter and cadvisor\n\nChange-Id: I6913756ff85b7cc626d50ce16d55085d3683db17\n'}, {'number': 2, 'created': '2020-07-09 11:10:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/254e62f4d74f50a4cb39fbbb56bb3ff040b81982', 'message': 'Performance: Split out prometheus node exporter and cadvisor\n\nChange-Id: I6913756ff85b7cc626d50ce16d55085d3683db17\n'}, {'number': 3, 'created': '2020-07-09 11:16:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/afffeea71c10026f0bce3c3299cdc83baa49ac53', 'message': 'Performance: Split out prometheus node exporter and cadvisor\n\nChange-Id: I6913756ff85b7cc626d50ce16d55085d3683db17\n'}, {'number': 4, 'created': '2020-07-09 12:30:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/f3250bc658381a05c6a030c846c23fda8fb4db97', 'message': 'Performance: Split out prometheus node exporter and cadvisor\n\nChange-Id: I6913756ff85b7cc626d50ce16d55085d3683db17\n'}, {'number': 5, 'created': '2020-07-09 14:51:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/aed64b0080ca06bd278742be7773ad0581803176', 'message': 'Performance: Split out prometheus node exporter and cadvisor\n\nExecuting tasks has a performance penalty, even if they are skipped [1].\nThe prometheus role deploys a few types of services:\n\n* core prometheus server and alert manager services\n* centralised service-based exporters (mysql, memcached, OpenStack,\n  etc.)\n* distributed exporters (node exporter, cadvisor) that are deployed to\n  all hosts\n\nThere is a significant overhead in executing the tasks associated with\nthe core services and centralised exporters against all hosts, even\nthough they are skipped.\n\nThis change extracts the node exporter and cadvisor deployment into a\nnew prometheus-all role, to avoid executing the prometheus role against\nall hosts. The new role is lightweight, and has a small number of tasks,\nwhich should significantly improve performance.\n\n[1]\nhttps://github.com/stackhpc/ansible-scaling/blob/master/doc/skip.md\n\nPartially-Implements: blueprint performance-improvements\n\nChange-Id: I6913756ff85b7cc626d50ce16d55085d3683db17\n'}, {'number': 6, 'created': '2020-07-20 11:24:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/83ca8c5a962874f0933ef0c7814e786582638bff', 'message': 'Performance: Split out prometheus node exporter and cadvisor\n\nExecuting tasks has a performance penalty, even if they are skipped [1].\nThe prometheus role deploys a few types of services:\n\n* core prometheus server and alert manager services\n* centralised service-based exporters (mysql, memcached, OpenStack,\n  etc.)\n* distributed exporters (node exporter, cadvisor) that are deployed to\n  all hosts\n\nThere is a significant overhead in executing the tasks associated with\nthe core services and centralised exporters against all hosts, even\nthough they are skipped.\n\nThis change extracts the node exporter and cadvisor deployment into a\nnew prometheus-all role, to avoid executing the prometheus role against\nall hosts. The new role is lightweight, and has a small number of tasks,\nwhich should significantly improve performance.\n\n[1]\nhttps://github.com/stackhpc/ansible-scaling/blob/master/doc/skip.md\n\nPartially-Implements: blueprint performance-improvements\n\nChange-Id: I6913756ff85b7cc626d50ce16d55085d3683db17\n'}, {'number': 7, 'created': '2020-08-25 19:09:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/7588dcc40d733ff03502511ade33640585b4fdc8', 'message': 'Performance: Split out prometheus node exporter and cadvisor\n\nExecuting tasks has a performance penalty, even if they are skipped [1].\nThe prometheus role deploys a few types of services:\n\n* core prometheus server and alert manager services\n* centralised service-based exporters (mysql, memcached, OpenStack,\n  etc.)\n* distributed exporters (node exporter, cadvisor) that are deployed to\n  all hosts\n\nThere is a significant overhead in executing the tasks associated with\nthe core services and centralised exporters against all hosts, even\nthough they are skipped.\n\nThis change extracts the node exporter and cadvisor deployment into a\nnew prometheus-all role, to avoid executing the prometheus role against\nall hosts. The new role is lightweight, and has a small number of tasks,\nwhich should significantly improve performance.\n\n[1]\nhttps://github.com/stackhpc/ansible-scaling/blob/master/doc/skip.md\n\nPartially-Implements: blueprint performance-improvements\n\nChange-Id: I6913756ff85b7cc626d50ce16d55085d3683db17\n'}, {'number': 8, 'created': '2021-02-22 14:07:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/b3cb090381cc44738cab9545134cd67625da0615', 'message': 'Performance: Split out prometheus node exporter and cadvisor\n\nExecuting tasks has a performance penalty, even if they are skipped [1].\nThe prometheus role deploys a few types of services:\n\n* core prometheus server and alert manager services\n* centralised service-based exporters (mysql, memcached, OpenStack,\n  etc.)\n* distributed node-based exporters (node exporter, cadvisor) that are\n  deployed to all hosts\n\nThere is a significant overhead in executing the tasks associated with\nthe core services and centralised exporters against all hosts, even\nthough they are skipped.\n\nThis change extracts the node exporter and cadvisor deployment into a\nnew prometheus-node-exporters role, to avoid executing the prometheus\nrole against all hosts. The new role is lightweight, and has a small\nnumber of tasks, which should significantly improve performance.\n\n[1]\nhttps://github.com/stackhpc/ansible-scaling/blob/master/doc/skip.md\n\nPartially-Implements: blueprint performance-improvements\n\nChange-Id: I6913756ff85b7cc626d50ce16d55085d3683db17\n'}, {'number': 9, 'created': '2021-08-25 16:27:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/4501102f3932050f26c9dae40f2b314ada66bb73', 'message': 'Performance: Split out prometheus node exporter and cadvisor\n\nExecuting tasks has a performance penalty, even if they are skipped [1].\nThe prometheus role deploys a few types of services:\n\n* core prometheus server and alert manager services\n* centralised service-based exporters (mysql, memcached, OpenStack,\n  etc.)\n* distributed node-based exporters (node exporter, cadvisor) that are\n  deployed to all hosts\n\nThere is a significant overhead in executing the tasks associated with\nthe core services and centralised exporters against all hosts, even\nthough they are skipped.\n\nThis change extracts the node exporter and cadvisor deployment into a\nnew prometheus-node-exporters role, to avoid executing the prometheus\nrole against all hosts. The new role is lightweight, and has a small\nnumber of tasks, which should significantly improve performance.\n\n[1]\nhttps://github.com/stackhpc/ansible-scaling/blob/master/doc/skip.md\n\nPartially-Implements: blueprint performance-improvements\n\nChange-Id: I6913756ff85b7cc626d50ce16d55085d3683db17\n'}, {'number': 10, 'created': '2023-05-22 09:32:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/f662b14e26459f85eae0ab572f009caac7d04111', 'message': 'Performance: Split out prometheus node exporter and cadvisor\n\nExecuting tasks has a performance penalty, even if they are skipped [1].\nThe prometheus role deploys a few types of services:\n\n* core prometheus server and alert manager services\n* centralised service-based exporters (mysql, memcached, OpenStack,\n  etc.)\n* distributed node-based exporters (node exporter, cadvisor) that are\n  deployed to all hosts\n\nThere is a significant overhead in executing the tasks associated with\nthe core services and centralised exporters against all hosts, even\nthough they are skipped.\n\nThis change extracts the node exporter and cadvisor deployment into a\nnew prometheus-node-exporters role, to avoid executing the prometheus\nrole against all hosts. The new role is lightweight, and has a small\nnumber of tasks, which should significantly improve performance.\n\n[1]\nhttps://github.com/stackhpc/ansible-scaling/blob/master/doc/skip.md\n\nPartially-Implements: blueprint performance-improvements\n\nChange-Id: I6913756ff85b7cc626d50ce16d55085d3683db17\n'}, {'number': 11, 'created': '2023-05-22 12:46:22.000000000', 'files': ['ansible/roles/prometheus-node-exporters/handlers/main.yml', 'ansible/roles/prometheus-node-exporters/templates/prometheus-node-exporter.json.j2', 'ansible/roles/prometheus-node-exporters/templates/prometheus-cadvisor.json.j2', 'ansible/roles/prometheus-node-exporters/tasks/upgrade.yml', 'ansible/roles/prometheus-node-exporters/tasks/reconfigure.yml', 'ansible/site.yml', 'ansible/roles/prometheus-node-exporters/tasks/check-containers.yml', 'releasenotes/notes/split-prometheus-role-d363716375b14143.yaml', 'ansible/roles/prometheus-node-exporters/tasks/deploy.yml', 'ansible/roles/prometheus-node-exporters/tasks/copy-certs.yml', 'ansible/roles/prometheus-node-exporters/tasks/precheck.yml', 'ansible/roles/prometheus-node-exporters/tasks/config.yml', 'ansible/roles/prometheus-node-exporters/tasks/deploy-containers.yml', 'ansible/roles/prometheus-node-exporters/tasks/check.yml', 'ansible/roles/prometheus-node-exporters/tasks/pull.yml', 'ansible/roles/prometheus-node-exporters/defaults/main.yml', 'ansible/roles/prometheus-node-exporters/tasks/main.yml', 'ansible/roles/prometheus-node-exporters/tasks/stop.yml', 'ansible/roles/prometheus/defaults/main.yml', 'ansible/roles/prometheus/tasks/precheck.yml', 'ansible/roles/prometheus/handlers/main.yml'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/00c7794c39b8c1b047230d9752557aa407afb5d5', 'message': 'Performance: Split out prometheus node exporter and cadvisor\n\nExecuting tasks has a performance penalty, even if they are skipped [1].\nThe prometheus role deploys a few types of services:\n\n* core prometheus server and alert manager services\n* centralised service-based exporters (mysql, memcached, OpenStack,\n  etc.)\n* distributed node-based exporters (node exporter, cadvisor) that are\n  deployed to all hosts\n\nThere is a significant overhead in executing the tasks associated with\nthe core services and centralised exporters against all hosts, even\nthough they are skipped.\n\nThis change extracts the node exporter and cadvisor deployment into a\nnew prometheus-node-exporters role, to avoid executing the prometheus\nrole against all hosts. The new role is lightweight, and has a small\nnumber of tasks, which should significantly improve performance.\n\n[1]\nhttps://github.com/stackhpc/ansible-scaling/blob/master/doc/skip.md\n\nPartially-Implements: blueprint performance-improvements\n\nChange-Id: I6913756ff85b7cc626d50ce16d55085d3683db17\n'}]",7,740082,00c7794c39b8c1b047230d9752557aa407afb5d5,46,9,11,14826,,,0,"Performance: Split out prometheus node exporter and cadvisor

Executing tasks has a performance penalty, even if they are skipped [1].
The prometheus role deploys a few types of services:

* core prometheus server and alert manager services
* centralised service-based exporters (mysql, memcached, OpenStack,
  etc.)
* distributed node-based exporters (node exporter, cadvisor) that are
  deployed to all hosts

There is a significant overhead in executing the tasks associated with
the core services and centralised exporters against all hosts, even
though they are skipped.

This change extracts the node exporter and cadvisor deployment into a
new prometheus-node-exporters role, to avoid executing the prometheus
role against all hosts. The new role is lightweight, and has a small
number of tasks, which should significantly improve performance.

[1]
https://github.com/stackhpc/ansible-scaling/blob/master/doc/skip.md

Partially-Implements: blueprint performance-improvements

Change-Id: I6913756ff85b7cc626d50ce16d55085d3683db17
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/82/740082/3 && git format-patch -1 --stdout FETCH_HEAD,"['ansible/roles/prometheus-all/tasks/config.yml', 'ansible/roles/prometheus-all/templates/prometheus-node-exporter.json.j2', 'ansible/roles/prometheus-all/defaults/main.yml', 'ansible/roles/prometheus-all/tasks/precheck.yml', 'ansible/site.yml', 'ansible/roles/prometheus-all/tasks/main.yml', 'ansible/roles/prometheus-all/templates/prometheus-cadvisor.json.j2', 'ansible/roles/prometheus-all/tasks/deploy.yml', 'ansible/roles/prometheus-all/tasks/stop.yml', 'ansible/roles/prometheus-all/handlers/main.yml', 'ansible/roles/prometheus-all/tasks/check-containers.yml', 'ansible/roles/prometheus-all/tasks/deploy-containers.yml', 'ansible/roles/prometheus-all/meta/main.yml', 'ansible/roles/prometheus-all/tasks/reconfigure.yml', 'ansible/roles/prometheus-all/tasks/copy-certs.yml', 'ansible/roles/prometheus-all/tasks/upgrade.yml', 'ansible/roles/prometheus/defaults/main.yml', 'ansible/roles/prometheus/tasks/precheck.yml', 'ansible/roles/prometheus/handlers/main.yml', 'ansible/roles/prometheus-all/tasks/check.yml', 'ansible/roles/prometheus-all/tasks/pull.yml']",21,9a1aaa7ac318dacd9ce84808c3de2e02c716afe4,bp/performance-improvements,"--- - name: Pulling prometheus images become: true kolla_docker: action: ""pull_image"" common_options: ""{{ docker_common_options }}"" image: ""{{ item.value.image }}"" when: - inventory_hostname in groups[item.value.group] - item.value.enabled | bool with_dict: ""{{ prometheus_services }}"" ",,234,103
openstack%2Fceilometer~master~Ieb4f08c08efdc95c13b33bae52c28495b051d78e,openstack/ceilometer,master,Ieb4f08c08efdc95c13b33bae52c28495b051d78e,Imported Translations from Zanata,ABANDONED,2023-07-08 03:15:49.000000000,2023-07-08 16:11:04.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-08 03:15:49.000000000', 'files': ['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/dc4ced468068fcaff23f83539183c26248129321', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: Ieb4f08c08efdc95c13b33bae52c28495b051d78e\n'}]",0,888007,dc4ced468068fcaff23f83539183c26248129321,3,1,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: Ieb4f08c08efdc95c13b33bae52c28495b051d78e
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/07/888007/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'],1,dc4ced468068fcaff23f83539183c26248129321,zanata/translations,"""POT-Creation-Date: 2023-07-07 08:57+0000\n""""PO-Revision-Date: 2023-07-07 09:07+0000\n""msgid ""20.0.0-13"" msgstr ""20.0.0-13""","""POT-Creation-Date: 2023-06-19 06:37+0000\n""""PO-Revision-Date: 2023-06-20 11:18+0000\n""msgid ""20.0.0-12"" msgstr ""20.0.0-12""",4,4
openstack%2Fproject-config~master~I93e6928d30db8a90b45329ca00f066b4ec1b4ae7,openstack/project-config,master,I93e6928d30db8a90b45329ca00f066b4ec1b4ae7,Fix unbound setup for debian-bookworm,MERGED,2023-07-04 07:49:19.000000000,2023-07-08 13:29:58.000000000,2023-07-04 09:46:55.000000000,"[{'_account_id': 13252}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 07:49:19.000000000', 'files': ['nodepool/elements/nodepool-base/pkg-map'], 'web_link': 'https://opendev.org/openstack/project-config/commit/3df74599243942aa691cda526f607a89c1a7ec7e', 'message': 'Fix unbound setup for debian-bookworm\n\ndns-root-data has been demoted to a ""Recommends"" dependency of unbound,\nwhich we don\'t install. Sadly the default unbound configuration is\nbroken without it.\n\nChange-Id: I93e6928d30db8a90b45329ca00f066b4ec1b4ae7\n'}]",1,887570,3df74599243942aa691cda526f607a89c1a7ec7e,9,2,1,13252,,,0,"Fix unbound setup for debian-bookworm

dns-root-data has been demoted to a ""Recommends"" dependency of unbound,
which we don't install. Sadly the default unbound configuration is
broken without it.

Change-Id: I93e6928d30db8a90b45329ca00f066b4ec1b4ae7
",git fetch https://review.opendev.org/openstack/project-config refs/changes/70/887570/1 && git format-patch -1 --stdout FETCH_HEAD,['nodepool/elements/nodepool-base/pkg-map'],1,3df74599243942aa691cda526f607a89c1a7ec7e,debian-bookworm,"{ ""release"": { ""debian"": { ""bookworm"": { ""unbound"": ""unbound dns-root-data"" } } } } ",,9,0
openstack%2Fneutron~stable%2Fzed~I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265,openstack/neutron,stable/zed,I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265,Don't allow deletion of the router ports without IP addresses,MERGED,2023-07-04 14:48:34.000000000,2023-07-08 12:35:57.000000000,2023-07-08 12:34:08.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 14:48:34.000000000', 'files': ['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/1fe05c561c27846af5a35e00f8b0e83a978c3c8f', 'message': ""Don't allow deletion of the router ports without IP addresses\n\nThis patch effectively reverts old patch [1]. From now on it will be not\nallowed to directly remove router ports which don't have fixed IPs\nassigned. Such ports will be treated as any other ports connected to the\nrouters.\nOriginally [1] was introduced to allow cleanup of the router ports for\nwhich subnets were deleted. But now it's not needed anymore as we\nprevent deletion of subnet if there are any ports with IP allocated from\nthat subnet.\n\nCloses-bug: #2025056\n\n[1] https://review.opendev.org/c/openstack/neutron/+/20424\n\nChange-Id: I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265\n(cherry picked from commit 32d589f03ed0d2744fe15173ebacafd18fced8a9)\n""}]",2,887615,1fe05c561c27846af5a35e00f8b0e83a978c3c8f,19,3,1,11975,,,0,"Don't allow deletion of the router ports without IP addresses

This patch effectively reverts old patch [1]. From now on it will be not
allowed to directly remove router ports which don't have fixed IPs
assigned. Such ports will be treated as any other ports connected to the
routers.
Originally [1] was introduced to allow cleanup of the router ports for
which subnets were deleted. But now it's not needed anymore as we
prevent deletion of subnet if there are any ports with IP allocated from
that subnet.

Closes-bug: #2025056

[1] https://review.opendev.org/c/openstack/neutron/+/20424

Change-Id: I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265
(cherry picked from commit 32d589f03ed0d2744fe15173ebacafd18fced8a9)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/15/887615/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py']",2,1fe05c561c27846af5a35e00f8b0e83a978c3c8f,bug/2025056-stable/2023.1-stable/zed," 'device_id': '44', 'id': 'f', } with testtools.ExpectedException(n_exc.ServicePortInUse): self.db.prevent_l3_port_deletion(mock.Mock(), None)"," 'id': 'f' } self.db.prevent_l3_port_deletion(None, None)",3,11
openstack%2Fopenstack-helm~master~Ie227e7d2dd297b6095a40f6114ef6b0a2f226790,openstack/openstack-helm,master,Ie227e7d2dd297b6095a40f6114ef6b0a2f226790,Run tests for older releases periodic-weekly,MERGED,2023-07-05 00:50:08.000000000,2023-07-08 07:39:37.000000000,2023-07-08 07:37:46.000000000,"[{'_account_id': 3009}, {'_account_id': 22348}, {'_account_id': 33330}]","[{'number': 1, 'created': '2023-07-05 00:50:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/a206f72ac33ca0eb91bc68c296edc92f6dd85b00', 'message': ""Run tests for older releases periodic-weekly\n\nOpenstack releases older than Yoga are now in\nextended maintenance. To reduce the CI\nfootprint we don't run test jobs for older\nrelases as part of the check/gate pipelines.\n\nInstead we are going to run those jobs as\npart of the periodic-weekly pipeline.\n\nSee the detailed description of extended maintenance\nstatus here\nhttps://docs.openstack.org/project-team-guide/stable-branches.html#maintenance-phases\n\nChange-Id: Ie227e7d2dd297b6095a40f6114ef6b0a2f226790\n""}, {'number': 2, 'created': '2023-07-05 00:57:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/efd9d80f629737acb4e5a6351bbfac50cd3301ec', 'message': ""Run tests for older releases periodic-weekly\n\nOpenstack releases older than Yoga are now in\nextended maintenance. To reduce the CI\nfootprint we don't run test jobs for older\nrelases as part of the check/gate pipelines.\n\nInstead we are going to run those jobs as\npart of the periodic-weekly pipeline.\n\nSee the detailed description of extended maintenance\nstatus here\nhttps://docs.openstack.org/project-team-guide/stable-branches.html#maintenance-phases\n\nChange-Id: Ie227e7d2dd297b6095a40f6114ef6b0a2f226790\n""}, {'number': 3, 'created': '2023-07-07 17:12:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/a2a1a6dc11516e740363208e2e3ef660eaedfd47', 'message': ""Run tests for older releases periodic-weekly\n\nOpenstack releases older than Yoga are now in\nextended maintenance. To reduce the CI\nfootprint we don't run test jobs for older\nrelases as part of the check/gate pipelines.\n\nInstead we are going to run those jobs as\npart of the periodic-weekly pipeline.\n\nSee the detailed description of extended maintenance\nstatus here\nhttps://docs.openstack.org/project-team-guide/stable-branches.html#maintenance-phases\n\nChange-Id: Ie227e7d2dd297b6095a40f6114ef6b0a2f226790\n""}, {'number': 4, 'created': '2023-07-08 05:10:41.000000000', 'files': ['zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/1a3b7b5c2579d8062802342ac977dc29118eda28', 'message': ""Run tests for older releases periodic-weekly\n\nOpenstack releases older than Yoga are now in\nextended maintenance. To reduce the CI\nfootprint we don't run test jobs for older\nrelases as part of the check/gate pipelines.\n\nInstead we are going to run those jobs as\npart of the periodic-weekly pipeline.\n\nSee the detailed description of extended maintenance\nstatus here\nhttps://docs.openstack.org/project-team-guide/stable-branches.html#maintenance-phases\n\nChange-Id: Ie227e7d2dd297b6095a40f6114ef6b0a2f226790\n""}]",7,887647,1a3b7b5c2579d8062802342ac977dc29118eda28,26,3,4,3009,,,0,"Run tests for older releases periodic-weekly

Openstack releases older than Yoga are now in
extended maintenance. To reduce the CI
footprint we don't run test jobs for older
relases as part of the check/gate pipelines.

Instead we are going to run those jobs as
part of the periodic-weekly pipeline.

See the detailed description of extended maintenance
status here
https://docs.openstack.org/project-team-guide/stable-branches.html#maintenance-phases

Change-Id: Ie227e7d2dd297b6095a40f6114ef6b0a2f226790
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/47/887647/4 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/project.yaml'],1,a206f72ac33ca0eb91bc68c296edc92f6dd85b00,, - openstack-helm-cinder-2023-1-ubuntu_focal - openstack-helm-compute-kit-2023-1-ubuntu_focal periodic-weekly: jobs: - openstack-helm-cinder-victoria-ubuntu_focal - openstack-helm-cinder-wallaby-ubuntu_focal - openstack-helm-cinder-xena-ubuntu_focal - openstack-helm-compute-kit-victoria-ubuntu_focal - openstack-helm-compute-kit-wallaby-ubuntu_focal - openstack-helm-compute-kit-xena-ubuntu_focal , - openstack-helm-cinder-victoria-ubuntu_focal - openstack-helm-cinder-wallaby-ubuntu_focal - openstack-helm-cinder-xena-ubuntu_focal - openstack-helm-compute-kit-victoria-ubuntu_focal - openstack-helm-compute-kit-wallaby-ubuntu_focal - openstack-helm-compute-kit-xena-ubuntu_focal - openstack-helm-cinder-wallaby-ubuntu_focal - openstack-helm-compute-kit-wallaby-ubuntu_focal,11,8
openstack%2Fpython-swiftclient~master~I757b76e3af5f667a670cdf65687f23ef2b486666,openstack/python-swiftclient,master,I757b76e3af5f667a670cdf65687f23ef2b486666,Declare py311 support,MERGED,2023-07-07 19:42:10.000000000,2023-07-08 06:54:06.000000000,2023-07-08 06:52:16.000000000,"[{'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-07 19:42:10.000000000', 'files': ['.zuul.yaml', 'setup.cfg'], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/90f565009a2d18d6f4f3660aa57673e99ba638a7', 'message': 'Declare py311 support\n\nAdd a voting job, update trove classifiers.\n\nChange-Id: I757b76e3af5f667a670cdf65687f23ef2b486666\n'}]",1,887994,90f565009a2d18d6f4f3660aa57673e99ba638a7,8,2,1,15343,,,0,"Declare py311 support

Add a voting job, update trove classifiers.

Change-Id: I757b76e3af5f667a670cdf65687f23ef2b486666
",git fetch https://review.opendev.org/openstack/python-swiftclient refs/changes/94/887994/1 && git format-patch -1 --stdout FETCH_HEAD,"['.zuul.yaml', 'setup.cfg']",2,90f565009a2d18d6f4f3660aa57673e99ba638a7,, Programming Language :: Python :: 3.11,,11,0
openstack%2Fpython-swiftclient~master~Iee5467e148d7f67134f8277148e300b340750103,openstack/python-swiftclient,master,Iee5467e148d7f67134f8277148e300b340750103,Translate '\'-delimited paths on Windows to be '/'-delimited,NEW,2015-11-03 01:37:09.000000000,2023-07-08 05:25:45.000000000,,"[{'_account_id': 6968}, {'_account_id': 13390}, {'_account_id': 15343}, {'_account_id': 18604}, {'_account_id': 22348}]","[{'number': 1, 'created': '2015-11-03 01:37:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/ab7e8bb53ed00ccebf4c741bdb35e1e4e5f6506a', 'message': 'swiftclient updated\n\nChange-Id: Iee5467e148d7f67134f8277148e300b340750103\n'}, {'number': 2, 'created': '2021-02-23 05:43:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/3025c6a0b07e31b8f7df41ce579e8b55ca9b77df', 'message': 'Translate \'\\\'-delimited paths on Windows to be \'/\'-delimited\n\nThis allows Windows clients to more easily upload ""standard"" object\nnames that will interact well with features like staticweb.\n\nChange-Id: Iee5467e148d7f67134f8277148e300b340750103\n'}, {'number': 3, 'created': '2021-02-23 05:54:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/aa8c3c43150c027484e9c4220c8ffce827baeeed', 'message': 'Translate \'\\\'-delimited paths on Windows to be \'/\'-delimited\n\nThis allows Windows clients to more easily upload ""standard"" object\nnames that will interact well with features like staticweb.\n\nChange-Id: Iee5467e148d7f67134f8277148e300b340750103\n'}, {'number': 4, 'created': '2021-04-08 23:57:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/4b83aa8468601057c9b3651a5661d59e78bee580', 'message': 'Translate \'\\\'-delimited paths on Windows to be \'/\'-delimited\n\nThis allows Windows clients to more easily upload ""standard"" object\nnames that will interact well with features like staticweb.\n\nChange-Id: Iee5467e148d7f67134f8277148e300b340750103\n'}, {'number': 5, 'created': '2022-01-12 05:24:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/85908256ce914bc1b46324c7b670d9ac3155973c', 'message': 'Translate \'\\\'-delimited paths on Windows to be \'/\'-delimited\n\nThis allows Windows clients to more easily upload ""standard"" object\nnames that will interact well with features like staticweb.\n\nCloses-Bug: #1511813\nCo-Authored-By: Tim Burke <tim.burke@gmail.com>\nChange-Id: Iee5467e148d7f67134f8277148e300b340750103\n'}, {'number': 6, 'created': '2022-10-17 17:04:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/5abdda5401a00af6a1d9042a4c26253b989fafc4', 'message': 'Translate \'\\\'-delimited paths on Windows to be \'/\'-delimited\n\nThis allows Windows clients to more easily upload ""standard"" object\nnames that will interact well with features like staticweb.\n\nCloses-Bug: #1511813\nCo-Authored-By: Tim Burke <tim.burke@gmail.com>\nChange-Id: Iee5467e148d7f67134f8277148e300b340750103\n'}, {'number': 7, 'created': '2023-07-07 20:13:24.000000000', 'files': ['swiftclient/shell.py', 'test/unit/test_shell.py'], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/2434b2dc628f3ded8223b7334566086c61b0828f', 'message': 'Translate \'\\\'-delimited paths on Windows to be \'/\'-delimited\n\nThis allows Windows clients to more easily upload ""standard"" object\nnames that will interact well with features like staticweb.\n\nCloses-Bug: #1511813\nCo-Authored-By: Tim Burke <tim.burke@gmail.com>\nChange-Id: Iee5467e148d7f67134f8277148e300b340750103\n'}]",4,241064,2434b2dc628f3ded8223b7334566086c61b0828f,24,5,7,18604,,,0,"Translate '\'-delimited paths on Windows to be '/'-delimited

This allows Windows clients to more easily upload ""standard"" object
names that will interact well with features like staticweb.

Closes-Bug: #1511813
Co-Authored-By: Tim Burke <tim.burke@gmail.com>
Change-Id: Iee5467e148d7f67134f8277148e300b340750103
",git fetch https://review.opendev.org/openstack/python-swiftclient refs/changes/64/241064/3 && git format-patch -1 --stdout FETCH_HEAD,['swiftclient/shell.py'],1,ab7e8bb53ed00ccebf4c741bdb35e1e4e5f6506a,," _dir = _dir.replace('\\','/') objs.extend(['/'.join([_dir, _f]) for _f in _fs])"," objs.extend([join(_dir, _f) for _f in _fs])",2,1
openstack%2Fopenstack-helm~master~I27600cc732039ef82d41cea8d1ef9bba2eb6001b,openstack/openstack-helm,master,I27600cc732039ef82d41cea8d1ef9bba2eb6001b,Run rally tests sequentially in compute-kit jobs,MERGED,2023-07-07 21:31:09.000000000,2023-07-08 04:02:05.000000000,2023-07-08 03:59:07.000000000,"[{'_account_id': 3009}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-07 21:31:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/6904397e8f97bdec6f28261a0e772fd8f97d4f5f', 'message': 'Run rally tests sequentially in compute-kit jobs\n\nChange-Id: I27600cc732039ef82d41cea8d1ef9bba2eb6001b\n'}, {'number': 2, 'created': '2023-07-08 01:08:32.000000000', 'files': ['zuul.d/jobs-openstack-helm.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/77fe3a0fb2a271630501e8773c7f8e563c570c08', 'message': 'Run rally tests sequentially in compute-kit jobs\n\nChange-Id: I27600cc732039ef82d41cea8d1ef9bba2eb6001b\n'}]",1,887999,77fe3a0fb2a271630501e8773c7f8e563c570c08,9,2,2,3009,,,0,"Run rally tests sequentially in compute-kit jobs

Change-Id: I27600cc732039ef82d41cea8d1ef9bba2eb6001b
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/99/887999/2 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/jobs-openstack-helm.yaml'],1,6904397e8f97bdec6f28261a0e772fd8f97d4f5f,sequential_rally_tests, - export OSH_TEST_TIMEOUT=1200;./tools/deployment/common/run-helm-tests.sh neutron - ./tools/deployment/common/run-helm-tests.sh nova - ./tools/deployment/common/run-helm-tests.sh glance - ./tools/deployment/common/run-helm-tests.sh keystone, - - export OSH_TEST_TIMEOUT=1200;./tools/deployment/common/run-helm-tests.sh neutron - ./tools/deployment/common/run-helm-tests.sh nova; ./tools/deployment/common/run-helm-tests.sh glance; ./tools/deployment/common/run-helm-tests.sh keystone;,4,4
openstack%2Fpython-swiftclient~master~I6ac1c04ba1831b1445fb0554b5e10fe3b1c26d7b,openstack/python-swiftclient,master,I6ac1c04ba1831b1445fb0554b5e10fe3b1c26d7b,Python 3.4 support is removed,ABANDONED,2017-03-08 12:42:44.000000000,2023-07-08 03:40:06.000000000,,[{'_account_id': 3}],"[{'number': 1, 'created': '2017-03-08 12:42:44.000000000', 'files': ['setup.cfg', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/python-swiftclient/commit/32783a783d56347afd4667bb7ea43e0fb56ad4ff', 'message': 'Python 3.4 support is removed\n\nIn setup.cfg and tox.ini the python 3.4 is removed\nbeacuse python 3.5 is available.\n\nChange-Id: I6ac1c04ba1831b1445fb0554b5e10fe3b1c26d7b\n'}]",0,443110,32783a783d56347afd4667bb7ea43e0fb56ad4ff,3,1,1,25005,,,0,"Python 3.4 support is removed

In setup.cfg and tox.ini the python 3.4 is removed
beacuse python 3.5 is available.

Change-Id: I6ac1c04ba1831b1445fb0554b5e10fe3b1c26d7b
",git fetch https://review.opendev.org/openstack/python-swiftclient refs/changes/10/443110/1 && git format-patch -1 --stdout FETCH_HEAD,"['setup.cfg', 'tox.ini']",2,32783a783d56347afd4667bb7ea43e0fb56ad4ff,pythonversion,"envlist = py27,py35,pypy,pep8","envlist = py27,py34,py35,pypy,pep8",1,2
openstack%2Frequirements~master~Ia3d9182cf067d3684e2ab534e436c97a6562682e,openstack/requirements,master,Ia3d9182cf067d3684e2ab534e436c97a6562682e,update constraint for keystoneauth1 to new release 5.2.1,MERGED,2023-06-19 14:13:35.000000000,2023-07-08 00:16:02.000000000,2023-07-08 00:15:06.000000000,"[{'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-19 14:13:35.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/24cab47230976257fc4beca351bea66f31c7d5d0', 'message': 'update constraint for keystoneauth1 to new release 5.2.1\n\nmeta: version: 5.2.1\nmeta: diff-start: -\nmeta: series: bobcat\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: no\nmeta: release:Author: Artem Goncharov <artem.goncharov@gmail.com>\nmeta: release:Commit: Artem Goncharov <artem.goncharov@gmail.com>\nmeta: release:Change-Id: Ifa7288f2cad259c0569979d7744dd944c1165571\nmeta: release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nmeta: release:Code-Review+1: David Wilde <dwilde@redhat.com>\nmeta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>\nmeta: release:Workflow+1: Thierry Carrez <thierry@openstack.org>\nChange-Id: Ia3d9182cf067d3684e2ab534e436c97a6562682e\n'}]",3,886406,24cab47230976257fc4beca351bea66f31c7d5d0,18,3,1,11131,,,0,"update constraint for keystoneauth1 to new release 5.2.1

meta: version: 5.2.1
meta: diff-start: -
meta: series: bobcat
meta: release-type: release
meta: pypi: yes
meta: first: no
meta: release:Author: Artem Goncharov <artem.goncharov@gmail.com>
meta: release:Commit: Artem Goncharov <artem.goncharov@gmail.com>
meta: release:Change-Id: Ifa7288f2cad259c0569979d7744dd944c1165571
meta: release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
meta: release:Code-Review+1: David Wilde <dwilde@redhat.com>
meta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>
meta: release:Workflow+1: Thierry Carrez <thierry@openstack.org>
Change-Id: Ia3d9182cf067d3684e2ab534e436c97a6562682e
",git fetch https://review.opendev.org/openstack/requirements refs/changes/06/886406/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,24cab47230976257fc4beca351bea66f31c7d5d0,new-release,keystoneauth1===5.2.1,keystoneauth1===5.2.0,1,1
openstack%2Fnova~stable%2Fussuri~I2209bf1b3320901cf603ec39163cf923b25b0359,openstack/nova,stable/ussuri,I2209bf1b3320901cf603ec39163cf923b25b0359,"database: Archive parent and child rows ""trees"" one at a time",NEW,2023-07-07 21:50:34.000000000,2023-07-07 23:36:46.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 21:50:34.000000000', 'files': ['nova/tests/functional/db/test_archive.py', 'releasenotes/notes/db-archive-performance-degradation-3fdabc43398149b1.yaml', 'nova/tests/unit/cmd/test_manage.py', 'nova/cmd/manage.py', 'nova/db/sqlalchemy/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/892424f055abc9c98da1e36af9c5a2c05573ab26', 'message': 'database: Archive parent and child rows ""trees"" one at a time\n\nPreviously, we archived deleted rows in batches of max_rows parents +\ntheir child rows in a single database transaction. Doing it that way\nlimited how high a value of max_rows could be specified by the caller\nbecause of the size of the database transaction it could generate.\n\nFor example, in a large scale deployment with hundreds of thousands of\ndeleted rows and constant server creation and deletion activity, a\nvalue of max_rows=1000 might exceed the database\'s configured maximum\npacket size or timeout due to a database deadlock, forcing the operator\nto use a much lower max_rows value like 100 or 50.\n\nAnd when the operator has e.g. 500,000 deleted instances rows (and\nmillions of deleted rows total) they are trying to archive, being\nforced to use a max_rows value several orders of magnitude lower than\nthe number of rows they need to archive was a poor user experience.\n\nThis changes the logic to archive one parent row and its foreign key\nrelated child rows one at a time in a single database transaction\nwhile limiting the total number of rows per table as soon as it reaches\n>= max_rows. Doing this will allow operators to choose more predictable\nvalues for max_rows and get more progress per invocation of\narchive_deleted_rows.\n\nCloses-Bug: #2024258\n\nChange-Id: I2209bf1b3320901cf603ec39163cf923b25b0359\n(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)\n(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)\n(cherry picked from commit 6972efdaa94d05952eb5b90e456b566c4c73360c)\n(cherry picked from commit b555279add583e6b0cee1bddb8d5642c68ad84a6)\n(cherry picked from commit 4dc6ec9dccba2695536cb3cbd5534e64003904c9)\n(cherry picked from commit 2d073fdd935d3df6511c16b0203dcc7933e68a89)\n(cherry picked from commit 125009b215f8d8ca0d28a3bbf06c9ade6e5840ab)\n'}]",0,888002,892424f055abc9c98da1e36af9c5a2c05573ab26,2,1,1,4690,,,0,"database: Archive parent and child rows ""trees"" one at a time

Previously, we archived deleted rows in batches of max_rows parents +
their child rows in a single database transaction. Doing it that way
limited how high a value of max_rows could be specified by the caller
because of the size of the database transaction it could generate.

For example, in a large scale deployment with hundreds of thousands of
deleted rows and constant server creation and deletion activity, a
value of max_rows=1000 might exceed the database's configured maximum
packet size or timeout due to a database deadlock, forcing the operator
to use a much lower max_rows value like 100 or 50.

And when the operator has e.g. 500,000 deleted instances rows (and
millions of deleted rows total) they are trying to archive, being
forced to use a max_rows value several orders of magnitude lower than
the number of rows they need to archive was a poor user experience.

This changes the logic to archive one parent row and its foreign key
related child rows one at a time in a single database transaction
while limiting the total number of rows per table as soon as it reaches
>= max_rows. Doing this will allow operators to choose more predictable
values for max_rows and get more progress per invocation of
archive_deleted_rows.

Closes-Bug: #2024258

Change-Id: I2209bf1b3320901cf603ec39163cf923b25b0359
(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)
(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)
(cherry picked from commit 6972efdaa94d05952eb5b90e456b566c4c73360c)
(cherry picked from commit b555279add583e6b0cee1bddb8d5642c68ad84a6)
(cherry picked from commit 4dc6ec9dccba2695536cb3cbd5534e64003904c9)
(cherry picked from commit 2d073fdd935d3df6511c16b0203dcc7933e68a89)
(cherry picked from commit 125009b215f8d8ca0d28a3bbf06c9ade6e5840ab)
",git fetch https://review.opendev.org/openstack/nova refs/changes/02/888002/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/functional/db/test_archive.py', 'releasenotes/notes/db-archive-performance-degradation-3fdabc43398149b1.yaml', 'nova/tests/unit/cmd/test_manage.py', 'nova/cmd/manage.py', 'nova/db/sqlalchemy/api.py']",5,892424f055abc9c98da1e36af9c5a2c05573ab26,," # This is a list of IDs of rows that should be archived from this table, # limited to a length of max_rows. # # extras = {tablename: number_of_extra_rows_archived} if not records: # Nothing to archive, so return. return rows_archived, deleted_instance_uuids, extras # Keep track of how many rows we accumulate for the insert+delete database # transaction and cap it as soon as it is >= max_rows. Because we will # archive all child rows of a parent row along with the parent at the same # time, we end up with extra rows to archive in addition to len(records). num_rows_in_batch = 0 # The sequence of query statements we will execute in a batch. These are # ordered: [child1, child1, parent1, child2, child2, child2, parent2, ...] # Parent + child ""trees"" are kept together to avoid FK constraint # violations. statements_in_batch = [] # The list of records in the batch. This is used for collecting deleted # instance UUIDs in the case of the 'instances' table. records_in_batch = [] # (melwitt): We will gather rows related by foreign key relationship for # each deleted row, one at a time. We do it this way to keep track of and # limit the total number of rows that will be archived in a single database # transaction. In a large scale database with potentially hundreds of # thousands of deleted rows, if we don't limit the size of the transaction # based on max_rows, we can get into a situation where we get stuck not # able to make much progress. The value of max_rows has to be 1) small # enough to not exceed the database's max packet size limit or timeout with # a deadlock but 2) large enough to make progress in an environment with a # constant high volume of create and delete traffic. By archiving each # parent + child rows tree one at a time, we can ensure meaningful progress # can be made while allowing the caller to predictably control the size of # the database transaction with max_rows. for record in records: metadata, conn, table, column, [record]) statements_in_batch.extend(fk_inserts + fk_deletes) # statement to add parent row to shadow table insert = shadow_table.insert(inline=True).\ from_select(columns, sql.select([table], column.in_([record]))) statements_in_batch.append(insert) # statement to remove parent row from main table delete = table.delete().where(column.in_([record])) statements_in_batch.append(delete) records_in_batch.append(record) # Check whether were have a full batch >= max_rows. Rows are counted as # the number of rows that will be moved in the database transaction. # So each insert+delete pair represents one row that will be moved. # 1 parent + its fks num_rows_in_batch += 1 + len(fk_inserts) if max_rows is not None and num_rows_in_batch >= max_rows: break # NOTE(tssurya): In order to facilitate the deletion of records from # instance_mappings, request_specs and instance_group_member tables in the # nova_api DB, the rows of deleted instances from the instances table are # stored prior to their deletion. Basically the uuids of the archived # instances are queried and returned. if tablename == ""instances"": query_select = sql.select( [table.c.uuid], table.c.id.in_(records_in_batch)) rows = conn.execute(query_select).fetchall() # deleted_instance_uuids = ['uuid1', 'uuid2', ...] deleted_instance_uuids = [r[0] for r in rows] try: # Group the insert and delete in a transaction. with conn.begin(): for statement in statements_in_batch: result = conn.execute(statement) result_tablename = statement.table.name # Add to archived row counts if not a shadow table. if not result_tablename.startswith(_SHADOW_TABLE_PREFIX): if result_tablename == tablename: # Number of tablename (parent) rows archived. rows_archived += result.rowcount else: # Number(s) of child rows archived. extras[result_tablename] += result.rowcount except db_exc.DBReferenceError as ex: # A foreign key constraint keeps us from deleting some of these rows # until we clean up a dependent table. Just skip this table for now; # we'll come back to it later. LOG.warning(""IntegrityError detected when archiving table "" ""%(tablename)s: %(error)s"", {'tablename': tablename, 'error': six.text_type(ex)})"," # {tablename: extra_rows_archived} if records: insert = shadow_table.insert(inline=True).\ from_select(columns, sql.select([table], column.in_(records))) delete = table.delete().where(column.in_(records)) metadata, conn, table, column, records) # NOTE(tssurya): In order to facilitate the deletion of records from # instance_mappings, request_specs and instance_group_member tables in # the nova_api DB, the rows of deleted instances from the instances # table are stored prior to their deletion. Basically the uuids of the # archived instances are queried and returned. if tablename == ""instances"": query_select = sql.select([table.c.uuid], table.c.id.in_(records)) rows = conn.execute(query_select).fetchall() deleted_instance_uuids = [r[0] for r in rows] try: # Group the insert and delete in a transaction. with conn.begin(): for fk_insert in fk_inserts: conn.execute(fk_insert) for fk_delete in fk_deletes: result_fk_delete = conn.execute(fk_delete) extras[fk_delete.table.name] += result_fk_delete.rowcount conn.execute(insert) result_delete = conn.execute(delete) rows_archived += result_delete.rowcount except db_exc.DBReferenceError as ex: # A foreign key constraint keeps us from deleting some of # these rows until we clean up a dependent table. Just # skip this table for now; we'll come back to it later. LOG.warning(""IntegrityError detected when archiving table "" ""%(tablename)s: %(error)s"", {'tablename': tablename, 'error': six.text_type(ex)})",200,43
openstack%2Fnova~stable%2Fvictoria~I2209bf1b3320901cf603ec39163cf923b25b0359,openstack/nova,stable/victoria,I2209bf1b3320901cf603ec39163cf923b25b0359,"database: Archive parent and child rows ""trees"" one at a time",NEW,2023-07-07 21:16:53.000000000,2023-07-07 22:53:58.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 21:16:53.000000000', 'files': ['nova/tests/functional/db/test_archive.py', 'releasenotes/notes/db-archive-performance-degradation-3fdabc43398149b1.yaml', 'nova/tests/unit/cmd/test_manage.py', 'nova/cmd/manage.py', 'nova/db/sqlalchemy/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/125009b215f8d8ca0d28a3bbf06c9ade6e5840ab', 'message': 'database: Archive parent and child rows ""trees"" one at a time\n\nPreviously, we archived deleted rows in batches of max_rows parents +\ntheir child rows in a single database transaction. Doing it that way\nlimited how high a value of max_rows could be specified by the caller\nbecause of the size of the database transaction it could generate.\n\nFor example, in a large scale deployment with hundreds of thousands of\ndeleted rows and constant server creation and deletion activity, a\nvalue of max_rows=1000 might exceed the database\'s configured maximum\npacket size or timeout due to a database deadlock, forcing the operator\nto use a much lower max_rows value like 100 or 50.\n\nAnd when the operator has e.g. 500,000 deleted instances rows (and\nmillions of deleted rows total) they are trying to archive, being\nforced to use a max_rows value several orders of magnitude lower than\nthe number of rows they need to archive was a poor user experience.\n\nThis changes the logic to archive one parent row and its foreign key\nrelated child rows one at a time in a single database transaction\nwhile limiting the total number of rows per table as soon as it reaches\n>= max_rows. Doing this will allow operators to choose more predictable\nvalues for max_rows and get more progress per invocation of\narchive_deleted_rows.\n\nConflicts:\n    nova/db/sqlalchemy/api.py\n\nNOTE(melwitt): The conflict is because change\nI23bb9e539d08f5c6202909054c2dd49b6c7a7a0e (Remove six.text_type (1/2))\nis not in Victoria.\n\nCloses-Bug: #2024258\n\nChange-Id: I2209bf1b3320901cf603ec39163cf923b25b0359\n(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)\n(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)\n(cherry picked from commit 6972efdaa94d05952eb5b90e456b566c4c73360c)\n(cherry picked from commit b555279add583e6b0cee1bddb8d5642c68ad84a6)\n(cherry picked from commit 4dc6ec9dccba2695536cb3cbd5534e64003904c9)\n(cherry picked from commit 2d073fdd935d3df6511c16b0203dcc7933e68a89)\n'}]",0,887998,125009b215f8d8ca0d28a3bbf06c9ade6e5840ab,2,1,1,4690,,,0,"database: Archive parent and child rows ""trees"" one at a time

Previously, we archived deleted rows in batches of max_rows parents +
their child rows in a single database transaction. Doing it that way
limited how high a value of max_rows could be specified by the caller
because of the size of the database transaction it could generate.

For example, in a large scale deployment with hundreds of thousands of
deleted rows and constant server creation and deletion activity, a
value of max_rows=1000 might exceed the database's configured maximum
packet size or timeout due to a database deadlock, forcing the operator
to use a much lower max_rows value like 100 or 50.

And when the operator has e.g. 500,000 deleted instances rows (and
millions of deleted rows total) they are trying to archive, being
forced to use a max_rows value several orders of magnitude lower than
the number of rows they need to archive was a poor user experience.

This changes the logic to archive one parent row and its foreign key
related child rows one at a time in a single database transaction
while limiting the total number of rows per table as soon as it reaches
>= max_rows. Doing this will allow operators to choose more predictable
values for max_rows and get more progress per invocation of
archive_deleted_rows.

Conflicts:
    nova/db/sqlalchemy/api.py

NOTE(melwitt): The conflict is because change
I23bb9e539d08f5c6202909054c2dd49b6c7a7a0e (Remove six.text_type (1/2))
is not in Victoria.

Closes-Bug: #2024258

Change-Id: I2209bf1b3320901cf603ec39163cf923b25b0359
(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)
(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)
(cherry picked from commit 6972efdaa94d05952eb5b90e456b566c4c73360c)
(cherry picked from commit b555279add583e6b0cee1bddb8d5642c68ad84a6)
(cherry picked from commit 4dc6ec9dccba2695536cb3cbd5534e64003904c9)
(cherry picked from commit 2d073fdd935d3df6511c16b0203dcc7933e68a89)
",git fetch https://review.opendev.org/openstack/nova refs/changes/98/887998/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/functional/db/test_archive.py', 'releasenotes/notes/db-archive-performance-degradation-3fdabc43398149b1.yaml', 'nova/tests/unit/cmd/test_manage.py', 'nova/cmd/manage.py', 'nova/db/sqlalchemy/api.py']",5,125009b215f8d8ca0d28a3bbf06c9ade6e5840ab,," # This is a list of IDs of rows that should be archived from this table, # limited to a length of max_rows. # # extras = {tablename: number_of_extra_rows_archived} if not records: # Nothing to archive, so return. return rows_archived, deleted_instance_uuids, extras # Keep track of how many rows we accumulate for the insert+delete database # transaction and cap it as soon as it is >= max_rows. Because we will # archive all child rows of a parent row along with the parent at the same # time, we end up with extra rows to archive in addition to len(records). num_rows_in_batch = 0 # The sequence of query statements we will execute in a batch. These are # ordered: [child1, child1, parent1, child2, child2, child2, parent2, ...] # Parent + child ""trees"" are kept together to avoid FK constraint # violations. statements_in_batch = [] # The list of records in the batch. This is used for collecting deleted # instance UUIDs in the case of the 'instances' table. records_in_batch = [] # (melwitt): We will gather rows related by foreign key relationship for # each deleted row, one at a time. We do it this way to keep track of and # limit the total number of rows that will be archived in a single database # transaction. In a large scale database with potentially hundreds of # thousands of deleted rows, if we don't limit the size of the transaction # based on max_rows, we can get into a situation where we get stuck not # able to make much progress. The value of max_rows has to be 1) small # enough to not exceed the database's max packet size limit or timeout with # a deadlock but 2) large enough to make progress in an environment with a # constant high volume of create and delete traffic. By archiving each # parent + child rows tree one at a time, we can ensure meaningful progress # can be made while allowing the caller to predictably control the size of # the database transaction with max_rows. for record in records: metadata, conn, table, column, [record]) statements_in_batch.extend(fk_inserts + fk_deletes) # statement to add parent row to shadow table insert = shadow_table.insert(inline=True).\ from_select(columns, sql.select([table], column.in_([record]))) statements_in_batch.append(insert) # statement to remove parent row from main table delete = table.delete().where(column.in_([record])) statements_in_batch.append(delete) records_in_batch.append(record) # Check whether were have a full batch >= max_rows. Rows are counted as # the number of rows that will be moved in the database transaction. # So each insert+delete pair represents one row that will be moved. # 1 parent + its fks num_rows_in_batch += 1 + len(fk_inserts) if max_rows is not None and num_rows_in_batch >= max_rows: break # NOTE(tssurya): In order to facilitate the deletion of records from # instance_mappings, request_specs and instance_group_member tables in the # nova_api DB, the rows of deleted instances from the instances table are # stored prior to their deletion. Basically the uuids of the archived # instances are queried and returned. if tablename == ""instances"": query_select = sql.select( [table.c.uuid], table.c.id.in_(records_in_batch)) rows = conn.execute(query_select).fetchall() # deleted_instance_uuids = ['uuid1', 'uuid2', ...] deleted_instance_uuids = [r[0] for r in rows] try: # Group the insert and delete in a transaction. with conn.begin(): for statement in statements_in_batch: result = conn.execute(statement) result_tablename = statement.table.name # Add to archived row counts if not a shadow table. if not result_tablename.startswith(_SHADOW_TABLE_PREFIX): if result_tablename == tablename: # Number of tablename (parent) rows archived. rows_archived += result.rowcount else: # Number(s) of child rows archived. extras[result_tablename] += result.rowcount except db_exc.DBReferenceError as ex: # A foreign key constraint keeps us from deleting some of these rows # until we clean up a dependent table. Just skip this table for now; # we'll come back to it later. LOG.warning(""IntegrityError detected when archiving table "" ""%(tablename)s: %(error)s"", {'tablename': tablename, 'error': six.text_type(ex)})"," # {tablename: extra_rows_archived} if records: insert = shadow_table.insert(inline=True).\ from_select(columns, sql.select([table], column.in_(records))) delete = table.delete().where(column.in_(records)) metadata, conn, table, column, records) # NOTE(tssurya): In order to facilitate the deletion of records from # instance_mappings, request_specs and instance_group_member tables in # the nova_api DB, the rows of deleted instances from the instances # table are stored prior to their deletion. Basically the uuids of the # archived instances are queried and returned. if tablename == ""instances"": query_select = sql.select([table.c.uuid], table.c.id.in_(records)) rows = conn.execute(query_select).fetchall() deleted_instance_uuids = [r[0] for r in rows] try: # Group the insert and delete in a transaction. with conn.begin(): for fk_insert in fk_inserts: conn.execute(fk_insert) for fk_delete in fk_deletes: result_fk_delete = conn.execute(fk_delete) extras[fk_delete.table.name] += result_fk_delete.rowcount conn.execute(insert) result_delete = conn.execute(delete) rows_archived += result_delete.rowcount except db_exc.DBReferenceError as ex: # A foreign key constraint keeps us from deleting some of # these rows until we clean up a dependent table. Just # skip this table for now; we'll come back to it later. LOG.warning(""IntegrityError detected when archiving table "" ""%(tablename)s: %(error)s"", {'tablename': tablename, 'error': six.text_type(ex)})",200,43
openstack%2Ftripleo-common~stable%2Fwallaby~Ib7c1b83eaba36f7bc7ea6538e73604ca5bc6bf01,openstack/tripleo-common,stable/wallaby,Ib7c1b83eaba36f7bc7ea6538e73604ca5bc6bf01,Use nova-libvirt image for LibvirtConfig,MERGED,2023-07-03 10:30:07.000000000,2023-07-07 22:43:04.000000000,2023-07-07 22:42:08.000000000,"[{'_account_id': 6926}, {'_account_id': 9816}, {'_account_id': 17216}, {'_account_id': 20733}, {'_account_id': 22348}, {'_account_id': 22954}, {'_account_id': 23811}]","[{'number': 1, 'created': '2023-07-03 10:30:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/2d5ae80c34b2d20382324639983bd6d7e88ecd85', 'message': 'Use nova-libvirt image for LibvirtConfig\n\nThis change moves the ContainerNovaLibvirtConfigImage param to use the\nnova-libvirt image instead of the nova-compute image.\n\nResolves: rhbz#2186553\nSigned-off-by: Brendan Shephard <bshephar@redhat.com>\nChange-Id: Ib7c1b83eaba36f7bc7ea6538e73604ca5bc6bf01\n'}, {'number': 2, 'created': '2023-07-04 11:23:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/ab2e63579c60293509ed8c3ca62569aeb2de5d8c', 'message': 'Use nova-libvirt image for LibvirtConfig\n\nThis change moves the ContainerNovaLibvirtConfigImage param to use the\nnova-libvirt image instead of the nova-compute image.\n\nResolves: rhbz#2186553\nSigned-off-by: Brendan Shephard <bshephar@redhat.com>\nChange-Id: Ib7c1b83eaba36f7bc7ea6538e73604ca5bc6bf01\n'}, {'number': 3, 'created': '2023-07-05 00:18:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/7ac21a8f72e243b86791d2196d1e355d2e973dc7', 'message': 'Use nova-libvirt image for LibvirtConfig\n\nThis change moves the ContainerNovaLibvirtConfigImage param to use the\nnova-libvirt image instead of the nova-compute image.\n\nResolves: rhbz#2186553\nSigned-off-by: Brendan Shephard <bshephar@redhat.com>\nChange-Id: Ib7c1b83eaba36f7bc7ea6538e73604ca5bc6bf01\n'}, {'number': 4, 'created': '2023-07-05 04:12:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/941d1e7bc5c38cf434ba942c215c4bd26e293233', 'message': 'Use nova-libvirt image for LibvirtConfig\n\nThis change moves the ContainerNovaLibvirtConfigImage param to use the\nnova-libvirt image instead of the nova-compute image.\n\nResolves: rhbz#2186553\nSigned-off-by: Brendan Shephard <bshephar@redhat.com>\nChange-Id: Ib7c1b83eaba36f7bc7ea6538e73604ca5bc6bf01\n'}, {'number': 5, 'created': '2023-07-05 05:03:26.000000000', 'files': ['container-images/tripleo_containers.yaml.j2', 'tripleo_common/tests/image/test_kolla_builder.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/4c109097e02ff9a7be217a2e6f053487b06cc950', 'message': 'Use nova-libvirt image for LibvirtConfig\n\nThis change moves the ContainerNovaLibvirtConfigImage param to use the\nnova-libvirt image instead of the nova-compute image.\n\nResolves: rhbz#2186553\nSigned-off-by: Brendan Shephard <bshephar@redhat.com>\nChange-Id: Ib7c1b83eaba36f7bc7ea6538e73604ca5bc6bf01\n'}]",17,887503,4c109097e02ff9a7be217a2e6f053487b06cc950,36,7,5,30073,,,0,"Use nova-libvirt image for LibvirtConfig

This change moves the ContainerNovaLibvirtConfigImage param to use the
nova-libvirt image instead of the nova-compute image.

Resolves: rhbz#2186553
Signed-off-by: Brendan Shephard <bshephar@redhat.com>
Change-Id: Ib7c1b83eaba36f7bc7ea6538e73604ca5bc6bf01
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/03/887503/1 && git format-patch -1 --stdout FETCH_HEAD,['container-images/tripleo_containers.yaml.j2'],1,2d5ae80c34b2d20382324639983bd6d7e88ecd85,, - ContainerNovaLibvirtConfigImage, - ContainerNovaLibvirtConfigImage,1,1
openstack%2Fnova~stable%2Fwallaby~I2209bf1b3320901cf603ec39163cf923b25b0359,openstack/nova,stable/wallaby,I2209bf1b3320901cf603ec39163cf923b25b0359,"database: Archive parent and child rows ""trees"" one at a time",NEW,2023-07-07 18:42:00.000000000,2023-07-07 22:32:01.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 18:42:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/0fce1a73711c8f1cc2cc17241ff1b212423baca7', 'message': 'database: Archive parent and child rows ""trees"" one at a time\n\nPreviously, we archived deleted rows in batches of max_rows parents +\ntheir child rows in a single database transaction. Doing it that way\nlimited how high a value of max_rows could be specified by the caller\nbecause of the size of the database transaction it could generate.\n\nFor example, in a large scale deployment with hundreds of thousands of\ndeleted rows and constant server creation and deletion activity, a\nvalue of max_rows=1000 might exceed the database\'s configured maximum\npacket size or timeout due to a database deadlock, forcing the operator\nto use a much lower max_rows value like 100 or 50.\n\nAnd when the operator has e.g. 500,000 deleted instances rows (and\nmillions of deleted rows total) they are trying to archive, being\nforced to use a max_rows value several orders of magnitude lower than\nthe number of rows they need to archive was a poor user experience.\n\nThis changes the logic to archive one parent row and its foreign key\nrelated child rows one at a time in a single database transaction\nwhile limiting the total number of rows per table as soon as it reaches\n>= max_rows. Doing this will allow operators to choose more predictable\nvalues for max_rows and get more progress per invocation of\narchive_deleted_rows.\n\nConflicts:\n    nova/cmd/manage.py\n\nNOTE(melwitt): The conflict is because change\nIbed67854a693c930effd4dba7aca6cd03b65bd92 (Add --task-log option to\nnova-manage db archive_deleted_rows) is not in Wallaby.\n\nCloses-Bug: #2024258\n\nChange-Id: I2209bf1b3320901cf603ec39163cf923b25b0359\n(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)\n(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)\n(cherry picked from commit 6972efdaa94d05952eb5b90e456b566c4c73360c)\n(cherry picked from commit b555279add583e6b0cee1bddb8d5642c68ad84a6)\n(cherry picked from commit 4dc6ec9dccba2695536cb3cbd5534e64003904c9)\n'}, {'number': 2, 'created': '2023-07-07 20:49:02.000000000', 'files': ['nova/tests/functional/db/test_archive.py', 'releasenotes/notes/db-archive-performance-degradation-3fdabc43398149b1.yaml', 'nova/tests/unit/cmd/test_manage.py', 'nova/cmd/manage.py', 'nova/db/sqlalchemy/api.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/2d073fdd935d3df6511c16b0203dcc7933e68a89', 'message': 'database: Archive parent and child rows ""trees"" one at a time\n\nPreviously, we archived deleted rows in batches of max_rows parents +\ntheir child rows in a single database transaction. Doing it that way\nlimited how high a value of max_rows could be specified by the caller\nbecause of the size of the database transaction it could generate.\n\nFor example, in a large scale deployment with hundreds of thousands of\ndeleted rows and constant server creation and deletion activity, a\nvalue of max_rows=1000 might exceed the database\'s configured maximum\npacket size or timeout due to a database deadlock, forcing the operator\nto use a much lower max_rows value like 100 or 50.\n\nAnd when the operator has e.g. 500,000 deleted instances rows (and\nmillions of deleted rows total) they are trying to archive, being\nforced to use a max_rows value several orders of magnitude lower than\nthe number of rows they need to archive was a poor user experience.\n\nThis changes the logic to archive one parent row and its foreign key\nrelated child rows one at a time in a single database transaction\nwhile limiting the total number of rows per table as soon as it reaches\n>= max_rows. Doing this will allow operators to choose more predictable\nvalues for max_rows and get more progress per invocation of\narchive_deleted_rows.\n\nConflicts:\n    nova/cmd/manage.py\n\nNOTE(melwitt): The conflict is because change\nIbed67854a693c930effd4dba7aca6cd03b65bd92 (Add --task-log option to\nnova-manage db archive_deleted_rows) is not in Wallaby.\n\nCloses-Bug: #2024258\n\nChange-Id: I2209bf1b3320901cf603ec39163cf923b25b0359\n(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)\n(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)\n(cherry picked from commit 6972efdaa94d05952eb5b90e456b566c4c73360c)\n(cherry picked from commit b555279add583e6b0cee1bddb8d5642c68ad84a6)\n(cherry picked from commit 4dc6ec9dccba2695536cb3cbd5534e64003904c9)\n'}]",0,887988,2d073fdd935d3df6511c16b0203dcc7933e68a89,6,1,2,4690,,,0,"database: Archive parent and child rows ""trees"" one at a time

Previously, we archived deleted rows in batches of max_rows parents +
their child rows in a single database transaction. Doing it that way
limited how high a value of max_rows could be specified by the caller
because of the size of the database transaction it could generate.

For example, in a large scale deployment with hundreds of thousands of
deleted rows and constant server creation and deletion activity, a
value of max_rows=1000 might exceed the database's configured maximum
packet size or timeout due to a database deadlock, forcing the operator
to use a much lower max_rows value like 100 or 50.

And when the operator has e.g. 500,000 deleted instances rows (and
millions of deleted rows total) they are trying to archive, being
forced to use a max_rows value several orders of magnitude lower than
the number of rows they need to archive was a poor user experience.

This changes the logic to archive one parent row and its foreign key
related child rows one at a time in a single database transaction
while limiting the total number of rows per table as soon as it reaches
>= max_rows. Doing this will allow operators to choose more predictable
values for max_rows and get more progress per invocation of
archive_deleted_rows.

Conflicts:
    nova/cmd/manage.py

NOTE(melwitt): The conflict is because change
Ibed67854a693c930effd4dba7aca6cd03b65bd92 (Add --task-log option to
nova-manage db archive_deleted_rows) is not in Wallaby.

Closes-Bug: #2024258

Change-Id: I2209bf1b3320901cf603ec39163cf923b25b0359
(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)
(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)
(cherry picked from commit 6972efdaa94d05952eb5b90e456b566c4c73360c)
(cherry picked from commit b555279add583e6b0cee1bddb8d5642c68ad84a6)
(cherry picked from commit 4dc6ec9dccba2695536cb3cbd5534e64003904c9)
",git fetch https://review.opendev.org/openstack/nova refs/changes/88/887988/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/functional/db/test_archive.py', 'releasenotes/notes/db-archive-performance-degradation-3fdabc43398149b1.yaml', 'nova/tests/unit/cmd/test_manage.py', 'nova/cmd/manage.py', 'nova/db/sqlalchemy/api.py']",5,0fce1a73711c8f1cc2cc17241ff1b212423baca7,," # This is a list of IDs of rows that should be archived from this table, # limited to a length of max_rows. # # extras = {tablename: number_of_extra_rows_archived} if not records: # Nothing to archive, so return. return rows_archived, deleted_instance_uuids, extras # Keep track of how many rows we accumulate for the insert+delete database # transaction and cap it as soon as it is >= max_rows. Because we will # archive all child rows of a parent row along with the parent at the same # time, we end up with extra rows to archive in addition to len(records). num_rows_in_batch = 0 # The sequence of query statements we will execute in a batch. These are # ordered: [child1, child1, parent1, child2, child2, child2, parent2, ...] # Parent + child ""trees"" are kept together to avoid FK constraint # violations. statements_in_batch = [] # The list of records in the batch. This is used for collecting deleted # instance UUIDs in the case of the 'instances' table. records_in_batch = [] # (melwitt): We will gather rows related by foreign key relationship for # each deleted row, one at a time. We do it this way to keep track of and # limit the total number of rows that will be archived in a single database # transaction. In a large scale database with potentially hundreds of # thousands of deleted rows, if we don't limit the size of the transaction # based on max_rows, we can get into a situation where we get stuck not # able to make much progress. The value of max_rows has to be 1) small # enough to not exceed the database's max packet size limit or timeout with # a deadlock but 2) large enough to make progress in an environment with a # constant high volume of create and delete traffic. By archiving each # parent + child rows tree one at a time, we can ensure meaningful progress # can be made while allowing the caller to predictably control the size of # the database transaction with max_rows. for record in records: metadata, conn, table, column, [record]) statements_in_batch.extend(fk_inserts + fk_deletes) # statement to add parent row to shadow table insert = shadow_table.insert(inline=True).\ from_select(columns, sql.select([table], column.in_([record]))) statements_in_batch.append(insert) # statement to remove parent row from main table delete = table.delete().where(column.in_([record])) statements_in_batch.append(delete) records_in_batch.append(record) # Check whether were have a full batch >= max_rows. Rows are counted as # the number of rows that will be moved in the database transaction. # So each insert+delete pair represents one row that will be moved. # 1 parent + its fks num_rows_in_batch += 1 + len(fk_inserts) if max_rows is not None and num_rows_in_batch >= max_rows: break # NOTE(tssurya): In order to facilitate the deletion of records from # instance_mappings, request_specs and instance_group_member tables in the # nova_api DB, the rows of deleted instances from the instances table are # stored prior to their deletion. Basically the uuids of the archived # instances are queried and returned. if tablename == ""instances"": query_select = sql.select( [table.c.uuid], table.c.id.in_(records_in_batch)) rows = conn.execute(query_select).fetchall() # deleted_instance_uuids = ['uuid1', 'uuid2', ...] deleted_instance_uuids = [r[0] for r in rows] try: # Group the insert and delete in a transaction. with conn.begin(): for statement in statements_in_batch: result = conn.execute(statement) result_tablename = statement.table.name # Add to archived row counts if not a shadow table. if not result_tablename.startswith(_SHADOW_TABLE_PREFIX): if result_tablename == tablename: # Number of tablename (parent) rows archived. rows_archived += result.rowcount else: # Number(s) of child rows archived. extras[result_tablename] += result.rowcount except db_exc.DBReferenceError as ex: # A foreign key constraint keeps us from deleting some of these rows # until we clean up a dependent table. Just skip this table for now; # we'll come back to it later. LOG.warning(""IntegrityError detected when archiving table "" ""%(tablename)s: %(error)s"", {'tablename': tablename, 'error': str(ex)})"," # {tablename: extra_rows_archived} if records: insert = shadow_table.insert(inline=True).\ from_select(columns, sql.select([table], column.in_(records))) delete = table.delete().where(column.in_(records)) metadata, conn, table, column, records) # NOTE(tssurya): In order to facilitate the deletion of records from # instance_mappings, request_specs and instance_group_member tables in # the nova_api DB, the rows of deleted instances from the instances # table are stored prior to their deletion. Basically the uuids of the # archived instances are queried and returned. if tablename == ""instances"": query_select = sql.select([table.c.uuid], table.c.id.in_(records)) rows = conn.execute(query_select).fetchall() deleted_instance_uuids = [r[0] for r in rows] try: # Group the insert and delete in a transaction. with conn.begin(): for fk_insert in fk_inserts: conn.execute(fk_insert) for fk_delete in fk_deletes: result_fk_delete = conn.execute(fk_delete) extras[fk_delete.table.name] += result_fk_delete.rowcount conn.execute(insert) result_delete = conn.execute(delete) rows_archived += result_delete.rowcount except db_exc.DBReferenceError as ex: # A foreign key constraint keeps us from deleting some of # these rows until we clean up a dependent table. Just # skip this table for now; we'll come back to it later. LOG.warning(""IntegrityError detected when archiving table "" ""%(tablename)s: %(error)s"", {'tablename': tablename, 'error': str(ex)})",201,43
openstack%2Fnova~stable%2Fussuri~If39f6afb6359c67aa38cf315ec90ffa386d5c142,openstack/nova,stable/ussuri,If39f6afb6359c67aa38cf315ec90ffa386d5c142,testing: Fix and robustify archive_deleted_rows test,NEW,2023-07-07 21:50:34.000000000,2023-07-07 22:17:18.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 21:50:34.000000000', 'files': ['nova/tests/functional/test_nova_manage.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/8164fe9989fbce74473a88cbe2ee39b5a44f5e3c', 'message': 'testing: Fix and robustify archive_deleted_rows test\n\nThe regexes in test_archive_deleted_rows for multiple cells were\nincorrect in that they were not isolating the search pattern and rather\ncould match with other rows in the result table as well, resulting in a\nfalse positive.\n\nThis fixes the regexes and also adds one more server to the test\nscenario in order to make sure archive_deleted_rows iterates at least\nonce to expose bugs that may be present in its internal iteration.\n\nThis patch is in preparation for a future patch that will change the\nlogic in archive_deleted_rows. Making this test more robust will more\nthoroughly test for regression.\n\nChange-Id: If39f6afb6359c67aa38cf315ec90ffa386d5c142\n(cherry picked from commit f6620d48c86fb1c5034c09da6411ea46b4d9c2ed)\n(cherry picked from commit 8823da84e9d07309b860c3ce3ad4c9ebd3652f86)\n(cherry picked from commit ecfa7c405710451247018f189416ada7bba6fd36)\n(cherry picked from commit 956208d879ef2d49ac5a6f6c2067ff299114d193)\n(cherry picked from commit 600dba5571e830c1e274e391fe86d498efcd7e53)\n(cherry picked from commit 7287070758e7f99c35eb4827b5552df784084a09)\n(cherry picked from commit afdf01aa1f95ab430f62661680b7b492f5f78ff8)\n'}]",0,888001,8164fe9989fbce74473a88cbe2ee39b5a44f5e3c,2,1,1,4690,,,0,"testing: Fix and robustify archive_deleted_rows test

The regexes in test_archive_deleted_rows for multiple cells were
incorrect in that they were not isolating the search pattern and rather
could match with other rows in the result table as well, resulting in a
false positive.

This fixes the regexes and also adds one more server to the test
scenario in order to make sure archive_deleted_rows iterates at least
once to expose bugs that may be present in its internal iteration.

This patch is in preparation for a future patch that will change the
logic in archive_deleted_rows. Making this test more robust will more
thoroughly test for regression.

Change-Id: If39f6afb6359c67aa38cf315ec90ffa386d5c142
(cherry picked from commit f6620d48c86fb1c5034c09da6411ea46b4d9c2ed)
(cherry picked from commit 8823da84e9d07309b860c3ce3ad4c9ebd3652f86)
(cherry picked from commit ecfa7c405710451247018f189416ada7bba6fd36)
(cherry picked from commit 956208d879ef2d49ac5a6f6c2067ff299114d193)
(cherry picked from commit 600dba5571e830c1e274e391fe86d498efcd7e53)
(cherry picked from commit 7287070758e7f99c35eb4827b5552df784084a09)
(cherry picked from commit afdf01aa1f95ab430f62661680b7b492f5f78ff8)
",git fetch https://review.opendev.org/openstack/nova refs/changes/01/888001/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/functional/test_nova_manage.py'],1,8164fe9989fbce74473a88cbe2ee39b5a44f5e3c,," server_ids_by_cell = collections.defaultdict(list) # Create two servers per cell to make sure archive for table iterates # at least once. for i in range(2): # Boot a server to cell1 server = self._build_server(az='nova:host1') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids_by_cell['cell1'].append(created_server['id']) # Boot a server to cell2 server = self._build_server(az='nova:host2') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids_by_cell['cell2'].append(created_server['id']) # Boot a server to cell0 (cause ERROR state prior to schedule) server = self._build_server() # Flavor m1.xlarge cannot be fulfilled server['flavorRef'] = 'http://fake.server/5' created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ERROR') server_ids_by_cell['cell0'].append(created_server['id']) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: self.api.delete_server(server_id) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) # 6 instances should have been archived (cell0, cell1, cell2) r""\| cell0\.instances\s+\| 2"") r""\| cell1\.instances\s+\| 2"") r""\| cell2\.instances\s+\| 2"") r""\| API_DB\.instance_mappings\s+\| 6"") r""\| API_DB\.request_specs\s+\| 6"") for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: self.assertRaises(exception.InstanceNotFound, objects.Instance.get_by_uuid, cctxt, server_id)"," # Boot a server to cell1 server_ids = {} server = self._build_server(az='nova:host1') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids['cell1'] = created_server['id'] # Boot a server to cell2 server = self._build_server(az='nova:host2') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids['cell2'] = created_server['id'] # Boot a server to cell0 (cause ERROR state prior to schedule) server = self._build_server() # Flavor m1.xlarge cannot be fulfilled server['flavorRef'] = 'http://fake.server/5' created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ERROR') server_ids['cell0'] = created_server['id'] for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) for cell_name in server_ids.keys(): self.api.delete_server(server_ids[cell_name]) for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) # Three instances should have been archived (cell0, cell1, cell2) r""| cell0\.instances.*\| 1.*"") r""| cell1\.instances.*\| 1.*"") r""| cell2\.instances.*\| 1.*"") r""| API_DB\.instance_mappings.*\| 3.*"") r""| API_DB\.request_specs.*\| 3.*"") for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: self.assertRaises(exception.InstanceNotFound, objects.Instance.get_by_uuid, cctxt, server_id)",54,40
openstack%2Fironic~master~I53bfd0dcc6289e51316795fbe352c70d608e4f31,openstack/ironic,master,I53bfd0dcc6289e51316795fbe352c70d608e4f31,Cleanup if images.fetch fails,NEW,2023-07-07 18:57:57.000000000,2023-07-07 22:09:32.000000000,,"[{'_account_id': 7130}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-07 18:57:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/afaf149bf929873292eedeb77d1b29dc7f3f1358', 'message': 'Cleanup if images.fetch fails\n\nCleanup if images.fetch fails as in some cases, we might get a stale\n.part file that is incomplete and corrupted (ie: full disk due to image\nconversion) and this prevents future deployment from working.\n\nChange-Id: I53bfd0dcc6289e51316795fbe352c70d608e4f31\n'}, {'number': 2, 'created': '2023-07-07 19:58:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/aee5bb5b40d1bb0abf88ebe82ad4710a96f784a9', 'message': 'Cleanup if images.fetch fails\n\nCleanup if images.fetch fails as in some cases, we might get a stale\n.part file that is incomplete and corrupted (ie: full disk due to image\nconversion) and this prevents future deployment from working.\n\nChange-Id: I53bfd0dcc6289e51316795fbe352c70d608e4f31\n'}, {'number': 3, 'created': '2023-07-07 20:24:50.000000000', 'files': ['ironic/drivers/modules/image_cache.py', 'ironic/tests/unit/drivers/modules/test_image_cache.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/8c93d7df8358b75b6cb948e1f1815f7604d694db', 'message': 'Cleanup if images.fetch fails\n\nCleanup if images.fetch fails as in some cases, we might get a stale\n.part file that is incomplete and corrupted (ie: full disk due to image\nconversion) and this prevents future deployment from working.\n\nCo-Authored-By: Julia Kreger <juliaashleykreger@gmail.com>\nChange-Id: I53bfd0dcc6289e51316795fbe352c70d608e4f31\n'}]",2,887990,8c93d7df8358b75b6cb948e1f1815f7604d694db,11,2,3,11655,,,0,"Cleanup if images.fetch fails

Cleanup if images.fetch fails as in some cases, we might get a stale
.part file that is incomplete and corrupted (ie: full disk due to image
conversion) and this prevents future deployment from working.

Co-Authored-By: Julia Kreger <juliaashleykreger@gmail.com>
Change-Id: I53bfd0dcc6289e51316795fbe352c70d608e4f31
",git fetch https://review.opendev.org/openstack/ironic refs/changes/90/887990/2 && git format-patch -1 --stdout FETCH_HEAD,"['ironic/drivers/modules/image_cache.py', 'ironic/tests/unit/common/test_images.py']",2,afaf149bf929873292eedeb77d1b29dc7f3f1358,," @mock.patch.object(os.path, 'exists', autospec=True) @mock.patch.object(image_service, 'get_image_service', autospec=True) @mock.patch.object(images, 'image_to_raw', autospec=True) @mock.patch.object(__builtin__, 'open', autospec=True) def test_fetch_image_service_force_raw_stale(self, open_mock, image_to_raw_mock, image_service_mock, file_exists_mock): mock_file_handle = mock.MagicMock(spec=file) mock_file_handle.__enter__.return_value = 'file' open_mock.return_value = mock_file_handle file_exists_mock.return_value = True images.fetch('context', 'image_href', 'path', force_raw=True) open_mock.assert_called_once_with('path', 'wb') image_service_mock.return_value.download.assert_called_once_with( 'image_href', 'file') image_to_raw_mock.assert_called_once_with( 'image_href', 'path', 'path.part') ",,23,0
openstack%2Fnova~stable%2Fvictoria~If39f6afb6359c67aa38cf315ec90ffa386d5c142,openstack/nova,stable/victoria,If39f6afb6359c67aa38cf315ec90ffa386d5c142,testing: Fix and robustify archive_deleted_rows test,NEW,2023-07-07 21:16:53.000000000,2023-07-07 21:43:25.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 21:16:53.000000000', 'files': ['nova/tests/functional/test_nova_manage.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/afdf01aa1f95ab430f62661680b7b492f5f78ff8', 'message': 'testing: Fix and robustify archive_deleted_rows test\n\nThe regexes in test_archive_deleted_rows for multiple cells were\nincorrect in that they were not isolating the search pattern and rather\ncould match with other rows in the result table as well, resulting in a\nfalse positive.\n\nThis fixes the regexes and also adds one more server to the test\nscenario in order to make sure archive_deleted_rows iterates at least\nonce to expose bugs that may be present in its internal iteration.\n\nThis patch is in preparation for a future patch that will change the\nlogic in archive_deleted_rows. Making this test more robust will more\nthoroughly test for regression.\n\nChange-Id: If39f6afb6359c67aa38cf315ec90ffa386d5c142\n(cherry picked from commit f6620d48c86fb1c5034c09da6411ea46b4d9c2ed)\n(cherry picked from commit 8823da84e9d07309b860c3ce3ad4c9ebd3652f86)\n(cherry picked from commit ecfa7c405710451247018f189416ada7bba6fd36)\n(cherry picked from commit 956208d879ef2d49ac5a6f6c2067ff299114d193)\n(cherry picked from commit 600dba5571e830c1e274e391fe86d498efcd7e53)\n(cherry picked from commit 7287070758e7f99c35eb4827b5552df784084a09)\n'}]",0,887997,afdf01aa1f95ab430f62661680b7b492f5f78ff8,2,1,1,4690,,,0,"testing: Fix and robustify archive_deleted_rows test

The regexes in test_archive_deleted_rows for multiple cells were
incorrect in that they were not isolating the search pattern and rather
could match with other rows in the result table as well, resulting in a
false positive.

This fixes the regexes and also adds one more server to the test
scenario in order to make sure archive_deleted_rows iterates at least
once to expose bugs that may be present in its internal iteration.

This patch is in preparation for a future patch that will change the
logic in archive_deleted_rows. Making this test more robust will more
thoroughly test for regression.

Change-Id: If39f6afb6359c67aa38cf315ec90ffa386d5c142
(cherry picked from commit f6620d48c86fb1c5034c09da6411ea46b4d9c2ed)
(cherry picked from commit 8823da84e9d07309b860c3ce3ad4c9ebd3652f86)
(cherry picked from commit ecfa7c405710451247018f189416ada7bba6fd36)
(cherry picked from commit 956208d879ef2d49ac5a6f6c2067ff299114d193)
(cherry picked from commit 600dba5571e830c1e274e391fe86d498efcd7e53)
(cherry picked from commit 7287070758e7f99c35eb4827b5552df784084a09)
",git fetch https://review.opendev.org/openstack/nova refs/changes/97/887997/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/functional/test_nova_manage.py'],1,afdf01aa1f95ab430f62661680b7b492f5f78ff8,," server_ids_by_cell = collections.defaultdict(list) # Create two servers per cell to make sure archive for table iterates # at least once. for i in range(2): # Boot a server to cell1 server = self._build_server(az='nova:host1') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids_by_cell['cell1'].append(created_server['id']) # Boot a server to cell2 server = self._build_server(az='nova:host2') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids_by_cell['cell2'].append(created_server['id']) # Boot a server to cell0 (cause ERROR state prior to schedule) server = self._build_server() # Flavor m1.xlarge cannot be fulfilled server['flavorRef'] = 'http://fake.server/5' created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ERROR') server_ids_by_cell['cell0'].append(created_server['id']) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: self.api.delete_server(server_id) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) # 6 instances should have been archived (cell0, cell1, cell2) r""\| cell0\.instances\s+\| 2"") r""\| cell1\.instances\s+\| 2"") r""\| cell2\.instances\s+\| 2"") r""\| API_DB\.instance_mappings\s+\| 6"") r""\| API_DB\.request_specs\s+\| 6"") for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: self.assertRaises(exception.InstanceNotFound, objects.Instance.get_by_uuid, cctxt, server_id)"," # Boot a server to cell1 server_ids = {} server = self._build_server(az='nova:host1') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids['cell1'] = created_server['id'] # Boot a server to cell2 server = self._build_server(az='nova:host2') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids['cell2'] = created_server['id'] # Boot a server to cell0 (cause ERROR state prior to schedule) server = self._build_server() # Flavor m1.xlarge cannot be fulfilled server['flavorRef'] = 'http://fake.server/5' created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ERROR') server_ids['cell0'] = created_server['id'] for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) for cell_name in server_ids.keys(): self.api.delete_server(server_ids[cell_name]) for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) # Three instances should have been archived (cell0, cell1, cell2) r""| cell0\.instances.*\| 1.*"") r""| cell1\.instances.*\| 1.*"") r""| cell2\.instances.*\| 1.*"") r""| API_DB\.instance_mappings.*\| 3.*"") r""| API_DB\.request_specs.*\| 3.*"") for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: self.assertRaises(exception.InstanceNotFound, objects.Instance.get_by_uuid, cctxt, server_id)",54,40
openstack%2Fneutron-lib~master~Ie74934754598292b125d2be7edb4bbcbb898a230,openstack/neutron-lib,master,Ie74934754598292b125d2be7edb4bbcbb898a230,Removed ``HasProjectPrimaryKeyIndex`` class,MERGED,2023-06-21 10:06:36.000000000,2023-07-07 20:49:17.000000000,2023-07-07 20:48:22.000000000,"[{'_account_id': 1131}, {'_account_id': 5948}, {'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-21 10:06:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-lib/commit/153f15c51627f37ace71cc62234d99063706ae85', 'message': 'Removed ``HasProjectPrimaryKeyIndex`` class\n\nA column that is primary key creates an index by default. There is no\nneed to create another one by passing index=True.\n\nRelated-Bug: #2024044\nChange-Id: Ie74934754598292b125d2be7edb4bbcbb898a230\n'}, {'number': 2, 'created': '2023-07-03 07:45:13.000000000', 'files': ['neutron_lib/db/model_base.py'], 'web_link': 'https://opendev.org/openstack/neutron-lib/commit/673e48a1890c721654ce0de9cd9e0897c791bd6a', 'message': 'Removed ``HasProjectPrimaryKeyIndex`` class\n\nA column that is primary key creates an index by default. There is no\nneed to create another one by passing index=True.\n\nRelated-Bug: #2024044\nChange-Id: Ie74934754598292b125d2be7edb4bbcbb898a230\n'}]",3,886589,673e48a1890c721654ce0de9cd9e0897c791bd6a,16,6,2,16688,,,0,"Removed ``HasProjectPrimaryKeyIndex`` class

A column that is primary key creates an index by default. There is no
need to create another one by passing index=True.

Related-Bug: #2024044
Change-Id: Ie74934754598292b125d2be7edb4bbcbb898a230
",git fetch https://review.opendev.org/openstack/neutron-lib refs/changes/89/886589/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron_lib/db/model_base.py'],1,153f15c51627f37ace71cc62234d99063706ae85,bug/2024044,,"class HasProjectPrimaryKeyIndex(HasProject): """"""Project mixin, add to subclasses that have a user."""""" # NOTE: project_id is just a free form string project_id = sa.Column(sa.String(db_const.PROJECT_ID_FIELD_SIZE), nullable=False, primary_key=True, index=True) ",0,8
openstack%2Fnova~stable%2Fyoga~I2209bf1b3320901cf603ec39163cf923b25b0359,openstack/nova,stable/yoga,I2209bf1b3320901cf603ec39163cf923b25b0359,"database: Archive parent and child rows ""trees"" one at a time",NEW,2023-07-07 18:16:50.000000000,2023-07-07 20:25:59.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 18:16:50.000000000', 'files': ['nova/tests/functional/db/test_archive.py', 'nova/db/main/api.py', 'releasenotes/notes/db-archive-performance-degradation-3fdabc43398149b1.yaml', 'nova/tests/unit/cmd/test_manage.py', 'nova/cmd/manage.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/b555279add583e6b0cee1bddb8d5642c68ad84a6', 'message': 'database: Archive parent and child rows ""trees"" one at a time\n\nPreviously, we archived deleted rows in batches of max_rows parents +\ntheir child rows in a single database transaction. Doing it that way\nlimited how high a value of max_rows could be specified by the caller\nbecause of the size of the database transaction it could generate.\n\nFor example, in a large scale deployment with hundreds of thousands of\ndeleted rows and constant server creation and deletion activity, a\nvalue of max_rows=1000 might exceed the database\'s configured maximum\npacket size or timeout due to a database deadlock, forcing the operator\nto use a much lower max_rows value like 100 or 50.\n\nAnd when the operator has e.g. 500,000 deleted instances rows (and\nmillions of deleted rows total) they are trying to archive, being\nforced to use a max_rows value several orders of magnitude lower than\nthe number of rows they need to archive was a poor user experience.\n\nThis changes the logic to archive one parent row and its foreign key\nrelated child rows one at a time in a single database transaction\nwhile limiting the total number of rows per table as soon as it reaches\n>= max_rows. Doing this will allow operators to choose more predictable\nvalues for max_rows and get more progress per invocation of\narchive_deleted_rows.\n\nConflicts:\n    nova/db/main/api.py\n\nNOTE(melwitt): The conflict is because change\nIc43c21038ee682f9733fbde42c6d24f8088815fc (db: Don\'t rely on autocommit\nbehavior) is not in Yoga.\n\nCloses-Bug: #2024258\n\nChange-Id: I2209bf1b3320901cf603ec39163cf923b25b0359\n(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)\n(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)\n(cherry picked from commit 6972efdaa94d05952eb5b90e456b566c4c73360c)\n'}]",0,887983,b555279add583e6b0cee1bddb8d5642c68ad84a6,3,1,1,4690,,,0,"database: Archive parent and child rows ""trees"" one at a time

Previously, we archived deleted rows in batches of max_rows parents +
their child rows in a single database transaction. Doing it that way
limited how high a value of max_rows could be specified by the caller
because of the size of the database transaction it could generate.

For example, in a large scale deployment with hundreds of thousands of
deleted rows and constant server creation and deletion activity, a
value of max_rows=1000 might exceed the database's configured maximum
packet size or timeout due to a database deadlock, forcing the operator
to use a much lower max_rows value like 100 or 50.

And when the operator has e.g. 500,000 deleted instances rows (and
millions of deleted rows total) they are trying to archive, being
forced to use a max_rows value several orders of magnitude lower than
the number of rows they need to archive was a poor user experience.

This changes the logic to archive one parent row and its foreign key
related child rows one at a time in a single database transaction
while limiting the total number of rows per table as soon as it reaches
>= max_rows. Doing this will allow operators to choose more predictable
values for max_rows and get more progress per invocation of
archive_deleted_rows.

Conflicts:
    nova/db/main/api.py

NOTE(melwitt): The conflict is because change
Ic43c21038ee682f9733fbde42c6d24f8088815fc (db: Don't rely on autocommit
behavior) is not in Yoga.

Closes-Bug: #2024258

Change-Id: I2209bf1b3320901cf603ec39163cf923b25b0359
(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)
(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)
(cherry picked from commit 6972efdaa94d05952eb5b90e456b566c4c73360c)
",git fetch https://review.opendev.org/openstack/nova refs/changes/83/887983/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/functional/db/test_archive.py', 'nova/db/main/api.py', 'releasenotes/notes/db-archive-performance-degradation-3fdabc43398149b1.yaml', 'nova/tests/unit/cmd/test_manage.py', 'nova/cmd/manage.py']",5,b555279add583e6b0cee1bddb8d5642c68ad84a6,," help='Maximum number of deleted rows to archive per table. Defaults ' 'to 1000. Note that this number is a soft limit and does not ' 'include the corresponding rows, if any, that are removed ' 'from the API database for deleted instances.') :param max_rows: Maximum number of deleted rows to archive per table. Note that this number is a soft limit and does not include the corresponding rows, if any, that are removed from the API database for deleted instances. # table_to_rows = {table_name: number_of_rows_archived} # deleted_instance_uuids = ['uuid1', 'uuid2', ...] table_to_rows, deleted_instance_uuids, total_rows_archived = \ for table_name, rows_archived in table_to_rows.items(): # deleted_instance_uuids does not necessarily mean that any # instances rows were archived because it is obtained by a query # separate from the archive queries. For example, if a # DBReferenceError was raised while processing the instances table, # we would have skipped the table and had 0 rows archived even # though deleted instances rows were found. instances_archived = table_to_rows.get('instances', 0) if deleted_instance_uuids and instances_archived: # archive. We check the values() in case we get something like # table_to_rows = {'instances': 0} back somehow. if not until_complete or not any(table_to_rows.values()):"," help='Maximum number of deleted rows to archive. Defaults to 1000. ' 'Note that this number does not include the corresponding ' 'rows, if any, that are removed from the API database for ' 'deleted instances.') :param max_rows: Maximum number of deleted rows to archive run, deleted_instance_uuids, total_rows_archived = \ for table_name, rows_archived in run.items(): if deleted_instance_uuids: # archive. if not until_complete or not run:",201,46
openstack%2Fnova~stable%2Fwallaby~If39f6afb6359c67aa38cf315ec90ffa386d5c142,openstack/nova,stable/wallaby,If39f6afb6359c67aa38cf315ec90ffa386d5c142,testing: Fix and robustify archive_deleted_rows test,NEW,2023-07-07 18:42:00.000000000,2023-07-07 20:17:10.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 18:42:00.000000000', 'files': ['nova/tests/functional/test_nova_manage.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/7287070758e7f99c35eb4827b5552df784084a09', 'message': 'testing: Fix and robustify archive_deleted_rows test\n\nThe regexes in test_archive_deleted_rows for multiple cells were\nincorrect in that they were not isolating the search pattern and rather\ncould match with other rows in the result table as well, resulting in a\nfalse positive.\n\nThis fixes the regexes and also adds one more server to the test\nscenario in order to make sure archive_deleted_rows iterates at least\nonce to expose bugs that may be present in its internal iteration.\n\nThis patch is in preparation for a future patch that will change the\nlogic in archive_deleted_rows. Making this test more robust will more\nthoroughly test for regression.\n\nConflicts:\n    nova/tests/functional/test_nova_manage.py\n\nNOTE(melwitt): The conflict is because change\nIbed67854a693c930effd4dba7aca6cd03b65bd92 (Add --task-log option to\nnova-manage db archive_deleted_rows) is not in Wallaby.\n\nChange-Id: If39f6afb6359c67aa38cf315ec90ffa386d5c142\n(cherry picked from commit f6620d48c86fb1c5034c09da6411ea46b4d9c2ed)\n(cherry picked from commit 8823da84e9d07309b860c3ce3ad4c9ebd3652f86)\n(cherry picked from commit ecfa7c405710451247018f189416ada7bba6fd36)\n(cherry picked from commit 956208d879ef2d49ac5a6f6c2067ff299114d193)\n(cherry picked from commit 600dba5571e830c1e274e391fe86d498efcd7e53)\n'}]",0,887987,7287070758e7f99c35eb4827b5552df784084a09,3,1,1,4690,,,0,"testing: Fix and robustify archive_deleted_rows test

The regexes in test_archive_deleted_rows for multiple cells were
incorrect in that they were not isolating the search pattern and rather
could match with other rows in the result table as well, resulting in a
false positive.

This fixes the regexes and also adds one more server to the test
scenario in order to make sure archive_deleted_rows iterates at least
once to expose bugs that may be present in its internal iteration.

This patch is in preparation for a future patch that will change the
logic in archive_deleted_rows. Making this test more robust will more
thoroughly test for regression.

Conflicts:
    nova/tests/functional/test_nova_manage.py

NOTE(melwitt): The conflict is because change
Ibed67854a693c930effd4dba7aca6cd03b65bd92 (Add --task-log option to
nova-manage db archive_deleted_rows) is not in Wallaby.

Change-Id: If39f6afb6359c67aa38cf315ec90ffa386d5c142
(cherry picked from commit f6620d48c86fb1c5034c09da6411ea46b4d9c2ed)
(cherry picked from commit 8823da84e9d07309b860c3ce3ad4c9ebd3652f86)
(cherry picked from commit ecfa7c405710451247018f189416ada7bba6fd36)
(cherry picked from commit 956208d879ef2d49ac5a6f6c2067ff299114d193)
(cherry picked from commit 600dba5571e830c1e274e391fe86d498efcd7e53)
",git fetch https://review.opendev.org/openstack/nova refs/changes/87/887987/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/functional/test_nova_manage.py'],1,7287070758e7f99c35eb4827b5552df784084a09,," server_ids_by_cell = collections.defaultdict(list) # Create two servers per cell to make sure archive for table iterates # at least once. for i in range(2): # Boot a server to cell1 server = self._build_server(az='nova:host1') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids_by_cell['cell1'].append(created_server['id']) # Boot a server to cell2 server = self._build_server(az='nova:host2') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids_by_cell['cell2'].append(created_server['id']) # Boot a server to cell0 (cause ERROR state prior to schedule) server = self._build_server() # Flavor m1.xlarge cannot be fulfilled server['flavorRef'] = 'http://fake.server/5' created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ERROR') server_ids_by_cell['cell0'].append(created_server['id']) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: self.api.delete_server(server_id) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) # 6 instances should have been archived (cell0, cell1, cell2) r""\| cell0\.instances\s+\| 2"") r""\| cell1\.instances\s+\| 2"") r""\| cell2\.instances\s+\| 2"") r""\| API_DB\.instance_mappings\s+\| 6"") r""\| API_DB\.request_specs\s+\| 6"") for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: self.assertRaises(exception.InstanceNotFound, objects.Instance.get_by_uuid, cctxt, server_id)"," # Boot a server to cell1 server_ids = {} server = self._build_server(az='nova:host1') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids['cell1'] = created_server['id'] # Boot a server to cell2 server = self._build_server(az='nova:host2') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids['cell2'] = created_server['id'] # Boot a server to cell0 (cause ERROR state prior to schedule) server = self._build_server() # Flavor m1.xlarge cannot be fulfilled server['flavorRef'] = 'http://fake.server/5' created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ERROR') server_ids['cell0'] = created_server['id'] for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) for cell_name in server_ids.keys(): self.api.delete_server(server_ids[cell_name]) for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) # Three instances should have been archived (cell0, cell1, cell2) r""| cell0\.instances.*\| 1.*"") r""| cell1\.instances.*\| 1.*"") r""| cell2\.instances.*\| 1.*"") r""| API_DB\.instance_mappings.*\| 3.*"") r""| API_DB\.request_specs.*\| 3.*"") for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: self.assertRaises(exception.InstanceNotFound, objects.Instance.get_by_uuid, cctxt, server_id)",54,40
openstack%2Fnova~stable%2Fxena~I2209bf1b3320901cf603ec39163cf923b25b0359,openstack/nova,stable/xena,I2209bf1b3320901cf603ec39163cf923b25b0359,"database: Archive parent and child rows ""trees"" one at a time",NEW,2023-07-07 18:34:50.000000000,2023-07-07 20:12:10.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 18:34:50.000000000', 'files': ['nova/tests/functional/db/test_archive.py', 'nova/db/main/api.py', 'releasenotes/notes/db-archive-performance-degradation-3fdabc43398149b1.yaml', 'nova/tests/unit/cmd/test_manage.py', 'nova/cmd/manage.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/4dc6ec9dccba2695536cb3cbd5534e64003904c9', 'message': 'database: Archive parent and child rows ""trees"" one at a time\n\nPreviously, we archived deleted rows in batches of max_rows parents +\ntheir child rows in a single database transaction. Doing it that way\nlimited how high a value of max_rows could be specified by the caller\nbecause of the size of the database transaction it could generate.\n\nFor example, in a large scale deployment with hundreds of thousands of\ndeleted rows and constant server creation and deletion activity, a\nvalue of max_rows=1000 might exceed the database\'s configured maximum\npacket size or timeout due to a database deadlock, forcing the operator\nto use a much lower max_rows value like 100 or 50.\n\nAnd when the operator has e.g. 500,000 deleted instances rows (and\nmillions of deleted rows total) they are trying to archive, being\nforced to use a max_rows value several orders of magnitude lower than\nthe number of rows they need to archive was a poor user experience.\n\nThis changes the logic to archive one parent row and its foreign key\nrelated child rows one at a time in a single database transaction\nwhile limiting the total number of rows per table as soon as it reaches\n>= max_rows. Doing this will allow operators to choose more predictable\nvalues for max_rows and get more progress per invocation of\narchive_deleted_rows.\n\nConflicts:\n    nova/db/main/api.py\n\nNOTE(melwitt): The conflicts are because the following changes are not\nin Xena:\n\n  * I36e43e30e07f4904c7b49925cefe804af45cff6c (db: Replace use of\n    legacy select() calling style)\n\n  * Ib5fcb841294b4e20fe085e8603d4132e97be7db9 (db: Replace\n    \'insert.inline\' parameter with \'Insert.inline()\' method)\n\nCloses-Bug: #2024258\n\nChange-Id: I2209bf1b3320901cf603ec39163cf923b25b0359\n(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)\n(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)\n(cherry picked from commit 6972efdaa94d05952eb5b90e456b566c4c73360c)\n(cherry picked from commit b555279add583e6b0cee1bddb8d5642c68ad84a6)\n'}]",0,887985,4dc6ec9dccba2695536cb3cbd5534e64003904c9,3,1,1,4690,,,0,"database: Archive parent and child rows ""trees"" one at a time

Previously, we archived deleted rows in batches of max_rows parents +
their child rows in a single database transaction. Doing it that way
limited how high a value of max_rows could be specified by the caller
because of the size of the database transaction it could generate.

For example, in a large scale deployment with hundreds of thousands of
deleted rows and constant server creation and deletion activity, a
value of max_rows=1000 might exceed the database's configured maximum
packet size or timeout due to a database deadlock, forcing the operator
to use a much lower max_rows value like 100 or 50.

And when the operator has e.g. 500,000 deleted instances rows (and
millions of deleted rows total) they are trying to archive, being
forced to use a max_rows value several orders of magnitude lower than
the number of rows they need to archive was a poor user experience.

This changes the logic to archive one parent row and its foreign key
related child rows one at a time in a single database transaction
while limiting the total number of rows per table as soon as it reaches
>= max_rows. Doing this will allow operators to choose more predictable
values for max_rows and get more progress per invocation of
archive_deleted_rows.

Conflicts:
    nova/db/main/api.py

NOTE(melwitt): The conflicts are because the following changes are not
in Xena:

  * I36e43e30e07f4904c7b49925cefe804af45cff6c (db: Replace use of
    legacy select() calling style)

  * Ib5fcb841294b4e20fe085e8603d4132e97be7db9 (db: Replace
    'insert.inline' parameter with 'Insert.inline()' method)

Closes-Bug: #2024258

Change-Id: I2209bf1b3320901cf603ec39163cf923b25b0359
(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)
(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)
(cherry picked from commit 6972efdaa94d05952eb5b90e456b566c4c73360c)
(cherry picked from commit b555279add583e6b0cee1bddb8d5642c68ad84a6)
",git fetch https://review.opendev.org/openstack/nova refs/changes/85/887985/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/functional/db/test_archive.py', 'nova/db/main/api.py', 'releasenotes/notes/db-archive-performance-degradation-3fdabc43398149b1.yaml', 'nova/tests/unit/cmd/test_manage.py', 'nova/cmd/manage.py']",5,4dc6ec9dccba2695536cb3cbd5534e64003904c9,," help='Maximum number of deleted rows to archive per table. Defaults ' 'to 1000. Note that this number is a soft limit and does not ' 'include the corresponding rows, if any, that are removed ' 'from the API database for deleted instances.') :param max_rows: Maximum number of deleted rows to archive per table. Note that this number is a soft limit and does not include the corresponding rows, if any, that are removed from the API database for deleted instances. # table_to_rows = {table_name: number_of_rows_archived} # deleted_instance_uuids = ['uuid1', 'uuid2', ...] table_to_rows, deleted_instance_uuids, total_rows_archived = \ for table_name, rows_archived in table_to_rows.items(): # deleted_instance_uuids does not necessarily mean that any # instances rows were archived because it is obtained by a query # separate from the archive queries. For example, if a # DBReferenceError was raised while processing the instances table, # we would have skipped the table and had 0 rows archived even # though deleted instances rows were found. instances_archived = table_to_rows.get('instances', 0) if deleted_instance_uuids and instances_archived: # archive. We check the values() in case we get something like # table_to_rows = {'instances': 0} back somehow. if not until_complete or not any(table_to_rows.values()):"," help='Maximum number of deleted rows to archive. Defaults to 1000. ' 'Note that this number does not include the corresponding ' 'rows, if any, that are removed from the API database for ' 'deleted instances.') :param max_rows: Maximum number of deleted rows to archive run, deleted_instance_uuids, total_rows_archived = \ for table_name, rows_archived in run.items(): if deleted_instance_uuids: # archive. if not until_complete or not run:",201,43
openstack%2Fswift~master~I8ed9b4ca5af90e9a64ec996725e59cba34f796a5,openstack/swift,master,I8ed9b4ca5af90e9a64ec996725e59cba34f796a5,"In the case where we can't stat the device, an error search in the Kernel logs must also be carried out, and the device unmounted if necessary",ABANDONED,2023-06-21 12:37:37.000000000,2023-07-07 20:06:08.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-06-21 12:37:37.000000000', 'files': ['bin/swift-drive-audit'], 'web_link': 'https://opendev.org/openstack/swift/commit/761863ada93b13a9cb1b1055942b7982fb9877c8', 'message': ""In the case where we can't stat the device, an error search in the Kernel logs must also be carried out, and the device unmounted if necessary\n\nChange-Id: I8ed9b4ca5af90e9a64ec996725e59cba34f796a5\n""}]",1,886633,761863ada93b13a9cb1b1055942b7982fb9877c8,4,1,1,36116,,,0,"In the case where we can't stat the device, an error search in the Kernel logs must also be carried out, and the device unmounted if necessary

Change-Id: I8ed9b4ca5af90e9a64ec996725e59cba34f796a5
",git fetch https://review.opendev.org/openstack/swift refs/changes/33/886633/1 && git format-patch -1 --stdout FETCH_HEAD,['bin/swift-drive-audit'],1,761863ada93b13a9cb1b1055942b7982fb9877c8,TOPIC-BRANCH," # In this case, an error search in the Kernel logs must also be carried out, and the device unmounted if necessary devices.append(device)",,2,0
openstack%2Fironic~master~I41baf76d8e0f20e5760e0d75af825ab6dd446e92,openstack/ironic,master,I41baf76d8e0f20e5760e0d75af825ab6dd446e92,Make sure there's no stale path_tmp,ABANDONED,2023-07-07 19:14:50.000000000,2023-07-07 20:02:56.000000000,,[],"[{'number': 1, 'created': '2023-07-07 19:14:50.000000000', 'files': ['ironic/drivers/modules/image_cache.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/691cbedd87d52fd9656cf4072ef194a52e78085f', 'message': ""Make sure there's no stale path_tmp\n\nWe might get a stale .part file that is incomplete and corrupted (ie: full disk\nue to image conversion, server reboots, oom) and this prevents future deployment\nfrom completing successfully.\n\nChange-Id: I41baf76d8e0f20e5760e0d75af825ab6dd446e92\n""}]",0,887991,691cbedd87d52fd9656cf4072ef194a52e78085f,4,0,1,7130,,,0,"Make sure there's no stale path_tmp

We might get a stale .part file that is incomplete and corrupted (ie: full disk
ue to image conversion, server reboots, oom) and this prevents future deployment
from completing successfully.

Change-Id: I41baf76d8e0f20e5760e0d75af825ab6dd446e92
",git fetch https://review.opendev.org/openstack/ironic refs/changes/91/887991/1 && git format-patch -1 --stdout FETCH_HEAD,['ironic/drivers/modules/image_cache.py'],1,691cbedd87d52fd9656cf4072ef194a52e78085f,stale_download," if os.path.exists(path_tmp): LOG.warning(""%s exist, assuming it's stale"" % (path_tmp)) os.remove(path_tmp)",,3,0
openstack%2Fnova~stable%2Fzed~I2209bf1b3320901cf603ec39163cf923b25b0359,openstack/nova,stable/zed,I2209bf1b3320901cf603ec39163cf923b25b0359,"database: Archive parent and child rows ""trees"" one at a time",NEW,2023-07-07 18:08:56.000000000,2023-07-07 19:55:54.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 18:08:56.000000000', 'files': ['nova/tests/functional/db/test_archive.py', 'nova/db/main/api.py', 'releasenotes/notes/db-archive-performance-degradation-3fdabc43398149b1.yaml', 'nova/tests/unit/cmd/test_manage.py', 'nova/cmd/manage.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/6972efdaa94d05952eb5b90e456b566c4c73360c', 'message': 'database: Archive parent and child rows ""trees"" one at a time\n\nPreviously, we archived deleted rows in batches of max_rows parents +\ntheir child rows in a single database transaction. Doing it that way\nlimited how high a value of max_rows could be specified by the caller\nbecause of the size of the database transaction it could generate.\n\nFor example, in a large scale deployment with hundreds of thousands of\ndeleted rows and constant server creation and deletion activity, a\nvalue of max_rows=1000 might exceed the database\'s configured maximum\npacket size or timeout due to a database deadlock, forcing the operator\nto use a much lower max_rows value like 100 or 50.\n\nAnd when the operator has e.g. 500,000 deleted instances rows (and\nmillions of deleted rows total) they are trying to archive, being\nforced to use a max_rows value several orders of magnitude lower than\nthe number of rows they need to archive was a poor user experience.\n\nThis changes the logic to archive one parent row and its foreign key\nrelated child rows one at a time in a single database transaction\nwhile limiting the total number of rows per table as soon as it reaches\n>= max_rows. Doing this will allow operators to choose more predictable\nvalues for max_rows and get more progress per invocation of\narchive_deleted_rows.\n\nCloses-Bug: #2024258\n\nChange-Id: I2209bf1b3320901cf603ec39163cf923b25b0359\n(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)\n(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)\n'}]",0,887981,6972efdaa94d05952eb5b90e456b566c4c73360c,3,1,1,4690,,,0,"database: Archive parent and child rows ""trees"" one at a time

Previously, we archived deleted rows in batches of max_rows parents +
their child rows in a single database transaction. Doing it that way
limited how high a value of max_rows could be specified by the caller
because of the size of the database transaction it could generate.

For example, in a large scale deployment with hundreds of thousands of
deleted rows and constant server creation and deletion activity, a
value of max_rows=1000 might exceed the database's configured maximum
packet size or timeout due to a database deadlock, forcing the operator
to use a much lower max_rows value like 100 or 50.

And when the operator has e.g. 500,000 deleted instances rows (and
millions of deleted rows total) they are trying to archive, being
forced to use a max_rows value several orders of magnitude lower than
the number of rows they need to archive was a poor user experience.

This changes the logic to archive one parent row and its foreign key
related child rows one at a time in a single database transaction
while limiting the total number of rows per table as soon as it reaches
>= max_rows. Doing this will allow operators to choose more predictable
values for max_rows and get more progress per invocation of
archive_deleted_rows.

Closes-Bug: #2024258

Change-Id: I2209bf1b3320901cf603ec39163cf923b25b0359
(cherry picked from commit 697fa3c000696da559e52b664c04cbd8d261c037)
(cherry picked from commit 75e4c86d90ae0229069fc2eb06bfb41436be7319)
",git fetch https://review.opendev.org/openstack/nova refs/changes/81/887981/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/functional/db/test_archive.py', 'nova/db/main/api.py', 'releasenotes/notes/db-archive-performance-degradation-3fdabc43398149b1.yaml', 'nova/tests/unit/cmd/test_manage.py', 'nova/cmd/manage.py']",5,6972efdaa94d05952eb5b90e456b566c4c73360c,," help='Maximum number of deleted rows to archive per table. Defaults ' 'to 1000. Note that this number is a soft limit and does not ' 'include the corresponding rows, if any, that are removed ' 'from the API database for deleted instances.') :param max_rows: Maximum number of deleted rows to archive per table. Note that this number is a soft limit and does not include the corresponding rows, if any, that are removed from the API database for deleted instances. # table_to_rows = {table_name: number_of_rows_archived} # deleted_instance_uuids = ['uuid1', 'uuid2', ...] table_to_rows, deleted_instance_uuids, total_rows_archived = \ for table_name, rows_archived in table_to_rows.items(): # deleted_instance_uuids does not necessarily mean that any # instances rows were archived because it is obtained by a query # separate from the archive queries. For example, if a # DBReferenceError was raised while processing the instances table, # we would have skipped the table and had 0 rows archived even # though deleted instances rows were found. instances_archived = table_to_rows.get('instances', 0) if deleted_instance_uuids and instances_archived: # archive. We check the values() in case we get something like # table_to_rows = {'instances': 0} back somehow. if not until_complete or not any(table_to_rows.values()):"," help='Maximum number of deleted rows to archive. Defaults to 1000. ' 'Note that this number does not include the corresponding ' 'rows, if any, that are removed from the API database for ' 'deleted instances.') :param max_rows: Maximum number of deleted rows to archive run, deleted_instance_uuids, total_rows_archived = \ for table_name, rows_archived in run.items(): if deleted_instance_uuids: # archive. if not until_complete or not run:",202,47
openstack%2Fnova~stable%2Fxena~If39f6afb6359c67aa38cf315ec90ffa386d5c142,openstack/nova,stable/xena,If39f6afb6359c67aa38cf315ec90ffa386d5c142,testing: Fix and robustify archive_deleted_rows test,NEW,2023-07-07 18:34:50.000000000,2023-07-07 19:17:27.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 18:34:50.000000000', 'files': ['nova/tests/functional/test_nova_manage.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/600dba5571e830c1e274e391fe86d498efcd7e53', 'message': 'testing: Fix and robustify archive_deleted_rows test\n\nThe regexes in test_archive_deleted_rows for multiple cells were\nincorrect in that they were not isolating the search pattern and rather\ncould match with other rows in the result table as well, resulting in a\nfalse positive.\n\nThis fixes the regexes and also adds one more server to the test\nscenario in order to make sure archive_deleted_rows iterates at least\nonce to expose bugs that may be present in its internal iteration.\n\nThis patch is in preparation for a future patch that will change the\nlogic in archive_deleted_rows. Making this test more robust will more\nthoroughly test for regression.\n\nChange-Id: If39f6afb6359c67aa38cf315ec90ffa386d5c142\n(cherry picked from commit f6620d48c86fb1c5034c09da6411ea46b4d9c2ed)\n(cherry picked from commit 8823da84e9d07309b860c3ce3ad4c9ebd3652f86)\n(cherry picked from commit ecfa7c405710451247018f189416ada7bba6fd36)\n(cherry picked from commit 956208d879ef2d49ac5a6f6c2067ff299114d193)\n'}]",0,887984,600dba5571e830c1e274e391fe86d498efcd7e53,3,1,1,4690,,,0,"testing: Fix and robustify archive_deleted_rows test

The regexes in test_archive_deleted_rows for multiple cells were
incorrect in that they were not isolating the search pattern and rather
could match with other rows in the result table as well, resulting in a
false positive.

This fixes the regexes and also adds one more server to the test
scenario in order to make sure archive_deleted_rows iterates at least
once to expose bugs that may be present in its internal iteration.

This patch is in preparation for a future patch that will change the
logic in archive_deleted_rows. Making this test more robust will more
thoroughly test for regression.

Change-Id: If39f6afb6359c67aa38cf315ec90ffa386d5c142
(cherry picked from commit f6620d48c86fb1c5034c09da6411ea46b4d9c2ed)
(cherry picked from commit 8823da84e9d07309b860c3ce3ad4c9ebd3652f86)
(cherry picked from commit ecfa7c405710451247018f189416ada7bba6fd36)
(cherry picked from commit 956208d879ef2d49ac5a6f6c2067ff299114d193)
",git fetch https://review.opendev.org/openstack/nova refs/changes/84/887984/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/functional/test_nova_manage.py'],1,600dba5571e830c1e274e391fe86d498efcd7e53,," server_ids_by_cell = collections.defaultdict(list) # Create two servers per cell to make sure archive for table iterates # at least once. for i in range(2): # Boot a server to cell1 server = self._build_server(az='nova:host1') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids_by_cell['cell1'].append(created_server['id']) # Boot a server to cell2 server = self._build_server(az='nova:host2') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids_by_cell['cell2'].append(created_server['id']) # Boot a server to cell0 (cause ERROR state prior to schedule) server = self._build_server() # Flavor m1.xlarge cannot be fulfilled server['flavorRef'] = 'http://fake.server/5' created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ERROR') server_ids_by_cell['cell0'].append(created_server['id']) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: self.api.delete_server(server_id) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) # 6 instances should have been archived (cell0, cell1, cell2) r""\| cell0\.instances\s+\| 2"") r""\| cell1\.instances\s+\| 2"") r""\| cell2\.instances\s+\| 2"") r""\| API_DB\.instance_mappings\s+\| 6"") r""\| API_DB\.request_specs\s+\| 6"") for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: self.assertRaises(exception.InstanceNotFound, objects.Instance.get_by_uuid, cctxt, server_id)"," # Boot a server to cell1 server_ids = {} server = self._build_server(az='nova:host1') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids['cell1'] = created_server['id'] # Boot a server to cell2 server = self._build_server(az='nova:host2') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids['cell2'] = created_server['id'] # Boot a server to cell0 (cause ERROR state prior to schedule) server = self._build_server() # Flavor m1.xlarge cannot be fulfilled server['flavorRef'] = 'http://fake.server/5' created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ERROR') server_ids['cell0'] = created_server['id'] for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) for cell_name in server_ids.keys(): self.api.delete_server(server_ids[cell_name]) for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) # Three instances should have been archived (cell0, cell1, cell2) r""| cell0\.instances.*\| 1.*"") r""| cell1\.instances.*\| 1.*"") r""| cell2\.instances.*\| 1.*"") r""| API_DB\.instance_mappings.*\| 3.*"") r""| API_DB\.request_specs.*\| 3.*"") for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: self.assertRaises(exception.InstanceNotFound, objects.Instance.get_by_uuid, cctxt, server_id)",54,40
openstack%2Fironic~stable%2Ftrain~I53bfd0dcc6289e51316795fbe352c70d608e4f31,openstack/ironic,stable/train,I53bfd0dcc6289e51316795fbe352c70d608e4f31,Cleanup if images.fetch fails,ABANDONED,2023-07-07 13:17:42.000000000,2023-07-07 19:15:31.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 13:17:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/77a61d46c2b35c7c44929b76eda70cbccfac24b5', 'message': 'Cleanup if images.fetch fails\n\nCleanup if images.fetch fails as in some cases, we might get a stale\n.part file that is incomplete and corrupted (ie: full disk due to image\nconversion) and this prevents future deployment from working.\n\nChange-Id: I53bfd0dcc6289e51316795fbe352c70d608e4f31\n'}, {'number': 2, 'created': '2023-07-07 17:00:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/bfd35a9b9c2bc67c4fb8b4f88d0c02a7b6001290', 'message': 'Cleanup if images.fetch fails\n\nCleanup if images.fetch fails as in some cases, we might get a stale\n.part file that is incomplete and corrupted (ie: full disk due to image\nconversion) and this prevents future deployment from working.\n\nChange-Id: I53bfd0dcc6289e51316795fbe352c70d608e4f31\n'}, {'number': 3, 'created': '2023-07-07 17:09:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a8dd65d8a5d03c2caa3b28060151c553ed832470', 'message': 'Cleanup if images.fetch fails\n\nCleanup if images.fetch fails as in some cases, we might get a stale\n.part file that is incomplete and corrupted (ie: full disk due to image\nconversion) and this prevents future deployment from working.\n\nChange-Id: I53bfd0dcc6289e51316795fbe352c70d608e4f31\n'}, {'number': 4, 'created': '2023-07-07 18:31:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9f32543f04a0ac9ec818898e737de17717e74a2c', 'message': 'Cleanup if images.fetch fails\n\nCleanup if images.fetch fails as in some cases, we might get a stale\n.part file that is incomplete and corrupted (ie: full disk due to image\nconversion) and this prevents future deployment from working.\n\nChange-Id: I53bfd0dcc6289e51316795fbe352c70d608e4f31\n'}, {'number': 5, 'created': '2023-07-07 18:33:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/f187053aaade360874b9865a7535429effc62747', 'message': 'Cleanup if images.fetch fails\n\nCleanup if images.fetch fails as in some cases, we might get a stale\n.part file that is incomplete and corrupted (ie: full disk due to image\nconversion) and this prevents future deployment from working.\n\nChange-Id: I53bfd0dcc6289e51316795fbe352c70d608e4f31\n'}, {'number': 6, 'created': '2023-07-07 18:52:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/52bf94308665c45b60e354e62bf07a42748422d0', 'message': 'Cleanup if images.fetch fails\n\nCleanup if images.fetch fails as in some cases, we might get a stale\n.part file that is incomplete and corrupted (ie: full disk due to image\nconversion) and this prevents future deployment from working.\n\nChange-Id: I53bfd0dcc6289e51316795fbe352c70d608e4f31\n'}, {'number': 7, 'created': '2023-07-07 19:09:40.000000000', 'files': ['ironic/drivers/modules/image_cache.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/064ec755c32b72c874cf5b7545470cce76f8eb16', 'message': 'Cleanup if images.fetch fails\n\nCleanup if images.fetch fails as in some cases, we might get a stale\n.part file that is incomplete and corrupted (ie: full disk due to image\nconversion) and this prevents future deployment from working.\n\nChange-Id: I53bfd0dcc6289e51316795fbe352c70d608e4f31\n'}]",41,887954,064ec755c32b72c874cf5b7545470cce76f8eb16,17,1,7,7130,,,0,"Cleanup if images.fetch fails

Cleanup if images.fetch fails as in some cases, we might get a stale
.part file that is incomplete and corrupted (ie: full disk due to image
conversion) and this prevents future deployment from working.

Change-Id: I53bfd0dcc6289e51316795fbe352c70d608e4f31
",git fetch https://review.opendev.org/openstack/ironic refs/changes/54/887954/1 && git format-patch -1 --stdout FETCH_HEAD,['ironic/drivers/modules/image_cache.py'],1,77a61d46c2b35c7c44929b76eda70cbccfac24b5,," try: images.fetch(context, image_href, path_tmp, force_raw=False) except: os.remove(path_tmp)"," images.fetch(context, image_href, path_tmp, force_raw=False)",4,1
openstack%2Fos-brick~master~I7cced8b9d8704c1782ac8583ce227efcc21b2847,openstack/os-brick,master,I7cced8b9d8704c1782ac8583ce227efcc21b2847,mypy: Fix failing mypy job,MERGED,2023-06-30 13:00:07.000000000,2023-07-07 19:03:44.000000000,2023-07-07 19:02:47.000000000,"[{'_account_id': 4523}, {'_account_id': 5314}, {'_account_id': 9535}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-30 13:00:07.000000000', 'files': ['os_brick/initiator/connectors/fibre_channel.py'], 'web_link': 'https://opendev.org/openstack/os-brick/commit/4d41c2986c5fc6389e1dc7a3053da03338dbda21', 'message': 'mypy: Fix failing mypy job\n\nCurrently fails w/ ""Unused type: ignore comment"" errors.\n\nChange-Id: I7cced8b9d8704c1782ac8583ce227efcc21b2847\n'}]",1,887390,4d41c2986c5fc6389e1dc7a3053da03338dbda21,18,4,1,4523,,,0,"mypy: Fix failing mypy job

Currently fails w/ ""Unused type: ignore comment"" errors.

Change-Id: I7cced8b9d8704c1782ac8583ce227efcc21b2847
",git fetch https://review.opendev.org/openstack/os-brick refs/changes/90/887390/1 && git format-patch -1 --stdout FETCH_HEAD,['os_brick/initiator/connectors/fibre_channel.py'],1,4d41c2986c5fc6389e1dc7a3053da03338dbda21,, if exc: raise exc, if exc: # type: ignore raise exc # type: ignore,2,2
openstack%2Fnova~stable%2Fyoga~If39f6afb6359c67aa38cf315ec90ffa386d5c142,openstack/nova,stable/yoga,If39f6afb6359c67aa38cf315ec90ffa386d5c142,testing: Fix and robustify archive_deleted_rows test,NEW,2023-07-07 18:16:50.000000000,2023-07-07 18:44:06.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 18:16:50.000000000', 'files': ['nova/tests/functional/test_nova_manage.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/956208d879ef2d49ac5a6f6c2067ff299114d193', 'message': 'testing: Fix and robustify archive_deleted_rows test\n\nThe regexes in test_archive_deleted_rows for multiple cells were\nincorrect in that they were not isolating the search pattern and rather\ncould match with other rows in the result table as well, resulting in a\nfalse positive.\n\nThis fixes the regexes and also adds one more server to the test\nscenario in order to make sure archive_deleted_rows iterates at least\nonce to expose bugs that may be present in its internal iteration.\n\nThis patch is in preparation for a future patch that will change the\nlogic in archive_deleted_rows. Making this test more robust will more\nthoroughly test for regression.\n\nChange-Id: If39f6afb6359c67aa38cf315ec90ffa386d5c142\n(cherry picked from commit f6620d48c86fb1c5034c09da6411ea46b4d9c2ed)\n(cherry picked from commit 8823da84e9d07309b860c3ce3ad4c9ebd3652f86)\n(cherry picked from commit ecfa7c405710451247018f189416ada7bba6fd36)\n'}]",0,887982,956208d879ef2d49ac5a6f6c2067ff299114d193,3,1,1,4690,,,0,"testing: Fix and robustify archive_deleted_rows test

The regexes in test_archive_deleted_rows for multiple cells were
incorrect in that they were not isolating the search pattern and rather
could match with other rows in the result table as well, resulting in a
false positive.

This fixes the regexes and also adds one more server to the test
scenario in order to make sure archive_deleted_rows iterates at least
once to expose bugs that may be present in its internal iteration.

This patch is in preparation for a future patch that will change the
logic in archive_deleted_rows. Making this test more robust will more
thoroughly test for regression.

Change-Id: If39f6afb6359c67aa38cf315ec90ffa386d5c142
(cherry picked from commit f6620d48c86fb1c5034c09da6411ea46b4d9c2ed)
(cherry picked from commit 8823da84e9d07309b860c3ce3ad4c9ebd3652f86)
(cherry picked from commit ecfa7c405710451247018f189416ada7bba6fd36)
",git fetch https://review.opendev.org/openstack/nova refs/changes/82/887982/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/functional/test_nova_manage.py'],1,956208d879ef2d49ac5a6f6c2067ff299114d193,," server_ids_by_cell = collections.defaultdict(list) # Create two servers per cell to make sure archive for table iterates # at least once. for i in range(2): # Boot a server to cell1 server = self._build_server(az='nova:host1') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids_by_cell['cell1'].append(created_server['id']) # Boot a server to cell2 server = self._build_server(az='nova:host2') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids_by_cell['cell2'].append(created_server['id']) # Boot a server to cell0 (cause ERROR state prior to schedule) server = self._build_server() # Flavor m1.xlarge cannot be fulfilled server['flavorRef'] = 'http://fake.server/5' created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ERROR') server_ids_by_cell['cell0'].append(created_server['id']) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: self.api.delete_server(server_id) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) # 6 instances should have been archived (cell0, cell1, cell2) r""\| cell0\.instances\s+\| 2"") r""\| cell1\.instances\s+\| 2"") r""\| cell2\.instances\s+\| 2"") r""\| API_DB\.instance_mappings\s+\| 6"") r""\| API_DB\.request_specs\s+\| 6"") for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: self.assertRaises(exception.InstanceNotFound, objects.Instance.get_by_uuid, cctxt, server_id)"," # Boot a server to cell1 server_ids = {} server = self._build_server(az='nova:host1') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids['cell1'] = created_server['id'] # Boot a server to cell2 server = self._build_server(az='nova:host2') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids['cell2'] = created_server['id'] # Boot a server to cell0 (cause ERROR state prior to schedule) server = self._build_server() # Flavor m1.xlarge cannot be fulfilled server['flavorRef'] = 'http://fake.server/5' created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ERROR') server_ids['cell0'] = created_server['id'] for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) for cell_name in server_ids.keys(): self.api.delete_server(server_ids[cell_name]) for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) # Three instances should have been archived (cell0, cell1, cell2) r""| cell0\.instances.*\| 1.*"") r""| cell1\.instances.*\| 1.*"") r""| cell2\.instances.*\| 1.*"") r""| API_DB\.instance_mappings.*\| 3.*"") r""| API_DB\.request_specs.*\| 3.*"") for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: self.assertRaises(exception.InstanceNotFound, objects.Instance.get_by_uuid, cctxt, server_id)",54,40
openstack%2Fnova~stable%2Fzed~If39f6afb6359c67aa38cf315ec90ffa386d5c142,openstack/nova,stable/zed,If39f6afb6359c67aa38cf315ec90ffa386d5c142,testing: Fix and robustify archive_deleted_rows test,NEW,2023-07-07 18:08:56.000000000,2023-07-07 18:36:31.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 18:08:56.000000000', 'files': ['nova/tests/functional/test_nova_manage.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/ecfa7c405710451247018f189416ada7bba6fd36', 'message': 'testing: Fix and robustify archive_deleted_rows test\n\nThe regexes in test_archive_deleted_rows for multiple cells were\nincorrect in that they were not isolating the search pattern and rather\ncould match with other rows in the result table as well, resulting in a\nfalse positive.\n\nThis fixes the regexes and also adds one more server to the test\nscenario in order to make sure archive_deleted_rows iterates at least\nonce to expose bugs that may be present in its internal iteration.\n\nThis patch is in preparation for a future patch that will change the\nlogic in archive_deleted_rows. Making this test more robust will more\nthoroughly test for regression.\n\nChange-Id: If39f6afb6359c67aa38cf315ec90ffa386d5c142\n(cherry picked from commit f6620d48c86fb1c5034c09da6411ea46b4d9c2ed)\n(cherry picked from commit 8823da84e9d07309b860c3ce3ad4c9ebd3652f86)\n'}]",0,887980,ecfa7c405710451247018f189416ada7bba6fd36,3,1,1,4690,,,0,"testing: Fix and robustify archive_deleted_rows test

The regexes in test_archive_deleted_rows for multiple cells were
incorrect in that they were not isolating the search pattern and rather
could match with other rows in the result table as well, resulting in a
false positive.

This fixes the regexes and also adds one more server to the test
scenario in order to make sure archive_deleted_rows iterates at least
once to expose bugs that may be present in its internal iteration.

This patch is in preparation for a future patch that will change the
logic in archive_deleted_rows. Making this test more robust will more
thoroughly test for regression.

Change-Id: If39f6afb6359c67aa38cf315ec90ffa386d5c142
(cherry picked from commit f6620d48c86fb1c5034c09da6411ea46b4d9c2ed)
(cherry picked from commit 8823da84e9d07309b860c3ce3ad4c9ebd3652f86)
",git fetch https://review.opendev.org/openstack/nova refs/changes/80/887980/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/functional/test_nova_manage.py'],1,ecfa7c405710451247018f189416ada7bba6fd36,," server_ids_by_cell = collections.defaultdict(list) # Create two servers per cell to make sure archive for table iterates # at least once. for i in range(2): # Boot a server to cell1 server = self._build_server(az='nova:host1') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids_by_cell['cell1'].append(created_server['id']) # Boot a server to cell2 server = self._build_server(az='nova:host2') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids_by_cell['cell2'].append(created_server['id']) # Boot a server to cell0 (cause ERROR state prior to schedule) server = self._build_server() # Flavor m1.xlarge cannot be fulfilled server['flavorRef'] = 'http://fake.server/5' created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ERROR') server_ids_by_cell['cell0'].append(created_server['id']) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: self.api.delete_server(server_id) for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) # 6 instances should have been archived (cell0, cell1, cell2) r""\| cell0\.instances\s+\| 2"") r""\| cell1\.instances\s+\| 2"") r""\| cell2\.instances\s+\| 2"") r""\| API_DB\.instance_mappings\s+\| 6"") r""\| API_DB\.request_specs\s+\| 6"") for cell_name, server_ids in server_ids_by_cell.items(): for server_id in server_ids: with context.target_cell( admin_context, self.cell_mappings[cell_name] ) as cctxt: self.assertRaises(exception.InstanceNotFound, objects.Instance.get_by_uuid, cctxt, server_id)"," # Boot a server to cell1 server_ids = {} server = self._build_server(az='nova:host1') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids['cell1'] = created_server['id'] # Boot a server to cell2 server = self._build_server(az='nova:host2') created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ACTIVE') server_ids['cell2'] = created_server['id'] # Boot a server to cell0 (cause ERROR state prior to schedule) server = self._build_server() # Flavor m1.xlarge cannot be fulfilled server['flavorRef'] = 'http://fake.server/5' created_server = self.api.post_server({'server': server}) self._wait_for_state_change(created_server, 'ERROR') server_ids['cell0'] = created_server['id'] for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) for cell_name in server_ids.keys(): self.api.delete_server(server_ids[cell_name]) for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: objects.Instance.get_by_uuid(cctxt, server_id) # Three instances should have been archived (cell0, cell1, cell2) r""| cell0\.instances.*\| 1.*"") r""| cell1\.instances.*\| 1.*"") r""| cell2\.instances.*\| 1.*"") r""| API_DB\.instance_mappings.*\| 3.*"") r""| API_DB\.request_specs.*\| 3.*"") for cell_name, server_id in server_ids.items(): with context.target_cell(admin_context, self.cell_mappings[cell_name]) as cctxt: self.assertRaises(exception.InstanceNotFound, objects.Instance.get_by_uuid, cctxt, server_id)",54,40
openstack%2Foctavia~master~I2c8b235835d1c00f81302ed881e18040cb2c7c16,openstack/octavia,master,I2c8b235835d1c00f81302ed881e18040cb2c7c16,[sqlalchemy2] Added missing relationships in models,MERGED,2023-02-27 07:52:40.000000000,2023-07-07 18:22:50.000000000,2023-07-07 18:21:46.000000000,"[{'_account_id': 11628}, {'_account_id': 22348}, {'_account_id': 32238}, {'_account_id': 34429}]","[{'number': 1, 'created': '2023-02-27 07:52:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f0c682e8c853dcd42cc32bf4c1db08000a6cbd12', 'message': '[sqlalchemy2] Added missing relationships in models\n\nSome missing relationships triggered issues when creating many objects\nin the same transaction, for instance a FlavorProfile, a Flavor and a\nLoadBalancer. The FlavorProfile was not created before the Flavor,\nviolating the foreign key constraint.\n\nChange-Id: I2c8b235835d1c00f81302ed881e18040cb2c7c16\n'}, {'number': 2, 'created': '2023-03-29 08:44:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/d62efd2b7e6e52ae8f0395cc7ed8b819d0235740', 'message': '[sqlalchemy2] Added missing relationships in models\n\nSome missing relationships triggered issues when creating many objects\nin the same transaction, for instance a FlavorProfile, a Flavor and a\nLoadBalancer. The FlavorProfile was not created before the Flavor,\nviolating the foreign key constraint.\n\nChange-Id: I2c8b235835d1c00f81302ed881e18040cb2c7c16\n'}, {'number': 3, 'created': '2023-04-24 06:50:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/43fbdee9dea0ebca15331fb682306fde67b12622', 'message': '[sqlalchemy2] Added missing relationships in models\n\nSome missing relationships triggered issues when creating many objects\nin the same transaction, for instance a FlavorProfile, a Flavor and a\nLoadBalancer. The FlavorProfile was not created before the Flavor,\nviolating the foreign key constraint.\n\nChange-Id: I2c8b235835d1c00f81302ed881e18040cb2c7c16\n'}, {'number': 4, 'created': '2023-05-03 07:07:19.000000000', 'files': ['octavia/db/models.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/e63e149b752575a6416dfcac61e2467f5d6f2846', 'message': '[sqlalchemy2] Added missing relationships in models\n\nSome missing relationships triggered issues when creating many objects\nin the same transaction, for instance a FlavorProfile, a Flavor and a\nLoadBalancer. The FlavorProfile was not created before the Flavor,\nviolating the foreign key constraint.\n\nChange-Id: I2c8b235835d1c00f81302ed881e18040cb2c7c16\n'}]",6,875364,e63e149b752575a6416dfcac61e2467f5d6f2846,26,4,4,29244,,,0,"[sqlalchemy2] Added missing relationships in models

Some missing relationships triggered issues when creating many objects
in the same transaction, for instance a FlavorProfile, a Flavor and a
LoadBalancer. The FlavorProfile was not created before the Flavor,
violating the foreign key constraint.

Change-Id: I2c8b235835d1c00f81302ed881e18040cb2c7c16
",git fetch https://review.opendev.org/openstack/octavia refs/changes/64/875364/4 && git format-patch -1 --stdout FETCH_HEAD,['octavia/db/models.py'],1,f0c682e8c853dcd42cc32bf4c1db08000a6cbd12,sqlalchemy2," flavor = orm.relationship(""Flavor"") flavor_profile = orm.relationship(""FlavorProfile"") availability_zone_profile = orm.relationship(""AvailabilityZoneProfile"")",,3,0
openstack%2Foctavia~master~I4507d79bd49a325dae825088db22f49a93ddd6f3,openstack/octavia,master,I4507d79bd49a325dae825088db22f49a93ddd6f3,[sqlalchemy2] Removal of cascade backrefs,MERGED,2022-10-14 06:19:17.000000000,2023-07-07 17:54:55.000000000,2023-07-07 17:53:52.000000000,"[{'_account_id': 11628}, {'_account_id': 22348}, {'_account_id': 32238}, {'_account_id': 34429}]","[{'number': 1, 'created': '2022-10-14 06:19:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/a92bb724b4a1407fdbc0e73c5e5244c2db17c3da', 'message': 'WIP [sqlalchemy2] Prepare the removal of cascade backrefs\n\n[0] https://docs.sqlalchemy.org/en/14/errors.html#\\\n    object-is-being-merged-into-a-session-along-the-backref-cascade\n\nChange-Id: I4507d79bd49a325dae825088db22f49a93ddd6f3\n'}, {'number': 2, 'created': '2023-02-27 07:52:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/b91b391135cee0068175c1f9c2c80dd90a751ad6', 'message': '[sqlalchemy2] Removal of cascade backrefs\n\n[0] https://docs.sqlalchemy.org/en/14/errors.html#\\\n    object-is-being-merged-into-a-session-along-the-backref-cascade\n\nChange-Id: I4507d79bd49a325dae825088db22f49a93ddd6f3\n'}, {'number': 3, 'created': '2023-03-29 08:44:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/fb5c6ebd07854d635a46b64ae49a05141a80400b', 'message': '[sqlalchemy2] Removal of cascade backrefs\n\n[0] https://docs.sqlalchemy.org/en/14/errors.html#\\\n    object-is-being-merged-into-a-session-along-the-backref-cascade\n\nChange-Id: I4507d79bd49a325dae825088db22f49a93ddd6f3\n'}, {'number': 4, 'created': '2023-04-24 06:50:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/f24e744858db889603ef8645af06e917a852e3c7', 'message': '[sqlalchemy2] Removal of cascade backrefs\n\n[0] https://docs.sqlalchemy.org/en/14/errors.html#\\\n    object-is-being-merged-into-a-session-along-the-backref-cascade\n\nChange-Id: I4507d79bd49a325dae825088db22f49a93ddd6f3\n'}, {'number': 5, 'created': '2023-05-03 07:07:19.000000000', 'files': ['octavia/tests/fixtures.py', 'octavia/db/models.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/022b407784ca199150833bdb5f94dca35d5591ac', 'message': '[sqlalchemy2] Removal of cascade backrefs\n\n[0] https://docs.sqlalchemy.org/en/14/errors.html#\\\n    object-is-being-merged-into-a-session-along-the-backref-cascade\n\nChange-Id: I4507d79bd49a325dae825088db22f49a93ddd6f3\n'}]",8,861315,022b407784ca199150833bdb5f94dca35d5591ac,29,4,5,29244,,,0,"[sqlalchemy2] Removal of cascade backrefs

[0] https://docs.sqlalchemy.org/en/14/errors.html#\
    object-is-being-merged-into-a-session-along-the-backref-cascade

Change-Id: I4507d79bd49a325dae825088db22f49a93ddd6f3
",git fetch https://review.opendev.org/openstack/octavia refs/changes/15/861315/4 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/tests/fixtures.py', 'octavia/db/models.py']",2,a92bb724b4a1407fdbc0e73c5e5244c2db17c3da,sqlalchemy2," back_populates=""default_pool"", cascade_backrefs=False) back_populates=""_default_listeners"", cascade_backrefs=False)"," back_populates=""default_pool"") back_populates=""_default_listeners"")",4,7
openstack%2Fopenstacksdk~master~Ib877d292f8adbf2fa0c51065f2917b3f1e263483,openstack/openstacksdk,master,Ib877d292f8adbf2fa0c51065f2917b3f1e263483,Bump the chunk_size to use CPU more efficiently,MERGED,2023-05-18 02:32:10.000000000,2023-07-07 17:43:42.000000000,2023-07-07 17:42:44.000000000,"[{'_account_id': 10239}, {'_account_id': 22348}, {'_account_id': 27900}]","[{'number': 1, 'created': '2023-05-18 02:32:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/be1c2ed5db0d0b36fe0f0635c2455d99df738f0d', 'message': 'Bump the chunk_size to use CPU more efficiently\n\nThe chunk_size used for downloading images was 1KiB for some time. That\nis okay for relatively small images but the client side of CPU can be a\nbottleneck especially for large images. Bump the default chunk_size from\n1KiB to 1MiB so we can use the client side CPU more efficiently.\n\n[1KiB chunk_size - current]\n\n$ time openstack image save IMAGE_689MB --file /dev/null\n\nreal    0m16.633s\nuser    0m12.633s\nsys     0m1.365s\n\n-> ~331 Mbps\n\n[1MiB chunk_size - patched]\n\n$ time openstack image save IMAGE_689MB --file /dev/null\n\nreal    0m4.896s\nuser    0m3.361s\nsys     0m0.724s\n\n-> ~1,125 Mbps\n\nStory: 2010759\nTask: 48044\nChange-Id: Ib877d292f8adbf2fa0c51065f2917b3f1e263483\n'}, {'number': 2, 'created': '2023-05-18 02:53:01.000000000', 'files': ['openstack/image/v1/_proxy.py', 'openstack/image/_download.py', 'examples/image/download.py', 'openstack/image/v2/_proxy.py', 'openstack/cloud/_image.py'], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/a72d46a9554ba50a36539dd046a6fdbf73de2808', 'message': 'Bump the chunk_size to use CPU more efficiently\n\nThe chunk_size used for downloading images was 1KiB for some time. That\nis okay for relatively small images but the client side of CPU can be a\nbottleneck especially for large images. Bump the default chunk_size from\n1KiB to 1MiB so we can use the client side CPU more efficiently.\n\n[1KiB chunk_size - current]\n\n$ time openstack image save IMAGE_689MB --file /dev/null\n\nreal    0m16.633s\nuser    0m12.633s\nsys     0m1.365s\n\n-> ~331 Mbps\n\n[1MiB chunk_size - patched]\n\n$ time openstack image save IMAGE_689MB --file /dev/null\n\nreal    0m4.896s\nuser    0m3.361s\nsys     0m0.724s\n\n-> ~1,125 Mbps\n\nStory: 2010759\nTask: 48044\nChange-Id: Ib877d292f8adbf2fa0c51065f2917b3f1e263483\n'}]",1,883461,a72d46a9554ba50a36539dd046a6fdbf73de2808,10,3,2,8108,,,0,"Bump the chunk_size to use CPU more efficiently

The chunk_size used for downloading images was 1KiB for some time. That
is okay for relatively small images but the client side of CPU can be a
bottleneck especially for large images. Bump the default chunk_size from
1KiB to 1MiB so we can use the client side CPU more efficiently.

[1KiB chunk_size - current]

$ time openstack image save IMAGE_689MB --file /dev/null

real    0m16.633s
user    0m12.633s
sys     0m1.365s

-> ~331 Mbps

[1MiB chunk_size - patched]

$ time openstack image save IMAGE_689MB --file /dev/null

real    0m4.896s
user    0m3.361s
sys     0m0.724s

-> ~1,125 Mbps

Story: 2010759
Task: 48044
Change-Id: Ib877d292f8adbf2fa0c51065f2917b3f1e263483
",git fetch https://review.opendev.org/openstack/openstacksdk refs/changes/61/883461/2 && git format-patch -1 --stdout FETCH_HEAD,"['openstack/image/v1/_proxy.py', 'openstack/image/_download.py', 'examples/image/download.py', 'openstack/image/v2/_proxy.py', 'openstack/cloud/_image.py']",5,be1c2ed5db0d0b36fe0f0635c2455d99df738f0d,chunk_size_1MiB," chunk_size=1024 * 1024, at one time. Defaults to 1 MiB"," chunk_size=1024, at one time. Defaults to 1024",9,7
openstack%2Fopenstack-tempest-skiplist~master~I5c96971efcb0e95b207ae514c376714ddbd193a9,openstack/openstack-tempest-skiplist,master,I5c96971efcb0e95b207ae514c376714ddbd193a9,"Revert ""Add fs020-rbac to skiplist - BZ2211604""",MERGED,2023-07-07 16:11:51.000000000,2023-07-07 17:16:50.000000000,2023-07-07 17:15:52.000000000,"[{'_account_id': 8367}, {'_account_id': 9976}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-07 16:11:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-tempest-skiplist/commit/51955a19ac9954ba09decb24adcdaa0afb94a667', 'message': 'Revert ""Add fs020-rbac to skiplist - BZ2211604""\n\nThis reverts commit 30b34879ce3d2b90cdc30bf914dcbca10ea6aaf2.\n\nReason for revert: ovn22.12-22.12.0-94.el9fdp with the fix is in the latest compose\n\nChange-Id: I5c96971efcb0e95b207ae514c376714ddbd193a9\n'}, {'number': 2, 'created': '2023-07-07 16:21:45.000000000', 'files': ['roles/validate-tempest/vars/tempest_skip.yml'], 'web_link': 'https://opendev.org/openstack/openstack-tempest-skiplist/commit/ec349ebef6942b2343be4584f734559f33eae241', 'message': 'Revert ""Add fs020-rbac to skiplist - BZ2211604""\n\nThis reverts commit 30b34879ce3d2b90cdc30bf914dcbca10ea6aaf2.\n\nReason for revert: ovn22.12-22.12.0-94.el9fdp with the fix is in the latest compose\n\nChange-Id: I5c96971efcb0e95b207ae514c376714ddbd193a9\n'}]",0,887880,ec349ebef6942b2343be4584f734559f33eae241,11,3,2,1955,,,0,"Revert ""Add fs020-rbac to skiplist - BZ2211604""

This reverts commit 30b34879ce3d2b90cdc30bf914dcbca10ea6aaf2.

Reason for revert: ovn22.12-22.12.0-94.el9fdp with the fix is in the latest compose

Change-Id: I5c96971efcb0e95b207ae514c376714ddbd193a9
",git fetch https://review.opendev.org/openstack/openstack-tempest-skiplist refs/changes/80/887880/2 && git format-patch -1 --stdout FETCH_HEAD,['roles/validate-tempest/vars/tempest_skip.yml'],1,51955a19ac9954ba09decb24adcdaa0afb94a667,fs020-rbac,, - periodic-tripleo-ci-rhel-9-ovb-1ctlr_2comp-featureset020-rbac-internal-rhos-17.1 - periodic-tripleo-ci-rhel-9-ovb-1ctlr_2comp-featureset020-rbac-internal-rhos-17.1,0,2
openstack%2Fswift~master~I035ef2dcc0d0e09337bd2e742baeff1fb62bf018,openstack/swift,master,I035ef2dcc0d0e09337bd2e742baeff1fb62bf018,Object-server: backfill unit test coverage for keep_cache_private,MERGED,2023-07-07 05:39:56.000000000,2023-07-07 17:05:41.000000000,2023-07-07 17:04:37.000000000,"[{'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-07 05:39:56.000000000', 'files': ['test/unit/obj/test_server.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/1b7cf29476a2f77ceab1a082100b0c7e7cb8dc93', 'message': 'Object-server: backfill unit test coverage for keep_cache_private\n\nChange-Id: I035ef2dcc0d0e09337bd2e742baeff1fb62bf018\n'}]",1,887910,1b7cf29476a2f77ceab1a082100b0c7e7cb8dc93,8,2,1,34930,,,0,"Object-server: backfill unit test coverage for keep_cache_private

Change-Id: I035ef2dcc0d0e09337bd2e742baeff1fb62bf018
",git fetch https://review.opendev.org/openstack/swift refs/changes/10/887910/1 && git format-patch -1 --stdout FETCH_HEAD,['test/unit/obj/test_server.py'],1,1b7cf29476a2f77ceab1a082100b0c7e7cb8dc93,keep_cache_tests," def test_GET_keep_cache_private_config_true(self): # Test swift.obj.server.ObjectController.GET that, when # 'keep_cache_private' is configured True, then # disk_file.reader will be called with keep_cache=True. # Set up a new ObjectController with customized configurations. conf = {'devices': self.testdir, 'mount_check': 'false', 'container_update_timeout': 0.0, 'keep_cache_private': 'True'} obj_controller = object_server.ObjectController( conf, logger=self.logger) obj_controller.bytes_per_sync = 1 timestamp = normalize_timestamp(time()) req = Request.blank('/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'PUT'}, headers={'X-Timestamp': timestamp, 'Content-Type': 'application/x-test'}) req.body = b'VERIFY' resp = req.get_response(obj_controller) self.assertEqual(resp.status_int, 201) # Request headers have neither 'X-Auth-Token' nor 'X-Storage-Token'. req = Request.blank('/sda1/p/a/c/o', headers={'Content-Type': 'application/x-test'}) reader_mock = mock.Mock(keep_cache=False) with mock.patch('swift.obj.diskfile.BaseDiskFile.reader', reader_mock): resp = req.get_response(obj_controller) reader_mock.assert_called_with(keep_cache=True) self.assertEqual(resp.status_int, 200) etag = '""%s""' % md5(b'VERIFY', usedforsecurity=False).hexdigest() self.assertEqual(dict(resp.headers), { 'Content-Type': 'application/x-test', 'Content-Length': '6', 'Etag': etag, 'X-Backend-Timestamp': timestamp, 'X-Timestamp': timestamp, 'X-Backend-Data-Timestamp': timestamp, 'X-Backend-Durable-Timestamp': timestamp, 'Last-Modified': strftime( '%a, %d %b %Y %H:%M:%S GMT', gmtime(math.ceil(float(timestamp)))), }) # Request headers have 'X-Auth-Token'. req = Request.blank('/sda1/p/a/c/o', headers={'Content-Type': 'application/x-test', 'X-Auth-Token': '2340lsdfhhjl02lxfjj'}) reader_mock = mock.Mock(keep_cache=False) with mock.patch('swift.obj.diskfile.BaseDiskFile.reader', reader_mock): resp = req.get_response(obj_controller) reader_mock.assert_called_with(keep_cache=True) self.assertEqual(resp.status_int, 200) # Request headers have 'X-Storage-Token'. req = Request.blank('/sda1/p/a/c/o', headers={'Content-Type': 'application/x-test', 'X-Storage-Token': '2340lsdfhhjl02lxfjj'}) reader_mock = mock.Mock(keep_cache=False) with mock.patch('swift.obj.diskfile.BaseDiskFile.reader', reader_mock): resp = req.get_response(obj_controller) reader_mock.assert_called_with(keep_cache=True) self.assertEqual(resp.status_int, 200) # Request headers have both 'X-Auth-Token' and 'X-Storage-Token'. req = Request.blank('/sda1/p/a/c/o', headers={'Content-Type': 'application/x-test', 'X-Auth-Token': '2340lsdfhhjl02lxfjj', 'X-Storage-Token': '2340lsdfhhjl02lxfjj'}) reader_mock = mock.Mock(keep_cache=False) with mock.patch('swift.obj.diskfile.BaseDiskFile.reader', reader_mock): resp = req.get_response(obj_controller) reader_mock.assert_called_with(keep_cache=True) self.assertEqual(resp.status_int, 200) def test_GET_keep_cache_private_config_false(self): # Test swift.obj.server.ObjectController.GET that, when # 'keep_cache_private' is configured false, then # disk_file.reader will be called with correct 'keep_cache'. # Set up a new ObjectController with customized configurations. conf = {'devices': self.testdir, 'mount_check': 'false', 'container_update_timeout': 0.0, 'keep_cache_private': 'false'} obj_controller = object_server.ObjectController( conf, logger=self.logger) obj_controller.bytes_per_sync = 1 timestamp = normalize_timestamp(time()) req = Request.blank('/sda1/p/a/c/o', environ={'REQUEST_METHOD': 'PUT'}, headers={'X-Timestamp': timestamp, 'Content-Type': 'application/x-test'}) req.body = b'VERIFY' resp = req.get_response(obj_controller) self.assertEqual(resp.status_int, 201) # Request headers have neither 'X-Auth-Token' nor 'X-Storage-Token'. req = Request.blank('/sda1/p/a/c/o', headers={'Content-Type': 'application/x-test'}) reader_mock = mock.Mock(keep_cache=False) with mock.patch('swift.obj.diskfile.BaseDiskFile.reader', reader_mock): resp = req.get_response(obj_controller) reader_mock.assert_called_with(keep_cache=True) self.assertEqual(resp.status_int, 200) etag = '""%s""' % md5(b'VERIFY', usedforsecurity=False).hexdigest() self.assertEqual(dict(resp.headers), { 'Content-Type': 'application/x-test', 'Content-Length': '6', 'Etag': etag, 'X-Backend-Timestamp': timestamp, 'X-Timestamp': timestamp, 'X-Backend-Data-Timestamp': timestamp, 'X-Backend-Durable-Timestamp': timestamp, 'Last-Modified': strftime( '%a, %d %b %Y %H:%M:%S GMT', gmtime(math.ceil(float(timestamp)))), }) # Request headers have 'X-Auth-Token'. req = Request.blank('/sda1/p/a/c/o', headers={'Content-Type': 'application/x-test', 'X-Auth-Token': '2340lsdfhhjl02lxfjj'}) reader_mock = mock.Mock(keep_cache=False) with mock.patch('swift.obj.diskfile.BaseDiskFile.reader', reader_mock): resp = req.get_response(obj_controller) reader_mock.assert_called_with(keep_cache=False) self.assertEqual(resp.status_int, 200) # Request headers have 'X-Storage-Token'. req = Request.blank('/sda1/p/a/c/o', headers={'Content-Type': 'application/x-test', 'X-Storage-Token': '2340lsdfhhjl02lxfjj'}) reader_mock = mock.Mock(keep_cache=False) with mock.patch('swift.obj.diskfile.BaseDiskFile.reader', reader_mock): resp = req.get_response(obj_controller) reader_mock.assert_called_with(keep_cache=False) self.assertEqual(resp.status_int, 200) # Request headers have both 'X-Auth-Token' and 'X-Storage-Token'. req = Request.blank('/sda1/p/a/c/o', headers={'Content-Type': 'application/x-test', 'X-Auth-Token': '2340lsdfhhjl02lxfjj', 'X-Storage-Token': '2340lsdfhhjl02lxfjj'}) reader_mock = mock.Mock(keep_cache=False) with mock.patch('swift.obj.diskfile.BaseDiskFile.reader', reader_mock): resp = req.get_response(obj_controller) reader_mock.assert_called_with(keep_cache=False) self.assertEqual(resp.status_int, 200) ",,144,0
openstack%2Fironic~master~Ie0af03b6d89f117cece9d2c02c1fac2e9252c99b,openstack/ironic,master,Ie0af03b6d89f117cece9d2c02c1fac2e9252c99b,[DEBUG] Moving back to Jammy,NEW,2023-07-07 15:34:58.000000000,2023-07-07 16:53:09.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 15:34:58.000000000', 'files': ['zuul.d/ironic-jobs.yaml', 'zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/ironic/commit/96fa2f5098b48fcad7709e4f79b1a0198182560d', 'message': '[DEBUG] Moving back to Jammy\n\nThis patch is to help us debug the failures of the 3\njobs when running with ubuntu Jammny\n\nChange-Id: Ie0af03b6d89f117cece9d2c02c1fac2e9252c99b\n'}]",0,887972,96fa2f5098b48fcad7709e4f79b1a0198182560d,4,1,1,15519,,,0,"[DEBUG] Moving back to Jammy

This patch is to help us debug the failures of the 3
jobs when running with ubuntu Jammny

Change-Id: Ie0af03b6d89f117cece9d2c02c1fac2e9252c99b
",git fetch https://review.opendev.org/openstack/ironic refs/changes/72/887972/1 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/ironic-jobs.yaml', 'zuul.d/project.yaml']",2,96fa2f5098b48fcad7709e4f79b1a0198182560d,debug-jammny,," - ironic-tox-unit-with-driver-libs - ironic-cross-sushy: voting: false - ironic-tempest-functional-python3 # NOTE(rpittau) moving to non-voting until we fix the tests # see also https://review.opendev.org/c/openstack/ironic-tempest-plugin/+/882312 - ironic-tempest-functional-rbac-scope-enforced: voting: false - ironic-grenade - ironic-grenade-skip-level: voting: false - ironic-tempest-bios-redfish-pxe - ironic-tempest-uefi-redfish-vmedia - ironic-tempest-partition-uefi-ipmi-pxe # NOTE(TheJulia) Marking multinode non-voting on 20210311 # Due to a high failure rate on limestone where the compute1 # machine never appears to be able to communicate across the # vxlan tunnel, possible mtu issue, but non-voting until we # understand it in mind for the upcoming release. - ironic-tempest-ipa-wholedisk-direct-tinyipa-multinode: voting: false - ironic-tempest-bios-ipmi-direct-tinyipa - ironic-tempest-bfv - ironic-tempest-ipa-partition-uefi-pxe-grub2 - metalsmith-integration-glance-centos8-legacy - metal3-integration # Non-voting jobs - ironic-tox-bandit: voting: false - ironic-inspector-tempest: voting: false - ironic-inspector-tempest-managed-non-standalone: voting: false - ironic-inspector-tempest-uefi-redfish-vmedia: voting: false - ironic-tempest-ipa-wholedisk-bios-ipmi-direct-dib: voting: false - ironic-tempest-ipxe-ipv6: voting: false - ironic-standalone-anaconda: voting: false - ironic-inspector-tempest-rbac-scope-enforced: voting: false - bifrost-integration-tinyipa-ubuntu-focal: voting: false - bifrost-integration-redfish-vmedia-uefi-centos-9: voting: false - ironic-tempest-pxe_ipmitool-postgres: voting: false - bifrost-benchmark-ironic: voting: false",0,54
openstack%2Fneutron~stable%2Fzed~Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326,openstack/neutron,stable/zed,Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326,Set result when lswitch port exist,MERGED,2023-07-06 07:49:13.000000000,2023-07-07 16:43:22.000000000,2023-07-07 16:41:55.000000000,"[{'_account_id': 1131}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 07:49:13.000000000', 'files': ['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_commands.py', 'neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/commands.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/bfdc1bf25a98c3601a8aeb1f80ccf7c74739ce06', 'message': 'Set result when lswitch port exist\n\nA TypeError was thrown during a synchronization\ncommand(neutron-ovn-db-sync-util) execution. From the code[1][2], it\ncan be seen. The result of the AddLSwitchPortCommand command will be\npassed as a parameter to the UpdateLSwitchPortQosOptionsCommand. But\nif the logical switch port exists, the result will not be set. Therefore,\nwhen the UpdateLSwitchPortQosOptionsCommand is executed, the port_id\nwill not be obtained, thereby throwing an exception TypeError.\nThis patch sets the result when the logical switch port exists.\n\n[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488\n[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505\n\nCloses-Bug: #2025202\n\nChange-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326\n(cherry picked from commit 65bbbcee76fc2a7501d4c28e3cc716f1f9a6f763)\n'}]",0,887761,bfdc1bf25a98c3601a8aeb1f80ccf7c74739ce06,9,3,1,16688,,,0,"Set result when lswitch port exist

A TypeError was thrown during a synchronization
command(neutron-ovn-db-sync-util) execution. From the code[1][2], it
can be seen. The result of the AddLSwitchPortCommand command will be
passed as a parameter to the UpdateLSwitchPortQosOptionsCommand. But
if the logical switch port exists, the result will not be set. Therefore,
when the UpdateLSwitchPortQosOptionsCommand is executed, the port_id
will not be obtained, thereby throwing an exception TypeError.
This patch sets the result when the logical switch port exists.

[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488
[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505

Closes-Bug: #2025202

Change-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326
(cherry picked from commit 65bbbcee76fc2a7501d4c28e3cc716f1f9a6f763)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/61/887761/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_commands.py', 'neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/commands.py']",3,bfdc1bf25a98c3601a8aeb1f80ccf7c74739ce06,bug/2025202, self.result = port.uuid,,20,1
openstack%2Fneutron~stable%2Fxena~Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326,openstack/neutron,stable/xena,Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326,Set result when lswitch port exist,MERGED,2023-07-06 07:49:37.000000000,2023-07-07 16:43:11.000000000,2023-07-07 16:42:03.000000000,"[{'_account_id': 1131}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 07:49:37.000000000', 'files': ['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_commands.py', 'neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/commands.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/ecd968a1b5c29f5ef5bbe378e925b3b3e8e7442d', 'message': 'Set result when lswitch port exist\n\nA TypeError was thrown during a synchronization\ncommand(neutron-ovn-db-sync-util) execution. From the code[1][2], it\ncan be seen. The result of the AddLSwitchPortCommand command will be\npassed as a parameter to the UpdateLSwitchPortQosOptionsCommand. But\nif the logical switch port exists, the result will not be set. Therefore,\nwhen the UpdateLSwitchPortQosOptionsCommand is executed, the port_id\nwill not be obtained, thereby throwing an exception TypeError.\nThis patch sets the result when the logical switch port exists.\n\n[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488\n[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505\n\nCloses-Bug: #2025202\n\nChange-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326\n(cherry picked from commit 65bbbcee76fc2a7501d4c28e3cc716f1f9a6f763)\n'}]",0,887764,ecd968a1b5c29f5ef5bbe378e925b3b3e8e7442d,9,3,1,16688,,,0,"Set result when lswitch port exist

A TypeError was thrown during a synchronization
command(neutron-ovn-db-sync-util) execution. From the code[1][2], it
can be seen. The result of the AddLSwitchPortCommand command will be
passed as a parameter to the UpdateLSwitchPortQosOptionsCommand. But
if the logical switch port exists, the result will not be set. Therefore,
when the UpdateLSwitchPortQosOptionsCommand is executed, the port_id
will not be obtained, thereby throwing an exception TypeError.
This patch sets the result when the logical switch port exists.

[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488
[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505

Closes-Bug: #2025202

Change-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326
(cherry picked from commit 65bbbcee76fc2a7501d4c28e3cc716f1f9a6f763)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/64/887764/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_commands.py', 'neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/commands.py']",3,ecd968a1b5c29f5ef5bbe378e925b3b3e8e7442d,bug/2025202, self.result = port.uuid,,20,1
openstack%2Fneutron~stable%2Fyoga~Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326,openstack/neutron,stable/yoga,Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326,Set result when lswitch port exist,MERGED,2023-07-06 07:49:24.000000000,2023-07-07 16:43:06.000000000,2023-07-07 16:41:59.000000000,"[{'_account_id': 1131}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 07:49:24.000000000', 'files': ['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_commands.py', 'neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/commands.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/6af9f70e0b07715c6908a23e7bdd8b2549fac397', 'message': 'Set result when lswitch port exist\n\nA TypeError was thrown during a synchronization\ncommand(neutron-ovn-db-sync-util) execution. From the code[1][2], it\ncan be seen. The result of the AddLSwitchPortCommand command will be\npassed as a parameter to the UpdateLSwitchPortQosOptionsCommand. But\nif the logical switch port exists, the result will not be set. Therefore,\nwhen the UpdateLSwitchPortQosOptionsCommand is executed, the port_id\nwill not be obtained, thereby throwing an exception TypeError.\nThis patch sets the result when the logical switch port exists.\n\n[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488\n[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505\n\nCloses-Bug: #2025202\n\nChange-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326\n(cherry picked from commit 65bbbcee76fc2a7501d4c28e3cc716f1f9a6f763)\n'}]",0,887762,6af9f70e0b07715c6908a23e7bdd8b2549fac397,9,3,1,16688,,,0,"Set result when lswitch port exist

A TypeError was thrown during a synchronization
command(neutron-ovn-db-sync-util) execution. From the code[1][2], it
can be seen. The result of the AddLSwitchPortCommand command will be
passed as a parameter to the UpdateLSwitchPortQosOptionsCommand. But
if the logical switch port exists, the result will not be set. Therefore,
when the UpdateLSwitchPortQosOptionsCommand is executed, the port_id
will not be obtained, thereby throwing an exception TypeError.
This patch sets the result when the logical switch port exists.

[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488
[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505

Closes-Bug: #2025202

Change-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326
(cherry picked from commit 65bbbcee76fc2a7501d4c28e3cc716f1f9a6f763)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/62/887762/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_commands.py', 'neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/commands.py']",3,6af9f70e0b07715c6908a23e7bdd8b2549fac397,bug/2025202, self.result = port.uuid,,20,1
openstack%2Fneutron~stable%2F2023.1~Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326,openstack/neutron,stable/2023.1,Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326,Set result when lswitch port exist,MERGED,2023-07-06 07:48:59.000000000,2023-07-07 16:43:04.000000000,2023-07-07 16:41:52.000000000,"[{'_account_id': 1131}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 07:48:59.000000000', 'files': ['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_commands.py', 'neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/commands.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/d9eb4ff47ca7b4f8b21d10fb5afc80dbe13d8424', 'message': 'Set result when lswitch port exist\n\nA TypeError was thrown during a synchronization\ncommand(neutron-ovn-db-sync-util) execution. From the code[1][2], it\ncan be seen. The result of the AddLSwitchPortCommand command will be\npassed as a parameter to the UpdateLSwitchPortQosOptionsCommand. But\nif the logical switch port exists, the result will not be set. Therefore,\nwhen the UpdateLSwitchPortQosOptionsCommand is executed, the port_id\nwill not be obtained, thereby throwing an exception TypeError.\nThis patch sets the result when the logical switch port exists.\n\n[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488\n[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505\n\nCloses-Bug: #2025202\n\nChange-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326\n(cherry picked from commit 65bbbcee76fc2a7501d4c28e3cc716f1f9a6f763)\n'}]",1,887760,d9eb4ff47ca7b4f8b21d10fb5afc80dbe13d8424,9,3,1,16688,,,0,"Set result when lswitch port exist

A TypeError was thrown during a synchronization
command(neutron-ovn-db-sync-util) execution. From the code[1][2], it
can be seen. The result of the AddLSwitchPortCommand command will be
passed as a parameter to the UpdateLSwitchPortQosOptionsCommand. But
if the logical switch port exists, the result will not be set. Therefore,
when the UpdateLSwitchPortQosOptionsCommand is executed, the port_id
will not be obtained, thereby throwing an exception TypeError.
This patch sets the result when the logical switch port exists.

[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488
[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505

Closes-Bug: #2025202

Change-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326
(cherry picked from commit 65bbbcee76fc2a7501d4c28e3cc716f1f9a6f763)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/60/887760/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_commands.py', 'neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/commands.py']",3,d9eb4ff47ca7b4f8b21d10fb5afc80dbe13d8424,bug/2025202, self.result = port.uuid,,20,1
openstack%2Fneutron~stable%2F2023.1~Ia3741abe5fa0c8ff65cf36d1ec31c089a1759f05,openstack/neutron,stable/2023.1,Ia3741abe5fa0c8ff65cf36d1ec31c089a1759f05,[FT] Move ``BaseOVSTestCase`` class to concurrency 1 executor,MERGED,2023-07-04 15:57:39.000000000,2023-07-07 16:33:46.000000000,2023-07-07 16:31:37.000000000,"[{'_account_id': 8313}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 15:57:39.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/neutron/commit/88d00e698f4c631c1db7eb7b32297e1c92e5524f', 'message': '[FT] Move ``BaseOVSTestCase`` class to concurrency 1 executor\n\nMove the ``BaseOVSTestCase`` class tests to the stestr executor with\nconcurrency=1. That will prevent that the minimum bandwidth tests\ninterfere among them.\n\nConflicts:\n    tox.ini\n\nCloses-Bug: #2025740\nChange-Id: Ia3741abe5fa0c8ff65cf36d1ec31c089a1759f05\n(cherry picked from commit 26a2266cf4e2395b0146902de2eb0a6966a037ec)\n'}]",1,887633,88d00e698f4c631c1db7eb7b32297e1c92e5524f,9,3,1,16688,,,0,"[FT] Move ``BaseOVSTestCase`` class to concurrency 1 executor

Move the ``BaseOVSTestCase`` class tests to the stestr executor with
concurrency=1. That will prevent that the minimum bandwidth tests
interfere among them.

Conflicts:
    tox.ini

Closes-Bug: #2025740
Change-Id: Ia3741abe5fa0c8ff65cf36d1ec31c089a1759f05
(cherry picked from commit 26a2266cf4e2395b0146902de2eb0a6966a037ec)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/33/887633/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,88d00e698f4c631c1db7eb7b32297e1c92e5524f,bug/2025740, stestr run --exclude-regex (.*MySQL\.|.*PostgreSQL\.|.*test_get_all_devices|.*BaseOVSTestCase\.) {posargs} stestr run --combine --concurrency 1 (.*MySQL\.|.*PostgreSQL\.|.*test_get_all_devices|.*BaseOVSTestCase\.) {posargs}, stestr run --exclude-regex (.*MySQL\.|.*PostgreSQL\.|.*test_get_all_devices) {posargs} stestr run --combine --concurrency 1 (.*MySQL\.|.*PostgreSQL\.|.*test_get_all_devices) {posargs},2,2
openstack%2Fneutron~stable%2Fxena~I1fb6a9ff94376ab5f695d311b6fa8034d73cf556,openstack/neutron,stable/xena,I1fb6a9ff94376ab5f695d311b6fa8034d73cf556,[OVN] Expose chassis hosting information in LSP,MERGED,2023-07-04 08:42:03.000000000,2023-07-07 16:32:54.000000000,2023-07-07 16:31:33.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 23567}]","[{'number': 1, 'created': '2023-07-04 08:42:03.000000000', 'files': ['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'neutron/common/ovn/constants.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/819339472ae9b6d20a299ed4e2e61856def70654', 'message': '[OVN] Expose chassis hosting information in LSP\n\nExpose chassis hosting information via LSP\'s external_ids. This allows\nfor projects such as ovn-bgp-agent NB driver that only connects to the\nNB database to consume such information (avoiding more connections to\nthe SB database).\n\nAlso stop populating the ""requested-chassis"" options for ports of the\ntype virtual because it\'s ignored by OVN.\n\nSee LP #2020058 for more information.\n\nConflicts:\n  neutron/common/ovn/constants.py\n  neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py\n  neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py\n\nCloses-Bug: #2020058\nChange-Id: I1fb6a9ff94376ab5f695d311b6fa8034d73cf556\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n(cherry picked from commit 28926957d691c8b0897c71fee59a888783d15974)\n'}]",1,887582,819339472ae9b6d20a299ed4e2e61856def70654,10,4,1,6773,,,0,"[OVN] Expose chassis hosting information in LSP

Expose chassis hosting information via LSP's external_ids. This allows
for projects such as ovn-bgp-agent NB driver that only connects to the
NB database to consume such information (avoiding more connections to
the SB database).

Also stop populating the ""requested-chassis"" options for ports of the
type virtual because it's ignored by OVN.

See LP #2020058 for more information.

Conflicts:
  neutron/common/ovn/constants.py
  neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py
  neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py

Closes-Bug: #2020058
Change-Id: I1fb6a9ff94376ab5f695d311b6fa8034d73cf556
Signed-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>
(cherry picked from commit 28926957d691c8b0897c71fee59a888783d15974)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/82/887582/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'neutron/common/ovn/constants.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py']",5,819339472ae9b6d20a299ed4e2e61856def70654,,"class TestOVNClient(TestOVNClientBase): def setUp(self): super(TestOVNClient, self).setUp() self.get_plugin = mock.patch( 'neutron_lib.plugins.directory.get_plugin').start() def test_update_lsp_host_info_up(self): context = mock.MagicMock() host_id = 'fake-binding-host-id' port_id = 'fake-port-id' db_port = mock.Mock( id=port_id, port_bindings=[mock.Mock(host=host_id)]) self.ovn_client.update_lsp_host_info(context, db_port) self.nb_idl.db_set.assert_called_once_with( 'Logical_Switch_Port', port_id, ('external_ids', {constants.OVN_HOST_ID_EXT_ID_KEY: host_id})) def test_update_lsp_host_info_down(self): context = mock.MagicMock() port_id = 'fake-port-id' db_port = mock.Mock(id=port_id) self.ovn_client.update_lsp_host_info(context, db_port, up=False) self.nb_idl.db_remove.assert_called_once_with( 'Logical_Switch_Port', port_id, 'external_ids', constants.OVN_HOST_ID_EXT_ID_KEY, if_exists=True) ",,81,3
openstack%2Fneutron~stable%2Fyoga~I1fb6a9ff94376ab5f695d311b6fa8034d73cf556,openstack/neutron,stable/yoga,I1fb6a9ff94376ab5f695d311b6fa8034d73cf556,[OVN] Expose chassis hosting information in LSP,MERGED,2023-07-04 08:41:55.000000000,2023-07-07 16:32:49.000000000,2023-07-07 16:31:30.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 23567}]","[{'number': 1, 'created': '2023-07-04 08:41:55.000000000', 'files': ['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'neutron/common/ovn/constants.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/3e6b2b77b2aa84260864c53bf7bcdd7611a848ef', 'message': '[OVN] Expose chassis hosting information in LSP\n\nExpose chassis hosting information via LSP\'s external_ids. This allows\nfor projects such as ovn-bgp-agent NB driver that only connects to the\nNB database to consume such information (avoiding more connections to\nthe SB database).\n\nAlso stop populating the ""requested-chassis"" options for ports of the\ntype virtual because it\'s ignored by OVN.\n\nSee LP #2020058 for more information.\n\nConflicts:\n  neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py\n\nCloses-Bug: #2020058\nChange-Id: I1fb6a9ff94376ab5f695d311b6fa8034d73cf556\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n(cherry picked from commit 28926957d691c8b0897c71fee59a888783d15974)\n'}]",0,887581,3e6b2b77b2aa84260864c53bf7bcdd7611a848ef,10,4,1,6773,,,0,"[OVN] Expose chassis hosting information in LSP

Expose chassis hosting information via LSP's external_ids. This allows
for projects such as ovn-bgp-agent NB driver that only connects to the
NB database to consume such information (avoiding more connections to
the SB database).

Also stop populating the ""requested-chassis"" options for ports of the
type virtual because it's ignored by OVN.

See LP #2020058 for more information.

Conflicts:
  neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py

Closes-Bug: #2020058
Change-Id: I1fb6a9ff94376ab5f695d311b6fa8034d73cf556
Signed-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>
(cherry picked from commit 28926957d691c8b0897c71fee59a888783d15974)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/81/887581/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'neutron/common/ovn/constants.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py']",5,3e6b2b77b2aa84260864c53bf7bcdd7611a848ef,," def test_update_lsp_host_info_up(self): context = mock.MagicMock() host_id = 'fake-binding-host-id' port_id = 'fake-port-id' db_port = mock.Mock( id=port_id, port_bindings=[mock.Mock(host=host_id)]) self.ovn_client.update_lsp_host_info(context, db_port) self.nb_idl.db_set.assert_called_once_with( 'Logical_Switch_Port', port_id, ('external_ids', {constants.OVN_HOST_ID_EXT_ID_KEY: host_id})) def test_update_lsp_host_info_down(self): context = mock.MagicMock() port_id = 'fake-port-id' db_port = mock.Mock(id=port_id) self.ovn_client.update_lsp_host_info(context, db_port, up=False) self.nb_idl.db_remove.assert_called_once_with( 'Logical_Switch_Port', port_id, 'external_ids', constants.OVN_HOST_ID_EXT_ID_KEY, if_exists=True) ",,76,3
openstack%2Fneutron~stable%2Fzed~I1fb6a9ff94376ab5f695d311b6fa8034d73cf556,openstack/neutron,stable/zed,I1fb6a9ff94376ab5f695d311b6fa8034d73cf556,[OVN] Expose chassis hosting information in LSP,MERGED,2023-07-04 08:41:45.000000000,2023-07-07 16:32:46.000000000,2023-07-07 16:31:26.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 23567}]","[{'number': 1, 'created': '2023-07-04 08:41:45.000000000', 'files': ['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'neutron/common/ovn/constants.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/984193b0dccd6cd080d60473de51f04bad704285', 'message': '[OVN] Expose chassis hosting information in LSP\n\nExpose chassis hosting information via LSP\'s external_ids. This allows\nfor projects such as ovn-bgp-agent NB driver that only connects to the\nNB database to consume such information (avoiding more connections to\nthe SB database).\n\nAlso stop populating the ""requested-chassis"" options for ports of the\ntype virtual because it\'s ignored by OVN.\n\nSee LP #2020058 for more information.\n\nConflicts:\n  neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py\n\nCloses-Bug: #2020058\nChange-Id: I1fb6a9ff94376ab5f695d311b6fa8034d73cf556\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n(cherry picked from commit 28926957d691c8b0897c71fee59a888783d15974)\n'}]",0,887580,984193b0dccd6cd080d60473de51f04bad704285,10,4,1,6773,,,0,"[OVN] Expose chassis hosting information in LSP

Expose chassis hosting information via LSP's external_ids. This allows
for projects such as ovn-bgp-agent NB driver that only connects to the
NB database to consume such information (avoiding more connections to
the SB database).

Also stop populating the ""requested-chassis"" options for ports of the
type virtual because it's ignored by OVN.

See LP #2020058 for more information.

Conflicts:
  neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py

Closes-Bug: #2020058
Change-Id: I1fb6a9ff94376ab5f695d311b6fa8034d73cf556
Signed-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>
(cherry picked from commit 28926957d691c8b0897c71fee59a888783d15974)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/80/887580/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'neutron/common/ovn/constants.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py']",5,984193b0dccd6cd080d60473de51f04bad704285,," def test_update_lsp_host_info_up(self): context = mock.MagicMock() host_id = 'fake-binding-host-id' port_id = 'fake-port-id' db_port = mock.Mock( id=port_id, port_bindings=[mock.Mock(host=host_id)]) self.ovn_client.update_lsp_host_info(context, db_port) self.nb_idl.db_set.assert_called_once_with( 'Logical_Switch_Port', port_id, ('external_ids', {constants.OVN_HOST_ID_EXT_ID_KEY: host_id})) def test_update_lsp_host_info_down(self): context = mock.MagicMock() port_id = 'fake-port-id' db_port = mock.Mock(id=port_id) self.ovn_client.update_lsp_host_info(context, db_port, up=False) self.nb_idl.db_remove.assert_called_once_with( 'Logical_Switch_Port', port_id, 'external_ids', constants.OVN_HOST_ID_EXT_ID_KEY, if_exists=True) ",,76,3
openstack%2Fneutron~stable%2F2023.1~I1fb6a9ff94376ab5f695d311b6fa8034d73cf556,openstack/neutron,stable/2023.1,I1fb6a9ff94376ab5f695d311b6fa8034d73cf556,[OVN] Expose chassis hosting information in LSP,MERGED,2023-07-04 08:41:30.000000000,2023-07-07 16:32:44.000000000,2023-07-07 16:31:22.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 23567}]","[{'number': 1, 'created': '2023-07-04 08:41:30.000000000', 'files': ['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'neutron/common/ovn/constants.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/82e94208bc45f9ef85c042ad85f6ff51da2f2548', 'message': '[OVN] Expose chassis hosting information in LSP\n\nExpose chassis hosting information via LSP\'s external_ids. This allows\nfor projects such as ovn-bgp-agent NB driver that only connects to the\nNB database to consume such information (avoiding more connections to\nthe SB database).\n\nAlso stop populating the ""requested-chassis"" options for ports of the\ntype virtual because it\'s ignored by OVN.\n\nSee LP #2020058 for more information.\n\nConflicts:\n  neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py\n\nCloses-Bug: #2020058\nChange-Id: I1fb6a9ff94376ab5f695d311b6fa8034d73cf556\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n(cherry picked from commit 28926957d691c8b0897c71fee59a888783d15974)\n'}]",2,887579,82e94208bc45f9ef85c042ad85f6ff51da2f2548,13,4,1,6773,,,0,"[OVN] Expose chassis hosting information in LSP

Expose chassis hosting information via LSP's external_ids. This allows
for projects such as ovn-bgp-agent NB driver that only connects to the
NB database to consume such information (avoiding more connections to
the SB database).

Also stop populating the ""requested-chassis"" options for ports of the
type virtual because it's ignored by OVN.

See LP #2020058 for more information.

Conflicts:
  neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py

Closes-Bug: #2020058
Change-Id: I1fb6a9ff94376ab5f695d311b6fa8034d73cf556
Signed-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>
(cherry picked from commit 28926957d691c8b0897c71fee59a888783d15974)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/79/887579/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'neutron/common/ovn/constants.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py']",5,82e94208bc45f9ef85c042ad85f6ff51da2f2548,," def test_update_lsp_host_info_up(self): context = mock.MagicMock() host_id = 'fake-binding-host-id' port_id = 'fake-port-id' db_port = mock.Mock( id=port_id, port_bindings=[mock.Mock(host=host_id)]) self.ovn_client.update_lsp_host_info(context, db_port) self.nb_idl.db_set.assert_called_once_with( 'Logical_Switch_Port', port_id, ('external_ids', {constants.OVN_HOST_ID_EXT_ID_KEY: host_id})) def test_update_lsp_host_info_down(self): context = mock.MagicMock() port_id = 'fake-port-id' db_port = mock.Mock(id=port_id) self.ovn_client.update_lsp_host_info(context, db_port, up=False) self.nb_idl.db_remove.assert_called_once_with( 'Logical_Switch_Port', port_id, 'external_ids', constants.OVN_HOST_ID_EXT_ID_KEY, if_exists=True) ",,76,3
openstack%2Ftripleo-common~stable%2Fwallaby~Id384780e8394b40a91761e7fbbc0f8e44263d681,openstack/tripleo-common,stable/wallaby,Id384780e8394b40a91761e7fbbc0f8e44263d681,Disable pam_loguinuid for crond,MERGED,2023-07-06 05:30:16.000000000,2023-07-07 16:27:01.000000000,2023-07-07 16:26:04.000000000,"[{'_account_id': 7144}, {'_account_id': 7414}, {'_account_id': 9816}, {'_account_id': 22348}, {'_account_id': 30073}]","[{'number': 1, 'created': '2023-07-06 05:30:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/7b58d68b482b0af90f31ff1d69fa36e4b0c613bb', 'message': 'Disable pan_loguinuid for crond\n\nCurrently we run some crond processes with -n option but the cron jobs\nfails because of pam errors. According to man page we have to remove\npam_loginuid.so in case we run crond in the foreground.\n\n~~~\n       -n     Tells the daemon to run in the foreground.  This can be\n              useful when starting it out of init. With this option is\n              needed to change pam setting.  /etc/pam.d/crond must not\n              enable pam_loginuid.so module.\n~~~\n\nThis was not a problem in CentOS8/RHEL8 likely because old podman added\nAUDIT_CONTROL by default but it is no longer enabled in the newer\npodman version we have in CentOS9/RHEL9.\n\nResolves: rhbz#2219765\nChange-Id: Id384780e8394b40a91761e7fbbc0f8e44263d681\n'}, {'number': 2, 'created': '2023-07-06 05:35:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/d64c182c1598b8a4171102e2ea374ce123ca1b38', 'message': 'Disable pan_loguinuid for crond\n\nCurrently we run some crond processes with -n option but the cron jobs\nfails because of pam errors. According to man page we have to remove\npam_loginuid.so in case we run crond in the foreground.\n\n~~~\n       -n     Tells the daemon to run in the foreground.  This can be\n              useful when starting it out of init. With this option is\n              needed to change pam setting.  /etc/pam.d/crond must not\n              enable pam_loginuid.so module.\n~~~\n\nResolves: rhbz#2219765\nChange-Id: Id384780e8394b40a91761e7fbbc0f8e44263d681\n'}, {'number': 3, 'created': '2023-07-06 08:13:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/27948542404739c34161df85eee3f7d7194a4a78', 'message': 'Disable pam_loguinuid for crond\n\nCurrently we run some crond processes with -n option but the cron jobs\nfails because of pam errors. According to man page we have to remove\npam_loginuid.so in case we run crond in the foreground.\n\n~~~\n       -n     Tells the daemon to run in the foreground.  This can be\n              useful when starting it out of init. With this option is\n              needed to change pam setting.  /etc/pam.d/crond must not\n              enable pam_loginuid.so module.\n~~~\n\nResolves: rhbz#2219765\nChange-Id: Id384780e8394b40a91761e7fbbc0f8e44263d681\n'}, {'number': 4, 'created': '2023-07-06 08:16:25.000000000', 'files': ['container-images/tcib/base/os/os.yaml', 'container-images/tcib/base/cron/cron.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/dfd9325672f24ed4954b248815eef2776f2605c4', 'message': 'Disable pam_loguinuid for crond\n\nCurrently we run some crond processes with -n option but the cron jobs\nfails because of pam errors. According to man page we have to remove\npam_loginuid.so in case we run crond in the foreground.\n\n~~~\n       -n     Tells the daemon to run in the foreground.  This can be\n              useful when starting it out of init. With this option is\n              needed to change pam setting.  /etc/pam.d/crond must not\n              enable pam_loginuid.so module.\n~~~\n\nResolves: rhbz#2219765\nChange-Id: Id384780e8394b40a91761e7fbbc0f8e44263d681\n'}]",1,887748,dfd9325672f24ed4954b248815eef2776f2605c4,18,5,4,9816,,,0,"Disable pam_loguinuid for crond

Currently we run some crond processes with -n option but the cron jobs
fails because of pam errors. According to man page we have to remove
pam_loginuid.so in case we run crond in the foreground.

~~~
       -n     Tells the daemon to run in the foreground.  This can be
              useful when starting it out of init. With this option is
              needed to change pam setting.  /etc/pam.d/crond must not
              enable pam_loginuid.so module.
~~~

Resolves: rhbz#2219765
Change-Id: Id384780e8394b40a91761e7fbbc0f8e44263d681
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/48/887748/4 && git format-patch -1 --stdout FETCH_HEAD,['container-images/tcib/base/base.yaml'],1,7b58d68b482b0af90f31ff1d69fa36e4b0c613bb,rhbz2219765,- run: sed -ri '/^session(\s)+required(\s+)pam_loginuid.so$/d' /etc/pam.d/crond sed -ri '/^-session(\s+)optional(\s+)pam_systemd.so$/d' /etc/pam.d/system-auth &&, sed -ri '/-session(\s+)optional(\s+)pam_systemd.so/d' /etc/pam.d/system-auth &&,2,1
openstack%2Fdesignate~stable%2F2023.1~I98c341b8f7138681e35d84707062f8d0e807c533,openstack/designate,stable/2023.1,I98c341b8f7138681e35d84707062f8d0e807c533,Fix list zones if shared with multiple projects,MERGED,2023-07-05 16:45:14.000000000,2023-07-07 16:26:11.000000000,2023-07-07 16:25:07.000000000,"[{'_account_id': 11628}, {'_account_id': 22348}, {'_account_id': 22623}]","[{'number': 1, 'created': '2023-07-05 16:45:14.000000000', 'files': ['releasenotes/notes/Fix-zone-list-when-zone-shared-more-than-once-288b57cafeba82df.yaml', 'designate/tests/test_storage/__init__.py', 'designate/storage/impl_sqlalchemy/__init__.py'], 'web_link': 'https://opendev.org/openstack/designate/commit/efeff8e8be0e73d461ac7206e4bf1350c5c684f6', 'message': 'Fix list zones if shared with multiple projects\n\nThis patch fixes a bug when listing zones or updating recordsets in\nzones that are shared with more than one project.\n\nCloses-Bug: #2025295\n(cherry picked from commit 011ebe2e7cd0df0c7f0869f0c7abbce79434821a)\nChange-Id: I98c341b8f7138681e35d84707062f8d0e807c533\n'}]",2,887728,efeff8e8be0e73d461ac7206e4bf1350c5c684f6,10,3,1,11628,,,0,"Fix list zones if shared with multiple projects

This patch fixes a bug when listing zones or updating recordsets in
zones that are shared with more than one project.

Closes-Bug: #2025295
(cherry picked from commit 011ebe2e7cd0df0c7f0869f0c7abbce79434821a)
Change-Id: I98c341b8f7138681e35d84707062f8d0e807c533
",git fetch https://review.opendev.org/openstack/designate refs/changes/28/887728/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/Fix-zone-list-when-zone-shared-more-than-once-288b57cafeba82df.yaml', 'designate/tests/test_storage/__init__.py', 'designate/storage/impl_sqlalchemy/__init__.py']",3,efeff8e8be0e73d461ac7206e4bf1350c5c684f6,bug/2025295," [tables.zones, shared_case]).outerjoin(tables.shared_zones).distinct()"," [tables.zones, shared_case]).outerjoin(tables.shared_zones)",24,1
openstack%2Fgovernance~master~I986570ec33354a2e7544fec8ea0fe07242f349e0,openstack/governance,master,I986570ec33354a2e7544fec8ea0fe07242f349e0,Create TOC for 2023 resolutions,ABANDONED,2023-07-07 16:03:49.000000000,2023-07-07 16:18:13.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 16:03:49.000000000', 'files': ['resolutions/index.rst'], 'web_link': 'https://opendev.org/openstack/governance/commit/07fcbde9bb49b2242ae3b3663be0df79f3ffe6dd', 'message': 'Create TOC for 2023 resolutions\n\nChange-Id: I986570ec33354a2e7544fec8ea0fe07242f349e0\n'}]",0,887974,07fcbde9bb49b2242ae3b3663be0df79f3ffe6dd,3,1,1,16465,,,0,"Create TOC for 2023 resolutions

Change-Id: I986570ec33354a2e7544fec8ea0fe07242f349e0
",git fetch https://review.opendev.org/openstack/governance refs/changes/74/887974/1 && git format-patch -1 --stdout FETCH_HEAD,['resolutions/index.rst'],1,07fcbde9bb49b2242ae3b3663be0df79f3ffe6dd,code-change,2023 ==== .. toctree:: :maxdepth: 1 :glob: :reversed: 2023* ,,10,0
openstack%2Fneutron~master~Iec8854749a1df81eb6a7154d3f951e176c69156d,openstack/neutron,master,Iec8854749a1df81eb6a7154d3f951e176c69156d,[OVN] Remove backwards compatibility with OVN < v20.09,NEW,2023-05-31 15:14:43.000000000,2023-07-07 16:13:25.000000000,,"[{'_account_id': 1131}, {'_account_id': 9845}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-05-31 15:14:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/95dab24c6210ec187a337755ba3384d6ed6236e1', 'message': '[OVN] Remove backwards compatibility with OVN < v20.09\n\nThis patch removes the compatibility with OVN under v20.09. That\nimplies the OVN Southbound definition has ""Chassis_Private"" table.\nAny previous check is removed from the code.\n\nThis patch also adds a sanity check, testing that the OVN Southbound\ndatabase definition is greater or equal to 2.9.0 [1].\n\nThe testing OVN NB and SB schemas are updated to the files contained in\nOVN v22.09. The new testing NB schema version is 6.3.9; the new testing\nSB schema version is 20.25.0.\n\n[1]https://github.com/ovn-org/ovn/commit/4adc10f58127e45b5883f2e7cb1c702720b95043\n\nCloses-Bug: #2002839\nChange-Id: Iec8854749a1df81eb6a7154d3f951e176c69156d\n'}, {'number': 2, 'created': '2023-07-07 12:47:28.000000000', 'files': ['neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovsdb_monitor.py', 'neutron/tests/functional/agent/ovn/metadata/test_metadata_agent.py', 'neutron/plugins/ml2/drivers/ovn/agent/neutron_agent.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/schemas/ovn-nb.ovsschema', 'neutron/tests/functional/base.py', 'neutron/cmd/sanity_check.py', 'neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovsdb_monitor.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovsdb_monitor.py', 'neutron/cmd/sanity/checks.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'neutron/agent/ovn/metadata/agent.py', 'releasenotes/notes/ovn-support-chassis_private-35192565e9ee2a00.yaml', 'neutron/tests/unit/plugins/ml2/drivers/ovn/agent/test_neutron_agent.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/schemas/ovn-sb.ovsschema', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/0470a85a5cb06e7cef3d9476b3521fc858fac11b', 'message': '[OVN] Remove backwards compatibility with OVN < v20.09\n\nThis patch removes the compatibility with OVN under v20.09. That\nimplies the OVN Southbound definition has ""Chassis_Private"" table.\nAny previous check is removed from the code.\n\nThis patch also adds a sanity check, testing that the OVN Southbound\ndatabase definition is greater or equal to 2.9.0 [1].\n\nThe testing OVN NB and SB schemas are updated to the files contained in\nOVN v22.09. The new testing NB schema version is 6.3.9; the new testing\nSB schema version is 20.25.0.\n\n[1]https://github.com/ovn-org/ovn/commit/4adc10f58127e45b5883f2e7cb1c702720b95043\n\nCloses-Bug: #2002839\nChange-Id: Iec8854749a1df81eb6a7154d3f951e176c69156d\n'}]",0,884898,0470a85a5cb06e7cef3d9476b3521fc858fac11b,10,4,2,16688,,,0,"[OVN] Remove backwards compatibility with OVN < v20.09

This patch removes the compatibility with OVN under v20.09. That
implies the OVN Southbound definition has ""Chassis_Private"" table.
Any previous check is removed from the code.

This patch also adds a sanity check, testing that the OVN Southbound
database definition is greater or equal to 2.9.0 [1].

The testing OVN NB and SB schemas are updated to the files contained in
OVN v22.09. The new testing NB schema version is 6.3.9; the new testing
SB schema version is 20.25.0.

[1]https://github.com/ovn-org/ovn/commit/4adc10f58127e45b5883f2e7cb1c702720b95043

Closes-Bug: #2002839
Change-Id: Iec8854749a1df81eb6a7154d3f951e176c69156d
",git fetch https://review.opendev.org/openstack/neutron refs/changes/98/884898/2 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovsdb_monitor.py', 'neutron/tests/functional/agent/ovn/metadata/test_metadata_agent.py', 'neutron/plugins/ml2/drivers/ovn/agent/neutron_agent.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/schemas/ovn-nb.ovsschema', 'neutron/tests/functional/base.py', 'neutron/cmd/sanity_check.py', 'neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovsdb_monitor.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovsdb_monitor.py', 'neutron/cmd/sanity/checks.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'neutron/agent/ovn/metadata/agent.py', 'releasenotes/notes/ovn-support-chassis_private-35192565e9ee2a00.yaml', 'neutron/tests/unit/plugins/ml2/drivers/ovn/agent/test_neutron_agent.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/schemas/ovn-sb.ovsschema', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py']",16,95dab24c6210ec187a337755ba3384d6ed6236e1,bug/2002839, self.agent_chassis_table = 'Chassis_Private', if impl_idl_ovn.OvsdbSbOvnIdl.schema_has_table('Chassis_Private'): self.agent_chassis_table = 'Chassis_Private' else: self.agent_chassis_table = 'Chassis',354,113
openstack%2Fopenstack-tempest-skiplist~master~I0f465f5dd32df6e5cc72f3a5edbce1e19a83f10f,openstack/openstack-tempest-skiplist,master,I0f465f5dd32df6e5cc72f3a5edbce1e19a83f10f,Add fs020-rbac to skiplist - BZ2211604,MERGED,2023-06-02 00:52:05.000000000,2023-07-07 16:11:51.000000000,2023-06-02 01:11:13.000000000,"[{'_account_id': 9976}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-02 00:52:05.000000000', 'files': ['roles/validate-tempest/vars/tempest_skip.yml'], 'web_link': 'https://opendev.org/openstack/openstack-tempest-skiplist/commit/30b34879ce3d2b90cdc30bf914dcbca10ea6aaf2', 'message': 'Add fs020-rbac to skiplist - BZ2211604\n\nRelated-Bug: #BZ2211604\nChange-Id: I0f465f5dd32df6e5cc72f3a5edbce1e19a83f10f\n'}]",0,885102,30b34879ce3d2b90cdc30bf914dcbca10ea6aaf2,8,2,1,9976,,,0,"Add fs020-rbac to skiplist - BZ2211604

Related-Bug: #BZ2211604
Change-Id: I0f465f5dd32df6e5cc72f3a5edbce1e19a83f10f
",git fetch https://review.opendev.org/openstack/openstack-tempest-skiplist refs/changes/02/885102/1 && git format-patch -1 --stdout FETCH_HEAD,['roles/validate-tempest/vars/tempest_skip.yml'],1,30b34879ce3d2b90cdc30bf914dcbca10ea6aaf2,fs020-rbac, - periodic-tripleo-ci-rhel-9-ovb-1ctlr_2comp-featureset020-rbac-internal-rhos-17.1 - periodic-tripleo-ci-rhel-9-ovb-1ctlr_2comp-featureset020-rbac-internal-rhos-17.1,,2,0
openstack%2Fkeystonemiddleware~stable%2F2023.1~I3b8263afbf0ccee88ceaac2040d5ad274f22d74a,openstack/keystonemiddleware,stable/2023.1,I3b8263afbf0ccee88ceaac2040d5ad274f22d74a,Make tox.ini tox 4.0.0 compatible/fix gate,MERGED,2023-06-30 19:25:12.000000000,2023-07-07 15:58:58.000000000,2023-07-07 15:57:55.000000000,"[{'_account_id': 7414}, {'_account_id': 7973}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-30 19:25:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/6ad1c2ab199ac0dcd5464e3a0348fdeca8e183ae', 'message': ""Make tox.ini tox 4.0.0 compatible/fix gate\n\n* Removed skipsdist=True to make sure placement available in the virtual\n  env. Without this, our entrypoints are not available.\n\n* Removed basepython = python3 as we assume all developer switched to\n  python3 in their env already\n\n* Removed ignore_basepython_conflict = True as without the basepython\n  definition generative targets now work without conflict\n\nSee [1] for a similar change made to placement.\n\nIt is also necessary to fix issues with the gate. For reasons that I\nhave yet to grok, a mock of 'requests.request' used in some test is no\nlonger functioning as expected. My guess is that something is now\nimporting requests before us and interfering with the mock but never\nmind - we can easily bypass the issue by mocking 'requests.post'\ninstead.\n\n[1] https://review.opendev.org/c/openstack/placement/+/868418/\n\nChange-Id: I3b8263afbf0ccee88ceaac2040d5ad274f22d74a\nSigned-off-by: Stephen Finucane <stephenfin@redhat.com>\n(cherry picked from commit 626df3a5e9e9d4fbea2649a9cfa0048083bdb88b)\n""}, {'number': 2, 'created': '2023-07-03 11:31:26.000000000', 'files': ['test-requirements.txt', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/keystonemiddleware/commit/ce29fcfb4cc76d890856d96def2e888ffee26fc2', 'message': ""Make tox.ini tox 4.0.0 compatible/fix gate\n\n* Removed skipsdist=True to make sure placement available in the virtual\n  env. Without this, our entrypoints are not available.\n\n* Removed basepython = python3 as we assume all developer switched to\n  python3 in their env already\n\n* Removed ignore_basepython_conflict = True as without the basepython\n  definition generative targets now work without conflict\n\nSee [1] for a similar change made to placement.\n\nIt is also necessary to fix issues with the gate. For reasons that I\nhave yet to grok, a mock of 'requests.request' used in some test is no\nlonger functioning as expected. My guess is that something is now\nimporting requests before us and interfering with the mock but never\nmind - we can easily bypass the issue by mocking 'requests.post'\ninstead.\n\nChanges:\n  keystonemiddleware/tests/unit/test_ec2_token_middleware.py\n  test-requirements.txt\n\nNOTE(stephenfin): It is necessary to revert the test changes since they\ndon't apply to this branch. It is also necessary to cap bandit since we\ncan't fix the timeout errors it is warning about without a feature\nbackport.\n\n[1] https://review.opendev.org/c/openstack/placement/+/868418/\n\nChange-Id: I3b8263afbf0ccee88ceaac2040d5ad274f22d74a\nSigned-off-by: Stephen Finucane <stephenfin@redhat.com>\n(cherry picked from commit 626df3a5e9e9d4fbea2649a9cfa0048083bdb88b)\n""}]",0,887189,ce29fcfb4cc76d890856d96def2e888ffee26fc2,10,4,2,7414,,,0,"Make tox.ini tox 4.0.0 compatible/fix gate

* Removed skipsdist=True to make sure placement available in the virtual
  env. Without this, our entrypoints are not available.

* Removed basepython = python3 as we assume all developer switched to
  python3 in their env already

* Removed ignore_basepython_conflict = True as without the basepython
  definition generative targets now work without conflict

See [1] for a similar change made to placement.

It is also necessary to fix issues with the gate. For reasons that I
have yet to grok, a mock of 'requests.request' used in some test is no
longer functioning as expected. My guess is that something is now
importing requests before us and interfering with the mock but never
mind - we can easily bypass the issue by mocking 'requests.post'
instead.

Changes:
  keystonemiddleware/tests/unit/test_ec2_token_middleware.py
  test-requirements.txt

NOTE(stephenfin): It is necessary to revert the test changes since they
don't apply to this branch. It is also necessary to cap bandit since we
can't fix the timeout errors it is warning about without a feature
backport.

[1] https://review.opendev.org/c/openstack/placement/+/868418/

Change-Id: I3b8263afbf0ccee88ceaac2040d5ad274f22d74a
Signed-off-by: Stephen Finucane <stephenfin@redhat.com>
(cherry picked from commit 626df3a5e9e9d4fbea2649a9cfa0048083bdb88b)
",git fetch https://review.opendev.org/openstack/keystonemiddleware refs/changes/89/887189/1 && git format-patch -1 --stdout FETCH_HEAD,"['keystonemiddleware/tests/unit/test_ec2_token_middleware.py', 'tox.ini']",2,6ad1c2ab199ac0dcd5464e3a0348fdeca8e183ae,tox-4-stable/2023.1,"minversion = 4.2.0 envlist = py3,pep8,releasenotescommands = stestr run {posargs}","minversion = 3.18.0 skipsdist = True envlist = py37,pep8,releasenotes ignore_basepython_conflict = Truecommands = stestr run {posargs} basepython = python3",24,24
openstack%2Fdevstack-plugin-ceph~stable%2F2023.1~I076527536e19f7fa2c0cd177bebb1df22db51a0a,openstack/devstack-plugin-ceph,stable/2023.1,I076527536e19f7fa2c0cd177bebb1df22db51a0a,Enable validation and disable block-migration,NEW,2023-07-07 12:44:38.000000000,2023-07-07 15:48:36.000000000,,"[{'_account_id': 13861}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-07 12:44:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack-plugin-ceph/commit/c46e478e3745aba10735e81bb28a877141215eda', 'message': 'Enable validation and disable block-migration\n\nOn the multinode job we need to enable validation like the base job\nand also disable block migration for live migration that we get from\nour parent job.\n\nChange-Id: I076527536e19f7fa2c0cd177bebb1df22db51a0a\n(cherry picked from commit bf4598d923bebe9ba31e6651a07596017208b37e)\n'}, {'number': 2, 'created': '2023-07-07 13:42:19.000000000', 'files': ['.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/devstack-plugin-ceph/commit/a9a5910e5ded91471496580d0a7d757073b7f9cc', 'message': 'Enable validation and disable block-migration\n\nOn the multinode job we need to enable validation like the base job\nand also disable block migration for live migration that we get from\nour parent job.\n\nChange-Id: I076527536e19f7fa2c0cd177bebb1df22db51a0a\n(cherry picked from commit bf4598d923bebe9ba31e6651a07596017208b37e)\n'}]",3,887879,a9a5910e5ded91471496580d0a7d757073b7f9cc,10,2,2,11604,,,0,"Enable validation and disable block-migration

On the multinode job we need to enable validation like the base job
and also disable block migration for live migration that we get from
our parent job.

Change-Id: I076527536e19f7fa2c0cd177bebb1df22db51a0a
(cherry picked from commit bf4598d923bebe9ba31e6651a07596017208b37e)
",git fetch https://review.opendev.org/openstack/devstack-plugin-ceph refs/changes/79/887879/2 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.yaml'],1,c46e478e3745aba10735e81bb28a877141215eda,bug/2025813, TEMPEST_RUN_VALIDATION: true USE_BLOCK_MIGRATION_FOR_LIVE_MIGRATION: false, TEMPEST_RUN_VALIDATION: false,2,1
openstack%2Fnova-specs~master~I2df34bbf4031384e008cbc642ca15291501dfe53,openstack/nova-specs,master,I2df34bbf4031384e008cbc642ca15291501dfe53,Add support for Napatech LinkVirt SmartNICs,MERGED,2022-09-26 12:41:48.000000000,2023-07-07 15:14:55.000000000,2023-07-07 15:13:46.000000000,"[{'_account_id': 7166}, {'_account_id': 7730}, {'_account_id': 11604}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-09-26 12:41:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/6bfa49d228524e07c722aff46d069cf9fa226b3a', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 2, 'created': '2022-09-26 12:57:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/42df37278c73ec7d12d14620cf08aa8df9f3429d', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 3, 'created': '2022-09-26 13:12:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/6d7c36b6f2e00900274f7d0ce64d9a144f6e6f98', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 4, 'created': '2022-09-28 13:17:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/679d5f2b2f0a9faaa6689462442cd903ebb779dc', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 5, 'created': '2023-01-10 09:59:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/dc09be840df59ccc803f036172772fc1e705f6d7', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 6, 'created': '2023-03-07 08:58:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/efa921b99b9a4a1fac42bef6f1b12866effa0ecb', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 7, 'created': '2023-03-15 12:00:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/63910f42b9b6e61c145511842ae4febf2a9595a6', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 8, 'created': '2023-03-15 12:10:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/1ce288e9c590a481b6282c940853cdc903bcec24', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 9, 'created': '2023-04-11 13:49:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/63e83725eb06ddf7d019f9cb209b4c1081cf90ec', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 10, 'created': '2023-05-05 08:27:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/a95069d093d29e12379368120c1037d484d3eb63', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 11, 'created': '2023-05-08 14:10:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/5760ff2b9537f7d39e703cd612ce2d0b0844aa9a', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 12, 'created': '2023-05-19 13:16:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/cd23fb4acde300f3940e184f89e20c64468b44b2', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 13, 'created': '2023-05-19 13:41:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/50bafb989c9e64fc7ae1691fb38589758ec6663f', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 14, 'created': '2023-05-19 13:43:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/3f842fc98c4af3908d99a24b10d594b159f89930', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 15, 'created': '2023-05-29 09:23:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/882db8abab1b9b4b2b677a89a077cf5a7a10973f', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 16, 'created': '2023-06-01 14:30:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/75d84a974271ca190d58ef593c336c64a7943c7d', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 17, 'created': '2023-06-01 14:53:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/f319a61f9c609de76f340521534ef8931db4a76a', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}, {'number': 18, 'created': '2023-06-01 15:13:23.000000000', 'files': ['specs/2023.2/approved/support-napatech-linkvirtualization-smartnic.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/f62bcfe652dc31e0047fc045b3f25be06bbe8d6a', 'message': 'Add support for Napatech LinkVirt SmartNICs\n\nNapatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.\n\nChange-Id: I2df34bbf4031384e008cbc642ca15291501dfe53\n'}]",87,859290,f62bcfe652dc31e0047fc045b3f25be06bbe8d6a,76,5,18,35239,,,0,"Add support for Napatech LinkVirt SmartNICs

Napatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC.

Change-Id: I2df34bbf4031384e008cbc642ca15291501dfe53
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/90/859290/18 && git format-patch -1 --stdout FETCH_HEAD,['specs/2023.1/approved/napatech-linkvirtualization-smartnic-support.rst'],1,6bfa49d228524e07c722aff46d069cf9fa226b3a,bug/2013540,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Support Napatech LinkVirtualization SmartNICs ========================================== https://blueprints.launchpad.net/nova/+spec/example Napatech LinkVirtualization SmartNICs offload network traffic switching, QoS, and tunnel encapsulation/decapsulation functions from the OVS running on the hypervisor to the on-board silicon. This patch updates the Nova source code to include support for a new VIF type corresponding to the virtual devices exposed by the LinkVirtualization SmartNIC. Problem description =================== Napatech SmartNICs can offload several computational resource intensive tasks from the hypervisor, such as packet switching, QoS enforcement, and V(x)LAN tunnel encapsulation/decapsulation. This patch includes changes to Nova and Os-vif codebases to support Napatech SmartNICs out-of-the-box. This change proposal does not add any new vnic types. The only vnic type supported by Napatech LinkVirtualization is `virtio-forwarder`. Use Cases --------- * An end user of Napatech SmartNIC should be able to run VMs over the hardware-offloaded switch fabric without having to patch OpenStack source code. Proposed change =============== * We propose to add a new VIF type `VIF_TYPE_LV_OVS` and the related VIF handling code to a function `nova_to_osvif_vif()`. * We propose to add a new os-vif `network` property called `network_type`. It is used by the LinkVirtualization ML2 driver. Driver code is open-source and is available online in Github. * We propose unit-tests pertinent to the proposed changes. Alternatives ------------ An alternative is not to support LinkVirtualization adapters and require users to patch OpenStack source code. Data model impact ----------------- None REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- None Performance Impact ------------------ Users will see a significant network performance increase when running over the hardware offloaded data-plane. Other deployer impact --------------------- In line with other SmartNIC offerings, the deployer will have to configure OVS-DPDK following the SmartNIC producer guidelines and update the PCI whitelist configuration. Developer impact ---------------- Noen Upgrade impact -------------- Os-vif change includes code to handle differences object version 1.1 and 1.2 ( addition of `network_type` key.) Implementation ============== Assignee(s) ----------- TBD Feature Liaison --------------- TBD Work Items ---------- * Nova change proposal: https://review.opendev.org/c/openstack/nova/+/XXXXXX * Os-vif change proposal: https://review.opendev.org/c/openstack/os-vif/+/XXXXXX * Neutron-lib change proposal: https://review.opendev.org/c/openstack/neutron-lib/+/XXXXXX Dependencies ============ * This blueprint is a prerequisite to update code in Neutron to support LinkVirtualization SmartNICs. This is in-line with support of other SmartNICs. Links to changes of all four components are given in the Work Items section. Testing ======= Code changes include unit-tests. Documentation Impact ==================== We are not introducing any new VNIC type, so there should be no impact on documentation. References ========== * Napatech LinkVirtualization: https://www.napatech.com/products/link-virtualization-software/ * Napatech OpenStack drivers: https://github.com/napatech/linkvirt-ovs-openstack-plugin History ======= .. list-table:: Revisions :header-rows: 1 * - Release Name - Description * - 2023.1 Antelope - Introduced",,169,0
openstack%2Fironic~master~Ia045e10564dba5ad6d2f90ab75dc4d289c90cf68,openstack/ironic,master,Ia045e10564dba5ad6d2f90ab75dc4d289c90cf68,Remove python 3.6 mock hack,MERGED,2023-06-27 08:40:05.000000000,2023-07-07 14:42:32.000000000,2023-07-07 14:41:12.000000000,"[{'_account_id': 4571}, {'_account_id': 10239}, {'_account_id': 10342}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-27 08:40:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/31355b140ca2791f828be3355b44398171f53481', 'message': ""Remove python 3.6 mock hack\n\nWe don't support Python 3.6 anymore and the function is not\nused anywhere.\n\nChange-Id: Ia045e10564dba5ad6d2f90ab75dc4d289c90cf68\n""}, {'number': 2, 'created': '2023-07-03 09:30:36.000000000', 'files': ['ironic/tests/base.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/e8d6a890f1956844b2ddfbb5d60d2ffa388062da', 'message': ""Remove python 3.6 mock hack\n\nWe don't support Python 3.6 anymore and the function is not\nused anywhere.\n\nChange-Id: Ia045e10564dba5ad6d2f90ab75dc4d289c90cf68\n""}]",5,887023,e8d6a890f1956844b2ddfbb5d60d2ffa388062da,32,4,2,23851,,,0,"Remove python 3.6 mock hack

We don't support Python 3.6 anymore and the function is not
used anywhere.

Change-Id: Ia045e10564dba5ad6d2f90ab75dc4d289c90cf68
",git fetch https://review.opendev.org/openstack/ironic refs/changes/23/887023/1 && git format-patch -1 --stdout FETCH_HEAD,['ironic/tests/base.py'],1,31355b140ca2791f828be3355b44398171f53481,remove-py36hack,,"from unittest import mock# NOTE(rpittau) this function allows autospec for classmethods and # staticmethods in Python 3.6, while no issue occurs in Python 3.7.4 # and later. # For more info please see: http://bugs.python.org/issue23078 def _patch_mock_callable(obj): if isinstance(obj, type): return True if getattr(obj, '__call__', None) is not None: return True if (isinstance(obj, (staticmethod, classmethod)) and mock._callable(obj.__func__)): return True return False ",0,16
openstack%2Freleases~master~I0bee8a1e7285d971dc7c6ca5d09f25271848d993,openstack/releases,master,I0bee8a1e7285d971dc7c6ca5d09f25271848d993,Bobcat-2 release for keystonemiddleware,MERGED,2023-07-03 09:31:29.000000000,2023-07-07 14:33:13.000000000,2023-07-07 14:33:13.000000000,"[{'_account_id': 7414}, {'_account_id': 16465}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-03 09:31:29.000000000', 'files': ['deliverables/bobcat/keystonemiddleware.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/bc54b2946e61cb7265fba73d78bb82d82ec6b269', 'message': 'Bobcat-2 release for keystonemiddleware\n\nThis is the Bobcat-2 milestone release for keystonemiddleware.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: I0bee8a1e7285d971dc7c6ca5d09f25271848d993\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",2,887483,bc54b2946e61cb7265fba73d78bb82d82ec6b269,8,5,1,17685,,,0,"Bobcat-2 release for keystonemiddleware

This is the Bobcat-2 milestone release for keystonemiddleware.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: I0bee8a1e7285d971dc7c6ca5d09f25271848d993
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/83/887483/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/keystonemiddleware.yaml'],1,bc54b2946e61cb7265fba73d78bb82d82ec6b269,bobcat-milestone-2, - version: 10.4.0 projects: - repo: openstack/keystonemiddleware hash: 22408f8da0e8a14a1a24dc9237448e78a5673cf9,,4,0
openstack%2Freleases~master~I15a4e9c8d036d2942daf2d749bd430643d0d0dcc,openstack/releases,master,I15a4e9c8d036d2942daf2d749bd430643d0d0dcc,Bobcat-2 release for os-vif,MERGED,2023-07-03 09:38:02.000000000,2023-07-07 14:24:54.000000000,2023-07-07 14:24:54.000000000,"[{'_account_id': 7166}, {'_account_id': 11604}, {'_account_id': 16207}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 28522}, {'_account_id': 34860}]","[{'number': 1, 'created': '2023-07-03 09:38:02.000000000', 'files': ['deliverables/bobcat/os-vif.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/541bbfccaa8af4bffb15c7a82598bf9010d97f9d', 'message': 'Bobcat-2 release for os-vif\n\nThis is the Bobcat-2 milestone release for os-vif.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: I15a4e9c8d036d2942daf2d749bd430643d0d0dcc\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",6,887486,541bbfccaa8af4bffb15c7a82598bf9010d97f9d,12,7,1,17685,,,0,"Bobcat-2 release for os-vif

This is the Bobcat-2 milestone release for os-vif.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: I15a4e9c8d036d2942daf2d749bd430643d0d0dcc
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/86/887486/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/os-vif.yaml'],1,541bbfccaa8af4bffb15c7a82598bf9010d97f9d,bobcat-milestone-2,releases: - version: 3.2.0 projects: - repo: openstack/os-vif hash: da742a849a1b8b2f3ca9485b38c5bb54c1bb6c75,,5,0
openstack%2Freleases~master~I327a1884a1c93ece4fe5d7531d9fe4a6db3bd742,openstack/releases,master,I327a1884a1c93ece4fe5d7531d9fe4a6db3bd742,Bobcat-2 release for python-glanceclient,MERGED,2023-07-03 10:02:26.000000000,2023-07-07 14:24:52.000000000,2023-07-07 14:24:52.000000000,"[{'_account_id': 17685}, {'_account_id': 19138}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-03 10:02:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/b711f17752f682fb63e52866429a4593f3577d1d', 'message': 'Bobcat-2 release for python-glanceclient\n\nThis is the Bobcat-2 milestone release for python-glanceclient.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: I327a1884a1c93ece4fe5d7531d9fe4a6db3bd742\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}, {'number': 2, 'created': '2023-07-06 10:51:53.000000000', 'files': ['deliverables/bobcat/python-glanceclient.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/8d8f40a75fec9e22e81be71b84a93f7ccf9cd901', 'message': 'Bobcat-2 release for python-glanceclient\n\nThis is the Bobcat-2 milestone release for python-glanceclient.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: I327a1884a1c93ece4fe5d7531d9fe4a6db3bd742\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",3,887496,8d8f40a75fec9e22e81be71b84a93f7ccf9cd901,12,4,2,17685,,,0,"Bobcat-2 release for python-glanceclient

This is the Bobcat-2 milestone release for python-glanceclient.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: I327a1884a1c93ece4fe5d7531d9fe4a6db3bd742
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/96/887496/2 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/python-glanceclient.yaml'],1,b711f17752f682fb63e52866429a4593f3577d1d,bobcat-milestone-2,releases: - version: 4.4.0 projects: - repo: openstack/python-glanceclient hash: f53d6714fda94b22517489dfeb72bc2874386ebb,,5,0
openstack%2Fbarbican~master~I984ca060e7c65e8b9374eaaf192afc0023fa9262,openstack/barbican,master,I984ca060e7c65e8b9374eaaf192afc0023fa9262,Add support for Vault KV path,NEW,2023-01-05 16:22:41.000000000,2023-07-07 14:23:01.000000000,,"[{'_account_id': 7973}, {'_account_id': 10342}, {'_account_id': 14250}, {'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 29543}]","[{'number': 1, 'created': '2023-01-05 16:22:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/5843c5bc2b5cffc5b5d5f41546631267ac1d1e63', 'message': 'Add support for Vault KV path\n\nThis commit adds support for a Vault path that is relative to\nthe root of the Vault KV store. This configuration is optional\nand will be a noop for existing deployments.\n\nDepends-On: If34c38c8f0a2f13ea90f564bfe5e933e5e748da4\nChange-Id: I984ca060e7c65e8b9374eaaf192afc0023fa9262\n'}, {'number': 2, 'created': '2023-01-06 12:08:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/74aaa37c7ff8e9d7d14cbbb419b315ef757b9634', 'message': 'Add support for Vault KV path\n\nThis commit adds support for a Vault path that is relative to\nthe root of the Vault KV store. This configuration is optional\nand will be a noop for existing deployments.\n\nDepends-On: If34c38c8f0a2f13ea90f564bfe5e933e5e748da4\nChange-Id: I984ca060e7c65e8b9374eaaf192afc0023fa9262\n'}, {'number': 3, 'created': '2023-01-16 22:38:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/636a99e3a055fb00e1bb905fa895d80414312f03', 'message': 'Add support for Vault KV path\n\nThis commit adds support for a Vault path that is relative to\nthe root of the Vault KV store. This configuration is optional\nand will be a noop for existing deployments.\n\nDepends-On: If34c38c8f0a2f13ea90f564bfe5e933e5e748da4\nChange-Id: I984ca060e7c65e8b9374eaaf192afc0023fa9262\n'}, {'number': 4, 'created': '2023-01-17 18:40:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/1ac52c15946fc14c5c570ce561037d717d606088', 'message': 'Add support for Vault KV path\n\nThis commit adds support for a Vault path that is relative to\nthe root of the Vault KV store. This configuration is optional\nand will be a noop for existing deployments.\n\nDepends-On: https://review.opendev.org/c/openstack/castellan/+/869386\nChange-Id: I984ca060e7c65e8b9374eaaf192afc0023fa9262\n'}, {'number': 5, 'created': '2023-06-30 15:51:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/004079cce9cc9c68fcfc8949ada24799355a4dc1', 'message': 'Add support for Vault KV path\n\nThis commit adds support for a Vault path that is relative to\nthe root of the Vault KV store. This configuration is optional\nand will be a noop for existing deployments.\n\nChange-Id: I984ca060e7c65e8b9374eaaf192afc0023fa9262\n'}, {'number': 6, 'created': '2023-06-30 15:51:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/f2a389c4b65ed0aff28c98ce12fac08323e57edb', 'message': 'Add support for Vault KV path\n\nThis commit adds support for a Vault path that is relative to\nthe root of the Vault KV store. This configuration is optional\nand will be a noop for existing deployments.\n\nChange-Id: I984ca060e7c65e8b9374eaaf192afc0023fa9262\n'}, {'number': 7, 'created': '2023-07-01 03:46:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/836622951283ef04463f2452db35a534022db8c7', 'message': 'Add support for Vault KV path\n\nThis commit adds support for a Vault path that is relative to\nthe root of the Vault KV store. This configuration is optional\nand will be a noop for existing deployments.\n\nChange-Id: I984ca060e7c65e8b9374eaaf192afc0023fa9262\nDepends-On: https://review.opendev.org/c/openstack/barbican/+/887439\n'}, {'number': 8, 'created': '2023-07-01 04:51:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/821998720757dde276e6a8b4e0fc2041c876d1f7', 'message': 'Add support for Vault KV path\n\nThis commit adds support for a Vault path that is relative to\nthe root of the Vault KV store. This configuration is optional\nand will be a noop for existing deployments.\n\nRaises required version of castellan to 4.2.0, which added vault\nkv support.\n\nChange-Id: I984ca060e7c65e8b9374eaaf192afc0023fa9262\n'}, {'number': 9, 'created': '2023-07-01 14:38:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/9bd36d04ad7d7f50eee9d4b23bcc5706bad9dfe1', 'message': 'Add support for Vault KV path\n\nThis commit adds support for a Vault path that is relative to\nthe root of the Vault KV store. This configuration is optional\nand will be a noop for existing deployments.\n\nRaises required version of castellan to 4.2.0, which added vault\nkv support.\n\nChange-Id: I984ca060e7c65e8b9374eaaf192afc0023fa9262\n'}, {'number': 10, 'created': '2023-07-01 16:27:44.000000000', 'files': ['requirements.txt', 'barbican/plugin/vault_secret_store.py'], 'web_link': 'https://opendev.org/openstack/barbican/commit/3bd86424cc20fd0605148a8e95531af3a47aa1da', 'message': 'Add support for Vault KV path\n\nThis commit adds support for a Vault path that is relative to\nthe root of the Vault KV store. This configuration is optional\nand will be a noop for existing deployments.\n\nRaises required version of castellan to 4.2.0, which added vault\nkv support.\n\nDepends-On: https://review.opendev.org/c/openstack/requirements/+/887446\nChange-Id: I984ca060e7c65e8b9374eaaf192afc0023fa9262\n'}]",10,869387,3bd86424cc20fd0605148a8e95531af3a47aa1da,34,6,10,29543,,,0,"Add support for Vault KV path

This commit adds support for a Vault path that is relative to
the root of the Vault KV store. This configuration is optional
and will be a noop for existing deployments.

Raises required version of castellan to 4.2.0, which added vault
kv support.

Depends-On: https://review.opendev.org/c/openstack/requirements/+/887446
Change-Id: I984ca060e7c65e8b9374eaaf192afc0023fa9262
",git fetch https://review.opendev.org/openstack/barbican refs/changes/87/869387/9 && git format-patch -1 --stdout FETCH_HEAD,['barbican/plugin/vault_secret_store.py'],1,5843c5bc2b5cffc5b5d5f41546631267ac1d1e63,hash-vault-kv-path," cfg.StrOpt('kv_path', help='Path relative to root of KV store in Vault to use.') vault_kv_path=conf.vault_plugin.kv_path,",,3,0
openstack%2Freleases~master~I6341099f2493590008bbb9a14d4e5a858dc4aa27,openstack/releases,master,I6341099f2493590008bbb9a14d4e5a858dc4aa27,Remove networking-odl from Bobcat release,MERGED,2023-07-03 10:42:58.000000000,2023-07-07 14:11:46.000000000,2023-07-07 14:11:46.000000000,"[{'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-03 10:42:58.000000000', 'files': ['deliverables/bobcat/networking-odl.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/4bf493a7f2c75cb6376986f06c90aaff46c405de', 'message': 'Remove networking-odl from Bobcat release\n\nnetworking-odl was deprecated May 2023, and should not be included in\nthe 2023.1 Bobcat final release.\n\nChange-Id: I6341099f2493590008bbb9a14d4e5a858dc4aa27\n'}]",1,887504,4bf493a7f2c75cb6376986f06c90aaff46c405de,8,5,1,308,,,0,"Remove networking-odl from Bobcat release

networking-odl was deprecated May 2023, and should not be included in
the 2023.1 Bobcat final release.

Change-Id: I6341099f2493590008bbb9a14d4e5a858dc4aa27
",git fetch https://review.opendev.org/openstack/releases refs/changes/04/887504/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/networking-odl.yaml'],1,4bf493a7f2c75cb6376986f06c90aaff46c405de,remove-networking-odl,,--- launchpad: networking-odl team: neutron type: other release-model: cycle-with-rc release-type: neutron include-pypi-link: true repository-settings: openstack/networking-odl: {} ,0,9
openstack%2Fneutron~stable%2F2023.1~I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265,openstack/neutron,stable/2023.1,I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265,Don't allow deletion of the router ports without IP addresses,MERGED,2023-07-04 14:48:08.000000000,2023-07-07 14:03:47.000000000,2023-07-07 14:02:32.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 14:48:08.000000000', 'files': ['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/854613167419984f227855b8a7a0589747ea6764', 'message': ""Don't allow deletion of the router ports without IP addresses\n\nThis patch effectively reverts old patch [1]. From now on it will be not\nallowed to directly remove router ports which don't have fixed IPs\nassigned. Such ports will be treated as any other ports connected to the\nrouters.\nOriginally [1] was introduced to allow cleanup of the router ports for\nwhich subnets were deleted. But now it's not needed anymore as we\nprevent deletion of subnet if there are any ports with IP allocated from\nthat subnet.\n\nCloses-bug: #2025056\n\n[1] https://review.opendev.org/c/openstack/neutron/+/20424\n\nChange-Id: I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265\n(cherry picked from commit 32d589f03ed0d2744fe15173ebacafd18fced8a9)\n""}]",4,887614,854613167419984f227855b8a7a0589747ea6764,18,3,1,11975,,,0,"Don't allow deletion of the router ports without IP addresses

This patch effectively reverts old patch [1]. From now on it will be not
allowed to directly remove router ports which don't have fixed IPs
assigned. Such ports will be treated as any other ports connected to the
routers.
Originally [1] was introduced to allow cleanup of the router ports for
which subnets were deleted. But now it's not needed anymore as we
prevent deletion of subnet if there are any ports with IP allocated from
that subnet.

Closes-bug: #2025056

[1] https://review.opendev.org/c/openstack/neutron/+/20424

Change-Id: I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265
(cherry picked from commit 32d589f03ed0d2744fe15173ebacafd18fced8a9)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/14/887614/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py']",2,854613167419984f227855b8a7a0589747ea6764,bug/2025056-stable/2023.1," 'device_id': '44', 'id': 'f', } with testtools.ExpectedException(n_exc.ServicePortInUse): self.db.prevent_l3_port_deletion(mock.Mock(), None)"," 'id': 'f' } self.db.prevent_l3_port_deletion(None, None)",3,11
openstack%2Freleases~master~I92e9df123756a849c0f37f50717d51be580afe44,openstack/releases,master,I92e9df123756a849c0f37f50717d51be580afe44,[oslo] Transition Rocky to End of Life,MERGED,2023-07-07 12:06:24.000000000,2023-07-07 13:31:29.000000000,2023-07-07 13:31:29.000000000,"[{'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 28522}, {'_account_id': 31245}]","[{'number': 1, 'created': '2023-07-07 12:06:24.000000000', 'files': ['deliverables/rocky/devstack-plugin-kafka.yaml', 'deliverables/rocky/devstack-plugin-amqp1.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/3d32a7f22c90199965c0538cc88a5e57f869e196', 'message': ""[oslo] Transition Rocky to End of Life\n\nAs discussed in the mail thread [1][2], this patch transition the Rocky\nbranch to End of Life. The last patch of the branch will be tagged with\nrocky-eol tag. stable/rocky branch cannot be used anymore and will be\ndeleted if this patch merges.\n\nThis is needed as stable/rocky is not actively maintained in recent\nperiod, thus gates are mostly broken due to job failures. Besides,\nby removing these branches, infra resources will be freed up, too.\n\nPlease try to identify any zuul job, that is defined outside of the\nrepositories in this patch (for example in openstack-zuul-jobs, etc.)\nand won't be used anymore if stable/rocky is deleted. Propose a job\nremoval patch for them.\n\nPlease +1 if the team is ready for us to proceed with this transition,\nor -1 if there are still some activity on the branch and the team wants\nto continue to maintain it.\n\n[1] https://lists.openstack.org/pipermail/openstack-discuss/2023-January/031922.html\n[2] https://lists.openstack.org/pipermail/openstack-discuss/2023-April/033386.html\n\nChange-Id: I92e9df123756a849c0f37f50717d51be580afe44\n""}]",1,887946,3d32a7f22c90199965c0538cc88a5e57f869e196,8,4,1,17685,,,0,"[oslo] Transition Rocky to End of Life

As discussed in the mail thread [1][2], this patch transition the Rocky
branch to End of Life. The last patch of the branch will be tagged with
rocky-eol tag. stable/rocky branch cannot be used anymore and will be
deleted if this patch merges.

This is needed as stable/rocky is not actively maintained in recent
period, thus gates are mostly broken due to job failures. Besides,
by removing these branches, infra resources will be freed up, too.

Please try to identify any zuul job, that is defined outside of the
repositories in this patch (for example in openstack-zuul-jobs, etc.)
and won't be used anymore if stable/rocky is deleted. Propose a job
removal patch for them.

Please +1 if the team is ready for us to proceed with this transition,
or -1 if there are still some activity on the branch and the team wants
to continue to maintain it.

[1] https://lists.openstack.org/pipermail/openstack-discuss/2023-January/031922.html
[2] https://lists.openstack.org/pipermail/openstack-discuss/2023-April/033386.html

Change-Id: I92e9df123756a849c0f37f50717d51be580afe44
",git fetch https://review.opendev.org/openstack/releases refs/changes/46/887946/1 && git format-patch -1 --stdout FETCH_HEAD,"['deliverables/rocky/devstack-plugin-kafka.yaml', 'deliverables/rocky/devstack-plugin-amqp1.yaml']",2,3d32a7f22c90199965c0538cc88a5e57f869e196,rocky-eol,releases: - version: rocky-eol projects: - repo: openstack/devstack-plugin-amqp1 hash: cf07f5f9c54e60fc49ca05057e3dee4ca6e76581,,10,0
openstack%2Freleases~master~I2b2a9c82d68d65dbb0fb270887ace4ff0a66febd,openstack/releases,master,I2b2a9c82d68d65dbb0fb270887ace4ff0a66febd,[Packaging-rpm] Transition Rocky to End of Life,MERGED,2023-07-07 12:06:15.000000000,2023-07-07 13:17:55.000000000,2023-07-07 13:17:55.000000000,"[{'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-07 12:06:15.000000000', 'files': ['deliverables/rocky/rpm-packaging.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/907ead1c5c38d269ea57ce4706013a8756c26ba5', 'message': ""[Packaging-rpm] Transition Rocky to End of Life\n\nAs discussed in the mail thread [1][2], this patch transition the Rocky\nbranch to End of Life. The last patch of the branch will be tagged with\nrocky-eol tag. stable/rocky branch cannot be used anymore and will be\ndeleted if this patch merges.\n\nThis is needed as stable/rocky is not actively maintained in recent\nperiod, thus gates are mostly broken due to job failures. Besides,\nby removing these branches, infra resources will be freed up, too.\n\nPlease try to identify any zuul job, that is defined outside of the\nrepositories in this patch (for example in openstack-zuul-jobs, etc.)\nand won't be used anymore if stable/rocky is deleted. Propose a job\nremoval patch for them.\n\nPlease +1 if the team is ready for us to proceed with this transition,\nor -1 if there are still some activity on the branch and the team wants\nto continue to maintain it.\n\n[1] https://lists.openstack.org/pipermail/openstack-discuss/2023-January/031922.html\n[2] https://lists.openstack.org/pipermail/openstack-discuss/2023-April/033386.html\n\nChange-Id: I2b2a9c82d68d65dbb0fb270887ace4ff0a66febd\n""}]",1,887945,907ead1c5c38d269ea57ce4706013a8756c26ba5,7,3,1,17685,,,0,"[Packaging-rpm] Transition Rocky to End of Life

As discussed in the mail thread [1][2], this patch transition the Rocky
branch to End of Life. The last patch of the branch will be tagged with
rocky-eol tag. stable/rocky branch cannot be used anymore and will be
deleted if this patch merges.

This is needed as stable/rocky is not actively maintained in recent
period, thus gates are mostly broken due to job failures. Besides,
by removing these branches, infra resources will be freed up, too.

Please try to identify any zuul job, that is defined outside of the
repositories in this patch (for example in openstack-zuul-jobs, etc.)
and won't be used anymore if stable/rocky is deleted. Propose a job
removal patch for them.

Please +1 if the team is ready for us to proceed with this transition,
or -1 if there are still some activity on the branch and the team wants
to continue to maintain it.

[1] https://lists.openstack.org/pipermail/openstack-discuss/2023-January/031922.html
[2] https://lists.openstack.org/pipermail/openstack-discuss/2023-April/033386.html

Change-Id: I2b2a9c82d68d65dbb0fb270887ace4ff0a66febd
",git fetch https://review.opendev.org/openstack/releases refs/changes/45/887945/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/rocky/rpm-packaging.yaml'],1,907ead1c5c38d269ea57ce4706013a8756c26ba5,rocky-eol,team: Packaging-rpmreleases: - version: rocky-eol projects: - repo: openstack/rpm-packaging hash: 2a488d9bd16c98665573e55a72b04ffe12dd99da,team: 'Packaging-rpm',6,1
openstack%2Fovn-octavia-provider~stable%2Fzed~Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd,openstack/ovn-octavia-provider,stable/zed,Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd,Ensure DVR is restablished on member on cascade deletion,MERGED,2023-07-06 10:56:26.000000000,2023-07-07 13:08:14.000000000,2023-07-07 13:07:16.000000000,"[{'_account_id': 6773}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 34451}]","[{'number': 1, 'created': '2023-07-06 10:56:26.000000000', 'files': ['ovn_octavia_provider/tests/unit/test_driver.py', 'ovn_octavia_provider/helper.py', 'ovn_octavia_provider/driver.py', 'ovn_octavia_provider/tests/unit/test_helper.py'], 'web_link': 'https://opendev.org/openstack/ovn-octavia-provider/commit/d7040c43703f0b49ae78d2ecfd904dd54bbc7152', 'message': 'Ensure DVR is restablished on member on cascade deletion\n\nTraffic to member, if they have FIPs gets centralized when they\nare part of a loadbalancer. However, when the loadbalancer gets\ndeleted, the traffic should be distributed again (if DVR was\nenabled). To do that this patch also considers the cascade deletion\n\nCloses-Bug: #2025637\nChange-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd\n(cherry picked from commit 20997b185f21aac8959d7517b4ffb7bc44c1b76a)\n'}]",3,887686,d7040c43703f0b49ae78d2ecfd904dd54bbc7152,17,4,1,23567,,,0,"Ensure DVR is restablished on member on cascade deletion

Traffic to member, if they have FIPs gets centralized when they
are part of a loadbalancer. However, when the loadbalancer gets
deleted, the traffic should be distributed again (if DVR was
enabled). To do that this patch also considers the cascade deletion

Closes-Bug: #2025637
Change-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd
(cherry picked from commit 20997b185f21aac8959d7517b4ffb7bc44c1b76a)
",git fetch https://review.opendev.org/openstack/ovn-octavia-provider refs/changes/86/887686/1 && git format-patch -1 --stdout FETCH_HEAD,"['ovn_octavia_provider/tests/unit/test_driver.py', 'ovn_octavia_provider/helper.py', 'ovn_octavia_provider/driver.py', 'ovn_octavia_provider/tests/unit/test_helper.py']",4,d7040c43703f0b49ae78d2ecfd904dd54bbc7152,," @mock.patch('ovn_octavia_provider.common.clients.get_neutron_client') def test_lb_delete_port_exception(self, del_port, net_cli):"," def test_lb_delete_port_exception(self, del_port):",37,4
openstack%2Fdesignate~stable%2F2023.1~I04c104cfc9ee2f03d0c5adca3c80bbfff20afb70,openstack/designate,stable/2023.1,I04c104cfc9ee2f03d0c5adca3c80bbfff20afb70,Fix TsigKeyring issues with dnspython 2.x,MERGED,2023-07-01 07:34:06.000000000,2023-07-07 13:07:56.000000000,2023-07-07 13:06:48.000000000,"[{'_account_id': 11628}, {'_account_id': 13252}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-01 07:34:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/designate/commit/aab7a1e61c58d341b5c816aedc58972b11e6a607', 'message': 'Fix TsigKeyring issues with dnspython 2.x\n\n- Fixed issues in TsigKeyring.\n- Fixed tsgi issues in mdns handler.\n- Fixed invalid secret used in tests.\n- Added additional test coverage.\n- Re-enabled previously broken test.\n\nCloses-Bug: #1982252\nChange-Id: I04c104cfc9ee2f03d0c5adca3c80bbfff20afb70\n(cherry picked from commit 38c591eaa1109e7ea4600563e505dbdbe8a59b37)\n'}, {'number': 2, 'created': '2023-07-01 16:05:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/designate/commit/158130bd713a31cc82f633a937b97acd3e3fd749', 'message': 'Fix TsigKeyring issues with dnspython 2.x\n\n- Fixed issues in TsigKeyring.\n- Fixed tsgi issues in mdns handler.\n- Fixed invalid secret used in tests.\n- Added additional test coverage.\n- Re-enabled previously broken test.\n\nCloses-Bug: #1982252\nChange-Id: I04c104cfc9ee2f03d0c5adca3c80bbfff20afb70\n(cherry picked from commit 38c591eaa1109e7ea4600563e505dbdbe8a59b37)\n'}, {'number': 3, 'created': '2023-07-02 09:49:09.000000000', 'files': ['designate/tests/__init__.py', 'designate/mdns/handler.py', 'designate/dnsutils.py', 'designate/tests/test_dnsutils.py', 'designate/tests/test_mdns/test_handler.py'], 'web_link': 'https://opendev.org/openstack/designate/commit/bc87a7daac78f973d945aef6fee6a55ac46c5ce4', 'message': 'Fix TsigKeyring issues with dnspython 2.x\n\n- Fixed issues in TsigKeyring.\n- Fixed tsgi issues in mdns handler.\n- Fixed invalid secret used in tests.\n- Added additional test coverage.\n- Re-enabled previously broken test.\n\nAdditionally modified unit test to provide a storage provider,\nas this does not exist in the next release.\n\nCloses-Bug: #1982252\nChange-Id: I04c104cfc9ee2f03d0c5adca3c80bbfff20afb70\n(cherry picked from commit 38c591eaa1109e7ea4600563e505dbdbe8a59b37)\n'}]",5,887450,bc87a7daac78f973d945aef6fee6a55ac46c5ce4,20,3,3,13252,,,0,"Fix TsigKeyring issues with dnspython 2.x

- Fixed issues in TsigKeyring.
- Fixed tsgi issues in mdns handler.
- Fixed invalid secret used in tests.
- Added additional test coverage.
- Re-enabled previously broken test.

Additionally modified unit test to provide a storage provider,
as this does not exist in the next release.

Closes-Bug: #1982252
Change-Id: I04c104cfc9ee2f03d0c5adca3c80bbfff20afb70
(cherry picked from commit 38c591eaa1109e7ea4600563e505dbdbe8a59b37)
",git fetch https://review.opendev.org/openstack/designate refs/changes/50/887450/2 && git format-patch -1 --stdout FETCH_HEAD,"['designate/tests/__init__.py', 'designate/mdns/handler.py', 'designate/dnsutils.py', 'designate/tests/test_dnsutils.py', 'designate/tests/test_mdns/test_handler.py']",5,aab7a1e61c58d341b5c816aedc58972b11e6a607,,"import dns.tsigkeyring request.use_tsig(dns.tsigkeyring.from_text( {'test-key-two': 'AnotherSecretKey'}) ) args = [request.keyname, request.keyring.secret, 300, request.id, request.tsig_error, b'', request.mac, request.keyalgorithm]","from unittest import expectedFailure @expectedFailure request.keyring = {request.keyname: ''} request.had_tsig = True args = [request.keyname, request.keyring[request.keyname], request.fudge, request.original_id, request.tsig_error, request.other_data, request.mac, request.keyalgorithm]",99,16
openstack%2Fpython-openstackclient~master~I1c18c2ec5eb4923e1ab8b3fc6199ef6f329b4a4d,openstack/python-openstackclient,master,I1c18c2ec5eb4923e1ab8b3fc6199ef6f329b4a4d,Add support for default security group rule CRUDs,NEW,2023-06-30 10:13:40.000000000,2023-07-07 12:37:40.000000000,,"[{'_account_id': 8313}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-30 10:13:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/2aa6b2115161f51ef48f6f9972217832fc5267a4', 'message': 'Add support for default security group rule CRUDs\n\nDepends-On: https://review.opendev.org/c/openstack/openstacksdk/+/887262\nChange-Id: I1c18c2ec5eb4923e1ab8b3fc6199ef6f329b4a4d\n'}, {'number': 2, 'created': '2023-06-30 10:27:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/f7a68898b1482e86f747f00ddcc16f0996111004', 'message': 'Add support for default security group rule CRUDs\n\nDepends-On: https://review.opendev.org/c/openstack/openstacksdk/+/887262\nChange-Id: I1c18c2ec5eb4923e1ab8b3fc6199ef6f329b4a4d\n'}, {'number': 3, 'created': '2023-07-07 08:52:55.000000000', 'files': ['openstackclient/tests/functional/network/v2/test_default_security_group_rule.py', 'openstackclient/network/utils.py', 'openstackclient/tests/unit/network/v2/test_security_group_rule_network.py', 'openstackclient/tests/unit/network/v2/test_security_group_rule_compute.py', 'doc/source/cli/command-objects/default-security-group-rule.rst', 'openstackclient/tests/unit/network/v2/fakes.py', 'openstackclient/network/v2/default_security_group_rule.py', 'releasenotes/notes/Add-default-security-group-rule-CRUD-2916568f829ea38c.yaml', 'setup.cfg', 'openstackclient/network/v2/security_group_rule.py', 'openstackclient/tests/unit/network/v2/test_default_security_group_rule.py'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/ee7f29f1ca38891cfb475c2043d88472404cce16', 'message': 'Add support for default security group rule CRUDs\n\nDepends-On: https://review.opendev.org/c/openstack/openstacksdk/+/887262\nChange-Id: I1c18c2ec5eb4923e1ab8b3fc6199ef6f329b4a4d\n'}]",1,887364,ee7f29f1ca38891cfb475c2043d88472404cce16,6,3,3,11975,,,0,"Add support for default security group rule CRUDs

Depends-On: https://review.opendev.org/c/openstack/openstacksdk/+/887262
Change-Id: I1c18c2ec5eb4923e1ab8b3fc6199ef6f329b4a4d
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/64/887364/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstackclient/tests/functional/network/v2/test_default_security_group_rule.py', 'openstackclient/network/utils.py', 'openstackclient/tests/unit/network/v2/test_security_group_rule_network.py', 'openstackclient/tests/unit/network/v2/test_security_group_rule_compute.py', 'doc/source/cli/command-objects/default-security-group-rule.rst', 'openstackclient/tests/unit/network/v2/fakes.py', 'openstackclient/network/v2/default_security_group_rule.py', 'releasenotes/notes/Add-default-security-group-rule-CRUD-2916568f829ea38c.yaml', 'setup.cfg', 'openstackclient/network/v2/security_group_rule.py', 'openstackclient/tests/unit/network/v2/test_default_security_group_rule.py']",11,2aa6b2115161f51ef48f6f9972217832fc5267a4,bug/1983053,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # from unittest import mock from unittest.mock import call from osc_lib import exceptions from openstackclient.network import utils as network_utils from openstackclient.network.v2 import default_security_group_rule from openstackclient.tests.unit.identity.v3 import fakes as identity_fakes from openstackclient.tests.unit.network.v2 import fakes as network_fakes from openstackclient.tests.unit import utils as tests_utils class TestDefaultSecurityGroupRule(network_fakes.TestNetworkV2): def setUp(self): super(TestDefaultSecurityGroupRule, self).setUp() # Get a shortcut to the network client self.network = self.app.client_manager.network class TestCreateDefaultSecurityGroupRule(TestDefaultSecurityGroupRule): expected_columns = ( 'description', 'direction', 'ether_type', 'id', 'port_range_max', 'port_range_min', 'protocol', 'remote_address_group_id', 'remote_group_id', 'remote_ip_prefix', 'used_in_default_sg', 'used_in_non_default_sg', ) expected_data = None def _setup_default_security_group_rule(self, attrs=None): self._default_sg_rule = ( network_fakes.FakeDefaultSecurityGroupRule. create_one_default_security_group_rule(attrs) ) self.network.create_default_security_group_rule = mock.Mock( return_value=self._default_sg_rule ) self.expected_data = ( self._default_sg_rule.description, self._default_sg_rule.direction, self._default_sg_rule.ether_type, self._default_sg_rule.id, self._default_sg_rule.port_range_max, self._default_sg_rule.port_range_min, self._default_sg_rule.protocol, self._default_sg_rule.remote_address_group_id, self._default_sg_rule.remote_group_id, self._default_sg_rule.remote_ip_prefix, self._default_sg_rule.used_in_default_sg, self._default_sg_rule.used_in_non_default_sg, ) def setUp(self): super(TestCreateDefaultSecurityGroupRule, self).setUp() # Get the command object to test self.cmd = default_security_group_rule.CreateDefaultSecurityGroupRule( self.app, self.namespace ) def test_create_all_remote_options(self): arglist = [ '--remote-ip', '10.10.0.0/24', '--remote-group', 'test-remote-group-id', '--remote-address-group', 'test-remote-address-group-id', ] self.assertRaises( tests_utils.ParserException, self.check_parser, self.cmd, arglist, [], ) def test_create_bad_ethertype(self): arglist = [ '--ethertype', 'foo', ] self.assertRaises( tests_utils.ParserException, self.check_parser, self.cmd, arglist, [], ) def test_lowercase_ethertype(self): arglist = [ '--ethertype', 'ipv4', ] parsed_args = self.check_parser(self.cmd, arglist, []) self.assertEqual('IPv4', parsed_args.ethertype) def test_lowercase_v6_ethertype(self): arglist = [ '--ethertype', 'ipv6', ] parsed_args = self.check_parser(self.cmd, arglist, []) self.assertEqual('IPv6', parsed_args.ethertype) def test_proper_case_ethertype(self): arglist = [ '--ethertype', 'IPv6', ] parsed_args = self.check_parser(self.cmd, arglist, []) self.assertEqual('IPv6', parsed_args.ethertype) def test_create_all_port_range_options(self): arglist = [ '--dst-port', '80:80', '--icmp-type', '3', '--icmp-code', '1', ] verifylist = [ ('dst_port', (80, 80)), ('icmp_type', 3), ('icmp_code', 1), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) self.assertRaises( exceptions.CommandError, self.cmd.take_action, parsed_args ) def test_create_default_rule(self): self._setup_default_security_group_rule( { 'protocol': 'tcp', 'port_range_max': 443, 'port_range_min': 443, } ) arglist = [ '--protocol', 'tcp', '--dst-port', str(self._default_sg_rule.port_range_min), ] verifylist = [ ( 'dst_port', ( self._default_sg_rule.port_range_min, self._default_sg_rule.port_range_max, ), ), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'port_range_max': self._default_sg_rule.port_range_max, 'port_range_min': self._default_sg_rule.port_range_min, 'protocol': self._default_sg_rule.protocol, 'remote_ip_prefix': self._default_sg_rule.remote_ip_prefix, 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_protocol_any(self): self._setup_default_security_group_rule( { 'protocol': None, 'remote_ip_prefix': '10.0.2.0/24', } ) arglist = [ '--protocol', 'any', '--remote-ip', self._default_sg_rule.remote_ip_prefix, ] verifylist = [ ('protocol', 'any'), ('remote_ip', self._default_sg_rule.remote_ip_prefix), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'protocol': self._default_sg_rule.protocol, 'remote_ip_prefix': self._default_sg_rule.remote_ip_prefix, 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_remote_address_group(self): self._setup_default_security_group_rule( { 'protocol': 'icmp', 'remote_address_group_id': 'remote-address-group-id', } ) arglist = [ '--protocol', 'icmp', '--remote-address-group', self._default_sg_rule.remote_address_group_id, ] verifylist = [ ('remote_address_group', self._default_sg_rule.remote_address_group_id), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'protocol': self._default_sg_rule.protocol, 'remote_address_group_id': self._default_sg_rule.remote_address_group_id, 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_remote_group(self): self._setup_default_security_group_rule( { 'protocol': 'tcp', 'port_range_max': 22, 'port_range_min': 22, } ) arglist = [ '--protocol', 'tcp', '--dst-port', str(self._default_sg_rule.port_range_min), '--ingress', '--remote-group', 'remote-group-id', ] verifylist = [ ( 'dst_port', ( self._default_sg_rule.port_range_min, self._default_sg_rule.port_range_max, ), ), ('ingress', True), ('remote_group', 'remote-group-id'), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'port_range_max': self._default_sg_rule.port_range_max, 'port_range_min': self._default_sg_rule.port_range_min, 'protocol': self._default_sg_rule.protocol, 'remote_group_id': 'remote-group-id', 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_source_group(self): self._setup_default_security_group_rule( { 'remote_group_id': 'remote-group-id', } ) arglist = [ '--ingress', '--remote-group', 'remote-group-id', ] verifylist = [ ('ingress', True), ('remote_group', 'remote-group-id'), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'protocol': self._default_sg_rule.protocol, 'remote_group_id': 'remote-group-id', 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_source_ip(self): self._setup_default_security_group_rule( { 'protocol': 'icmp', 'remote_ip_prefix': '10.0.2.0/24', } ) arglist = [ '--protocol', self._default_sg_rule.protocol, '--remote-ip', self._default_sg_rule.remote_ip_prefix, ] verifylist = [ ('protocol', self._default_sg_rule.protocol), ('remote_ip', self._default_sg_rule.remote_ip_prefix), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'protocol': self._default_sg_rule.protocol, 'remote_ip_prefix': self._default_sg_rule.remote_ip_prefix, 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_remote_ip(self): self._setup_default_security_group_rule( { 'protocol': 'icmp', 'remote_ip_prefix': '10.0.2.0/24', } ) arglist = [ '--protocol', self._default_sg_rule.protocol, '--remote-ip', self._default_sg_rule.remote_ip_prefix, ] verifylist = [ ('protocol', self._default_sg_rule.protocol), ('remote_ip', self._default_sg_rule.remote_ip_prefix), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'protocol': self._default_sg_rule.protocol, 'remote_ip_prefix': self._default_sg_rule.remote_ip_prefix, 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_tcp_with_icmp_type(self): arglist = [ '--protocol', 'tcp', '--icmp-type', '15', ] verifylist = [ ('protocol', 'tcp'), ('icmp_type', 15), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) self.assertRaises( exceptions.CommandError, self.cmd.take_action, parsed_args ) def test_create_icmp_code(self): arglist = [ '--protocol', '1', '--icmp-code', '1', ] verifylist = [ ('protocol', '1'), ('icmp_code', 1), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) self.assertRaises( exceptions.CommandError, self.cmd.take_action, parsed_args ) def test_create_icmp_code_zero(self): self._setup_default_security_group_rule( { 'port_range_min': 15, 'port_range_max': 0, 'protocol': 'icmp', 'remote_ip_prefix': '0.0.0.0/0', } ) arglist = [ '--protocol', self._default_sg_rule.protocol, '--icmp-type', str(self._default_sg_rule.port_range_min), '--icmp-code', str(self._default_sg_rule.port_range_max), ] verifylist = [ ('protocol', self._default_sg_rule.protocol), ('icmp_code', self._default_sg_rule.port_range_max), ('icmp_type', self._default_sg_rule.port_range_min), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_icmp_code_greater_than_zero(self): self._setup_default_security_group_rule( { 'port_range_min': 15, 'port_range_max': 18, 'protocol': 'icmp', 'remote_ip_prefix': '0.0.0.0/0', } ) arglist = [ '--protocol', self._default_sg_rule.protocol, '--icmp-type', str(self._default_sg_rule.port_range_min), '--icmp-code', str(self._default_sg_rule.port_range_max), ] verifylist = [ ('protocol', self._default_sg_rule.protocol), ('icmp_type', self._default_sg_rule.port_range_min), ('icmp_code', self._default_sg_rule.port_range_max), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_icmp_code_negative_value(self): self._setup_default_security_group_rule( { 'port_range_min': 15, 'port_range_max': None, 'protocol': 'icmp', 'remote_ip_prefix': '0.0.0.0/0', } ) arglist = [ '--protocol', self._default_sg_rule.protocol, '--icmp-type', str(self._default_sg_rule.port_range_min), '--icmp-code', '-2', ] verifylist = [ ('protocol', self._default_sg_rule.protocol), ('icmp_type', self._default_sg_rule.port_range_min), ('icmp_code', -2), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_icmp_type(self): self._setup_default_security_group_rule( { 'port_range_min': 15, 'protocol': 'icmp', 'remote_ip_prefix': '0.0.0.0/0', } ) arglist = [ '--icmp-type', str(self._default_sg_rule.port_range_min), '--protocol', self._default_sg_rule.protocol, ] verifylist = [ ('dst_port', None), ('icmp_type', self._default_sg_rule.port_range_min), ('icmp_code', None), ('protocol', self._default_sg_rule.protocol), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'port_range_min': self._default_sg_rule.port_range_min, 'protocol': self._default_sg_rule.protocol, 'remote_ip_prefix': self._default_sg_rule.remote_ip_prefix, 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_icmp_type_zero(self): self._setup_default_security_group_rule( { 'port_range_min': 0, 'protocol': 'icmp', 'remote_ip_prefix': '0.0.0.0/0', } ) arglist = [ '--icmp-type', str(self._default_sg_rule.port_range_min), '--protocol', self._default_sg_rule.protocol, ] verifylist = [ ('dst_port', None), ('icmp_type', self._default_sg_rule.port_range_min), ('icmp_code', None), ('protocol', self._default_sg_rule.protocol), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'port_range_min': self._default_sg_rule.port_range_min, 'protocol': self._default_sg_rule.protocol, 'remote_ip_prefix': self._default_sg_rule.remote_ip_prefix, 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_icmp_type_greater_than_zero(self): self._setup_default_security_group_rule( { 'port_range_min': 13, # timestamp 'protocol': 'icmp', 'remote_ip_prefix': '0.0.0.0/0', } ) arglist = [ '--icmp-type', str(self._default_sg_rule.port_range_min), '--protocol', self._default_sg_rule.protocol, ] verifylist = [ ('dst_port', None), ('icmp_type', self._default_sg_rule.port_range_min), ('icmp_code', None), ('protocol', self._default_sg_rule.protocol), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'port_range_min': self._default_sg_rule.port_range_min, 'protocol': self._default_sg_rule.protocol, 'remote_ip_prefix': self._default_sg_rule.remote_ip_prefix, 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_icmp_type_negative_value(self): self._setup_default_security_group_rule( { 'port_range_min': None, # timestamp 'protocol': 'icmp', 'remote_ip_prefix': '0.0.0.0/0', } ) arglist = [ '--icmp-type', '-13', '--protocol', self._default_sg_rule.protocol, ] verifylist = [ ('dst_port', None), ('icmp_type', -13), ('icmp_code', None), ('protocol', self._default_sg_rule.protocol), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'protocol': self._default_sg_rule.protocol, 'remote_ip_prefix': self._default_sg_rule.remote_ip_prefix, 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_ipv6_icmp_type_code(self): self._setup_default_security_group_rule( { 'ether_type': 'IPv6', 'port_range_min': 139, 'port_range_max': 2, 'protocol': 'ipv6-icmp', 'remote_ip_prefix': '::/0', } ) arglist = [ '--icmp-type', str(self._default_sg_rule.port_range_min), '--icmp-code', str(self._default_sg_rule.port_range_max), '--protocol', self._default_sg_rule.protocol, ] verifylist = [ ('dst_port', None), ('icmp_type', self._default_sg_rule.port_range_min), ('icmp_code', self._default_sg_rule.port_range_max), ('protocol', self._default_sg_rule.protocol), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'port_range_min': self._default_sg_rule.port_range_min, 'port_range_max': self._default_sg_rule.port_range_max, 'protocol': self._default_sg_rule.protocol, 'remote_ip_prefix': self._default_sg_rule.remote_ip_prefix, 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_icmpv6_type(self): self._setup_default_security_group_rule( { 'ether_type': 'IPv6', 'port_range_min': 139, 'protocol': 'icmpv6', 'remote_ip_prefix': '::/0', } ) arglist = [ '--icmp-type', str(self._default_sg_rule.port_range_min), '--protocol', self._default_sg_rule.protocol, ] verifylist = [ ('dst_port', None), ('icmp_type', self._default_sg_rule.port_range_min), ('icmp_code', None), ('protocol', self._default_sg_rule.protocol), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'port_range_min': self._default_sg_rule.port_range_min, 'protocol': self._default_sg_rule.protocol, 'remote_ip_prefix': self._default_sg_rule.remote_ip_prefix, 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) def test_create_with_description(self): self._setup_default_security_group_rule( { 'description': 'Setting SGR', } ) arglist = [ '--description', self._default_sg_rule.description, ] verifylist = [ ('description', self._default_sg_rule.description), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.create_default_security_group_rule.\ assert_called_once_with(**{ 'description': self._default_sg_rule.description, 'direction': self._default_sg_rule.direction, 'ethertype': self._default_sg_rule.ether_type, 'protocol': self._default_sg_rule.protocol, 'remote_ip_prefix': self._default_sg_rule.remote_ip_prefix, 'used_in_default_sg': False, 'used_in_non_default_sg': True}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, data) class TestDeleteDefaultSecurityGroupRule( TestDefaultSecurityGroupRule): # The default security group rules to be deleted. _default_sg_rules = ( network_fakes.FakeDefaultSecurityGroupRule. create_default_security_group_rules(count=2)) def setUp(self): super(TestDeleteDefaultSecurityGroupRule, self).setUp() self.network.delete_default_security_group_rule = mock.Mock( return_value=None) self.network.find_default_security_group_rule = ( network_fakes.FakeDefaultSecurityGroupRule. get_default_security_group_rules(self._default_sg_rules)) # Get the command object to test self.cmd = default_security_group_rule.DeleteDefaultSecurityGroupRule( self.app, self.namespace ) def test_default_security_group_rule_delete(self): arglist = [ self._default_sg_rules[0].id, ] verifylist = [ ('rule', [self._default_sg_rules[0].id]), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) result = self.cmd.take_action(parsed_args) self.network.delete_default_security_group_rule.\ assert_called_once_with(self._default_sg_rules[0]) self.assertIsNone(result) def test_multi_default_security_group_rules_delete(self): arglist = [] verifylist = [] for s in self._default_sg_rules: arglist.append(s.id) verifylist = [ ('rule', arglist), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) result = self.cmd.take_action(parsed_args) calls = [] for s in self._default_sg_rules: calls.append(call(s)) self.network.delete_default_security_group_rule.assert_has_calls(calls) self.assertIsNone(result) def test_multi_default_security_group_rules_delete_with_exception(self): arglist = [ self._default_sg_rules[0].id, 'unexist_rule', ] verifylist = [ ('rule', [self._default_sg_rules[0].id, 'unexist_rule']), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) find_mock_result = [ self._default_sg_rules[0], exceptions.CommandError, ] self.network.find_default_security_group_rule = mock.Mock( side_effect=find_mock_result ) try: self.cmd.take_action(parsed_args) self.fail('CommandError should be raised.') except exceptions.CommandError as e: self.assertEqual('1 of 2 default rules failed to delete.', str(e)) self.network.find_default_security_group_rule.assert_any_call( self._default_sg_rules[0].id, ignore_missing=False ) self.network.find_default_security_group_rule.assert_any_call( 'unexist_rule', ignore_missing=False ) self.network.delete_default_security_group_rule.\ assert_called_once_with(self._default_sg_rules[0]) class TestListDefaultSecurityGroupRule( TestDefaultSecurityGroupRule): # The security group rule to be listed. _default_sg_rule_tcp = ( network_fakes.FakeDefaultSecurityGroupRule.\ create_one_default_security_group_rule({ 'protocol': 'tcp', 'port_range_max': 80, 'port_range_min': 80})) _default_sg_rule_icmp = ( network_fakes.FakeDefaultSecurityGroupRule.\ create_one_default_security_group_rule({ 'protocol': 'icmp', 'remote_ip_prefix': '10.0.2.0/24'})) _default_sg_rules = [ _default_sg_rule_tcp, _default_sg_rule_icmp, ] expected_columns = ( 'ID', 'IP Protocol', 'Ethertype', 'IP Range', 'Port Range', 'Direction', 'Remote Security Group', 'Remote Address Group', 'Used in default Security Group', 'Used in custom Security Group', ) expected_data = [] expected_data_no_group = [] for _default_sg_rule in _default_sg_rules: expected_data.append( ( _default_sg_rule.id, _default_sg_rule.protocol, _default_sg_rule.ether_type, _default_sg_rule.remote_ip_prefix, network_utils.format_network_port_range( _default_sg_rule ), _default_sg_rule.direction, _default_sg_rule.remote_group_id, _default_sg_rule.remote_address_group_id, _default_sg_rule.used_in_default_sg, _default_sg_rule.used_in_non_default_sg, ) ) def setUp(self): super(TestListDefaultSecurityGroupRule, self).setUp() self.network.default_security_group_rules = mock.Mock( return_value=self._default_sg_rules ) # Get the command object to test self.cmd = default_security_group_rule.ListDefaultSecurityGroupRule( self.app, self.namespace ) def test_list_default(self): self._default_sg_rule_tcp.port_range_min = 80 parsed_args = self.check_parser(self.cmd, [], []) columns, data = self.cmd.take_action(parsed_args) self.network.default_security_group_rules.assert_called_once_with(**{}) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, list(data)) def test_list_with_protocol(self): self._default_sg_rule_tcp.port_range_min = 80 arglist = [ '--protocol', 'tcp', ] verifylist = [ ('protocol', 'tcp'), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.default_security_group_rules.assert_called_once_with( **{ 'protocol': 'tcp', } ) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, list(data)) def test_list_with_ingress(self): self._default_sg_rule_tcp.port_range_min = 80 arglist = [ '--ingress', ] verifylist = [ ('ingress', True), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.default_security_group_rules.assert_called_once_with( **{ 'direction': 'ingress', } ) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, list(data)) def test_list_with_wrong_egress(self): self._default_sg_rule_tcp.port_range_min = 80 arglist = [ '--egress', ] verifylist = [ ('egress', True), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.default_security_group_rules.assert_called_once_with( **{ 'direction': 'egress', } ) self.assertEqual(self.expected_columns, columns) self.assertEqual(self.expected_data, list(data)) class TestShowDefaultSecurityGroupRule( TestDefaultSecurityGroupRule): # The default security group rule to be shown. _default_sg_rule = ( network_fakes.FakeDefaultSecurityGroupRule. create_one_default_security_group_rule()) columns = ( 'description', 'direction', 'ether_type', 'id', 'port_range_max', 'port_range_min', 'protocol', 'remote_address_group_id', 'remote_group_id', 'remote_ip_prefix', 'used_in_default_sg', 'used_in_non_default_sg', ) data = ( _default_sg_rule.description, _default_sg_rule.direction, _default_sg_rule.ether_type, _default_sg_rule.id, _default_sg_rule.port_range_max, _default_sg_rule.port_range_min, _default_sg_rule.protocol, _default_sg_rule.remote_address_group_id, _default_sg_rule.remote_group_id, _default_sg_rule.remote_ip_prefix, _default_sg_rule.used_in_default_sg, _default_sg_rule.used_in_non_default_sg, ) def setUp(self): super(TestShowDefaultSecurityGroupRule, self).setUp() self.network.find_default_security_group_rule = mock.Mock( return_value=self._default_sg_rule ) # Get the command object to test self.cmd = default_security_group_rule.ShowDefaultSecurityGroupRule( self.app, self.namespace ) def test_show_no_options(self): self.assertRaises( tests_utils.ParserException, self.check_parser, self.cmd, [], [] ) def test_show_all_options(self): arglist = [ self._default_sg_rule.id, ] verifylist = [ ('rule', self._default_sg_rule.id), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) columns, data = self.cmd.take_action(parsed_args) self.network.find_default_security_group_rule.assert_called_once_with( self._default_sg_rule.id, ignore_missing=False ) self.assertEqual(self.columns, columns) self.assertEqual(self.data, data) ",,1749,114
openstack%2Fopenstack-zuul-jobs~master~Ib6005dcd7eefbe693a400ef016103ed1df064d83,openstack/openstack-zuul-jobs,master,Ib6005dcd7eefbe693a400ef016103ed1df064d83,WIP: Cleanup orphan jobs after stable/train EOL of nova,NEW,2023-07-07 12:26:15.000000000,2023-07-07 12:26:46.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-07 12:26:15.000000000', 'files': ['playbooks/legacy/grenade-dsvm-neutron-multinode-zero-downtime/post.yaml', 'playbooks/legacy/tempest-dsvm-nova-libvirt-kvm-apr/post.yaml', 'zuul.d/zuul-legacy-jobs.yaml', 'playbooks/legacy/grenade-dsvm-neutron-multinode-zero-downtime/run.yaml', 'playbooks/legacy/tempest-dsvm-nova-libvirt-kvm-apr/run.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-zuul-jobs/commit/c7b72e8b4741006a938304a031e09306bc71e244', 'message': ""WIP: Cleanup orphan jobs after stable/train EOL of nova\n\nThese jobs were used only in nova's stable/train, but that went to EOL,\nso job definitions can be removed from here.\n\nChange-Id: Ib6005dcd7eefbe693a400ef016103ed1df064d83\n""}]",0,887950,c7b72e8b4741006a938304a031e09306bc71e244,3,1,1,17685,,,0,"WIP: Cleanup orphan jobs after stable/train EOL of nova

These jobs were used only in nova's stable/train, but that went to EOL,
so job definitions can be removed from here.

Change-Id: Ib6005dcd7eefbe693a400ef016103ed1df064d83
",git fetch https://review.opendev.org/openstack/openstack-zuul-jobs refs/changes/50/887950/1 && git format-patch -1 --stdout FETCH_HEAD,"['playbooks/legacy/grenade-dsvm-neutron-multinode-zero-downtime/post.yaml', 'playbooks/legacy/tempest-dsvm-nova-libvirt-kvm-apr/post.yaml', 'zuul.d/zuul-legacy-jobs.yaml', 'playbooks/legacy/grenade-dsvm-neutron-multinode-zero-downtime/run.yaml', 'playbooks/legacy/tempest-dsvm-nova-libvirt-kvm-apr/run.yaml']",5,c7b72e8b4741006a938304a031e09306bc71e244,nova_train_eol,,"- hosts: all name: Autoconverted job legacy-tempest-dsvm-nova-libvirt-kvm-apr from old job gate-tempest-dsvm-nova-libvirt-kvm-apr-ubuntu-xenial-nv tasks: - name: Ensure legacy workspace directory file: path: '{{ ansible_user_dir }}/workspace' state: directory - shell: cmd: | set -e set -x cat > clonemap.yaml << EOF clonemap: - name: openstack/devstack-gate dest: devstack-gate EOF /usr/zuul-env/bin/zuul-cloner -m clonemap.yaml --cache-dir /opt/git \ https://opendev.org \ openstack/devstack-gate executable: /bin/bash chdir: '{{ ansible_user_dir }}/workspace' environment: '{{ zuul | zuul_legacy_vars }}' - shell: cmd: | set -e set -x cat << 'EOF' >>""/tmp/dg-local.conf"" [[local|localrc]] enable_plugin devstack-plugin-additional-pkg-repos https://opendev.org/x/devstack-plugin-additional-pkg-repos EOF executable: /bin/bash chdir: '{{ ansible_user_dir }}/workspace' environment: '{{ zuul | zuul_legacy_vars }}' - shell: cmd: | set -e set -x export PYTHONUNBUFFERED=true export DEVSTACK_GATE_TEMPEST=1 export DEVSTACK_GATE_TEMPEST_FULL=1 export PROJECTS=""x/devstack-plugin-additional-pkg-repos $PROJECTS"" # We want to test the latest libvirt with kvm version with that plugin # TODO: (markus_z) rename that in the apr repo to ""libvirt-kvm"" # to make the chosen virtualization technology explicit. export LATEST_CODEBASES=libvirt export BRANCH_OVERRIDE=default if [ ""$BRANCH_OVERRIDE"" != ""default"" ] ; then export OVERRIDE_ZUUL_BRANCH=$BRANCH_OVERRIDE fi cp devstack-gate/devstack-vm-gate-wrap.sh ./safe-devstack-vm-gate-wrap.sh ./safe-devstack-vm-gate-wrap.sh executable: /bin/bash chdir: '{{ ansible_user_dir }}/workspace' environment: '{{ zuul | zuul_legacy_vars }}' ",0,165
openstack%2Foctavia~master~I59f1ed85383c078f505c654b1acf3e2a22d11faa,openstack/octavia,master,I59f1ed85383c078f505c654b1acf3e2a22d11faa,Fix octavia-status with amphorav2,MERGED,2023-03-29 08:13:34.000000000,2023-07-07 11:55:26.000000000,2023-07-07 11:53:26.000000000,"[{'_account_id': 11628}, {'_account_id': 22348}, {'_account_id': 29244}, {'_account_id': 32238}, {'_account_id': 34429}]","[{'number': 1, 'created': '2023-03-29 08:13:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/cfe19cd9f52c64cd180360360ca66acfc9a82873', 'message': 'Fix octavia-status with amphorav2\n\nChange-Id: I59f1ed85383c078f505c654b1acf3e2a22d11faa\n'}, {'number': 2, 'created': '2023-04-05 15:04:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/bfd743324daea3b8d071d42b8afff67ad136c1fe', 'message': 'Fix octavia-status with amphorav2\n\nChange-Id: I59f1ed85383c078f505c654b1acf3e2a22d11faa\n'}, {'number': 3, 'created': '2023-04-07 08:30:00.000000000', 'files': ['octavia/cmd/status.py', 'octavia/tests/unit/cmd/test_status.py', 'releasenotes/notes/fix-octavia-status-amphorav2-038fe77a2189b99f.yaml'], 'web_link': 'https://opendev.org/openstack/octavia/commit/1771f6acf9255e0ba80a8e5e68a8d1865daaae30', 'message': 'Fix octavia-status with amphorav2\n\nChange-Id: I59f1ed85383c078f505c654b1acf3e2a22d11faa\n'}]",4,878816,1771f6acf9255e0ba80a8e5e68a8d1865daaae30,23,5,3,29244,,,0,"Fix octavia-status with amphorav2

Change-Id: I59f1ed85383c078f505c654b1acf3e2a22d11faa
",git fetch https://review.opendev.org/openstack/octavia refs/changes/16/878816/1 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/cmd/status.py', 'octavia/tests/unit/cmd/test_status.py', 'releasenotes/notes/fix-octavia-status-amphorav2-038fe77a2189b99f.yaml']",3,cfe19cd9f52c64cd180360360ca66acfc9a82873,amphorav1-removal,--- fixes: - | Fixed a bug in octavia-status which reported an incorrect status for the *amphorav2* driver when using the default *amphora* alias. ,,8,4
openstack%2Foctavia~master~I16430fa52db02e7445203994220673c1037d764c,openstack/octavia,master,I16430fa52db02e7445203994220673c1037d764c,Fix upgrade check not working,MERGED,2023-03-05 22:18:44.000000000,2023-07-07 11:24:45.000000000,2023-07-07 11:23:37.000000000,"[{'_account_id': 22348}, {'_account_id': 29244}, {'_account_id': 32238}, {'_account_id': 34429}]","[{'number': 1, 'created': '2023-03-05 22:18:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/c27e90136203579e6e8c956e06780ce40d1a7ad1', 'message': ""Fix upgrade check not working\n\nThe octavia-status upgrade check command fails because\nthe initialization of Policy() requires oslo_config to\nbe initialized already which it isn't.\n\nThis can be reproduced in master by running:\n\n  tox -e venv -- octavia-status upgrade check\n\nI can observe this issue multiple relases back which\nprobably means this has been broken for a long time.\n\nWe shouldn't need to init Policy() because it just\nloads policy rules which is not checked in upgrade\ncheck anyway.\n\nChange-Id: I16430fa52db02e7445203994220673c1037d764c\n""}, {'number': 2, 'created': '2023-03-06 15:30:25.000000000', 'files': ['octavia/cmd/status.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/47de8fa3444cc7b1b68d486ad8c06d294a9ef9de', 'message': ""Fix upgrade check not working\n\nThe octavia-status upgrade check command fails because\nthe initialization of Policy() requires oslo_config to\nbe initialized already which it isn't.\n\nThis can be reproduced in master by running:\n\n  tox -e venv -- octavia-status upgrade check\n\nI can observe this issue multiple relases back which\nprobably means this has been broken for a long time.\n\nWe shouldn't need to init Policy() because it just\nloads policy rules which is not checked in upgrade\ncheck anyway.\n\nChange-Id: I16430fa52db02e7445203994220673c1037d764c\n""}]",7,876480,47de8fa3444cc7b1b68d486ad8c06d294a9ef9de,24,4,2,16137,,,0,"Fix upgrade check not working

The octavia-status upgrade check command fails because
the initialization of Policy() requires oslo_config to
be initialized already which it isn't.

This can be reproduced in master by running:

  tox -e venv -- octavia-status upgrade check

I can observe this issue multiple relases back which
probably means this has been broken for a long time.

We shouldn't need to init Policy() because it just
loads policy rules which is not checked in upgrade
check anyway.

Change-Id: I16430fa52db02e7445203994220673c1037d764c
",git fetch https://review.opendev.org/openstack/octavia refs/changes/80/876480/2 && git format-patch -1 --stdout FETCH_HEAD,['octavia/cmd/status.py'],1,c27e90136203579e6e8c956e06780ce40d1a7ad1,,, policy.Policy(),0,1
openstack%2Fcinder~master~I1f025542a2509e36919ece01b29064377dbbe189,openstack/cinder,master,I1f025542a2509e36919ece01b29064377dbbe189,HPE 3par: Unable to create clone of replicated vol,NEW,2023-05-30 12:37:20.000000000,2023-07-07 10:35:58.000000000,,"[{'_account_id': 22348}, {'_account_id': 30615}, {'_account_id': 35075}]","[{'number': 1, 'created': '2023-05-30 12:37:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/14c39631a48d32900840c5bbf9e9fc89e91689c7', 'message': 'DNM: WIP: HPE 3par: Unable to create clone of replicated vol\n\nWork in progress\n\nTwo possibilities of clone volume:\n1] same size, online copy\n\nExisting behaviour: start clone & return from function.\nError occur because clone is not yet complete and\ncode tries to create vol on secondary array.\n\n2] size is different, offline copy\n\nExisting behaviour: (i) create new replicated vol.\n(ii) during clone operation below error occur:\nVolume is involved in remote copy\n(iii) Since clone operation fails, delete new replicated vol (as cleanup).\n\nTo overcome both possibilities, code changes are done.\n\nFor clone of replicated vol, create offline copy only.\nSteps:\n(i) Create new vol without replication.\n(ii) Perform clone operation; wait till completion.\n(iii) Create vol on secondary array.\n\nChange-Id: I1f025542a2509e36919ece01b29064377dbbe189\n'}, {'number': 2, 'created': '2023-05-31 06:42:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/662ee102a6b465f1775d93e532f848e8deb15e0b', 'message': 'WIP: HPE 3par: Unable to create clone of replicated vol\n\nWork in progress\n\nTwo possibilities of clone volume:\n1] same size, online copy\n\nExisting behaviour: start clone & return from function.\nError occur because clone is not yet complete and\ncode tries to create vol on secondary array.\n\n2] size is different, offline copy\n\nExisting behaviour: (i) create new replicated vol.\n(ii) during clone operation below error occur:\nVolume is involved in remote copy\n(iii) Since clone operation fails, delete new replicated vol (as cleanup).\n\nTo overcome both possibilities, code changes are done.\n\nFor clone of replicated vol, create offline copy only.\nSteps:\n(i) Create new vol without replication.\n(ii) Perform clone operation; wait till completion.\n(iii) Create vol on secondary array.\n\nCloses-Bug: #2021941\nChange-Id: I1f025542a2509e36919ece01b29064377dbbe189\n'}, {'number': 3, 'created': '2023-06-05 10:30:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/4b509d3d61fe56bed369cac92d3fe8dae5d2cf36', 'message': 'WIP: HPE 3par: Unable to create clone of replicated vol\n\nWork in progress\n\nTwo possibilities of clone volume:\n1] same size, online copy\n\nExisting behaviour: start clone & return from function.\nError occur because clone is not yet complete and\ncode tries to create vol on secondary array.\n\n2] size is different, offline copy\n\nExisting behaviour: (i) create new replicated vol.\n(ii) during clone operation below error occur:\nVolume is involved in remote copy\n(iii) Since clone operation fails, delete new replicated vol (as cleanup).\n\nTo overcome both possibilities, code changes are done.\n\nFor clone of replicated vol, create offline copy only.\nSteps:\n(i) Create new vol without replication.\n(ii) Perform clone operation; wait till completion.\n(iii) Create vol on secondary array.\n\nCloses-Bug: #2021941\nChange-Id: I1f025542a2509e36919ece01b29064377dbbe189\n'}, {'number': 4, 'created': '2023-06-05 10:45:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b14bf620b69f66a81491fe012918677e4529eea9', 'message': 'WIP: HPE 3par: Unable to create clone of replicated vol\n\nWork in progress\n\nTwo possibilities of clone volume:\n1] same size, online copy\n\nExisting behaviour: start clone & return from function.\nError occur because clone is not yet complete and\ncode tries to create vol on secondary array.\n\n2] size is different, offline copy\n\nExisting behaviour: (i) create new replicated vol.\n(ii) during clone operation below error occur:\nVolume is involved in remote copy\n(iii) Since clone operation fails, delete new replicated vol (as cleanup).\n\nTo overcome both possibilities, code changes are done.\n\nFor clone of replicated vol, create offline copy only.\nSteps:\n(i) Create new vol without replication.\n(ii) Perform clone operation; wait till completion.\n(iii) Create vol on secondary array.\n\nCloses-Bug: #2021941\nChange-Id: I1f025542a2509e36919ece01b29064377dbbe189\n'}, {'number': 5, 'created': '2023-06-05 10:55:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/eddb02574a603f750e370cd9dd27a389c6556ed9', 'message': 'WIP: HPE 3par: Unable to create clone of replicated vol\n\nWork in progress\n\nTwo possibilities of clone volume:\n1] same size, online copy\n\nExisting behaviour: start clone & return from function.\nError occur because clone is not yet complete and\ncode tries to create vol on secondary array.\n\n2] size is different, offline copy\n\nExisting behaviour: (i) create new replicated vol.\n(ii) during clone operation below error occur:\nVolume is involved in remote copy\n(iii) Since clone operation fails, delete new replicated vol (as cleanup).\n\nTo overcome both possibilities, code changes are done.\n\nFor clone of replicated vol, create offline copy only.\nSteps:\n(i) Create new vol without replication.\n(ii) Perform clone operation; wait till completion.\n(iii) Create vol on secondary array.\n\nCloses-Bug: #2021941\nChange-Id: I1f025542a2509e36919ece01b29064377dbbe189\n'}, {'number': 6, 'created': '2023-06-05 12:02:11.000000000', 'files': ['releasenotes/notes/hpe-3par-clone-of-repl-vol-914a6e0e105996b4.yaml', 'cinder/volume/drivers/hpe/hpe_3par_base.py', 'cinder/tests/unit/volume/drivers/hpe/test_hpe3par.py', 'cinder/volume/drivers/hpe/hpe_3par_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/34a2fe4d99d1f9dc774b7f16b4ab9a35108f5c4f', 'message': 'HPE 3par: Unable to create clone of replicated vol\n\nTwo possibilities of clone volume:\n1] same size, online copy\n\nExisting behaviour: start clone & return from function.\nError occur because clone is not yet complete and\ncode tries to create vol on secondary array.\n\n2] size is different, offline copy\n\nExisting behaviour: (i) create new replicated vol.\n(ii) during clone operation below error occur:\nVolume is involved in remote copy\n(iii) Since clone operation fails, delete new replicated vol (as cleanup).\n\nTo overcome both possibilities, code changes are done.\n\nFor clone of replicated vol, create offline copy only.\nSteps:\n(i) Create new vol without replication.\n(ii) Perform clone operation; wait till completion.\n(iii) Create vol on secondary array.\n\nCloses-Bug: #2021941\nChange-Id: I1f025542a2509e36919ece01b29064377dbbe189\n'}]",5,884594,34a2fe4d99d1f9dc774b7f16b4ab9a35108f5c4f,62,3,6,29122,,,0,"HPE 3par: Unable to create clone of replicated vol

Two possibilities of clone volume:
1] same size, online copy

Existing behaviour: start clone & return from function.
Error occur because clone is not yet complete and
code tries to create vol on secondary array.

2] size is different, offline copy

Existing behaviour: (i) create new replicated vol.
(ii) during clone operation below error occur:
Volume is involved in remote copy
(iii) Since clone operation fails, delete new replicated vol (as cleanup).

To overcome both possibilities, code changes are done.

For clone of replicated vol, create offline copy only.
Steps:
(i) Create new vol without replication.
(ii) Perform clone operation; wait till completion.
(iii) Create vol on secondary array.

Closes-Bug: #2021941
Change-Id: I1f025542a2509e36919ece01b29064377dbbe189
",git fetch https://review.opendev.org/openstack/cinder refs/changes/94/884594/6 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/drivers/hpe/hpe_3par_common.py'],1,14c39631a48d32900840c5bbf9e9fc89e91689c7,I1f025542a2509e36919ece01b29064377dbbe189," def create_volume(self, volume, perform_replica_flag=True): if perform_replica_flag: if (self._volume_of_replicated_type(volume, hpe_tiramisu_check=True) and self._do_volume_replication_setup(volume)): replication_flag = True back_up_process and vol_chap_enabled) and not ( self._volume_of_replicated_type(volume, hpe_tiramisu_check=True)): model_update = self.create_volume(volume, perform_replica_flag=False) # v2 replication check LOG.debug(""RT: v2 replication check"") replication_flag = False if (self._volume_of_replicated_type(volume, hpe_tiramisu_check=True) and self._do_volume_replication_setup(volume)): replication_flag = True "," def create_volume(self, volume): if (self._volume_of_replicated_type(volume, hpe_tiramisu_check=True) and self._do_volume_replication_setup(volume)): replication_flag = True back_up_process and vol_chap_enabled): if (self._volume_of_replicated_type(volume, hpe_tiramisu_check=True) and self._do_volume_replication_setup(volume)): replication_flag = True model_update = self.create_volume(volume)",19,11
openstack%2Fovn-octavia-provider~stable%2Fxena~Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd,openstack/ovn-octavia-provider,stable/xena,Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd,Ensure DVR is restablished on member on cascade deletion,MERGED,2023-07-06 10:56:55.000000000,2023-07-07 10:26:56.000000000,2023-07-07 10:25:50.000000000,"[{'_account_id': 6773}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 34451}]","[{'number': 1, 'created': '2023-07-06 10:56:55.000000000', 'files': ['ovn_octavia_provider/tests/unit/test_driver.py', 'ovn_octavia_provider/helper.py', 'ovn_octavia_provider/driver.py', 'ovn_octavia_provider/tests/unit/test_helper.py'], 'web_link': 'https://opendev.org/openstack/ovn-octavia-provider/commit/cf17c25aa63a37fc9add2b96a5b778e3190018c7', 'message': 'Ensure DVR is restablished on member on cascade deletion\n\nTraffic to member, if they have FIPs gets centralized when they\nare part of a loadbalancer. However, when the loadbalancer gets\ndeleted, the traffic should be distributed again (if DVR was\nenabled). To do that this patch also considers the cascade deletion\n\nCloses-Bug: #2025637\nChange-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd\n(cherry picked from commit 20997b185f21aac8959d7517b4ffb7bc44c1b76a)\n'}]",0,887688,cf17c25aa63a37fc9add2b96a5b778e3190018c7,9,4,1,23567,,,0,"Ensure DVR is restablished on member on cascade deletion

Traffic to member, if they have FIPs gets centralized when they
are part of a loadbalancer. However, when the loadbalancer gets
deleted, the traffic should be distributed again (if DVR was
enabled). To do that this patch also considers the cascade deletion

Closes-Bug: #2025637
Change-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd
(cherry picked from commit 20997b185f21aac8959d7517b4ffb7bc44c1b76a)
",git fetch https://review.opendev.org/openstack/ovn-octavia-provider refs/changes/88/887688/1 && git format-patch -1 --stdout FETCH_HEAD,"['ovn_octavia_provider/tests/unit/test_driver.py', 'ovn_octavia_provider/helper.py', 'ovn_octavia_provider/driver.py', 'ovn_octavia_provider/tests/unit/test_helper.py']",4,cf17c25aa63a37fc9add2b96a5b778e3190018c7,," @mock.patch('ovn_octavia_provider.common.clients.get_neutron_client') def test_lb_delete_port_exception(self, del_port, net_cli):"," def test_lb_delete_port_exception(self, del_port):",37,4
openstack%2Fovn-octavia-provider~stable%2Fwallaby~Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd,openstack/ovn-octavia-provider,stable/wallaby,Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd,Ensure DVR is restablished on member on cascade deletion,MERGED,2023-07-06 10:57:11.000000000,2023-07-07 10:23:46.000000000,2023-07-07 10:22:35.000000000,"[{'_account_id': 6773}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 34451}]","[{'number': 1, 'created': '2023-07-06 10:57:11.000000000', 'files': ['ovn_octavia_provider/tests/unit/test_driver.py', 'ovn_octavia_provider/helper.py', 'ovn_octavia_provider/driver.py', 'ovn_octavia_provider/tests/unit/test_helper.py'], 'web_link': 'https://opendev.org/openstack/ovn-octavia-provider/commit/3fa41be337aa19ec3affff20cded74810c63a17e', 'message': 'Ensure DVR is restablished on member on cascade deletion\n\nTraffic to member, if they have FIPs gets centralized when they\nare part of a loadbalancer. However, when the loadbalancer gets\ndeleted, the traffic should be distributed again (if DVR was\nenabled). To do that this patch also considers the cascade deletion\n\nCloses-Bug: #2025637\nChange-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd\n(cherry picked from commit 20997b185f21aac8959d7517b4ffb7bc44c1b76a)\n'}]",0,887689,3fa41be337aa19ec3affff20cded74810c63a17e,9,4,1,23567,,,0,"Ensure DVR is restablished on member on cascade deletion

Traffic to member, if they have FIPs gets centralized when they
are part of a loadbalancer. However, when the loadbalancer gets
deleted, the traffic should be distributed again (if DVR was
enabled). To do that this patch also considers the cascade deletion

Closes-Bug: #2025637
Change-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd
(cherry picked from commit 20997b185f21aac8959d7517b4ffb7bc44c1b76a)
",git fetch https://review.opendev.org/openstack/ovn-octavia-provider refs/changes/89/887689/1 && git format-patch -1 --stdout FETCH_HEAD,"['ovn_octavia_provider/tests/unit/test_driver.py', 'ovn_octavia_provider/helper.py', 'ovn_octavia_provider/driver.py', 'ovn_octavia_provider/tests/unit/test_helper.py']",4,3fa41be337aa19ec3affff20cded74810c63a17e,," @mock.patch('ovn_octavia_provider.common.clients.get_neutron_client') def test_lb_delete_port_exception(self, del_port, net_cli):"," def test_lb_delete_port_exception(self, del_port):",37,4
openstack%2Fovn-octavia-provider~stable%2Fyoga~Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd,openstack/ovn-octavia-provider,stable/yoga,Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd,Ensure DVR is restablished on member on cascade deletion,MERGED,2023-07-06 10:56:37.000000000,2023-07-07 10:23:44.000000000,2023-07-07 10:22:33.000000000,"[{'_account_id': 6773}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 34451}]","[{'number': 1, 'created': '2023-07-06 10:56:37.000000000', 'files': ['ovn_octavia_provider/tests/unit/test_driver.py', 'ovn_octavia_provider/helper.py', 'ovn_octavia_provider/driver.py', 'ovn_octavia_provider/tests/unit/test_helper.py'], 'web_link': 'https://opendev.org/openstack/ovn-octavia-provider/commit/72cd160214d211710f2b512a0ac96dd35c798c9f', 'message': 'Ensure DVR is restablished on member on cascade deletion\n\nTraffic to member, if they have FIPs gets centralized when they\nare part of a loadbalancer. However, when the loadbalancer gets\ndeleted, the traffic should be distributed again (if DVR was\nenabled). To do that this patch also considers the cascade deletion\n\nCloses-Bug: #2025637\nChange-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd\n(cherry picked from commit 20997b185f21aac8959d7517b4ffb7bc44c1b76a)\n'}]",0,887687,72cd160214d211710f2b512a0ac96dd35c798c9f,9,4,1,23567,,,0,"Ensure DVR is restablished on member on cascade deletion

Traffic to member, if they have FIPs gets centralized when they
are part of a loadbalancer. However, when the loadbalancer gets
deleted, the traffic should be distributed again (if DVR was
enabled). To do that this patch also considers the cascade deletion

Closes-Bug: #2025637
Change-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd
(cherry picked from commit 20997b185f21aac8959d7517b4ffb7bc44c1b76a)
",git fetch https://review.opendev.org/openstack/ovn-octavia-provider refs/changes/87/887687/1 && git format-patch -1 --stdout FETCH_HEAD,"['ovn_octavia_provider/tests/unit/test_driver.py', 'ovn_octavia_provider/helper.py', 'ovn_octavia_provider/driver.py', 'ovn_octavia_provider/tests/unit/test_helper.py']",4,72cd160214d211710f2b512a0ac96dd35c798c9f,," @mock.patch('ovn_octavia_provider.common.clients.get_neutron_client') def test_lb_delete_port_exception(self, del_port, net_cli):"," def test_lb_delete_port_exception(self, del_port):",37,4
openstack%2Fneutron~stable%2F2023.1~I1fef7e225d631d581cb9f25982ba2e09b6f35fa8,openstack/neutron,stable/2023.1,I1fef7e225d631d581cb9f25982ba2e09b6f35fa8,[2023.1 Only] Switch to 2023.1 neutron-tempest-plugin jobs,MERGED,2023-07-06 14:20:19.000000000,2023-07-07 10:04:53.000000000,2023-07-07 10:02:37.000000000,"[{'_account_id': 8313}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 14:20:19.000000000', 'files': ['zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/neutron/commit/e0c387f4e088b1c61943074fc7d161e3f883d38a', 'message': '[2023.1 Only] Switch to 2023.1 neutron-tempest-plugin jobs\n\nChange-Id: I1fef7e225d631d581cb9f25982ba2e09b6f35fa8\n'}]",0,887854,e0c387f4e088b1c61943074fc7d161e3f883d38a,8,3,1,13861,,,0,"[2023.1 Only] Switch to 2023.1 neutron-tempest-plugin jobs

Change-Id: I1fef7e225d631d581cb9f25982ba2e09b6f35fa8
",git fetch https://review.opendev.org/openstack/neutron refs/changes/54/887854/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/project.yaml'],1,e0c387f4e088b1c61943074fc7d161e3f883d38a,antelope_release, - neutron-tempest-plugin-jobs-2023-1, - neutron-tempest-plugin-jobs,1,1
openstack%2Fskyline-apiserver~master~Ie7e552e9514b91b97a6d6a87b1ee477d56f58710,openstack/skyline-apiserver,master,Ie7e552e9514b91b97a6d6a87b1ee477d56f58710,docs: update kolla-ansible deploy skyline steps,NEW,2023-07-07 09:48:39.000000000,2023-07-07 10:03:31.000000000,,"[{'_account_id': 6282}, {'_account_id': 22348}, {'_account_id': 28706}]","[{'number': 1, 'created': '2023-07-07 09:48:39.000000000', 'files': ['kolla/README.md', 'kolla/README-zh_CN.md'], 'web_link': 'https://opendev.org/openstack/skyline-apiserver/commit/69271313e3a28036da2f89215f930a645a6ed242', 'message': 'docs: update kolla-ansible deploy skyline steps\n\nremove kolla-ansible skyline patch\nsince that has been merged into kolla-ansible.\n\nChange-Id: Ie7e552e9514b91b97a6d6a87b1ee477d56f58710\n'}]",0,887938,69271313e3a28036da2f89215f930a645a6ed242,2,3,1,35271,,,0,"docs: update kolla-ansible deploy skyline steps

remove kolla-ansible skyline patch
since that has been merged into kolla-ansible.

Change-Id: Ie7e552e9514b91b97a6d6a87b1ee477d56f58710
",git fetch https://review.opendev.org/openstack/skyline-apiserver refs/changes/38/887938/1 && git format-patch -1 --stdout FETCH_HEAD,"['kolla/README.md', 'kolla/README-zh_CN.md']",2,69271313e3a28036da2f89215f930a645a6ed242,,,"- kolla skyline patch包 : `https://opendev.org/skyline/skyline-apiserver/src/branch/master/kolla/kolla-xxxxxxx.diff` - kolla-ansible skyline patch包 : `https://opendev.org/skyline/skyline-apiserver/src/branch/master/kolla/kolla-ansible-xxxxxxx.diff` curl https://opendev.org/skyline/skyline-apiserver/src/branch/master/kolla/kolla-xxxxxxx.diff -o kolla.diff git apply --check /opt/kolla.diff git apply /opt/kolla.diff- 若部署环境已安装 kolla 获取 kolla 安装目录 ```shell KOLLA_PACKAGE_PATH=$(python3 -c ""import kolla;from pathlib import Path;print(Path(kolla.__file__).parents[1])"") KOLLA_DATA_FILES_PATH=$(python3 -c ""import kolla;from pathlib import Path;print(Path(kolla.__file__).parents[4].joinpath('share/kolla'))"") ``` 安装 kolla patch 包 ```shell curl https://opendev.org/skyline/skyline-apiserver/src/branch/master/kolla/kolla-xxxxxxx.diff -o /opt/kolla.diff cd ${KOLLA_PACKAGE_PATH} git apply --check --include='kolla/*' /opt/kolla.diff git apply --include='kolla/*' /opt/kolla.diff cd ${KOLLA_DATA_FILES_PATH} git apply --check --include='docker/*' /opt/kolla.diff git apply --include='docker/*' /opt/kolla.diff ``` curl https://opendev.org/skyline/skyline-apiserver/src/branch/master/kolla/kolla-ansible-xxxxxxx.diff -o kolla-ansible.diff git apply --check /opt/kolla-ansible.diff git apply /opt/kolla-ansible.diff- 若部署环境已安装 kolla-ansible 获取 kolla-ansible 安装目录 ```shell KOLLA_ANSIBLE_DATA_FILES_PATH=$(python3 -c ""import kolla_ansible;from pathlib import Path;print(Path(kolla_ansible.__file__).parents[4].joinpath('share/kolla-ansible'))"") ``` 安装 kolla-ansible patch 包 ```shell curl https://opendev.org/skyline/skyline-apiserver/src/branch/master/kolla/kolla-ansible-xxxxxxx.diff -o /opt/kolla-ansible.diff cd ${KOLLA_ANSIBLE_DATA_FILES_PATH} git apply --check --include='ansible/*' /opt/kolla-ansible.diff git apply --include='ansible/*' /opt/kolla-ansible.diff ``` ",0,92
openstack%2Fovn-octavia-provider~stable%2F2023.1~Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd,openstack/ovn-octavia-provider,stable/2023.1,Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd,Ensure DVR is restablished on member on cascade deletion,MERGED,2023-07-06 10:56:02.000000000,2023-07-07 09:58:55.000000000,2023-07-07 09:57:59.000000000,"[{'_account_id': 6773}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 34451}]","[{'number': 1, 'created': '2023-07-06 10:56:02.000000000', 'files': ['ovn_octavia_provider/tests/unit/test_driver.py', 'ovn_octavia_provider/helper.py', 'ovn_octavia_provider/driver.py', 'ovn_octavia_provider/tests/unit/test_helper.py'], 'web_link': 'https://opendev.org/openstack/ovn-octavia-provider/commit/b423ff94260828389b020700148a0096c6fb52c4', 'message': 'Ensure DVR is restablished on member on cascade deletion\n\nTraffic to member, if they have FIPs gets centralized when they\nare part of a loadbalancer. However, when the loadbalancer gets\ndeleted, the traffic should be distributed again (if DVR was\nenabled). To do that this patch also considers the cascade deletion\n\nCloses-Bug: #2025637\nChange-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd\n(cherry picked from commit 20997b185f21aac8959d7517b4ffb7bc44c1b76a)\n'}]",1,887685,b423ff94260828389b020700148a0096c6fb52c4,9,4,1,23567,,,0,"Ensure DVR is restablished on member on cascade deletion

Traffic to member, if they have FIPs gets centralized when they
are part of a loadbalancer. However, when the loadbalancer gets
deleted, the traffic should be distributed again (if DVR was
enabled). To do that this patch also considers the cascade deletion

Closes-Bug: #2025637
Change-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd
(cherry picked from commit 20997b185f21aac8959d7517b4ffb7bc44c1b76a)
",git fetch https://review.opendev.org/openstack/ovn-octavia-provider refs/changes/85/887685/1 && git format-patch -1 --stdout FETCH_HEAD,"['ovn_octavia_provider/tests/unit/test_driver.py', 'ovn_octavia_provider/helper.py', 'ovn_octavia_provider/driver.py', 'ovn_octavia_provider/tests/unit/test_helper.py']",4,b423ff94260828389b020700148a0096c6fb52c4,," @mock.patch('ovn_octavia_provider.common.clients.get_neutron_client') def test_lb_delete_port_exception(self, del_port, net_cli):"," def test_lb_delete_port_exception(self, del_port):",37,4
openstack%2Fkolla-ansible~master~I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e,openstack/kolla-ansible,master,I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e,[CI]Fix designate scenario,ABANDONED,2023-02-17 16:57:57.000000000,2023-07-07 09:01:24.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-02-17 16:57:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/d358d26e6ba15827ab4d8342f3eb9c66b49b8b46', 'message': '[CI]Fix designate scenario\n\nChange-Id: I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e\n'}, {'number': 2, 'created': '2023-02-19 09:48:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/78381dc60aac7a93331d353b0b3374210cf3bb94', 'message': '[CI]Fix designate scenario\n\nChange-Id: I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e\n'}, {'number': 3, 'created': '2023-02-19 15:51:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/bddd7b864c923bf484fc6859eef74c7d7946d5ce', 'message': '[CI]Fix designate scenario\n\nChange-Id: I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e\n'}, {'number': 4, 'created': '2023-02-20 13:12:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/f9726eb28c7eaff38df84e9321c8e03d767ca210', 'message': '[CI]Fix designate scenario\n\nChange-Id: I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e\n'}, {'number': 5, 'created': '2023-02-20 14:11:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/35380880a26725a48e759fb210fd7fec3f5c97ce', 'message': '[CI]Fix designate scenario\n\nChange-Id: I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e\n'}, {'number': 6, 'created': '2023-02-21 01:31:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/4b336ef4fe89cb95577c38d07b71e289c92c18ef', 'message': '[CI]Fix designate scenario\n\nChange-Id: I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e\n'}, {'number': 7, 'created': '2023-02-21 04:40:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/aba51e92c902158b2e4308624e5e350eafba571b', 'message': '[CI]Fix designate scenario\n\nChange-Id: I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e\n'}, {'number': 8, 'created': '2023-02-21 09:13:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/c99e741b726d53b5146752d77ba53f0dc0db5cf5', 'message': '[CI]Fix designate scenario\n\nChange-Id: I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e\n'}, {'number': 9, 'created': '2023-02-21 12:56:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/c203d681db9b1f0e4f0ae0f3c8274b5a7d63e049', 'message': '[CI]Fix designate scenario\n\nChange-Id: I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e\n'}, {'number': 10, 'created': '2023-02-22 15:07:11.000000000', 'files': ['tests/templates/globals-default.j2'], 'web_link': 'https://opendev.org/openstack/kolla-ansible/commit/f57d6bf98c57e94aad2e36e102a66934f9599a7f', 'message': '[CI]Fix designate scenario\n\nChange-Id: I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e\n'}]",0,874280,f57d6bf98c57e94aad2e36e102a66934f9599a7f,26,1,10,26285,,,0,"[CI]Fix designate scenario

Change-Id: I01b0afc0d32d1d3aed4e6d9fe6a1849a5ba6541e
",git fetch https://review.opendev.org/openstack/kolla-ansible refs/changes/80/874280/6 && git format-patch -1 --stdout FETCH_HEAD,"['tests/test-magnum.sh', 'zuul.d/project.yaml']",2,d358d26e6ba15827ab4d8342f3eb9c66b49b8b46,designate-ci, # - kolla-ansible-centos9s # - kolla-ansible-debian # - kolla-ansible-openeuler # - kolla-ansible-rocky9 # - kolla-ansible-ubuntu # - kolla-ansible-rocky9-kvm # - kolla-ansible-ubuntu-kvm # - kolla-ansible-rocky9-multinode-ipv6 # - kolla-ansible-ubuntu-multinode-ipv6 # - kolla-ansible-rocky9-bifrost # - kolla-ansible-ubuntu-bifrost # - kolla-ansible-rocky9-zun # - kolla-ansible-debian-zun # - kolla-ansible-ubuntu-zun # - kolla-ansible-rocky9-swift # - kolla-ansible-ubuntu-swift # - kolla-ansible-rocky9-scenario-nfv # - kolla-ansible-rocky9-octavia # - kolla-ansible-ubuntu-octavia # - kolla-ansible-rocky9-masakari # - kolla-ansible-ubuntu-masakari # - kolla-ansible-rocky9-ironic # - kolla-ansible-debian-ironic # - kolla-ansible-ubuntu-ironic # - kolla-ansible-rocky9-upgrade # - kolla-ansible-debian-upgrade # - kolla-ansible-ubuntu-upgrade # - kolla-ansible-ubuntu-binary-upgrade # - kolla-ansible-ubuntu-cells # - kolla-ansible-rocky9-cells # - kolla-ansible-rocky9-mariadb # - kolla-ansible-ubuntu-mariadb # - kolla-ansible-rocky9-ovn # - kolla-ansible-ubuntu-ovn # - kolla-ansible-rocky9-upgrade-ovn # - kolla-ansible-ubuntu-upgrade-ovn # - kolla-ansible-rocky9-prometheus-opensearch # - kolla-ansible-ubuntu-prometheus-opensearch # - kolla-ansible-rocky9-prometheus-opensearch-upgrade # - kolla-ansible-ubuntu-prometheus-opensearch-upgrade # - kolla-ansible-rocky9-venus # - kolla-ansible-ubuntu-venus # - kolla-ansible-rocky9-cephadm # - kolla-ansible-ubuntu-cephadm # - kolla-ansible-rocky9-upgrade-cephadm # - kolla-ansible-ubuntu-upgrade-cephadm # - kolla-ansible-rocky9-hashi-vault, - kolla-ansible-centos9s - kolla-ansible-debian - kolla-ansible-openeuler - kolla-ansible-rocky9 - kolla-ansible-ubuntu - kolla-ansible-rocky9-kvm - kolla-ansible-ubuntu-kvm - kolla-ansible-rocky9-multinode-ipv6 - kolla-ansible-ubuntu-multinode-ipv6 - kolla-ansible-rocky9-bifrost - kolla-ansible-ubuntu-bifrost - kolla-ansible-rocky9-zun - kolla-ansible-debian-zun - kolla-ansible-ubuntu-zun - kolla-ansible-rocky9-swift - kolla-ansible-ubuntu-swift - kolla-ansible-rocky9-scenario-nfv - kolla-ansible-rocky9-octavia - kolla-ansible-ubuntu-octavia - kolla-ansible-rocky9-masakari - kolla-ansible-ubuntu-masakari - kolla-ansible-rocky9-ironic - kolla-ansible-debian-ironic - kolla-ansible-ubuntu-ironic - kolla-ansible-rocky9-upgrade - kolla-ansible-debian-upgrade - kolla-ansible-ubuntu-upgrade - kolla-ansible-ubuntu-binary-upgrade - kolla-ansible-ubuntu-cells - kolla-ansible-rocky9-cells - kolla-ansible-rocky9-mariadb - kolla-ansible-ubuntu-mariadb - kolla-ansible-rocky9-ovn - kolla-ansible-ubuntu-ovn - kolla-ansible-rocky9-upgrade-ovn - kolla-ansible-ubuntu-upgrade-ovn - kolla-ansible-rocky9-prometheus-opensearch - kolla-ansible-ubuntu-prometheus-opensearch - kolla-ansible-rocky9-prometheus-opensearch-upgrade - kolla-ansible-ubuntu-prometheus-opensearch-upgrade - kolla-ansible-rocky9-venus - kolla-ansible-ubuntu-venus - kolla-ansible-rocky9-cephadm - kolla-ansible-ubuntu-cephadm - kolla-ansible-rocky9-upgrade-cephadm - kolla-ansible-ubuntu-upgrade-cephadm - kolla-ansible-rocky9-hashi-vault,49,47
openstack%2Fproject-config~master~Idd8e0b269738e771995bf4744f44dcd187c8d46e,openstack/project-config,master,Idd8e0b269738e771995bf4744f44dcd187c8d46e,Normalize projects.yaml,MERGED,2023-07-07 02:51:20.000000000,2023-07-07 08:56:06.000000000,2023-07-07 08:50:36.000000000,"[{'_account_id': 6547}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-07 02:51:20.000000000', 'files': ['gerrit/projects.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/39d0b03ce6cbc0f4ee188b339322bd0b893e028e', 'message': 'Normalize projects.yaml\n\nChange-Id: Idd8e0b269738e771995bf4744f44dcd187c8d46e\n'}]",1,887909,39d0b03ce6cbc0f4ee188b339322bd0b893e028e,7,2,1,11131,,,0,"Normalize projects.yaml

Change-Id: Idd8e0b269738e771995bf4744f44dcd187c8d46e
",git fetch https://review.opendev.org/openstack/project-config refs/changes/09/887909/1 && git format-patch -1 --stdout FETCH_HEAD,['gerrit/projects.yaml'],1,39d0b03ce6cbc0f4ee188b339322bd0b893e028e,project-yaml-normalization,, upstream: https://github.com/openstack-charmers/charm-barbican-k8s.git upstream: https://github.com/freyes/charm-heat-k8s.git,0,2
openstack%2Fceilometer~master~I0cfb559ca95f9457c48a9173331f2ccb7660d25c,openstack/ceilometer,master,I0cfb559ca95f9457c48a9173331f2ccb7660d25c,Imported Translations from Zanata,MERGED,2023-06-21 03:32:08.000000000,2023-07-07 08:43:09.000000000,2023-07-07 08:42:05.000000000,"[{'_account_id': 4264}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-21 03:32:08.000000000', 'files': ['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'], 'web_link': 'https://opendev.org/openstack/ceilometer/commit/cb7244148df3a2cf205a102da030c7d3f96a0152', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I0cfb559ca95f9457c48a9173331f2ccb7660d25c\n'}]",0,886560,cb7244148df3a2cf205a102da030c7d3f96a0152,7,2,1,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: I0cfb559ca95f9457c48a9173331f2ccb7660d25c
",git fetch https://review.opendev.org/openstack/ceilometer refs/changes/60/886560/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'],1,cb7244148df3a2cf205a102da030c7d3f96a0152,zanata/translations,"""POT-Creation-Date: 2023-06-19 06:37+0000\n""""PO-Revision-Date: 2023-06-20 11:18+0000\n""msgid ""20.0.0-12"" msgstr ""20.0.0-12""","""POT-Creation-Date: 2023-06-15 19:40+0000\n""""PO-Revision-Date: 2023-06-17 02:35+0000\n""msgid ""20.0.0-11"" msgstr ""20.0.0-11""",4,4
openstack%2Foctavia~stable%2Fzed~I3706fd5e12e17be37edce974563c6806d4f09709,openstack/octavia,stable/zed,I3706fd5e12e17be37edce974563c6806d4f09709,allowed_cidr validation for additional_vips,NEW,2023-04-15 20:31:52.000000000,2023-07-07 08:35:22.000000000,,"[{'_account_id': 22348}, {'_account_id': 29244}]","[{'number': 1, 'created': '2023-04-15 20:31:52.000000000', 'files': ['octavia/tests/unit/network/drivers/neutron/test_allowed_address_pairs.py', 'octavia/network/drivers/neutron/allowed_address_pairs.py', 'releasenotes/notes/allowed_cidr-validation-for-additional_vips-175c32824cc7ee95.yaml', 'octavia/tests/functional/api/v2/test_listener.py', 'octavia/api/v2/controllers/listener.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/68d464fa82cdd52cfabbdcbd3d94c86eb1453a3e', 'message': 'allowed_cidr validation for additional_vips\n\nThe validation for the allowed_cidr parameter did not take into account\nthe IP version of additional vIPs. The parameter was rejected if the IP\nversion did not match the IP version of the primary vIP. As additional\nvIPs can have a different IP version from the primary vIP all vIPs must\nbe checked during validation.\n\nChange-Id: I3706fd5e12e17be37edce974563c6806d4f09709\n(cherry picked from commit 60f579b64a8f603e5a67860a90cd413a0e9ca381)\n'}]",0,880504,68d464fa82cdd52cfabbdcbd3d94c86eb1453a3e,3,2,1,11290,,,0,"allowed_cidr validation for additional_vips

The validation for the allowed_cidr parameter did not take into account
the IP version of additional vIPs. The parameter was rejected if the IP
version did not match the IP version of the primary vIP. As additional
vIPs can have a different IP version from the primary vIP all vIPs must
be checked during validation.

Change-Id: I3706fd5e12e17be37edce974563c6806d4f09709
(cherry picked from commit 60f579b64a8f603e5a67860a90cd413a0e9ca381)
",git fetch https://review.opendev.org/openstack/octavia refs/changes/04/880504/1 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/tests/unit/network/drivers/neutron/test_allowed_address_pairs.py', 'octavia/network/drivers/neutron/allowed_address_pairs.py', 'releasenotes/notes/allowed_cidr-validation-for-additional_vips-175c32824cc7ee95.yaml', 'octavia/tests/functional/api/v2/test_listener.py', 'octavia/api/v2/controllers/listener.py']",5,68d464fa82cdd52cfabbdcbd3d94c86eb1453a3e,multi-vip-allowed-cidr-stable/2023.1-stable/zed," def _validate_cidr_compatible_with_vip(self, vips, allowed_cidrs): for cidr in allowed_cidrs: for vip in vips: # Check if CIDR IP version matches VIP IP version if (common_utils.is_cidr_ipv6(cidr) == common_utils.is_ipv6(vip)): break else: msg = _(""CIDR %(cidr)s IP version incompatible with all VIPs "" ""%(vips)s IP version."") detail=msg % {'cidr': cidr, 'vips': vips}) lb_db = self.repositories.load_balancer.get( lock_session, id=lb_id) vip_addresses = [lb_db.vip.ip_address] vip_addresses.extend([vip.ip_address for vip in lb_db.additional_vips]) self._validate_cidr_compatible_with_vip(vip_addresses, allowed_cidrs) vip_addresses = [db_listener.load_balancer.vip.ip_address] vip_addresses.extend( [vip.ip_address for vip in db_listener.load_balancer.additional_vips] ) vip_addresses, listener.allowed_cidrs)"," def _validate_cidr_compatible_with_vip(self, vip, allowed_cidrs): for cidr in allowed_cidrs: # Check if CIDR IP version matches VIP IP version if common_utils.is_cidr_ipv6(cidr) != common_utils.is_ipv6(vip): msg = _(""CIDR %(cidr)s IP version incompatible with VIP "" ""%(vip)s IP version."") detail=msg % {'cidr': cidr, 'vip': vip}) vip_db = self.repositories.vip.get( lock_session, load_balancer_id=lb_id) vip_address = vip_db.ip_address self._validate_cidr_compatible_with_vip(vip_address, allowed_cidrs) vip_address = db_listener.load_balancer.vip.ip_address vip_address, listener.allowed_cidrs)",107,37
openstack%2Fironic~master~I145af7d2d1172bc6e4b6b826b633b6b61cbb3384,openstack/ironic,master,I145af7d2d1172bc6e4b6b826b633b6b61cbb3384,[DNM] Test new inspection code path,ABANDONED,2023-04-25 16:20:41.000000000,2023-07-07 08:32:54.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-04-25 16:20:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0da87ad487dc8285a5f97ba4e20ee3a332d0eb04', 'message': '[DNM] Test new inspection code path\n\nChange-Id: I145af7d2d1172bc6e4b6b826b633b6b61cbb3384\n'}, {'number': 2, 'created': '2023-05-03 17:19:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/86697305f79b7950f74ee0b3c80fc7dcea06312a', 'message': '[DNM] Test new inspection code path\n\nChange-Id: I145af7d2d1172bc6e4b6b826b633b6b61cbb3384\n'}, {'number': 3, 'created': '2023-05-05 15:10:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7d7a93c6ed2bb672e35124d0d065a932edc2f1ee', 'message': '[DNM] Test new inspection code path\n\nChange-Id: I145af7d2d1172bc6e4b6b826b633b6b61cbb3384\n'}, {'number': 4, 'created': '2023-05-26 15:19:28.000000000', 'files': ['ironic/drivers/modules/inspector/interface.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/4395040813a2cc55cb647105dbfe7e68413f60b2', 'message': '[DNM] Test new inspection code path\n\nChange-Id: I145af7d2d1172bc6e4b6b826b633b6b61cbb3384\n'}]",4,881492,4395040813a2cc55cb647105dbfe7e68413f60b2,16,1,4,10239,,,0,"[DNM] Test new inspection code path

Change-Id: I145af7d2d1172bc6e4b6b826b633b6b61cbb3384
",git fetch https://review.opendev.org/openstack/ironic refs/changes/92/881492/1 && git format-patch -1 --stdout FETCH_HEAD,['ironic/drivers/modules/inspector/interface.py'],1,0da87ad487dc8285a5f97ba4e20ee3a332d0eb04,inspector," # FIXME(dtantsur): do not merge this, only for testing!!! endpoint = deploy_utils.get_ironic_api_url() + ""/v1/continue_inspection"" # end of FIXME",,4,0
openstack%2Fglance~master~I10904e8f2f524306181080365bf17251dedcb336,openstack/glance,master,I10904e8f2f524306181080365bf17251dedcb336,Sort locations based on store weight,ABANDONED,2023-07-07 08:22:38.000000000,2023-07-07 08:25:08.000000000,,[],"[{'number': 1, 'created': '2023-07-07 08:22:38.000000000', 'files': ['glance/tests/unit/common/test_utils.py', 'glance/tests/functional/v2/test_images.py', 'glance/common/utils.py', 'releasenotes/notes/store-weight-3ed3ee612579bc25.yaml', 'glance/api/v2/images.py', 'glance/db/__init__.py'], 'web_link': 'https://opendev.org/openstack/glance/commit/6ab2640f66a00275525b1bb0b2b96158162e968a', 'message': 'Sort locations based on store weight\n\nRelated to blueprint store-weight\nChange-Id: I2383a476cb7e79c7efecdf33203cff0b50ef3bbb\n\nChange-Id: I10904e8f2f524306181080365bf17251dedcb336\n'}]",0,887932,6ab2640f66a00275525b1bb0b2b96158162e968a,2,0,1,9303,,,0,"Sort locations based on store weight

Related to blueprint store-weight
Change-Id: I2383a476cb7e79c7efecdf33203cff0b50ef3bbb

Change-Id: I10904e8f2f524306181080365bf17251dedcb336
",git fetch https://review.opendev.org/openstack/glance refs/changes/32/887932/1 && git format-patch -1 --stdout FETCH_HEAD,"['glance/tests/unit/common/test_utils.py', 'glance/common/utils.py', 'glance/tests/functional/v2/test_images.py', 'releasenotes/notes/store-weight-3ed3ee612579bc25.yaml', 'glance/api/v2/images.py', 'glance/db/__init__.py']",6,6ab2640f66a00275525b1bb0b2b96158162e968a,bp/store-weight,"from glance.common import utils as common_utils locations=common_utils.sort_image_locations(locations),","from glance.common import location_strategy locations=location_strategy.get_ordered_locations(locations),",164,4
openstack%2Fcharm-ceph-mon~master~I1ca4316aaf4f0b855a12aa582a8188c88e926fa6,openstack/charm-ceph-mon,master,I1ca4316aaf4f0b855a12aa582a8188c88e926fa6,Fix ceph-mon upgrade path,MERGED,2023-07-05 19:28:00.000000000,2023-07-07 08:19:47.000000000,2023-07-07 08:19:47.000000000,"[{'_account_id': 15382}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 19:28:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ceph-mon/commit/6bd661f222f58013e9de71f838ab72d7fb3284b9', 'message': 'Fix ceph-mon upgrade path\n\nThis PR makes some small changes in the upgrade path logic by\nproviding a fallback method of fetching the current ceph-mon\nversion and adding additional checks to see if the upgrade can\nbe done in a sane way.\n\nCloses-Bug: #2024253\nChange-Id: I1ca4316aaf4f0b855a12aa582a8188c88e926fa6\n'}, {'number': 2, 'created': '2023-07-06 19:59:45.000000000', 'files': ['src/ceph_hooks.py', 'unit_tests/test_upgrade.py'], 'web_link': 'https://opendev.org/openstack/charm-ceph-mon/commit/1a41aa24ce82c411da936ef3f21c61a57c059155', 'message': 'Fix ceph-mon upgrade path\n\nThis PR makes some small changes in the upgrade path logic by\nproviding a fallback method of fetching the current ceph-mon\nversion and adding additional checks to see if the upgrade can\nbe done in a sane way.\n\nCloses-Bug: #2024253\nChange-Id: I1ca4316aaf4f0b855a12aa582a8188c88e926fa6\n'}]",10,887733,1a41aa24ce82c411da936ef3f21c61a57c059155,12,3,2,33717,,,0,"Fix ceph-mon upgrade path

This PR makes some small changes in the upgrade path logic by
providing a fallback method of fetching the current ceph-mon
version and adding additional checks to see if the upgrade can
be done in a sane way.

Closes-Bug: #2024253
Change-Id: I1ca4316aaf4f0b855a12aa582a8188c88e926fa6
",git fetch https://review.opendev.org/openstack/charm-ceph-mon refs/changes/33/887733/1 && git format-patch -1 --stdout FETCH_HEAD,"['src/ceph_hooks.py', 'unit_tests/test_upgrade.py']",2,6bd661f222f58013e9de71f838ab72d7fb3284b9,fix-upgrade,"from charms_ceph.utils import resolve_ceph_version as resolve_ceph_version_orig @patch('ceph_hooks.ceph.resolve_ceph_version') @patch('ceph_hooks.subprocess.check_output') @patch('ceph_hooks.add_source') @patch('ceph_hooks.ceph.is_bootstrapped') @patch('ceph_hooks.hookenv') @patch('ceph_hooks.ceph.roll_monitor_cluster') def test_check_for_upgrade_no_current_version(self, roll_monitor_cluster, hookenv, is_bootstrapped, add_source, check_output, resolve_ceph_version): _resolve_first = True def _resolve_version(arg): nonlocal _resolve_first if _resolve_first: _resolve_first = False return None return resolve_ceph_version_orig(arg) resolve_ceph_version.side_effect = _resolve_version check_output.return_value = b"""""" ceph version 16.2.13 (123) pacific (stable)"""""" is_bootstrapped.return_value = True hookenv.config.side_effect = self.test_config self.test_config.set('source', 'cloud:focal-yoga') check_for_upgrade() roll_monitor_cluster.assert_called() add_source.assert_not_called() @patch('ceph_hooks.ceph.resolve_ceph_version') @patch('ceph_hooks.subprocess.check_output') @patch('ceph_hooks.add_source') @patch('ceph_hooks.ceph.is_bootstrapped') @patch('ceph_hooks.hookenv') @patch('ceph_hooks.ceph.roll_monitor_cluster') def test_check_for_upgrade_no_versions(self, roll_monitor_cluster, hookenv, is_bootstrapped, add_source, check_output, resolve_ceph_version): resolve_ceph_version.return_value = None check_output.return_value = b"""""" ceph version 17.2.5 (456) quincy (stable)"""""" is_bootstrapped.return_value = True hookenv.config.side_effect = self.test_config check_for_upgrade() roll_monitor_cluster.assert_not_called() add_source.assert_not_called()",,74,0
openstack%2Frequirements~master~I186780a029c15de15764f1491f43737de8066b12,openstack/requirements,master,I186780a029c15de15764f1491f43737de8066b12,Bump XStatic-jQuery upper bound,MERGED,2023-07-05 15:26:03.000000000,2023-07-07 08:10:30.000000000,2023-07-07 08:09:39.000000000,"[{'_account_id': 6914}, {'_account_id': 8648}, {'_account_id': 11904}, {'_account_id': 13252}, {'_account_id': 14288}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-05 15:26:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/a47d4ce03c692716a08ba84388e0f85b5518b336', 'message': '[DNM] Upgrade XStatic-jQuery to 3.3.2.1 for Horizon\n\nChange-Id: I186780a029c15de15764f1491f43737de8066b12\n'}, {'number': 2, 'created': '2023-07-05 15:32:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/7f254dd56b8f3b685e8f461636c86cd9cf41b760', 'message': '[DNM] Upgrade XStatic-jQuery to 3.3.2.1 for Horizon\n\nChange-Id: I186780a029c15de15764f1491f43737de8066b12\n'}, {'number': 3, 'created': '2023-07-05 15:34:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/891dbf8959004064fe867023a4f3a46035669a7f', 'message': '[DNM] Upgrade XStatic-jQuery to 3.3.2.1 for Horizon\n\nChange-Id: I186780a029c15de15764f1491f43737de8066b12\n'}, {'number': 4, 'created': '2023-07-05 18:41:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/7abf5f00aeabf20bf36f043aeda30b926342e4e4', 'message': ""Upgrade XStatic-jQuery to 3.3.2.1 for Horizon\n\nSince we upgraded XStatic-JQuery-Migrate to 3.3.2.1 in patch\nhttps://review.opendev.org/c/openstack/requirements/+/883402 we now\nneed newer jquery, because that version of jquery-migrate won't\nwork with jquery 1.2.\n\nChange-Id: I186780a029c15de15764f1491f43737de8066b12\n""}, {'number': 5, 'created': '2023-07-06 13:39:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/6568c974a06294c9d9f38c8b69cbc38f1a285e7b', 'message': ""Bump XStatic-jQuery upper bound\n\nThis commit bumps the upper bound of XStatic-jQuery to <3.4.\nSince we upgraded XStatic-JQuery-Migrate to 3.3.2.1 in patch\nhttps://review.opendev.org/c/openstack/requirements/+/883402 we now\nneed newer jquery, because that version of jquery-migrate won't\nwork with jquery 1.2.\n\nNote: upper-constraints.txt will be updated in a seprate patch.\n\nChange-Id: I186780a029c15de15764f1491f43737de8066b12\n""}, {'number': 6, 'created': '2023-07-06 16:13:41.000000000', 'files': ['global-requirements.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/5881dcaff7dcea00b6954e9cf540467370185f55', 'message': ""Bump XStatic-jQuery upper bound\n\nThis commit bumps the upper bound of XStatic-jQuery to <3.\nSince we upgraded XStatic-JQuery-Migrate to 3.3.2.1 in patch\nhttps://review.opendev.org/c/openstack/requirements/+/883402 we now\nneed newer jquery, because that version of jquery-migrate won't\nwork with jquery 1.2.\n\nNote: upper-constraints.txt will be updated in a seprate patch.\n\nChange-Id: I186780a029c15de15764f1491f43737de8066b12\n""}]",3,887720,5881dcaff7dcea00b6954e9cf540467370185f55,22,7,6,29313,,,0,"Bump XStatic-jQuery upper bound

This commit bumps the upper bound of XStatic-jQuery to <3.
Since we upgraded XStatic-JQuery-Migrate to 3.3.2.1 in patch
https://review.opendev.org/c/openstack/requirements/+/883402 we now
need newer jquery, because that version of jquery-migrate won't
work with jquery 1.2.

Note: upper-constraints.txt will be updated in a seprate patch.

Change-Id: I186780a029c15de15764f1491f43737de8066b12
",git fetch https://review.opendev.org/openstack/requirements refs/changes/20/887720/4 && git format-patch -1 --stdout FETCH_HEAD,"['global-requirements.txt', 'upper-constraints.txt']",2,a47d4ce03c692716a08ba84388e0f85b5518b336,,XStatic-jQuery===3.3.2.1,XStatic-jQuery===1.12.4.1,2,2
openstack%2Fswift~master~Ic3be7432c28c7c6846423b803a49fd89fc990914,openstack/swift,master,Ic3be7432c28c7c6846423b803a49fd89fc990914,"internal_client: Add iter_{shard_ranges,namespaces} interfaces",NEW,2023-03-16 06:31:17.000000000,2023-07-07 07:30:35.000000000,,"[{'_account_id': 1179}, {'_account_id': 7233}, {'_account_id': 7847}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-03-16 06:31:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/4d7acc7e440be20b0a075931349e02955b2da5f8', 'message': ""WIP: internal_client: Add iter_shard_ranges interface\n\nThe container sharder already goes behind the scenes in internal_client\nto uses make_request to grab shard_ranges from the root. With the change\nto updating shardrange cache keys, we've also found downstream clients\nthat need to be updated. Further, we're looking to give the\nobject-updater the ability to probe for shard_ranges from either root or\ncache.\n\nThe rest of the sharding smarts lives in the proxy. So it seems an apt\nmoment to provide an internal_client shard_ranges interface to allow\nother components (upstream and down) to pull shards and allows us to\ntinker under the hood without breaking everything else.\n\nChange-Id: Ic3be7432c28c7c6846423b803a49fd89fc990914\n""}, {'number': 2, 'created': '2023-03-20 10:06:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/5e82cc1453c99de64deb9fa24481415877ce1975', 'message': ""WIP: internal_client: Add iter_shard_ranges interface\n\nThe container sharder already goes behind the scenes in internal_client\nto uses make_request to grab shard_ranges from the root. With the change\nto updating shardrange cache keys, we've also found downstream clients\nthat need to be updated. Further, we're looking to give the\nobject-updater the ability to probe for shard_ranges from either root or\ncache.\n\nThe rest of the sharding smarts lives in the proxy. So it seems an apt\nmoment to provide an internal_client shard_ranges interface to allow\nother components (upstream and down) to pull shards and allows us to\ntinker under the hood without breaking everything else.\n\nChange-Id: Ic3be7432c28c7c6846423b803a49fd89fc990914\n""}, {'number': 3, 'created': '2023-03-22 02:45:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/7360d99badd8b267093deaa1329f5c5e80b966e5', 'message': ""WIP: internal_client: Add iter_shard_ranges interface\n\nThe container sharder already goes behind the scenes in internal_client\nto uses make_request to grab shard_ranges from the root. With the change\nto updating shardrange cache keys, we've also found downstream clients\nthat need to be updated. Further, we're looking to give the\nobject-updater the ability to probe for shard_ranges from either root or\ncache.\n\nThe rest of the sharding smarts lives in the proxy. So it seems an apt\nmoment to provide an internal_client shard_ranges interface to allow\nother components (upstream and down) to pull shards and allows us to\ntinker under the hood without breaking everything else.\n\nChange-Id: Ic3be7432c28c7c6846423b803a49fd89fc990914\n""}, {'number': 4, 'created': '2023-03-23 09:19:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0146acbfd8b490b664920166f20a9ec31abe35ef', 'message': ""WIP: internal_client: Add iter_shard_ranges interface\n\nThe container sharder already goes behind the scenes in internal_client\nto uses make_request to grab shard_ranges from the root. With the change\nto updating shardrange cache keys, we've also found downstream clients\nthat need to be updated. Further, we're looking to give the\nobject-updater the ability to probe for shard_ranges from either root or\ncache.\n\nThe rest of the sharding smarts lives in the proxy. So it seems an apt\nmoment to provide an internal_client shard_ranges interface to allow\nother components (upstream and down) to pull shards and allows us to\ntinker under the hood without breaking everything else.\n\nChange-Id: Ic3be7432c28c7c6846423b803a49fd89fc990914\n""}, {'number': 5, 'created': '2023-03-24 02:09:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/1daa155d0ba31130eb562803026c2f13d56a769b', 'message': ""internal_client: Add iter_shard_ranges interface\n\nThe container sharder already goes behind the scenes in internal_client\nto uses make_request to grab shard_ranges from the root. With the change\nto updating shardrange cache keys, we've also found downstream clients\nthat need to be updated. Further, we're looking to give the\nobject-updater the ability to probe for shard_ranges from either root or\ncache.\n\nThe rest of the sharding smarts lives in the proxy. So it seems an apt\nmoment to provide an internal_client shard_ranges interface to allow\nother components (upstream and down) to pull shards and allows us to\ntinker under the hood without breaking everything else.\n\nChange-Id: Ic3be7432c28c7c6846423b803a49fd89fc990914\n""}, {'number': 6, 'created': '2023-03-31 06:04:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/e095c377ccf9a2536075259d5941bc2bdedc053f', 'message': ""internal_client: Add iter_shard_ranges interface\n\nThe container sharder already goes behind the scenes in internal_client\nto uses make_request to grab shard_ranges from the root. With the change\nto updating shardrange cache keys, we've also found downstream clients\nthat need to be updated. Further, we're looking to give the\nobject-updater the ability to probe for shard_ranges from either root or\ncache.\n\nThe rest of the sharding smarts lives in the proxy. So it seems an apt\nmoment to provide an internal_client shard_ranges interface to allow\nother components (upstream and down) to pull shards and allows us to\ntinker under the hood without breaking everything else.\n\nChange-Id: Ic3be7432c28c7c6846423b803a49fd89fc990914\n""}, {'number': 7, 'created': '2023-04-20 03:52:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/0a14fd3dfc6ab1b2da93d9d3138f8aaad2d2658e', 'message': ""internal_client: Add iter_shard_ranges interface\n\nThe container sharder already goes behind the scenes in internal_client\nto uses make_request to grab shard_ranges from the root. With the change\nto updating shardrange cache keys, we've also found downstream clients\nthat need to be updated. Further, we're looking to give the\nobject-updater the ability to probe for shard_ranges from either root or\ncache.\n\nThe rest of the sharding smarts lives in the proxy. So it seems an apt\nmoment to provide an internal_client shard_ranges interface to allow\nother components (upstream and down) to pull shards and allows us to\ntinker under the hood without breaking everything else.\n\nChange-Id: Ic3be7432c28c7c6846423b803a49fd89fc990914\n""}, {'number': 8, 'created': '2023-04-26 02:21:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/bbee1494fce1d33300a3ab6487bf6f18b97c99bb', 'message': ""internal_client: Add iter_shard_ranges interface\n\nThe container sharder already goes behind the scenes in internal_client\nto uses make_request to grab shard_ranges from the root. With the change\nto updating shardrange cache keys, we've also found downstream clients\nthat need to be updated. Further, we're looking to give the\nobject-updater the ability to probe for shard_ranges from either root or\ncache.\n\nThe rest of the sharding smarts lives in the proxy. So it seems an apt\nmoment to provide an internal_client shard_ranges interface to allow\nother components (upstream and down) to pull shards and allows us to\ntinker under the hood without breaking everything else.\n\nChange-Id: Ic3be7432c28c7c6846423b803a49fd89fc990914\n""}, {'number': 9, 'created': '2023-04-28 05:16:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/46ffe6c73fc0b1e9f1001f1288823a0365ce13f1', 'message': ""internal_client: Add iter_shard_ranges interface\n\nThe container sharder already goes behind the scenes in internal_client\nto uses make_request to grab shard_ranges from the root. With the change\nto updating shardrange cache keys, we've also found downstream clients\nthat need to be updated. Further, we're looking to give the\nobject-updater the ability to probe for shard_ranges from either root or\ncache.\n\nThe rest of the sharding smarts lives in the proxy. So it seems an apt\nmoment to provide an internal_client shard_ranges interface to allow\nother components (upstream and down) to pull shards and allows us to\ntinker under the hood without breaking everything else.\n\nChange-Id: Ic3be7432c28c7c6846423b803a49fd89fc990914\n""}, {'number': 10, 'created': '2023-06-27 06:44:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/726e879f95946e3bdedac254a8b6bbdb52348095', 'message': ""internal_client: Add iter_{shard_ranges,namespaces} interfaces\n\nThe container sharder already goes behind the scenes in internal_client\nto uses make_request to grab shard_ranges from the root. With the change\nto updating shardrange cache keys, we've also found downstream clients\nthat needed to be updated. Further, we're looking to give the\nobject-updater the ability to probe for shard_ranges from either root or\ncache.\n\nThe rest of the sharding smarts lives in the proxy and clients that want\nto talk sharding use the internal client, so it makes sense to provide\nan interface through internal client to do just that.\n\nA single iter_shard_ranges isn't quite enough however. Now that we use\nsmaller namespace objects to store shardranges into memcache there is a\nchance when a client calls this interface it could get shardrange dicts\nback from the container server OR namespace object dicts back from\nmemcache.\nThis patch creates 2 new interfaces:\n\n   iter_shard_ranges\n   iter_namespaces\n\nThe iter_shard_ranges interface is more expensive, as it'll use x-newest\nto _always_ go back to the container servers to the latest shard ranges.\nThis is exactly what the container-sharder uses the interface for.\nBecause it always hits container server, it'll always return shard_range\ndicts.\n\nThe inter_namespaces interface on the otherhand isn't as costly, it\ndoesn't use x-newest, so will happy return the first response from\neither a container server or memcache. As such in anycase the response\nwill in the form of namespace object dicts. Either directly from\nmemcache or shard_range dicts converted to namespace dicts.\n\nAll this will allow us to tinker under the hood in the future without\nbreaking any upstream or downstream client.\n\nChange-Id: Ic3be7432c28c7c6846423b803a49fd89fc990914\n""}, {'number': 11, 'created': '2023-07-03 09:08:22.000000000', 'files': ['test/unit/common/test_internal_client.py', 'test/unit/container/test_sharder.py', 'swift/common/utils/__init__.py', 'swift/container/sharder.py', 'swift/common/internal_client.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/740359f2e5474578d74d5bf2c68faffe97f0ea52', 'message': ""internal_client: Add iter_{shard_ranges,namespaces} interfaces\n\nThe container sharder already goes behind the scenes in internal_client\nto use make_request to grab shard_ranges from the root. With the change\nto updating shardrange cache keys, we've also found downstream clients\nthat needed to be updated. Further, we're looking to give the\nobject-updater the ability to probe for shard_ranges from either root or\ncache.\nThe rest of the sharding smarts lives in the proxy and clients that want\nto talk sharding use the internal client, so it makes sense to provide\nan interface through internal client to do just that.\n\nUnfortunately a single iter_shard_ranges isn't quite enough now that we use\nsmaller Namespace objects to store shardranges into memcache. Now there is a\nchance tp ether get a ShardRange or a Namespace record back. The latter happens\nwhen retrieving shardranges from memcache.\nThis patch creates 2 new interfaces:\n\n   iter_shard_ranges\n   iter_namespaces\n\nThe iter_shard_ranges interface is more expensive, as it'll use x-newest\nto _always_ go back to the container servers to the latest shard ranges.\nThis is exactly what the container-sharder uses the interface for.\nBecause it always hits container server, it'll always return a generator\nof ShardRange objects.\n\nThe iter_namespaces interface on the other hand isn't as costly, it\ndoesn't use x-newest, so will happy return the first response from\neither a container server or memcache. As such no matter what the\nresulting response it'll be converted into Namespace objects.\nwill be in the form of namespace object dicts. Either directly from\nmemcache or shard_range dicts converted to Namespace objects.\n\nAll this will allow us to tinker under the hood in the future without\nbreaking any upstream or downstream client.\n\nChange-Id: Ic3be7432c28c7c6846423b803a49fd89fc990914\n""}]",103,877584,740359f2e5474578d74d5bf2c68faffe97f0ea52,55,4,11,7233,,,0,"internal_client: Add iter_{shard_ranges,namespaces} interfaces

The container sharder already goes behind the scenes in internal_client
to use make_request to grab shard_ranges from the root. With the change
to updating shardrange cache keys, we've also found downstream clients
that needed to be updated. Further, we're looking to give the
object-updater the ability to probe for shard_ranges from either root or
cache.
The rest of the sharding smarts lives in the proxy and clients that want
to talk sharding use the internal client, so it makes sense to provide
an interface through internal client to do just that.

Unfortunately a single iter_shard_ranges isn't quite enough now that we use
smaller Namespace objects to store shardranges into memcache. Now there is a
chance tp ether get a ShardRange or a Namespace record back. The latter happens
when retrieving shardranges from memcache.
This patch creates 2 new interfaces:

   iter_shard_ranges
   iter_namespaces

The iter_shard_ranges interface is more expensive, as it'll use x-newest
to _always_ go back to the container servers to the latest shard ranges.
This is exactly what the container-sharder uses the interface for.
Because it always hits container server, it'll always return a generator
of ShardRange objects.

The iter_namespaces interface on the other hand isn't as costly, it
doesn't use x-newest, so will happy return the first response from
either a container server or memcache. As such no matter what the
resulting response it'll be converted into Namespace objects.
will be in the form of namespace object dicts. Either directly from
memcache or shard_range dicts converted to Namespace objects.

All this will allow us to tinker under the hood in the future without
breaking any upstream or downstream client.

Change-Id: Ic3be7432c28c7c6846423b803a49fd89fc990914
",git fetch https://review.opendev.org/openstack/swift refs/changes/84/877584/9 && git format-patch -1 --stdout FETCH_HEAD,"['swift/container/sharder.py', 'swift/common/internal_client.py']",2,4d7acc7e440be20b0a075931349e02955b2da5f8,listing_sr_cache," acceptable_statuses=(2, HTTP_NOT_FOUND), other_params=None, other_headers=None): :param other_params: Dictionary of other params to add to the request. :param other_headers: Dictionary of additional headers to add to the request. params = other_params or {} headers = other_headers or {} if marker and not isinstance(marker, bytes): params['marker'] = bytes_to_wsgi(quote(marker)) if end_marker and not isinstance(end_marker, bytes): params['end_marker'] = bytes_to_wsgi(quote(end_marker)) if prefix and not isinstance(prefix, bytes): params['prefix'] = bytes_to_wsgi(quote(prefix)) params.setdefault('format', 'json') 'GET', '%s' % path, headers, acceptable_statuses, params=params) def iter_shard_ranges( self, account, container, marker='', end_marker='', prefix='', acceptable_statuses=(2, HTTP_NOT_FOUND), includes=None, states=[], newest=False, include_deleted=False, override_deleted=False): """""" Returns an iterator of shard range dicts from a container. :param account: The container's account. :param container: Container to iterate objects on. :param marker: Prefix of first desired item, defaults to ''. :param end_marker: Last item returned will be 'less' than this, defaults to ''. :param prefix: Prefix of objects :param acceptable_statuses: List of status for valid responses, defaults to (2, HTTP_NOT_FOUND). :param includes: Optional, return the shardrange to which this obj belongs. :param states: An optional list of either the string or integer representation of :data:`~swift.common.utils.ShardRange.STATES`. :param newest: Wait for a response from all primaries and return the latest/newest response. :param include_deleted: Include shardranges marked as deleted. :param override_deleted: Check a container even if it's marked as deleted. :raises UnexpectedResponse: Exception raised when requests fail to get a response with an acceptable status :raises Exception: Exception is raised when code fails in an unexpected way. """""" path = self.make_path(account, container) headers = {'X-Backend-Record-Type': 'shard', 'X-Backend-Override-Deleted': str(override_deleted), 'X-Backend-Include-Deleted': str(include_deleted)} if newest: headers['X-Newest'] = 'true' params = {} if includes: params['includes'] = bytes_to_wsgi(quote(includes)) if states: params['states'] = "","".join(str(s) for s in states) return self._iter_items(path, marker, end_marker, prefix, acceptable_statuses, params, headers) "," acceptable_statuses=(2, HTTP_NOT_FOUND)): if not isinstance(marker, bytes): if not isinstance(end_marker, bytes): if not isinstance(prefix, bytes): 'GET', '%s?format=json&marker=%s&end_marker=%s&prefix=%s' % (path, bytes_to_wsgi(quote(marker)), bytes_to_wsgi(quote(end_marker)), bytes_to_wsgi(quote(prefix))), {}, acceptable_statuses)",67,35
openstack%2Fopenstack-ansible~master~Ief8759e19d935aec9d8cfa855b1b0ba2b0c83424,openstack/openstack-ansible,master,Ief8759e19d935aec9d8cfa855b1b0ba2b0c83424,Enable S3 API by default,MERGED,2023-05-29 22:11:40.000000000,2023-07-07 06:58:35.000000000,2023-07-07 06:56:28.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 28619}]","[{'number': 1, 'created': '2023-05-29 22:11:40.000000000', 'files': ['releasenotes/notes/s3-api-enabled-by-default-53e6602aeb4d9ff1.yaml', 'inventory/group_vars/ceph-rgw.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/e00689d50fb6b311988afc2e8a3459ce3af15d47', 'message': ""Enable S3 API by default\n\nWhen only 'swift' is specified in `rgw_enable_apis`, sending a http\nrequest to the base RadosGW API URL('/') returns '405 Method Not\nAllowed'.\nIt causes an important issue, because when any change is made to RadosGW\nconfiguration via ceph-ansible, the 'restart ceph rgws' handler is\ntriggered that use restart_rgw_daemon.sh[1] script to restart radosgw\nservice.\nBoth curl and wget used by this script return non-zero return code on\n'405 Method Not Allowed' response, causing ceph-ansible playbook to fail.\n\nAs a solution 's3' api can be enabled by default. With S3 API enabled,\nbase RadosGW API URL('/') returns 200 instead of 405 RC.\nThis change affects only environments using integrated ceph-ansible.\n\n[1] https://github.com/ceph/ceph-ansible/blob/stable-7.0/roles/ceph-handler/templates/restart_rgw_daemon.sh.j2#L68\n\nChange-Id: Ief8759e19d935aec9d8cfa855b1b0ba2b0c83424\n""}]",0,884645,e00689d50fb6b311988afc2e8a3459ce3af15d47,8,3,1,32666,,,0,"Enable S3 API by default

When only 'swift' is specified in `rgw_enable_apis`, sending a http
request to the base RadosGW API URL('/') returns '405 Method Not
Allowed'.
It causes an important issue, because when any change is made to RadosGW
configuration via ceph-ansible, the 'restart ceph rgws' handler is
triggered that use restart_rgw_daemon.sh[1] script to restart radosgw
service.
Both curl and wget used by this script return non-zero return code on
'405 Method Not Allowed' response, causing ceph-ansible playbook to fail.

As a solution 's3' api can be enabled by default. With S3 API enabled,
base RadosGW API URL('/') returns 200 instead of 405 RC.
This change affects only environments using integrated ceph-ansible.

[1] https://github.com/ceph/ceph-ansible/blob/stable-7.0/roles/ceph-handler/templates/restart_rgw_daemon.sh.j2#L68

Change-Id: Ief8759e19d935aec9d8cfa855b1b0ba2b0c83424
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/45/884645/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/s3-api-enabled-by-default-53e6602aeb4d9ff1.yaml', 'inventory/group_vars/ceph-rgw.yml']",2,e00689d50fb6b311988afc2e8a3459ce3af15d47,tls-backend," rgw_enable_apis: 'swift, s3' rgw_s3_auth_use_keystone: 'true'"," rgw_enable_apis: swift # For S3 support, update/add below rows # rgw_enable_apis: 'swift, s3' # rgw_s3_auth_use_keystone: 'true'",6,4
openstack%2Fopenstack-ansible~master~If451b4bc12bb0b0bfe3fe20494e99a07b59ab798,openstack/openstack-ansible,master,If451b4bc12bb0b0bfe3fe20494e99a07b59ab798,Return PIP_OPTS for load_nodepool_pip_opts,MERGED,2023-07-06 16:36:33.000000000,2023-07-07 06:57:37.000000000,2023-07-07 06:56:26.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 32666}]","[{'number': 1, 'created': '2023-07-06 16:36:33.000000000', 'files': ['scripts/scripts-library.sh'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/d458b1f46a8279e74e93ee2ba25b68921de302e5', 'message': ""Return PIP_OPTS for load_nodepool_pip_opts\n\nAt the moment for some scenarios, like linters, where ansible bootstrap\nis skipped, PIP_OPTS are undefined and linters fail due to that.\n\nWith patch we define PIP_OPTS to an empty value if\nwe're not in CI.\n\nAlternatively we can patch scenarios independently\nnot to rely on existance of PIP_OPTS or export default in\ngate-check-commit instead.\n\nChange-Id: If451b4bc12bb0b0bfe3fe20494e99a07b59ab798\n""}]",0,887862,d458b1f46a8279e74e93ee2ba25b68921de302e5,8,3,1,28619,,,0,"Return PIP_OPTS for load_nodepool_pip_opts

At the moment for some scenarios, like linters, where ansible bootstrap
is skipped, PIP_OPTS are undefined and linters fail due to that.

With patch we define PIP_OPTS to an empty value if
we're not in CI.

Alternatively we can patch scenarios independently
not to rely on existance of PIP_OPTS or export default in
gate-check-commit instead.

Change-Id: If451b4bc12bb0b0bfe3fe20494e99a07b59ab798
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/62/887862/1 && git format-patch -1 --stdout FETCH_HEAD,['scripts/scripts-library.sh'],1,d458b1f46a8279e74e93ee2ba25b68921de302e5,," else export PIP_OPTS=${PIP_OPTS:-""""}",,2,0
openstack%2Fopenstack-ansible~master~I1553ba549ba36ab23f999cb256e731520cbb5d09,openstack/openstack-ansible,master,I1553ba549ba36ab23f999cb256e731520cbb5d09,Adjust default value for *_backend_ssl,MERGED,2023-07-06 09:55:39.000000000,2023-07-07 06:55:46.000000000,2023-07-07 06:53:38.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 28619}]","[{'number': 1, 'created': '2023-07-06 09:55:39.000000000', 'files': ['inventory/group_vars/horizon_all/haproxy_service.yml', 'inventory/group_vars/blazar_all/haproxy_service.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/e88bf6b19c1c03f2d56d2be26180547b2fe8b5de', 'message': 'Adjust default value for *_backend_ssl\n\nI forgot to set a proper(openstack_service_backend_ssl) default value\nfor *_backend_ssl variables in a few places.\nThis patch fixes my mistake.\n\nChange-Id: I1553ba549ba36ab23f999cb256e731520cbb5d09\n'}]",0,887785,e88bf6b19c1c03f2d56d2be26180547b2fe8b5de,8,3,1,32666,,,0,"Adjust default value for *_backend_ssl

I forgot to set a proper(openstack_service_backend_ssl) default value
for *_backend_ssl variables in a few places.
This patch fixes my mistake.

Change-Id: I1553ba549ba36ab23f999cb256e731520cbb5d09
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/85/887785/1 && git format-patch -1 --stdout FETCH_HEAD,"['inventory/group_vars/horizon_all/haproxy_service.yml', 'inventory/group_vars/blazar_all/haproxy_service.yml']",2,e88bf6b19c1c03f2d56d2be26180547b2fe8b5de,tls-backend," haproxy_backend_ssl: ""{{ blazar_backend_ssl | default(openstack_service_backend_ssl) }}"""," haproxy_backend_ssl: ""{{ blazar_backend_ssl | default(False) }}""",2,2
openstack%2Fopenstack-ansible~master~I276ccd7e49db7e7ffe4f6f6c22ab1a82edc34688,openstack/openstack-ansible,master,I276ccd7e49db7e7ffe4f6f6c22ab1a82edc34688,Add TLS support to ceph-rgw backends,MERGED,2023-05-29 15:30:53.000000000,2023-07-07 06:54:47.000000000,2023-07-07 06:53:33.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 28619}]","[{'number': 1, 'created': '2023-05-29 15:30:53.000000000', 'files': ['playbooks/ceph-rgw-install.yml', 'inventory/group_vars/haproxy/haproxy.yml', 'inventory/group_vars/ceph-rgw.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/14f69fbb5d78f655ac55e2dc780f412b28c0ee9d', 'message': 'Add TLS support to ceph-rgw backends\n\nBy overriding the variable `ceph_rgw_backend_ssl: True` HTTPS will\nbe enabled, disabling HTTP support on the ceph-rgw backend api.\n\nThe ansible-role-pki is used to generate the required TLS\ncertificates if this functionality is enabled.\n\nChange-Id: I276ccd7e49db7e7ffe4f6f6c22ab1a82edc34688\n'}]",3,884633,14f69fbb5d78f655ac55e2dc780f412b28c0ee9d,11,3,1,32666,,,0,"Add TLS support to ceph-rgw backends

By overriding the variable `ceph_rgw_backend_ssl: True` HTTPS will
be enabled, disabling HTTP support on the ceph-rgw backend api.

The ansible-role-pki is used to generate the required TLS
certificates if this functionality is enabled.

Change-Id: I276ccd7e49db7e7ffe4f6f6c22ab1a82edc34688
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/33/884633/1 && git format-patch -1 --stdout FETCH_HEAD,"['playbooks/ceph-rgw-install.yml', 'inventory/group_vars/haproxy/haproxy.yml', 'inventory/group_vars/ceph-rgw.yml']",3,14f69fbb5d78f655ac55e2dc780f412b28c0ee9d,tls-backend,"### ### Backend TLS ### # Ceph configuration options to enable TLS on ceph-rgw radosgw_frontend_ssl_certificate: ""{{ ceph_rgw_backend_ssl is truthy | ternary(ceph_rgw_ssl_cert, '') }}"" # Ceph-ansible requires to include private key in `radosgw_frontend_ssl_certificate` # which is not possible with ansible-role-pki. # That is why `ssl_private_key` is defined in `radosgw_frontend_options`. radosgw_frontend_options: ""{{ ceph_rgw_backend_ssl is truthy | ternary('ssl_private_key=' + ceph_rgw_ssl_key, '') }}"" # Define if communication between haproxy and service backends should be # encrypted with TLS. ceph_rgw_backend_ssl: ""{{ openstack_service_backend_ssl | default(False) }}"" # Storage location for SSL certificate authority ceph_rgw_pki_dir: ""{{ openstack_pki_dir | default('/etc/openstack_deploy/pki') }}"" # Delegated host for operating the certificate authority ceph_rgw_pki_setup_host: ""{{ openstack_pki_setup_host | default('localhost') }}"" # ceph_rgw server certificate ceph_rgw_pki_keys_path: ""{{ ceph_rgw_pki_dir ~ '/certs/private/' }}"" ceph_rgw_pki_certs_path: ""{{ ceph_rgw_pki_dir ~ '/certs/certs/' }}"" ceph_rgw_pki_intermediate_cert_name: ""{{ openstack_pki_service_intermediate_cert_name | default('ExampleCorpIntermediate') }}"" ceph_rgw_pki_regen_cert: '' ceph_rgw_pki_san: ""{{ openstack_pki_san | default('DNS:' ~ ansible_facts['hostname'] ~ ',IP:' ~ management_address) }}"" ceph_rgw_pki_certificates: - name: ""ceph_rgw_{{ ansible_facts['hostname'] }}"" provider: ownca cn: ""{{ ansible_facts['hostname'] }}"" san: ""{{ ceph_rgw_pki_san }}"" signed_by: ""{{ ceph_rgw_pki_intermediate_cert_name }}"" # ceph_rgw destination files for SSL certificates ceph_rgw_ssl_cert: /etc/ceph/ceph-rgw.pem ceph_rgw_ssl_key: /etc/ceph/ceph-rgw.key # Installation details for SSL certificates ceph_rgw_pki_install_certificates: - src: ""{{ ceph_rgw_user_ssl_cert | default(ceph_rgw_pki_certs_path ~ 'ceph_rgw_' ~ ansible_facts['hostname'] ~ '-chain.crt') }}"" dest: ""{{ ceph_rgw_ssl_cert }}"" owner: ""ceph"" group: ""ceph"" mode: ""0644"" - src: ""{{ ceph_rgw_user_ssl_key | default(ceph_rgw_pki_keys_path ~ 'ceph_rgw_' ~ ansible_facts['hostname'] ~ '.key.pem') }}"" dest: ""{{ ceph_rgw_ssl_key }}"" owner: ""ceph"" group: ""ceph"" mode: ""0600"" # Define user-provided SSL certificates #ceph_rgw_user_ssl_cert: <path to cert on ansible deployment host> #ceph_rgw_user_ssl_key: <path to cert on ansible deployment host>",,78,0
openstack%2Fopenstack-ansible~master~I92953a14dd311a60b169165c5a8e61dd98466033,openstack/openstack-ansible,master,I92953a14dd311a60b169165c5a8e61dd98466033,Restore an ability for HAProxy to bind on interal IP,MERGED,2023-05-30 08:53:11.000000000,2023-07-07 06:53:36.000000000,2023-07-07 06:53:36.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 28619}]","[{'number': 1, 'created': '2023-05-30 08:53:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/d29055c8b80b119979e5991c6743d59dc9bc530a', 'message': 'Allow using domain name as internal_lb_vip_address\n\nChange-Id: I92953a14dd311a60b169165c5a8e61dd98466033\n'}, {'number': 2, 'created': '2023-06-30 10:20:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/d23e61572ebe41742c809852edcf57364406c04f', 'message': 'Allow using domain name as internal_lb_vip_address\n\nChange-Id: I92953a14dd311a60b169165c5a8e61dd98466033\n'}, {'number': 3, 'created': '2023-07-03 14:31:53.000000000', 'files': ['inventory/group_vars/repo_all.yml', 'inventory/group_vars/galera_all.yml', 'inventory/group_vars/nova_all/haproxy_service.yml', 'inventory/group_vars/neutron_all/haproxy_service.yml', 'inventory/group_vars/rabbitmq_all.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible/commit/64054e4cada6aacb87f7b3feee88fece95c5127a', 'message': 'Restore an ability for HAProxy to bind on interal IP\n\nAccording to the docs [1], there is an ability for HAProxy to bind\nspecifically on IP-address, ""while preserving the names for TLS-\ncertificates and endpoint URIs"".\n\nFor internal endpoint this supposed to be done by setting\n`internal_lb_vip_address` and `haproxy_bind_internal_lb_vip_address`\nbut was broken due to the fact that for:\n* `haproxy_galera_service`\n* `haproxy_opendaylight_neutron_service`\n* `haproxy_opendaylight_websocket_service`\n* `haproxy_nova_api_metadata_service`\n* `haproxy_rabbitmq_service`\n* `haproxy_repo_service`\n`haproxy_bind` was explicitly set to `[internal_lb_vip_address]` and\noverriding `haproxy_bind_internal_lb_vip_address` would result in\nwrong certificate paths (with FQDN in names, which does not exist)\nfor these frontends.\n\n[1] https://docs.openstack.org/openstack-ansible-haproxy_server/latest/configure-haproxy.html#overriding-the-address-haproxy-will-bind-to\n\nChange-Id: I92953a14dd311a60b169165c5a8e61dd98466033\n'}]",7,884662,64054e4cada6aacb87f7b3feee88fece95c5127a,20,3,3,34653,,,0,"Restore an ability for HAProxy to bind on interal IP

According to the docs [1], there is an ability for HAProxy to bind
specifically on IP-address, ""while preserving the names for TLS-
certificates and endpoint URIs"".

For internal endpoint this supposed to be done by setting
`internal_lb_vip_address` and `haproxy_bind_internal_lb_vip_address`
but was broken due to the fact that for:
* `haproxy_galera_service`
* `haproxy_opendaylight_neutron_service`
* `haproxy_opendaylight_websocket_service`
* `haproxy_nova_api_metadata_service`
* `haproxy_rabbitmq_service`
* `haproxy_repo_service`
`haproxy_bind` was explicitly set to `[internal_lb_vip_address]` and
overriding `haproxy_bind_internal_lb_vip_address` would result in
wrong certificate paths (with FQDN in names, which does not exist)
for these frontends.

[1] https://docs.openstack.org/openstack-ansible-haproxy_server/latest/configure-haproxy.html#overriding-the-address-haproxy-will-bind-to

Change-Id: I92953a14dd311a60b169165c5a8e61dd98466033
",git fetch https://review.opendev.org/openstack/openstack-ansible refs/changes/62/884662/1 && git format-patch -1 --stdout FETCH_HEAD,"['inventory/group_vars/repo_all.yml', 'inventory/group_vars/galera_all.yml', 'inventory/group_vars/nova_all/haproxy_service.yml', 'inventory/group_vars/neutron_all/haproxy_service.yml', 'inventory/group_vars/rabbitmq_all.yml']",5,d29055c8b80b119979e5991c6743d59dc9bc530a,multiceph," haproxy_bind: ""{{ [haproxy_bind_internal_lb_vip_address] }}"""," haproxy_bind: ""{{ [internal_lb_vip_address] }}""",6,6
openstack%2Fopenstack-ansible-galera_server~master~Id5ae73222a1109ad13b0b70ba3d02063d931ff90,openstack/openstack-ansible-galera_server,master,Id5ae73222a1109ad13b0b70ba3d02063d931ff90,Remove warn argument for command/shell,MERGED,2023-07-06 16:18:58.000000000,2023-07-07 06:50:45.000000000,2023-07-07 06:49:52.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 32666}]","[{'number': 1, 'created': '2023-07-06 16:18:58.000000000', 'files': ['tasks/galera_server_post_install.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-galera_server/commit/cef3aa94f6a39b62f8a3fc337d6b00b4dacb7226', 'message': ""Remove warn argument for command/shell\n\nSince ansible-core 2.14 you can't use warn as module argument.\n\nChange-Id: Id5ae73222a1109ad13b0b70ba3d02063d931ff90\n""}]",1,887861,cef3aa94f6a39b62f8a3fc337d6b00b4dacb7226,10,3,1,28619,,,0,"Remove warn argument for command/shell

Since ansible-core 2.14 you can't use warn as module argument.

Change-Id: Id5ae73222a1109ad13b0b70ba3d02063d931ff90
",git fetch https://review.opendev.org/openstack/openstack-ansible-galera_server refs/changes/61/887861/1 && git format-patch -1 --stdout FETCH_HEAD,['tasks/galera_server_post_install.yml'],1,cef3aa94f6a39b62f8a3fc337d6b00b4dacb7226,osa/core-2.15,, warn: no,0,1
openstack%2Fopenstack-ansible-rabbitmq_server~master~Ie2e783d065f32b906ee1554abaf5dc3b24236ca8,openstack/openstack-ansible-rabbitmq_server,master,Ie2e783d065f32b906ee1554abaf5dc3b24236ca8,Adjust wildcard definition,MERGED,2023-07-06 09:21:17.000000000,2023-07-07 06:29:06.000000000,2023-07-07 06:28:08.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 32666}]","[{'number': 1, 'created': '2023-07-06 09:21:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-rabbitmq_server/commit/00fbf1f61c0f4507a948e833776a92d5db2818da', 'message': ""Adjust wildcard definition\n\nThere was no reason to adjust rabbitmq_package_version previously, as\nreplaced by the wildcard part is never changed, so patching that doesn't\nhave any practical sense. We also return `-1` to erlang spec, so that\nwildcard would match only the part we expect to change.\n\nThis partially reverts I99683a031f935b579d38ae457c484c9a150344c6\n\nChange-Id: Ie2e783d065f32b906ee1554abaf5dc3b24236ca8\n""}, {'number': 2, 'created': '2023-07-06 09:39:35.000000000', 'files': ['vars/redhat.yml', 'vars/debian.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-rabbitmq_server/commit/2fc53a3a03889dc742892b83a1c9d4f30d258e0d', 'message': ""Adjust wildcard definition\n\nThere was no reason to adjust rabbitmq_package_version previously, as\nreplaced by the wildcard part is never changed, so patching that doesn't\nhave any practical sense. We also return `-1` to erlang spec, so that\nwildcard would match only the part we expect to change.\n\nThis partially reverts I99683a031f935b579d38ae457c484c9a150344c6\n\nChange-Id: Ie2e783d065f32b906ee1554abaf5dc3b24236ca8\n""}]",1,887779,2fc53a3a03889dc742892b83a1c9d4f30d258e0d,11,3,2,28619,,,0,"Adjust wildcard definition

There was no reason to adjust rabbitmq_package_version previously, as
replaced by the wildcard part is never changed, so patching that doesn't
have any practical sense. We also return `-1` to erlang spec, so that
wildcard would match only the part we expect to change.

This partially reverts I99683a031f935b579d38ae457c484c9a150344c6

Change-Id: Ie2e783d065f32b906ee1554abaf5dc3b24236ca8
",git fetch https://review.opendev.org/openstack/openstack-ansible-rabbitmq_server refs/changes/79/887779/1 && git format-patch -1 --stdout FETCH_HEAD,"['vars/redhat.yml', 'vars/debian.yml']",2,00fbf1f61c0f4507a948e833776a92d5db2818da,,"_rabbitmq_package_version: ""3.11.17-1""_rabbitmq_erlang_version_spec: ""{{ (rabbitmq_install_method == 'external_repo') | ternary('1:25.3.2*-1', '1:22.*') }}""","_rabbitmq_package_version: ""3.11.17*""_rabbitmq_erlang_version_spec: ""{{ (rabbitmq_install_method == 'external_repo') | ternary('1:25.3.2*', '1:22.*') }}""",4,4
openstack%2Fbifrost~stable%2F2023.1~I54ab52bbaec98ab94314698bc13083760d090206,openstack/bifrost,stable/2023.1,I54ab52bbaec98ab94314698bc13083760d090206,chore: allow ironic-inspector to work with IPv6 disabled,MERGED,2023-03-15 15:14:30.000000000,2023-07-07 06:09:07.000000000,2023-07-07 06:08:03.000000000,"[{'_account_id': 4571}, {'_account_id': 10342}, {'_account_id': 14760}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-03-15 15:14:30.000000000', 'files': ['playbooks/roles/bifrost-ironic-install/templates/ironic-inspector.conf.j2'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/867a4c466cef639df1939ccc52f154c98fae5c22', 'message': 'chore: allow ironic-inspector to work with IPv6 disabled\n\nIf IPv6 is disabled then ironic-inspector would fail to start. Update\nthe template to have ironic-inspector listen on the `internal_ip` of\nthe system.\n\nChange-Id: I54ab52bbaec98ab94314698bc13083760d090206\n(cherry picked from commit 83d56c72bb9af1ab60c08e7438f7b74ed5394b4d)\n'}]",1,877494,867a4c466cef639df1939ccc52f154c98fae5c22,12,4,1,10239,,,0,"chore: allow ironic-inspector to work with IPv6 disabled

If IPv6 is disabled then ironic-inspector would fail to start. Update
the template to have ironic-inspector listen on the `internal_ip` of
the system.

Change-Id: I54ab52bbaec98ab94314698bc13083760d090206
(cherry picked from commit 83d56c72bb9af1ab60c08e7438f7b74ed5394b4d)
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/94/877494/1 && git format-patch -1 --stdout FETCH_HEAD,['playbooks/roles/bifrost-ironic-install/templates/ironic-inspector.conf.j2'],1,867a4c466cef639df1939ccc52f154c98fae5c22,jlvillal/ipv4-stable/2023.1,{% else %} listen_address = {{ internal_ip }},,2,0
openstack%2Fnova~master~I04ce2b00903c0ad884bbe844ff45b7db62338072,openstack/nova,master,I04ce2b00903c0ad884bbe844ff45b7db62338072,libvirt: retry libvirt connection on live_migration_monitor,NEW,2022-12-09 06:03:03.000000000,2023-07-07 04:55:42.000000000,,"[{'_account_id': 7730}, {'_account_id': 11604}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-12-09 06:03:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/3d7c59e15f6189aaec801993ddfe15716f0b1532', 'message': 'libvirt: retry libvirt connection on live_migration_monitor\n\nWhen libvirtd is restarted, libvirtd aborts live migrations and\ndisconnect client connection. In this case, current nova fails to\ncontinue live_migration_monitor and fails with unclean state.\n\nBy this commit, at least for the cases that the domain is still on the\nsource host, nova can know that the live migration is cancelled and thus\ncan roll back cleanly.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nChange-Id: I04ce2b00903c0ad884bbe844ff45b7db62338072\n'}, {'number': 2, 'created': '2022-12-15 08:00:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/1680b139050aded74b11e7e3719fef87534777dd', 'message': 'libvirt: retry libvirt connection on live_migration_monitor\n\nWhen libvirtd is restarted, libvirtd aborts live migrations and\ndisconnect client connection. In this case, current nova fails to\ncontinue live_migration_monitor and fails with unclean state.\n\nBy this commit, at least for the cases that the domain is still on the\nsource host, nova can know that the live migration is cancelled and thus\ncan roll back cleanly.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nCloses-Bug: #1999607\nChange-Id: I04ce2b00903c0ad884bbe844ff45b7db62338072\n'}, {'number': 3, 'created': '2023-01-20 09:10:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/51d888c053aa38d94937d850fb4c8afc3fddbaaf', 'message': 'libvirt: retry libvirt connection on live_migration_monitor\n\nWhen libvirtd is restarted, libvirtd aborts live migrations and\ndisconnect client connection. In this case, current nova fails to\ncontinue live_migration_monitor and fails with unclean state.\n\nBy this commit, at least for the cases that the domain is still on the\nsource host, nova can know that the live migration is cancelled and thus\ncan roll back cleanly.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nCloses-Bug: #1999607\nChange-Id: I04ce2b00903c0ad884bbe844ff45b7db62338072\n'}, {'number': 4, 'created': '2023-02-16 03:44:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/fe2fea741def7bdaf0e8cde85dd703144cf02313', 'message': 'libvirt: retry libvirt connection on live_migration_monitor\n\nWhen libvirtd is restarted, libvirtd aborts live migrations and\ndisconnect client connection. In this case, current nova fails to\ncontinue live_migration_monitor and fails with unclean state.\n\nBy this commit, at least for the cases that the domain is still on the\nsource host, nova can know that the live migration is cancelled and thus\ncan roll back cleanly.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nCloses-Bug: #1999607\nChange-Id: I04ce2b00903c0ad884bbe844ff45b7db62338072\n'}, {'number': 5, 'created': '2023-03-14 10:57:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9fbc3cf3c1db3c774148997786331593a8c93c85', 'message': 'libvirt: retry libvirt connection on live_migration_monitor\n\nWhen libvirtd is restarted, libvirtd aborts live migrations and\ndisconnect client connection. In this case, current nova fails to\ncontinue live_migration_monitor and fails with unclean state.\n\nBy this commit, at least for the cases that the domain is still on the\nsource host, nova can know that the live migration is cancelled and thus\ncan roll back cleanly.\n\nThis commit changes the behavior of\ntest_live_migration_monitor_job_stats_internal_error, because by this\ncommit libvirt driver will now retry after single LibvirtException.\nTherefore this commit also changes the test case, as this is expected\nchange.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nCloses-Bug: #1999607\nChange-Id: I04ce2b00903c0ad884bbe844ff45b7db62338072\n'}, {'number': 6, 'created': '2023-03-16 06:07:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a5b7a452b23971b1c24e8a3616fc9a7c6d51bb1b', 'message': 'libvirt: retry libvirt connection on live_migration_monitor\n\nWhen libvirtd is restarted, libvirtd aborts live migrations and\ndisconnect client connection. In this case, current nova fails to\ncontinue live_migration_monitor and fails with unclean state.\n\nBy this commit, at least for the cases that the domain is still on the\nsource host, nova can know that the live migration is cancelled and thus\ncan roll back cleanly.\n\nThis commit changes the behavior of\ntest_live_migration_monitor_job_stats_internal_error, because by this\ncommit libvirt driver will now retry after single LibvirtException.\nTherefore this commit also changes the test case, as this is expected\nchange.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nCloses-Bug: #1999607\nChange-Id: I04ce2b00903c0ad884bbe844ff45b7db62338072\n'}, {'number': 7, 'created': '2023-03-16 06:39:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9a7a5af44e1f5bddf26bf00cee734f2b3251efe4', 'message': 'libvirt: retry libvirt connection on live_migration_monitor\n\nWhen libvirtd is restarted, libvirtd aborts live migrations and\ndisconnect client connection. In this case, current nova fails to\ncontinue live_migration_monitor and fails with unclean state.\n\nBy this commit, at least for the cases that the domain is still on the\nsource host, nova can know that the live migration is cancelled and thus\ncan roll back cleanly.\n\nThis commit changes the behavior of\ntest_live_migration_monitor_job_stats_internal_error, because by this\ncommit libvirt driver will now retry after single LibvirtException.\nTherefore this commit also changes the test case, as this is expected\nchange.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nCloses-Bug: #1999607\nChange-Id: I04ce2b00903c0ad884bbe844ff45b7db62338072\n'}, {'number': 8, 'created': '2023-03-16 07:22:06.000000000', 'files': ['nova/virt/libvirt/driver.py', 'nova/tests/functional/regressions/test_bug_1999607.py', 'releasenotes/notes/bug_1999607-libvirtd-restart-while-live-migration-a3c7d8a2c982f342.yaml', 'nova/tests/unit/virt/libvirt/test_driver.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/e6457fa581142458772bbf1fe0fef9e6240c73d5', 'message': 'libvirt: retry libvirt connection on live_migration_monitor\n\nWhen libvirtd is restarted, libvirtd aborts live migrations and\ndisconnect client connection. In this case, current nova fails to\ncontinue live_migration_monitor and fails with unclean state.\n\nBy this commit, at least for the cases that the domain is still on the\nsource host, nova can know that the live migration is cancelled and thus\ncan roll back cleanly.\n\nThis commit changes the behavior of\ntest_live_migration_monitor_job_stats_internal_error, because by this\ncommit libvirt driver will now retry after single LibvirtException.\nTherefore this commit also changes the test case, as this is expected\nchange.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nCloses-Bug: #1999607\nChange-Id: I04ce2b00903c0ad884bbe844ff45b7db62338072\n'}]",20,867077,e6457fa581142458772bbf1fe0fef9e6240c73d5,58,3,8,35587,,,0,"libvirt: retry libvirt connection on live_migration_monitor

When libvirtd is restarted, libvirtd aborts live migrations and
disconnect client connection. In this case, current nova fails to
continue live_migration_monitor and fails with unclean state.

By this commit, at least for the cases that the domain is still on the
source host, nova can know that the live migration is cancelled and thus
can roll back cleanly.

This commit changes the behavior of
test_live_migration_monitor_job_stats_internal_error, because by this
commit libvirt driver will now retry after single LibvirtException.
Therefore this commit also changes the test case, as this is expected
change.

Signed-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>
Closes-Bug: #1999607
Change-Id: I04ce2b00903c0ad884bbe844ff45b7db62338072
",git fetch https://review.opendev.org/openstack/nova refs/changes/77/867077/7 && git format-patch -1 --stdout FETCH_HEAD,['nova/virt/libvirt/driver.py'],1,3d7c59e15f6189aaec801993ddfe15716f0b1532,bug/1999607," monitor_interval_sec = 0.5 while True: retries = 120 for i in range(retries): try: if i > 0: # libvirt connection was lost on previous iteration. # @guest is tied to libvirt connection, so the # instance should be renewed. guest = self._host.get_guest(instance) info = guest.get_job_info() break except libvirt.libvirtError: if i == retries - 1: # Giving up with the result that nova cannot determine # on which hypervisor the virtual machine is running. LOG.exception(""Cannot connect to libvirt, and cannot "" ""know the result of migration job. "" ""Giving up."", instance=instance) raise LOG.info(""Cannot connect to libvirt, but live migration "" ""should be ongoing. Retry %(current)d / %(max)d"", {'current': i + 1, 'max': retries}, instance=instance) time.sleep(monitor_interval_sec) time.sleep(monitor_interval_sec)", while True: info = guest.get_job_info() time.sleep(0.5),25,2
openstack%2Ffuturist~master~Idb13271f5ea698cd382986d9cdd9c378516ac2ad,openstack/futurist,master,Idb13271f5ea698cd382986d9cdd9c378516ac2ad,GreenThreadPoolExecutor to accept multiple shutdown()s,NEW,2022-12-05 08:49:37.000000000,2023-07-07 04:55:28.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2022-12-05 08:49:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/futurist/commit/8637cc49a02d38ba34ceb136aa636d8c5023b50a', 'message': 'GreenThreadPoolExecutor to accept multiple shutdown()s\n\nCurrent GreenThreadPoolExecutor does nothing on shutdown() if it is not\nthe first time.\n\nIf wait=False was given on the first time, then even if\nshutdown(wait=True) was called after that, jobs are not guaranteed not\nto be running.\n\nCurrent document says this function is safe to call several times, so\nthe second call should also succeed.\nhttps://docs.openstack.org/futurist/latest/reference/index.html#executors\n\nBy this commit, fix the behavior of GreenThreadPoolExecutor and add a\ntest.\n\nfuturist.ProcessPoolExecutor also did not pass this test, but testing by\nthe original concurrent.futures.ProcessPoolExecutor raised on\nshutdown(wait=False), so did not change the behavior on this commit.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nChange-Id: Idb13271f5ea698cd382986d9cdd9c378516ac2ad\n'}, {'number': 2, 'created': '2022-12-13 05:29:36.000000000', 'files': ['futurist/_futures.py', 'futurist/tests/test_executors.py'], 'web_link': 'https://opendev.org/openstack/futurist/commit/8167b857ca60a7707e9368e11af0b15edae91b8c', 'message': 'GreenThreadPoolExecutor to accept multiple shutdown()s\n\nCurrent GreenThreadPoolExecutor does nothing on shutdown() if it is not\nthe first time.\n\nIf wait=False was given on the first time, then even if\nshutdown(wait=True) was called after that, jobs are not guaranteed not\nto be running.\n\nCurrent document says this function is safe to call several times, so\nthe second call should also succeed.\nhttps://docs.openstack.org/futurist/latest/reference/index.html#executors\n\nBy this commit, fix the behavior of GreenThreadPoolExecutor and add a\ntest.\n\nfuturist.ProcessPoolExecutor also did not pass this test, but testing by\nthe original concurrent.futures.ProcessPoolExecutor raised on\nshutdown(wait=False), so did not change the behavior on this commit.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nChange-Id: Idb13271f5ea698cd382986d9cdd9c378516ac2ad\n'}]",6,866553,8167b857ca60a7707e9368e11af0b15edae91b8c,8,1,2,35587,,,0,"GreenThreadPoolExecutor to accept multiple shutdown()s

Current GreenThreadPoolExecutor does nothing on shutdown() if it is not
the first time.

If wait=False was given on the first time, then even if
shutdown(wait=True) was called after that, jobs are not guaranteed not
to be running.

Current document says this function is safe to call several times, so
the second call should also succeed.
https://docs.openstack.org/futurist/latest/reference/index.html#executors

By this commit, fix the behavior of GreenThreadPoolExecutor and add a
test.

futurist.ProcessPoolExecutor also did not pass this test, but testing by
the original concurrent.futures.ProcessPoolExecutor raised on
shutdown(wait=False), so did not change the behavior on this commit.

Signed-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>
Change-Id: Idb13271f5ea698cd382986d9cdd9c378516ac2ad
",git fetch https://review.opendev.org/openstack/futurist refs/changes/53/866553/2 && git format-patch -1 --stdout FETCH_HEAD,"['futurist/_futures.py', 'futurist/tests/test_executors.py']",2,8637cc49a02d38ba34ceb136aa636d8c5023b50a,feature/green-threadpool-executor-shutdown-again," def test_shutdown_again(self): if isinstance(self.executor, futurist.ProcessPoolExecutor): # Skipping this behavior as intended behavior, # because concurrent.futures.ProcessPoolExecutor raises # on shutdown(wait=False) return self.executor.submit(delayed, 0.2) self.executor.shutdown(wait=False) self.executor.shutdown(wait=True) self.assertEqual(1, self.executor.statistics.executed) self.executor.shutdown(wait=True) self.executor.shutdown(wait=False) ",,16,4
openstack%2Fnova~master~Ic63d8c51bd0a6c7845ae9eb44cf33fc3c341bf9e,openstack/nova,master,Ic63d8c51bd0a6c7845ae9eb44cf33fc3c341bf9e,libvirt: add sftp driver,NEW,2022-12-06 08:32:50.000000000,2023-07-07 04:54:18.000000000,,"[{'_account_id': 7730}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-12-06 08:32:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/57c5d94da85afe4c8e37a3fb5345497559d97b75', 'message': 'libvirt: add sftp driver\n\nsshd can be configured to accept only sftp access by force-command\noption. Both ssh driver and rsync driver cannot use with internal-sftp\nrestriction. Also, ChrootDirectory option is combined well with\ninternal-sftp command. This commit supports the situation that the ssh\nserver is configured with ChrootDirectory and still have enough\nprivilege to access the necessary directories.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nChange-Id: Ic63d8c51bd0a6c7845ae9eb44cf33fc3c341bf9e\n'}, {'number': 2, 'created': '2022-12-07 04:57:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/9abc100c98facff11233fd894a41cc84a5aefad6', 'message': 'libvirt: add sftp driver\n\nsshd can be configured to accept only sftp access by force-command\noption. Both ssh driver and rsync driver cannot use with internal-sftp\nrestriction. Also, ChrootDirectory option is combined well with\ninternal-sftp command. This commit supports the situation that the ssh\nserver is configured with ChrootDirectory and still have enough\nprivilege to access the necessary directories.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nChange-Id: Ic63d8c51bd0a6c7845ae9eb44cf33fc3c341bf9e\n'}, {'number': 3, 'created': '2022-12-07 07:24:11.000000000', 'files': ['nova/conf/libvirt.py', 'nova/virt/libvirt/volume/remotefs.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/8a26e653640b2af37a71157dbb32829283f17e6f', 'message': 'libvirt: add sftp driver\n\nsshd can be configured to accept only sftp access by force-command\noption. Both ssh driver and rsync driver cannot use with internal-sftp\nrestriction. Also, ChrootDirectory option is combined well with\ninternal-sftp command. This commit supports the situation that the ssh\nserver is configured with ChrootDirectory and still have enough\nprivilege to access the necessary directories.\n\nSigned-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>\nChange-Id: Ic63d8c51bd0a6c7845ae9eb44cf33fc3c341bf9e\n'}]",13,866672,8a26e653640b2af37a71157dbb32829283f17e6f,33,2,3,35587,,,0,"libvirt: add sftp driver

sshd can be configured to accept only sftp access by force-command
option. Both ssh driver and rsync driver cannot use with internal-sftp
restriction. Also, ChrootDirectory option is combined well with
internal-sftp command. This commit supports the situation that the ssh
server is configured with ChrootDirectory and still have enough
privilege to access the necessary directories.

Signed-off-by: Hiroki Narukawa <hnarukaw@yahoo-corp.jp>
Change-Id: Ic63d8c51bd0a6c7845ae9eb44cf33fc3c341bf9e
",git fetch https://review.opendev.org/openstack/nova refs/changes/72/866672/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/conf/libvirt.py', 'nova/virt/libvirt/volume/remotefs.py']",2,57c5d94da85afe4c8e37a3fb5345497559d97b75,feature/sftp-driver,"from nova import exception # Behavior slightly differs on each implementation # Not idempotent # When /a/a.txt exists but /a/b does not exist, # after copying /c/b inside into /a/b, /a/b/a.txt exists. # Idempotent # When /a/a.txt exists but /a/b does not exist, # after copying /c/b inside into /a/b, /a/b/b/a.txt exists. class SftpDriver(RemoteFilesystemDriver): def create_file(self, host, dst_path, on_execute, on_completion): with utils.tempdir() as tempdir: dir_path = os.path.dirname(os.path.normpath(dst_path)) # Create target dir inside temporary directory local_tmp_dir = os.path.join(tempdir, dir_path.strip(os.path.sep)) processutils.execute('mkdir', '-p', local_tmp_dir, on_execute=on_execute, on_completion=on_completion) # Create file in directory file_name = os.path.basename(os.path.normpath(dst_path)) local_tmp_file = os.path.join(local_tmp_dir, file_name) processutils.execute('touch', local_tmp_file) self.copy_file(local_tmp_file, host + ':' + dst_path, on_execute=on_execute, on_completion=on_completion, compression=False) def remove_file(self, host, dst, on_execute, on_completion): args = ['sftp', '-b', '-', host] cmd = '@rm ' + self._remote_path(dst) processutils.execute( *args, process_input=cmd, on_execute=on_execute, on_completion=on_completion) def create_dir(self, host, dst_path, on_execute, on_completion): if host is None: try: os.mkdir(dst_path) except FileExistsError: pass return try: args = ['sftp', '-b', '-', host] cmd = '@mkdir ' + self._remote_path(dst_path) processutils.execute( *args, process_input=cmd, on_execute=on_execute, on_completion=on_completion) except processutils.ProcessExecutionError: # sftp has no command equivalent to mkdir -p is_dir = self._is_dir(host, dst_path) if not is_dir: raise exception.NovaException('%s exists, but not a directory' % dst_path) def remove_dir(self, host, dst, on_execute, on_completion): (dirs, files) = self._list_files(host, dst) for d in dirs: self.remove_dir(host, os.path.join(dst, d), on_execute=None, on_completion=None) for f in files: self.remove_file(host, os.path.join(dst, f), on_execute=None, on_completion=None) args = ['sftp', '-b', '-', host] cmd = '@rmdir ' + self._remote_path(dst) processutils.execute( *args, process_input=cmd, on_execute=on_execute, on_completion=on_completion) # Not idempotent # When /a/a.txt exists but /a/b does not exist, # after copying /c/b inside into /a/b, /a/b/a.txt exists. def copy_file(self, src, dst, on_execute, on_completion, compression): if ':' in src: src_host = src.split(':')[0] src_path = src.split(':')[1] else: src_host = None src_path = src if ':' in dst: dst_host = dst.split(':')[0] dst_path = dst.split(':')[1] else: dst_host = None dst_path = dst if src_host is None and dst_host is None: # simple local copy args = ['cp', '-rf', src_path, dst_path] processutils.execute( *args, on_execute=on_execute, on_completion=on_completion) return if src_host is not None and dst_host is not None: with utils.tempdir() as tempdir: self.copy_file(src, tempdir, None, None, compression) self.copy_file(os.path.join(tempdir, os.path.basename(src_path)), dst, on_execute, on_completion, compression) return # After here, exactly one of src and dst is remote if self._is_dir(src_host, src_path): self.create_dir(dst_host, os.path.join(dst_path, os.path.basename(src)), None, None) # As far as ploop disks are in fact directories we add '-r' argument if src_host is not None: host = src_host sftp_cmd = '@get -r ' + self._remote_path(src_path) + ' ' + dst_path else: host = dst_host sftp_cmd = '@put -r ' + src_path + ' ' + self._remote_path(dst_path) args = ['sftp', '-b', '-', host] if compression: args.append('-C') processutils.execute( *args, process_input=sftp_cmd, on_execute=on_execute, on_completion=on_completion) def _list_files(self, host, dst_path): if host is None: dirs = [] files = [] for f in os.listdir(dst_path): if os.path.isdir(f): dirs.append(f) if os.path.isfile(f): files.append(f) return (dirs, files) args = ['sftp', '-b', '-', host] # Only basename is displayed when executed with -l sftp_cmd = '@ls -1al ' + self._remote_path(dst_path) ls_out = processutils.execute(*args, process_input=sftp_cmd)[0] dirs = [] files = [] for line in ls_out.split(""\n""): if line == '': continue line_split = line.split(' ') is_dir = line_split[0][0] == 'd' name = line_split[-1] if name == ('.') or name == ('..'): continue if is_dir: dirs.append(name) else: files.append(name) return (dirs, files) def _is_dir(self, host, dst_path): if host is None: if not os.path.exists(dst_path): raise exception.NovaException('Path not found: %s' % dst_path) return os.path.isdir(dst_path) args = ['sftp', '-b', '-', host] # Full path is displayed when executed without -l sftp_cmd = '@ls -1a ' + self._remote_path(dst_path) ls_out = processutils.execute(*args, process_input=sftp_cmd)[0] for name in ls_out.split(""\n""): if name == os.path.join(self._remote_path(dst_path), '.'): return True return False def _remote_path(self, path): sftp_remote_chroot = CONF.libvirt.sftp_remote_chroot if sftp_remote_chroot is None: return path chroot_canonical_path = os.path.abspath(sftp_remote_chroot) target_canonical_path = os.path.abspath(path) if os.path.commonpath([chroot_canonical_path, target_canonical_path]) != chroot_canonical_path: raise exception.NovaException('Trying to access outside the chroot: accessing %(path)s, ' 'chroot %(chroot)s' % {'path': path, 'chroot': sftp_remote_chroot}) return path[len(chroot_canonical_path):]",,207,2
openstack%2Fironic~master~I34f58f4e77e7757b89247fd64f5fcde26f679453,openstack/ironic,master,I34f58f4e77e7757b89247fd64f5fcde26f679453,Add hold steps,MERGED,2023-03-30 16:18:46.000000000,2023-07-07 04:31:22.000000000,2023-07-07 04:28:36.000000000,"[{'_account_id': 4571}, {'_account_id': 10239}, {'_account_id': 10342}, {'_account_id': 11655}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-03-30 16:18:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/7313809ca3fb312ea3d375d97b11179b70099771', 'message': 'WIP: Add hold steps\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n'}, {'number': 2, 'created': '2023-03-30 22:07:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/691e1939a3b1ee13425e23771cd22d628e500704', 'message': 'WIP: Add hold steps\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n'}, {'number': 3, 'created': '2023-04-05 21:26:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/6a1e638ef5e307e9044dae6939bfe56018740cd5', 'message': 'WIP: Add hold steps\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n'}, {'number': 4, 'created': '2023-04-05 21:59:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/03c850d5f65f79d3cc1ea005ac65c1f056f263a9', 'message': 'WIP: Add hold steps\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n'}, {'number': 5, 'created': '2023-05-05 00:24:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/b5b0a9686b5a174198f9df459070ecc77d26ca62', 'message': 'WIP: Add hold steps\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n'}, {'number': 6, 'created': '2023-05-05 00:45:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/77e2103e4ab2b45a2dd6915ea41ef7bf25ca1e19', 'message': 'WIP: Add hold steps\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n'}, {'number': 7, 'created': '2023-05-05 00:49:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/fc70efce352fbba97a579a0d84a21050c3d4f252', 'message': 'WIP: Add hold steps\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n'}, {'number': 8, 'created': '2023-05-19 18:19:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/9ad890d2312efad9d02fa95c6abcd5e9cf047e97', 'message': ""Add hold steps\n\n* Updates API version to 1.84 to permit an ``unhold`` verb\n* Adds the ``deploy hold`` and ``clean hold`` provision states\n  to the internal state machine.\n* Adds on documentation on steps to help provide greater clarity\n  to Ironic's users on how to utilize steps. It should be noted\n  this documentation also includes the power state reserved step\n  names from the DPU functionality patch.\n* Fixes the state machine diagram. Changes type to PNG as SVG\n  rendering is broken due to python libraries utilized for SVG\n  generation which do not work on more recent Python versions.\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n""}, {'number': 9, 'created': '2023-05-24 15:10:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/3c76788c71fc49e3800e24b955d225188de1c35a', 'message': ""Add hold steps\n\n* Updates API version to 1.84 to permit an ``unhold`` verb\n* Adds the ``deploy hold`` and ``clean hold`` provision states\n  to the internal state machine.\n* Adds on documentation on steps to help provide greater clarity\n  to Ironic's users on how to utilize steps. It should be noted\n  this documentation also includes the power state reserved step\n  names from the DPU functionality patch.\n* Fixes the state machine diagram. Changes type to PNG as SVG\n  rendering is broken due to python libraries utilized for SVG\n  generation which do not work on more recent Python versions.\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n""}, {'number': 10, 'created': '2023-05-24 21:37:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/c91bfb90672b9cd4c74407c19fe8d59a3658a722', 'message': ""Add hold steps\n\n* Updates API version to 1.84 to permit an ``unhold`` verb\n* Adds the ``deploy hold`` and ``clean hold`` provision states\n  to the internal state machine.\n* Adds on documentation on steps to help provide greater clarity\n  to Ironic's users on how to utilize steps. It should be noted\n  this documentation also includes the power state reserved step\n  names from the DPU functionality patch.\n* Fixes the state machine diagram. Changes type to PNG as SVG\n  rendering is broken due to python libraries utilized for SVG\n  generation which do not work on more recent Python versions.\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n""}, {'number': 11, 'created': '2023-05-24 23:18:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/23045403719a99d46fe429497fd164bbf779977c', 'message': ""Add hold steps\n\n* Updates API version to 1.84 to permit an ``unhold`` verb\n* Adds the ``deploy hold`` and ``clean hold`` provision states\n  to the internal state machine.\n* Adds on documentation on steps to help provide greater clarity\n  to Ironic's users on how to utilize steps. It should be noted\n  this documentation also includes the power state reserved step\n  names from the DPU functionality patch.\n* Fixes the state machine diagram. Changes type to PNG as SVG\n  rendering is broken due to python libraries utilized for SVG\n  generation which do not work on more recent Python versions.\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n""}, {'number': 12, 'created': '2023-05-25 14:27:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/637d294cb15ebd19dea3a5b830bc839d67027d15', 'message': ""Add hold steps\n\n* Updates API version to 1.84 to permit an ``unhold`` verb\n* Adds the ``deploy hold`` and ``clean hold`` provision states\n  to the internal state machine.\n* Adds on documentation on steps to help provide greater clarity\n  to Ironic's users on how to utilize steps. It should be noted\n  this documentation also includes the power state reserved step\n  names from the DPU functionality patch.\n* Fixes the state machine diagram. Changes type to PNG as SVG\n  rendering is broken due to python libraries utilized for SVG\n  generation which do not work on more recent Python versions.\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n""}, {'number': 13, 'created': '2023-06-24 16:06:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/0d7aa19a545954fb4a58f3d0e9377e6d4bba2771', 'message': ""WIP Add hold steps\n\n* Updates API version to 1.84 to permit an ``unhold`` verb\n* Adds the ``deploy hold`` and ``clean hold`` provision states\n  to the internal state machine.\n* Adds on documentation on steps to help provide greater clarity\n  to Ironic's users on how to utilize steps. It should be noted\n  this documentation also includes the power state reserved step\n  names from the DPU functionality patch.\n* Fixes the state machine diagram. Changes type to PNG as SVG\n  rendering is broken due to python libraries utilized for SVG\n  generation which do not work on more recent Python versions.\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n""}, {'number': 14, 'created': '2023-06-30 16:21:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/a8c158d2661c4ea976b2b1ad4795c8c53413b812', 'message': ""Add hold steps\n\n* Updates API version to 1.85 to permit an ``unhold`` verb\n* Adds the ``deploy hold`` and ``clean hold`` provision states\n  to the internal state machine.\n* Adds on documentation on steps to help provide greater clarity\n  to Ironic's users on how to utilize steps. It should be noted\n  this documentation also includes the power state reserved step\n  names from the DPU functionality patch.\n* Fixes the state machine diagram. Changes type to PNG as SVG\n  rendering is broken due to python libraries utilized for SVG\n  generation which do not work on more recent Python versions.\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n""}, {'number': 15, 'created': '2023-06-30 16:37:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic/commit/4aa66f53509d6ac57315ad776439c595ca538ee6', 'message': ""Add hold steps\n\n* Updates API version to 1.85 to permit an ``unhold`` verb\n* Adds the ``deploy hold`` and ``clean hold`` provision states\n  to the internal state machine.\n* Adds on documentation on steps to help provide greater clarity\n  to Ironic's users on how to utilize steps. It should be noted\n  this documentation also includes the power state reserved step\n  names from the DPU functionality patch.\n* Fixes the state machine diagram. Changes type to PNG as SVG\n  rendering is broken due to python libraries utilized for SVG\n  generation which do not work on more recent Python versions.\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n""}, {'number': 16, 'created': '2023-06-30 21:36:03.000000000', 'files': ['doc/source/admin/index.rst', 'ironic/tests/unit/conductor/test_manager.py', 'ironic/tests/unit/conductor/test_cleaning.py', 'ironic/conductor/steps.py', 'ironic/api/controllers/v1/utils.py', 'ironic/common/release_mappings.py', 'ironic/common/states.py', 'doc/source/admin/steps.rst', 'releasenotes/notes/add-hold-states-7be5804d6f3a119a.yaml', 'tools/states_to_dot.py', 'ironic/tests/unit/drivers/modules/test_agent_base.py', 'ironic/tests/unit/api/controllers/v1/test_node.py', 'ironic/conductor/cleaning.py', 'doc/source/contributor/webapi-version-history.rst', 'ironic/conductor/manager.py', 'doc/source/user/states.rst', 'ironic/api/controllers/v1/versions.py', 'ironic/drivers/modules/agent_base.py', 'ironic/tests/unit/conductor/test_deployments.py', 'doc/source/images/states.svg', 'ironic/conductor/deployments.py', 'tox.ini', 'doc/source/images/states.png', 'ironic/api/controllers/v1/node.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/c4e3100d5c99f276aba2d2ed3d55dd9e6650c276', 'message': ""Add hold steps\n\n* Updates API version to 1.85 to permit an ``unhold`` verb\n* Adds the ``deploy hold`` and ``clean hold`` provision states\n  to the internal state machine.\n* Adds on documentation on steps to help provide greater clarity\n  to Ironic's users on how to utilize steps. It should be noted\n  this documentation also includes the power state reserved step\n  names from the DPU functionality patch.\n* Fixes the state machine diagram. Changes type to PNG as SVG\n  rendering is broken due to python libraries utilized for SVG\n  generation which do not work on more recent Python versions.\n\nChange-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453\n""}]",44,879060,c4e3100d5c99f276aba2d2ed3d55dd9e6650c276,61,5,16,11655,,,0,"Add hold steps

* Updates API version to 1.85 to permit an ``unhold`` verb
* Adds the ``deploy hold`` and ``clean hold`` provision states
  to the internal state machine.
* Adds on documentation on steps to help provide greater clarity
  to Ironic's users on how to utilize steps. It should be noted
  this documentation also includes the power state reserved step
  names from the DPU functionality patch.
* Fixes the state machine diagram. Changes type to PNG as SVG
  rendering is broken due to python libraries utilized for SVG
  generation which do not work on more recent Python versions.

Change-Id: I34f58f4e77e7757b89247fd64f5fcde26f679453
",git fetch https://review.opendev.org/openstack/ironic refs/changes/60/879060/11 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/admin/index.rst', 'ironic/tests/unit/drivers/modules/test_agent_base.py', 'ironic/tests/unit/conductor/test_manager.py', 'doc/source/admin/reserved-functional-steps.rst', 'ironic/tests/unit/conductor/test_cleaning.py', 'ironic/conductor/cleaning.py', 'ironic/conductor/manager.py', 'ironic/drivers/modules/agent_base.py', 'ironic/conductor/steps.py', 'ironic/tests/unit/conductor/test_deployments.py', 'ironic/common/states.py', 'ironic/conductor/deployments.py']",12,7313809ca3fb312ea3d375d97b11179b70099771,sleep_step," if conductor_steps.reserved_step_name_handler(task, step): return",,193,8
openstack%2Foctavia~master~I9ca1b2e5c222a9d7dca21c943f375cc07043fe72,openstack/octavia,master,I9ca1b2e5c222a9d7dca21c943f375cc07043fe72,Imported Translations from Zanata,MERGED,2023-06-22 04:12:39.000000000,2023-07-07 04:07:21.000000000,2023-07-07 04:06:06.000000000,"[{'_account_id': 11628}, {'_account_id': 22348}, {'_account_id': 26285}, {'_account_id': 34429}]","[{'number': 1, 'created': '2023-06-22 04:12:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/28810bdcf1d11bab047af30e95fdb34b0590ed82', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I9ca1b2e5c222a9d7dca21c943f375cc07043fe72\n'}, {'number': 2, 'created': '2023-06-23 03:59:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/2f10afcdcdc4877d79a3667b8961aa809c4158a7', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I9ca1b2e5c222a9d7dca21c943f375cc07043fe72\n'}, {'number': 3, 'created': '2023-07-04 02:58:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/octavia/commit/aff5a184d589bb1477ee5ebcdb45bd160d9b8314', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I9ca1b2e5c222a9d7dca21c943f375cc07043fe72\n'}, {'number': 4, 'created': '2023-07-07 03:00:47.000000000', 'files': ['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'], 'web_link': 'https://opendev.org/openstack/octavia/commit/a9359ab7b3a094968cc128d8c22eacc86a93d2e6', 'message': 'Imported Translations from Zanata\n\nFor more information about this automatic import see:\nhttps://docs.openstack.org/i18n/latest/reviewing-translation-import.html\n\nChange-Id: I9ca1b2e5c222a9d7dca21c943f375cc07043fe72\n'}]",1,886699,a9359ab7b3a094968cc128d8c22eacc86a93d2e6,15,4,4,11131,,,0,"Imported Translations from Zanata

For more information about this automatic import see:
https://docs.openstack.org/i18n/latest/reviewing-translation-import.html

Change-Id: I9ca1b2e5c222a9d7dca21c943f375cc07043fe72
",git fetch https://review.opendev.org/openstack/octavia refs/changes/99/886699/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po'],1,28810bdcf1d11bab047af30e95fdb34b0590ed82,zanata/translations,"""POT-Creation-Date: 2023-06-21 17:35+0000\n""""PO-Revision-Date: 2023-06-21 09:03+0000\n""msgid ""10.0.0-59"" msgstr ""10.0.0-59"" msgid ""11.0.0-18"" msgstr ""11.0.0-18"" msgid ""12.0.0-8"" msgstr ""12.0.0-8"" msgid ""12.0.0.0rc1-34"" msgstr ""12.0.0.0rc1-34"" msgid ""2023.1 Series Release Notes"" msgstr ""2023.1 Series Release Notes"" msgid ""5.1.2-37"" msgstr ""5.1.2-37""msgid ""7.1.2-35"" msgstr ""7.1.2-35""msgid ""8.0.1-67"" msgstr ""8.0.1-67""msgid ""9.1.0-17"" msgstr ""9.1.0-17""msgid ""Zed Series Release Notes"" msgstr ""Zed Series Release Notes"" ","""POT-Creation-Date: 2023-05-16 18:42+0000\n""""PO-Revision-Date: 2023-05-08 11:50+0000\n""msgid ""5.1.2-36"" msgstr ""5.1.2-36""msgid ""7.1.2-34"" msgstr ""7.1.2-34""msgid ""8.0.1-66"" msgstr ""8.0.1-66""msgid ""9.1.0-16"" msgstr ""9.1.0-16""",28,10
openstack%2Fneutron~stable%2Fyoga~I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265,openstack/neutron,stable/yoga,I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265,Don't allow deletion of the router ports without IP addresses,MERGED,2023-07-04 14:49:17.000000000,2023-07-07 02:41:30.000000000,2023-07-07 02:39:38.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 14:49:17.000000000', 'files': ['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/4f043fb7ad672d3aa71674ec820b3be5f9aaad9f', 'message': ""Don't allow deletion of the router ports without IP addresses\n\nThis patch effectively reverts old patch [1]. From now on it will be not\nallowed to directly remove router ports which don't have fixed IPs\nassigned. Such ports will be treated as any other ports connected to the\nrouters.\nOriginally [1] was introduced to allow cleanup of the router ports for\nwhich subnets were deleted. But now it's not needed anymore as we\nprevent deletion of subnet if there are any ports with IP allocated from\nthat subnet.\n\nCloses-bug: #2025056\n\n[1] https://review.opendev.org/c/openstack/neutron/+/20424\n\nChange-Id: I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265\n(cherry picked from commit 32d589f03ed0d2744fe15173ebacafd18fced8a9)\n""}]",2,887616,4f043fb7ad672d3aa71674ec820b3be5f9aaad9f,17,3,1,11975,,,0,"Don't allow deletion of the router ports without IP addresses

This patch effectively reverts old patch [1]. From now on it will be not
allowed to directly remove router ports which don't have fixed IPs
assigned. Such ports will be treated as any other ports connected to the
routers.
Originally [1] was introduced to allow cleanup of the router ports for
which subnets were deleted. But now it's not needed anymore as we
prevent deletion of subnet if there are any ports with IP allocated from
that subnet.

Closes-bug: #2025056

[1] https://review.opendev.org/c/openstack/neutron/+/20424

Change-Id: I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265
(cherry picked from commit 32d589f03ed0d2744fe15173ebacafd18fced8a9)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/16/887616/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py']",2,4f043fb7ad672d3aa71674ec820b3be5f9aaad9f,bug/2025056-stable/2023.1-stable/zed-stable/yoga," 'device_id': '44', 'id': 'f', } with testtools.ExpectedException(n_exc.ServicePortInUse): self.db.prevent_l3_port_deletion(mock.Mock(), None)"," 'id': 'f' } self.db.prevent_l3_port_deletion(None, None)",3,11
openstack%2Fcharm-cinder-lvm~master~I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc,openstack/charm-cinder-lvm,master,I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc,Fix target_helper config for >= Wallaby,NEW,2022-02-09 15:24:04.000000000,2023-07-07 02:02:22.000000000,,"[{'_account_id': 2424}, {'_account_id': 10058}, {'_account_id': 10366}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}, {'_account_id': 29625}]","[{'number': 1, 'created': '2022-02-09 15:24:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/e0a0f253cb98eb9b75b7ecc95a6a06a9c5e45834', 'message': ""Include additional parameter needed for cinder-lvm migration\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 2, 'created': '2022-08-19 18:58:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/d5ed2f658f073b49df6af3cef437ca8e3b0e3155', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 3, 'created': '2022-08-31 19:13:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/0f6d8906720972939626769a24772ef86b82cd88', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 4, 'created': '2022-09-20 11:29:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/bec96c9ed7c4654c1704a3fec94a41916e240378', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 5, 'created': '2022-09-20 13:27:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/1f983149b5406f4b6be0e8e565ec7c7377e53142', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 6, 'created': '2022-10-11 12:15:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/96f2be47ea5f1f955497d9a11aedc201701d109e', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 7, 'created': '2022-10-11 14:51:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/a1366909d9129d63ea6b79ce98511f440a60c512', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 8, 'created': '2022-10-13 19:37:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/f9ce4b472100dae79c79d0f2b5ce1e0619ec8a0b', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 9, 'created': '2022-10-21 18:55:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/0780db3b219ae1b9ae2f92273e578c0f9329ee30', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit f9ce4b472100dae79c79d0f2b5ce1e0619ec8a0b)\n""}, {'number': 10, 'created': '2023-03-13 20:24:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/1e29e45644a01d8de12ef1564166ee355dafddd3', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 11, 'created': '2023-03-21 19:58:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/d332e4e7d018f53db4ae5b69c35f37dcd231a8aa', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nDepends-on: I76abbd29ca910fe4c4d62da09e2d2dd3b5c798a6\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 12, 'created': '2023-03-22 14:54:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/858ea0f21eaf68793f6dc7a7306fe15157f5bec3', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nDepends-on: I76abbd29ca910fe4c4d62da09e2d2dd3b5c798a6\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 13, 'created': '2023-03-23 11:54:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/d25274589b82c7e8731fff458d7285ead52fbd39', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 14, 'created': '2023-03-23 14:20:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/ae8094a0aebd073caa3152a357cd94b47eb77029', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 15, 'created': '2023-04-25 21:19:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/71718d359e32564c3666c84403cbc8399a8deb4d', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 16, 'created': '2023-06-16 17:47:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/f904d5380a327ce4de496b7f0b04dab09b2232b5', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 17, 'created': '2023-06-19 15:25:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/3deadd06a9534a4781501dd34343d36345ca91f9', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 18, 'created': '2023-06-19 18:58:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/3acbbbcf170d76733456156295866ae3470b14e7', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 19, 'created': '2023-07-02 15:08:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/2628cacc1ee688833a4b1e892f854eb5aa09b99d', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 20, 'created': '2023-07-06 15:17:47.000000000', 'files': ['src/reactive/cinder_lvm_handlers.py', 'src/lib/charm/openstack/cinder_lvm.py', 'unit_tests/test_lib_charm_openstack_cinder_lvm.py'], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/bc942d2d42efecdb7bd22ef61e2ba71fe3d93167', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}]",46,828574,bc942d2d42efecdb7bd22ef61e2ba71fe3d93167,93,7,20,33717,,,0,"Fix target_helper config for >= Wallaby

Starting with the release of Wallaby, the 'tgtadm' package is
now the default target helper, which doesn't work for cinder-lvm.
As such, we need to explicitly set the new package to 'lioadm'.

Closes-Bug: #1949074
Change-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc
",git fetch https://review.opendev.org/openstack/charm-cinder-lvm refs/changes/74/828574/19 && git format-patch -1 --stdout FETCH_HEAD,"['src/lib/charm/openstack/cinder_lvm.py', 'unit_tests/test_lib_charm_openstack_cinder_lvm.py']",2,e0a0f253cb98eb9b75b7ecc95a6a06a9c5e45834,fix-target-helper," self.patch_object(cinder_lvm, 'os_release') cinder_lvm.os_release.return_value = 'victoria' def test_cinder_wallaby_upgrade(self): charm = self._patch_config_and_charm({}) config = charm.cinder_configuration() self.assertNotIn(('target_helper', 'lioadm'), config) cinder_lvm.os_release.return_value = 'wallaby' config = charm.cinder_configuration() self.assertIn(('target_helper', 'lioadm'), config)",,15,0
openstack%2Fneutron~stable%2F2023.1~I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd,openstack/neutron,stable/2023.1,I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd,[OVN] Prevent Trunk creation/deletion with parent port bound,MERGED,2023-07-04 13:27:43.000000000,2023-07-07 01:27:50.000000000,2023-07-07 01:26:31.000000000,"[{'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 13:27:43.000000000', 'files': ['neutron/tests/unit/db/test_l3_dvr_db.py', 'neutron/tests/unit/services/trunk/test_plugin.py', 'neutron/db/l3_dvr_db.py', 'releasenotes/notes/ovn-trunk-check-parent-port-eeca2eceaca9d158.yaml', 'neutron/services/trunk/drivers/ovn/trunk_driver.py', 'neutron/common/utils.py', 'neutron/services/trunk/plugin.py', 'neutron/tests/functional/services/trunk/drivers/ovn/test_trunk_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/2f48c24d412ee07d7cc609cf8379de83380324e3', 'message': '[OVN] Prevent Trunk creation/deletion with parent port bound\n\nThis patch imitates the ML2/OVS Trunk driver behaviour. When the\ntrunk parent port is bound:\n* A new trunk cannot be created using this parent port.\n* If the port is assigned as parent port of a trunk, this\n  trunk cannot be deleted.\n\nConflicts:\n    neutron/common/utils.py\n\nCloses-Bug: #2022059\nChange-Id: I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd\n(cherry picked from commit 833a6d82cd705548130cdac73a88d388f52c7824)\n'}]",3,887600,2f48c24d412ee07d7cc609cf8379de83380324e3,10,4,1,16688,,,0,"[OVN] Prevent Trunk creation/deletion with parent port bound

This patch imitates the ML2/OVS Trunk driver behaviour. When the
trunk parent port is bound:
* A new trunk cannot be created using this parent port.
* If the port is assigned as parent port of a trunk, this
  trunk cannot be deleted.

Conflicts:
    neutron/common/utils.py

Closes-Bug: #2022059
Change-Id: I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd
(cherry picked from commit 833a6d82cd705548130cdac73a88d388f52c7824)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/00/887600/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/db/test_l3_dvr_db.py', 'neutron/tests/unit/services/trunk/test_plugin.py', 'neutron/db/l3_dvr_db.py', 'releasenotes/notes/ovn-trunk-check-parent-port-eeca2eceaca9d158.yaml', 'neutron/services/trunk/drivers/ovn/trunk_driver.py', 'neutron/common/utils.py', 'neutron/services/trunk/plugin.py', 'neutron/tests/functional/services/trunk/drivers/ovn/test_trunk_driver.py']",8,2f48c24d412ee07d7cc609cf8379de83380324e3,bug/2022059,"from neutron_lib.callbacks import exceptions as n_exc def test_trunk_create_parent_port_bound(self): with self.network() as network: with self.subnet(network=network) as subnet: with self.port(subnet=subnet) as parent_port: pb = port_obj.PortBinding.get_objects( self.context, port_id=parent_port['port']['id']) port_obj.PortBinding.update_object( self.context, {'vif_type': portbindings.VIF_TYPE_OVS}, port_id=pb[0].port_id, host=pb[0].host) tenant_id = uuidutils.generate_uuid() trunk = {'trunk': { 'port_id': parent_port['port']['id'], 'tenant_id': tenant_id, 'project_id': tenant_id, 'admin_state_up': True, 'name': 'trunk', 'sub_ports': []}} self.assertRaises(n_exc.CallbackFailure, self.trunk_plugin.create_trunk, self.context, trunk) def test_trunk_delete_parent_port_bound(self): with self.trunk() as trunk: bp = port_obj.PortBinding.get_objects( self.context, port_id=trunk['port_id']) port_obj.PortBinding.update_object( self.context, {'vif_type': portbindings.VIF_TYPE_OVS}, port_id=bp[0].port_id, host=bp[0].host) self.assertRaises(n_exc.CallbackFailure, self.trunk_plugin.delete_trunk, self.context, trunk['id'])",,79,19
openstack%2Fneutron~stable%2Fzed~I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd,openstack/neutron,stable/zed,I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd,[OVN] Prevent Trunk creation/deletion with parent port bound,MERGED,2023-07-04 13:29:47.000000000,2023-07-07 01:27:49.000000000,2023-07-07 01:26:35.000000000,"[{'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 13:29:47.000000000', 'files': ['neutron/tests/unit/db/test_l3_dvr_db.py', 'neutron/tests/unit/services/trunk/test_plugin.py', 'neutron/db/l3_dvr_db.py', 'releasenotes/notes/ovn-trunk-check-parent-port-eeca2eceaca9d158.yaml', 'neutron/services/trunk/drivers/ovn/trunk_driver.py', 'neutron/common/utils.py', 'neutron/services/trunk/plugin.py', 'neutron/tests/functional/services/trunk/drivers/ovn/test_trunk_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/0d499808f1f3ec6cf40bb87eb32789c463401338', 'message': '[OVN] Prevent Trunk creation/deletion with parent port bound\n\nThis patch imitates the ML2/OVS Trunk driver behaviour. When the\ntrunk parent port is bound:\n* A new trunk cannot be created using this parent port.\n* If the port is assigned as parent port of a trunk, this\n  trunk cannot be deleted.\n\nConflicts:\n    neutron/common/utils.py\n\nCloses-Bug: #2022059\nChange-Id: I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd\n(cherry picked from commit 833a6d82cd705548130cdac73a88d388f52c7824)\n(cherry picked from commit 2f48c24d412ee07d7cc609cf8379de83380324e3)\n'}]",1,887601,0d499808f1f3ec6cf40bb87eb32789c463401338,9,4,1,16688,,,0,"[OVN] Prevent Trunk creation/deletion with parent port bound

This patch imitates the ML2/OVS Trunk driver behaviour. When the
trunk parent port is bound:
* A new trunk cannot be created using this parent port.
* If the port is assigned as parent port of a trunk, this
  trunk cannot be deleted.

Conflicts:
    neutron/common/utils.py

Closes-Bug: #2022059
Change-Id: I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd
(cherry picked from commit 833a6d82cd705548130cdac73a88d388f52c7824)
(cherry picked from commit 2f48c24d412ee07d7cc609cf8379de83380324e3)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/01/887601/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/db/test_l3_dvr_db.py', 'neutron/tests/unit/services/trunk/test_plugin.py', 'neutron/db/l3_dvr_db.py', 'releasenotes/notes/ovn-trunk-check-parent-port-eeca2eceaca9d158.yaml', 'neutron/services/trunk/drivers/ovn/trunk_driver.py', 'neutron/common/utils.py', 'neutron/services/trunk/plugin.py', 'neutron/tests/functional/services/trunk/drivers/ovn/test_trunk_driver.py']",8,0d499808f1f3ec6cf40bb87eb32789c463401338,bug/2022059,"from neutron_lib.callbacks import exceptions as n_exc def test_trunk_create_parent_port_bound(self): with self.network() as network: with self.subnet(network=network) as subnet: with self.port(subnet=subnet) as parent_port: pb = port_obj.PortBinding.get_objects( self.context, port_id=parent_port['port']['id']) port_obj.PortBinding.update_object( self.context, {'vif_type': portbindings.VIF_TYPE_OVS}, port_id=pb[0].port_id, host=pb[0].host) tenant_id = uuidutils.generate_uuid() trunk = {'trunk': { 'port_id': parent_port['port']['id'], 'tenant_id': tenant_id, 'project_id': tenant_id, 'admin_state_up': True, 'name': 'trunk', 'sub_ports': []}} self.assertRaises(n_exc.CallbackFailure, self.trunk_plugin.create_trunk, self.context, trunk) def test_trunk_delete_parent_port_bound(self): with self.trunk() as trunk: bp = port_obj.PortBinding.get_objects( self.context, port_id=trunk['port_id']) port_obj.PortBinding.update_object( self.context, {'vif_type': portbindings.VIF_TYPE_OVS}, port_id=bp[0].port_id, host=bp[0].host) self.assertRaises(n_exc.CallbackFailure, self.trunk_plugin.delete_trunk, self.context, trunk['id'])",,79,19
openstack%2Fansible-role-python_venv_build~master~I4ceeee4bf6c1020851824450ec7b30f6d14573f3,openstack/ansible-role-python_venv_build,master,I4ceeee4bf6c1020851824450ec7b30f6d14573f3,Remove warn argument for command/shell,MERGED,2023-06-30 14:56:07.000000000,2023-07-07 00:12:56.000000000,2023-07-07 00:12:02.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 32666}]","[{'number': 1, 'created': '2023-06-30 14:56:07.000000000', 'files': ['tasks/python_venv_install_symlink.yml'], 'web_link': 'https://opendev.org/openstack/ansible-role-python_venv_build/commit/0e1abb285872c2f677e3e382f9e7fd13abc76ed9', 'message': ""Remove warn argument for command/shell\n\nSince ansible-core 2.14 you can't use warn as module argument.\n\nInstead, a tag is used to instruct ansible-lint to\nsupress alerts.\n\nChange-Id: I4ceeee4bf6c1020851824450ec7b30f6d14573f3\n""}]",1,887384,0e1abb285872c2f677e3e382f9e7fd13abc76ed9,10,3,1,28619,,,0,"Remove warn argument for command/shell

Since ansible-core 2.14 you can't use warn as module argument.

Instead, a tag is used to instruct ansible-lint to
supress alerts.

Change-Id: I4ceeee4bf6c1020851824450ec7b30f6d14573f3
",git fetch https://review.opendev.org/openstack/ansible-role-python_venv_build refs/changes/84/887384/1 && git format-patch -1 --stdout FETCH_HEAD,['tasks/python_venv_install_symlink.yml'],1,0e1abb285872c2f677e3e382f9e7fd13abc76ed9,osa/core-2.15, tags: - skip_ansible_lint, args: warn: no,2,2
openstack%2Fansible-role-systemd_mount~master~I7287d449b8fd0ad970e37aa63b5cb25f88197858,openstack/ansible-role-systemd_mount,master,I7287d449b8fd0ad970e37aa63b5cb25f88197858,Remove warn argument for command/shell,MERGED,2023-06-30 13:28:43.000000000,2023-07-07 00:05:50.000000000,2023-07-07 00:04:57.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 32666}]","[{'number': 1, 'created': '2023-06-30 13:28:43.000000000', 'files': ['tasks/systemd_mounts.yml'], 'web_link': 'https://opendev.org/openstack/ansible-role-systemd_mount/commit/34b6061ab56857976447e50104f585340dde0b05', 'message': ""Remove warn argument for command/shell\n\nSince ansible-core 2.14 you can't use warn as module argument.\n\nIt's being removed to avoid module failure.\n\nChange-Id: I7287d449b8fd0ad970e37aa63b5cb25f88197858\n""}]",0,887378,34b6061ab56857976447e50104f585340dde0b05,8,3,1,28619,,,0,"Remove warn argument for command/shell

Since ansible-core 2.14 you can't use warn as module argument.

It's being removed to avoid module failure.

Change-Id: I7287d449b8fd0ad970e37aa63b5cb25f88197858
",git fetch https://review.opendev.org/openstack/ansible-role-systemd_mount refs/changes/78/887378/1 && git format-patch -1 --stdout FETCH_HEAD,['tasks/systemd_mounts.yml'],1,34b6061ab56857976447e50104f585340dde0b05,osa/core-2.15,, args: warn: no,0,2
openstack%2Fopenstacksdk~master~I68cbb0408ec008f2234d0bed77fd25d4e5b43169,openstack/openstacksdk,master,I68cbb0408ec008f2234d0bed77fd25d4e5b43169,Treat server as a dict in add_server_interfaces,NEW,2023-07-06 22:11:32.000000000,2023-07-06 23:48:25.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-06 22:11:32.000000000', 'files': ['openstack/cloud/meta.py'], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/144c370b3541b31ac0cf9e9b77d7284cbf39d549', 'message': 'Treat server as a dict in add_server_interfaces\n\nSDK is generally favoring treating objects as dictionaries internally.\nIn addition to promoting consistency and facilitating any future changes\naround the Server proxy objects, updating this method to use exclusively\ndictionary access will allow callers to submit a simple dictionary as\nreturned by nova in place of a Server proxy object.\n\nChange-Id: I68cbb0408ec008f2234d0bed77fd25d4e5b43169\n'}]",0,887906,144c370b3541b31ac0cf9e9b77d7284cbf39d549,2,1,1,1,,,0,"Treat server as a dict in add_server_interfaces

SDK is generally favoring treating objects as dictionaries internally.
In addition to promoting consistency and facilitating any future changes
around the Server proxy objects, updating this method to use exclusively
dictionary access will allow callers to submit a simple dictionary as
returned by nova in place of a Server proxy object.

Change-Id: I68cbb0408ec008f2234d0bed77fd25d4e5b43169
",git fetch https://review.opendev.org/openstack/openstacksdk refs/changes/06/887906/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/cloud/meta.py'],1,144c370b3541b31ac0cf9e9b77d7284cbf39d549,, server['access_ipv4'] = server['private_v4'] else: server['access_ipv4'] = server['public_v4'] server['access_ipv6'] = server['public_v6'], server.access_ipv4 = server.private_v4 else: server.access_ipv4 = server.public_v4 server.access_ipv6 = server.public_v6,3,3
openstack%2Fansible-role-pki~master~I453d2db4eeefb78735db54cf9e6fa8fa5f89b069,openstack/ansible-role-pki,master,I453d2db4eeefb78735db54cf9e6fa8fa5f89b069,Convert loop labels to strings,MERGED,2023-06-30 12:50:02.000000000,2023-07-06 23:38:10.000000000,2023-07-06 23:37:19.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 32666}]","[{'number': 1, 'created': '2023-06-30 12:50:02.000000000', 'files': ['tasks/main_certs.yml', 'tasks/standalone/install_ca.yml'], 'web_link': 'https://opendev.org/openstack/ansible-role-pki/commit/1139b8a18f17b1c0755e579e23cc68b6f9be5aaf', 'message': ""Convert loop labels to strings\n\nSince ansible-core 2.15 it's a requirement to have loop\nlabels as strings. In order to remain code readable, we move\ndefinition of label to vars and convert them to json string for\noutput.\n\nChange-Id: I453d2db4eeefb78735db54cf9e6fa8fa5f89b069\n""}]",0,887374,1139b8a18f17b1c0755e579e23cc68b6f9be5aaf,9,3,1,28619,,,0,"Convert loop labels to strings

Since ansible-core 2.15 it's a requirement to have loop
labels as strings. In order to remain code readable, we move
definition of label to vars and convert them to json string for
output.

Change-Id: I453d2db4eeefb78735db54cf9e6fa8fa5f89b069
",git fetch https://review.opendev.org/openstack/ansible-role-pki refs/changes/74/887374/1 && git format-patch -1 --stdout FETCH_HEAD,"['tasks/main_certs.yml', 'tasks/standalone/install_ca.yml']",2,1139b8a18f17b1c0755e579e23cc68b6f9be5aaf,osa/core-2.15," label: ""{{ loop_label | to_json }}"" vars: loop_label:", label:,9,3
openstack%2Frequirements~master~I37fe5174dbd7deadb350aa9956581a944202ffbc,openstack/requirements,master,I37fe5174dbd7deadb350aa9956581a944202ffbc,Bump oslo.db to 12.3.2,MERGED,2023-07-06 09:41:58.000000000,2023-07-06 22:44:39.000000000,2023-07-06 22:42:52.000000000,"[{'_account_id': 14288}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-06 09:41:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/requirements/commit/ed8e05b925d137ef4a51b1930f4aa8cf4de6c2d5', 'message': ""Bump oslo.db to 12.3.2\n\nWe are currently blocked from upgrading to oslo.db 13.x since that\nremoves support for things like sqlalchemy-migrate, which unfortunately\nare still being used by a few projects. However, there's an annoying bug\nthat is causing spurious warning messages during test runs in various\nprojects. This has been fixed on the most recent stable branch [1], so\nwe can at least bump our upper constraint to drag this in while waiting\nfor a chance to use 13.x.\n\nChange-Id: I37fe5174dbd7deadb350aa9956581a944202ffbc\nSigned-off-by: Stephen Finucane <stephenfin@redhat.com>\n""}, {'number': 2, 'created': '2023-07-06 09:44:12.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/95075f38c1a083b268dd3f5177ad30dc71a170d1', 'message': ""Bump oslo.db to 12.3.2\n\nWe are currently blocked from upgrading to oslo.db 13.x since that\nremoves support for things like sqlalchemy-migrate, which unfortunately\nare still being used by a few projects. However, there's an annoying bug\nthat is causing spurious warning messages during test runs in various\nprojects. This has been fixed on the most recent stable branch [1], so\nwe can at least bump our upper constraint to drag this in while waiting\nfor a chance to use 13.x.\n\n[1] https://review.opendev.org/c/openstack/oslo.db/+/880617\n\nChange-Id: I37fe5174dbd7deadb350aa9956581a944202ffbc\nSigned-off-by: Stephen Finucane <stephenfin@redhat.com>\n""}]",0,887781,95075f38c1a083b268dd3f5177ad30dc71a170d1,9,3,2,15334,,,0,"Bump oslo.db to 12.3.2

We are currently blocked from upgrading to oslo.db 13.x since that
removes support for things like sqlalchemy-migrate, which unfortunately
are still being used by a few projects. However, there's an annoying bug
that is causing spurious warning messages during test runs in various
projects. This has been fixed on the most recent stable branch [1], so
we can at least bump our upper constraint to drag this in while waiting
for a chance to use 13.x.

[1] https://review.opendev.org/c/openstack/oslo.db/+/880617

Change-Id: I37fe5174dbd7deadb350aa9956581a944202ffbc
Signed-off-by: Stephen Finucane <stephenfin@redhat.com>
",git fetch https://review.opendev.org/openstack/requirements refs/changes/81/887781/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,ed8e05b925d137ef4a51b1930f4aa8cf4de6c2d5,oslo.db,oslo.db===12.3.2,oslo.db===12.3.1,1,1
openstack%2Frequirements~master~Ia1946f42a187c69872744211fef719bc963b81dc,openstack/requirements,master,Ia1946f42a187c69872744211fef719bc963b81dc,update constraint for python-ironicclient to new release 5.3.0,MERGED,2023-07-06 09:04:18.000000000,2023-07-06 22:43:45.000000000,2023-07-06 22:42:49.000000000,"[{'_account_id': 14288}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-06 09:04:18.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/84bce05233e74771e9c18436cbcb91c0bc01a35b', 'message': 'update constraint for python-ironicclient to new release 5.3.0\n\nmeta: version: 5.3.0\nmeta: diff-start: -\nmeta: series: bobcat\nmeta: branch: master\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: no\nmeta: release:Author: Elod Illes <elod.illes@est.tech>\nmeta: release:Commit: Jay Faulkner <jay@jvf.cc>\nmeta: release:Change-Id: I49a0d84f09177dfba153bd1ec62c698c77bae4ea\nmeta: release:Code-Review+1: Jay Faulkner <jay@jvf.cc>\nmeta: release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nmeta: release:Code-Review+1: Iury Gregory Melo Ferreira <iurygregory@gmail.com>\nmeta: release:Workflow+1: Thierry Carrez <thierry@openstack.org>\nmeta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>\nChange-Id: Ia1946f42a187c69872744211fef719bc963b81dc\n'}]",0,887777,84bce05233e74771e9c18436cbcb91c0bc01a35b,8,3,1,11131,,,0,"update constraint for python-ironicclient to new release 5.3.0

meta: version: 5.3.0
meta: diff-start: -
meta: series: bobcat
meta: branch: master
meta: release-type: release
meta: pypi: yes
meta: first: no
meta: release:Author: Elod Illes <elod.illes@est.tech>
meta: release:Commit: Jay Faulkner <jay@jvf.cc>
meta: release:Change-Id: I49a0d84f09177dfba153bd1ec62c698c77bae4ea
meta: release:Code-Review+1: Jay Faulkner <jay@jvf.cc>
meta: release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
meta: release:Code-Review+1: Iury Gregory Melo Ferreira <iurygregory@gmail.com>
meta: release:Workflow+1: Thierry Carrez <thierry@openstack.org>
meta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>
Change-Id: Ia1946f42a187c69872744211fef719bc963b81dc
",git fetch https://review.opendev.org/openstack/requirements refs/changes/77/887777/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,84bce05233e74771e9c18436cbcb91c0bc01a35b,new-release,python-ironicclient===5.3.0,python-ironicclient===5.2.0,1,1
openstack%2Frequirements~stable%2Fzed~Ib78698d66a261704afe4ae5c43a39218305dcc87,openstack/requirements,stable/zed,Ib78698d66a261704afe4ae5c43a39218305dcc87,update constraint for oslo.messaging to new release 14.0.1,MERGED,2023-07-06 10:13:23.000000000,2023-07-06 22:42:57.000000000,2023-07-06 22:42:57.000000000,"[{'_account_id': 14288}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-06 10:13:23.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/ce23ac59f84228b1747c22311992169cf827cff9', 'message': 'update constraint for oslo.messaging to new release 14.0.1\n\nmeta: version: 14.0.1\nmeta: diff-start: -\nmeta: series: zed\nmeta: branch: stable/zed\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: no\nmeta: release:Author: Hervé Beraud <hberaud@redhat.com>\nmeta: release:Commit: Hervé Beraud <hberaud@redhat.com>\nmeta: release:Change-Id: If65debbef4ae6527f7a811ace99cf69e4b0d5339\nmeta: release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nmeta: release:Workflow+1: Elod Illes <elod.illes@est.tech>\nmeta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>\nChange-Id: Ib78698d66a261704afe4ae5c43a39218305dcc87\n'}]",0,887787,ce23ac59f84228b1747c22311992169cf827cff9,8,3,1,11131,,,0,"update constraint for oslo.messaging to new release 14.0.1

meta: version: 14.0.1
meta: diff-start: -
meta: series: zed
meta: branch: stable/zed
meta: release-type: release
meta: pypi: yes
meta: first: no
meta: release:Author: Hervé Beraud <hberaud@redhat.com>
meta: release:Commit: Hervé Beraud <hberaud@redhat.com>
meta: release:Change-Id: If65debbef4ae6527f7a811ace99cf69e4b0d5339
meta: release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
meta: release:Workflow+1: Elod Illes <elod.illes@est.tech>
meta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>
Change-Id: Ib78698d66a261704afe4ae5c43a39218305dcc87
",git fetch https://review.opendev.org/openstack/requirements refs/changes/87/887787/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,ce23ac59f84228b1747c22311992169cf827cff9,new-release,oslo.messaging===14.0.1,oslo.messaging===14.0.0,1,1
openstack%2Frequirements~stable%2Fyoga~I40e4712dabbf4bc4034d28c9b7ab8de6d88b5fbc,openstack/requirements,stable/yoga,I40e4712dabbf4bc4034d28c9b7ab8de6d88b5fbc,update constraint for oslo.messaging to new release 12.13.1,MERGED,2023-07-06 10:01:50.000000000,2023-07-06 22:42:54.000000000,2023-07-06 22:42:54.000000000,"[{'_account_id': 14288}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-06 10:01:50.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/fb131ba4a3bd11ff87094d96805c925c08b3dfdc', 'message': 'update constraint for oslo.messaging to new release 12.13.1\n\nmeta: version: 12.13.1\nmeta: diff-start: -\nmeta: series: yoga\nmeta: branch: stable/yoga\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: no\nmeta: release:Author: Hervé Beraud <hberaud@redhat.com>\nmeta: release:Commit: Hervé Beraud <hberaud@redhat.com>\nmeta: release:Change-Id: Ice1fa6e981ca88240f6bdc7ce47f823854d1da40\nmeta: release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nmeta: release:Workflow+1: Elod Illes <elod.illes@est.tech>\nmeta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>\nChange-Id: I40e4712dabbf4bc4034d28c9b7ab8de6d88b5fbc\n'}]",0,887786,fb131ba4a3bd11ff87094d96805c925c08b3dfdc,8,3,1,11131,,,0,"update constraint for oslo.messaging to new release 12.13.1

meta: version: 12.13.1
meta: diff-start: -
meta: series: yoga
meta: branch: stable/yoga
meta: release-type: release
meta: pypi: yes
meta: first: no
meta: release:Author: Hervé Beraud <hberaud@redhat.com>
meta: release:Commit: Hervé Beraud <hberaud@redhat.com>
meta: release:Change-Id: Ice1fa6e981ca88240f6bdc7ce47f823854d1da40
meta: release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
meta: release:Workflow+1: Elod Illes <elod.illes@est.tech>
meta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>
Change-Id: I40e4712dabbf4bc4034d28c9b7ab8de6d88b5fbc
",git fetch https://review.opendev.org/openstack/requirements refs/changes/86/887786/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,fb131ba4a3bd11ff87094d96805c925c08b3dfdc,new-release,oslo.messaging===12.13.1,oslo.messaging===12.13.0,1,1
openstack%2Fansible-hardening~master~Ie448fa182db8c1c9f64744ea72f27f285aa64366,openstack/ansible-hardening,master,Ie448fa182db8c1c9f64744ea72f27f285aa64366,Remove warn argument for command/shell,MERGED,2023-06-30 13:05:56.000000000,2023-07-06 22:34:31.000000000,2023-07-06 22:33:35.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 32666}]","[{'number': 1, 'created': '2023-06-30 13:05:56.000000000', 'files': ['tasks/rhel7stig/dnf.yml', 'handlers/main.yml', 'tasks/rhel7stig/file_perms.yml', 'tasks/rhel7stig/async_tasks.yml'], 'web_link': 'https://opendev.org/openstack/ansible-hardening/commit/2c7889852c7e8c6e8ecd14e4ce5304f01792cc01', 'message': ""Remove warn argument for command/shell\n\nSince ansible-core 2.14 you can't use warn as module argument.\n\nInstead, noqa should be used to instruct ansible-lint to\nsupress alerts.\n\nChange-Id: Ie448fa182db8c1c9f64744ea72f27f285aa64366\n""}]",0,887376,2c7889852c7e8c6e8ecd14e4ce5304f01792cc01,8,3,1,28619,,,0,"Remove warn argument for command/shell

Since ansible-core 2.14 you can't use warn as module argument.

Instead, noqa should be used to instruct ansible-lint to
supress alerts.

Change-Id: Ie448fa182db8c1c9f64744ea72f27f285aa64366
",git fetch https://review.opendev.org/openstack/ansible-hardening refs/changes/76/887376/1 && git format-patch -1 --stdout FETCH_HEAD,"['tasks/rhel7stig/dnf.yml', 'handlers/main.yml', 'tasks/rhel7stig/file_perms.yml', 'tasks/rhel7stig/async_tasks.yml']",4,2c7889852c7e8c6e8ecd14e4ce5304f01792cc01,osa/core-2.15," shell: ""rpm -Va > {{ temp_dir }}/rpmverify.txt"" # noqa: command-instead-of-module"," shell: ""rpm -Va > {{ temp_dir }}/rpmverify.txt"" args: warn: no",6,20
openstack%2Frequirements~master~I7732bbfa8b5bda15df9499042253a14271c3bd86,openstack/requirements,master,I7732bbfa8b5bda15df9499042253a14271c3bd86,update constraint for ovsdbapp to new release 2.4.0,MERGED,2023-07-06 08:57:34.000000000,2023-07-06 22:34:18.000000000,2023-07-06 22:32:27.000000000,"[{'_account_id': 14288}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-06 08:57:34.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/b2163742523f5753660e997cef981fe537a8fe9d', 'message': 'update constraint for ovsdbapp to new release 2.4.0\n\nmeta: version: 2.4.0\nmeta: diff-start: -\nmeta: series: bobcat\nmeta: branch: master\nmeta: release-type: release\nmeta: pypi: no\nmeta: first: no\nmeta: release:Author: Elod Illes <elod.illes@est.tech>\nmeta: release:Commit: Elod Illes <elod.illes@est.tech>\nmeta: release:Change-Id: I42f7f568e413cb42d22ea7158d87d27dd20e2fb6\nmeta: release:Code-Review+1: likui <likui@yovole.com>\nmeta: release:Code-Review+1: Rodolfo Alonso <ralonsoh@redhat.com>\nmeta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>\nmeta: release:Code-Review+1: Slawek Kaplonski <skaplons@redhat.com>\nmeta: release:Code-Review+1: Lajos Katona <katonalala@gmail.com>\nmeta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>\nmeta: release:Code-Review+1: Lucas Alvares Gomes <lucasagomes@gmail.com>\nmeta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>\nChange-Id: I7732bbfa8b5bda15df9499042253a14271c3bd86\n'}]",0,887773,b2163742523f5753660e997cef981fe537a8fe9d,8,3,1,11131,,,0,"update constraint for ovsdbapp to new release 2.4.0

meta: version: 2.4.0
meta: diff-start: -
meta: series: bobcat
meta: branch: master
meta: release-type: release
meta: pypi: no
meta: first: no
meta: release:Author: Elod Illes <elod.illes@est.tech>
meta: release:Commit: Elod Illes <elod.illes@est.tech>
meta: release:Change-Id: I42f7f568e413cb42d22ea7158d87d27dd20e2fb6
meta: release:Code-Review+1: likui <likui@yovole.com>
meta: release:Code-Review+1: Rodolfo Alonso <ralonsoh@redhat.com>
meta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>
meta: release:Code-Review+1: Slawek Kaplonski <skaplons@redhat.com>
meta: release:Code-Review+1: Lajos Katona <katonalala@gmail.com>
meta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>
meta: release:Code-Review+1: Lucas Alvares Gomes <lucasagomes@gmail.com>
meta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>
Change-Id: I7732bbfa8b5bda15df9499042253a14271c3bd86
",git fetch https://review.opendev.org/openstack/requirements refs/changes/73/887773/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,b2163742523f5753660e997cef981fe537a8fe9d,new-release,ovsdbapp===2.4.0,ovsdbapp===2.3.0,1,1
openstack%2Frequirements~master~I1cf5cb92825b90cc894117c54ec24cd718a3ba71,openstack/requirements,master,I1cf5cb92825b90cc894117c54ec24cd718a3ba71,update constraint for python-troveclient to new release 8.2.0,MERGED,2023-07-06 08:40:26.000000000,2023-07-06 22:33:33.000000000,2023-07-06 22:32:21.000000000,"[{'_account_id': 14288}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-06 08:40:26.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/18333c0360d4054c00a4171a6f7ee996507418a7', 'message': 'update constraint for python-troveclient to new release 8.2.0\n\nmeta: version: 8.2.0\nmeta: diff-start: -\nmeta: series: bobcat\nmeta: branch: master\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: yes\nmeta: release:Author: Elod Illes <elod.illes@est.tech>\nmeta: release:Commit: Elod Illes <elod.illes@est.tech>\nmeta: release:Change-Id: I65447a5b4897567c9524e28a049919a2b73ff635\nmeta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>\nmeta: release:Code-Review+1: wu.chunyang <wchy1001@gmail.com>\nmeta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>\nmeta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>\nChange-Id: I1cf5cb92825b90cc894117c54ec24cd718a3ba71\n'}]",0,887769,18333c0360d4054c00a4171a6f7ee996507418a7,8,3,1,11131,,,0,"update constraint for python-troveclient to new release 8.2.0

meta: version: 8.2.0
meta: diff-start: -
meta: series: bobcat
meta: branch: master
meta: release-type: release
meta: pypi: yes
meta: first: yes
meta: release:Author: Elod Illes <elod.illes@est.tech>
meta: release:Commit: Elod Illes <elod.illes@est.tech>
meta: release:Change-Id: I65447a5b4897567c9524e28a049919a2b73ff635
meta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>
meta: release:Code-Review+1: wu.chunyang <wchy1001@gmail.com>
meta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>
meta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>
Change-Id: I1cf5cb92825b90cc894117c54ec24cd718a3ba71
",git fetch https://review.opendev.org/openstack/requirements refs/changes/69/887769/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,18333c0360d4054c00a4171a6f7ee996507418a7,new-release,python-troveclient===8.2.0,python-troveclient===8.1.0,1,1
openstack%2Frequirements~master~I9ed31162f7db71ac6135e069fe035cb3abe86b7d,openstack/requirements,master,I9ed31162f7db71ac6135e069fe035cb3abe86b7d,update constraint for octavia-lib to new release 3.3.0,MERGED,2023-07-06 08:41:52.000000000,2023-07-06 22:32:24.000000000,2023-07-06 22:32:24.000000000,"[{'_account_id': 14288}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-06 08:41:52.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/d6b4fead39a2b1f1c71ceebf3c1c2996c1d131f8', 'message': 'update constraint for octavia-lib to new release 3.3.0\n\nmeta: version: 3.3.0\nmeta: diff-start: -\nmeta: series: bobcat\nmeta: branch: master\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: yes\nmeta: release:Author: Elod Illes <elod.illes@est.tech>\nmeta: release:Commit: Elod Illes <elod.illes@est.tech>\nmeta: release:Change-Id: I82779805f32939fb4fdf404875a41361501d2a78\nmeta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>\nmeta: release:Code-Review+1: Gregory Thiemonge <gthiemon@redhat.com>\nmeta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>\nmeta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>\nChange-Id: I9ed31162f7db71ac6135e069fe035cb3abe86b7d\n'}]",0,887770,d6b4fead39a2b1f1c71ceebf3c1c2996c1d131f8,7,3,1,11131,,,0,"update constraint for octavia-lib to new release 3.3.0

meta: version: 3.3.0
meta: diff-start: -
meta: series: bobcat
meta: branch: master
meta: release-type: release
meta: pypi: yes
meta: first: yes
meta: release:Author: Elod Illes <elod.illes@est.tech>
meta: release:Commit: Elod Illes <elod.illes@est.tech>
meta: release:Change-Id: I82779805f32939fb4fdf404875a41361501d2a78
meta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>
meta: release:Code-Review+1: Gregory Thiemonge <gthiemon@redhat.com>
meta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>
meta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>
Change-Id: I9ed31162f7db71ac6135e069fe035cb3abe86b7d
",git fetch https://review.opendev.org/openstack/requirements refs/changes/70/887770/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,d6b4fead39a2b1f1c71ceebf3c1c2996c1d131f8,new-release,octavia-lib===3.3.0,octavia-lib===3.2.0,1,1
openstack%2Fmanila~master~Ia1a3ee69bef76b52e4e6df1e73488c018ac0f3c9,openstack/manila,master,Ia1a3ee69bef76b52e4e6df1e73488c018ac0f3c9,Conditional Import for FIPS Compliance,NEW,2023-06-30 17:29:27.000000000,2023-07-06 22:27:50.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-06-30 17:29:27.000000000', 'files': ['manila/exception.py', 'manila/utils.py'], 'web_link': 'https://opendev.org/openstack/manila/commit/fb44b04282f12b5ab643918ddf0087b35a733de6', 'message': 'Conditional Import for FIPS Compliance\n\nConditionally Import Parakimo\n\nChange-Id: Ia1a3ee69bef76b52e4e6df1e73488c018ac0f3c9\n'}]",1,887422,fb44b04282f12b5ab643918ddf0087b35a733de6,9,1,1,32594,,,0,"Conditional Import for FIPS Compliance

Conditionally Import Parakimo

Change-Id: Ia1a3ee69bef76b52e4e6df1e73488c018ac0f3c9
",git fetch https://review.opendev.org/openstack/manila refs/changes/22/887422/1 && git format-patch -1 --stdout FETCH_HEAD,"['manila/exception.py', 'manila/utils.py']",2,fb44b04282f12b5ab643918ddf0087b35a733de6,fips-compliance,try: import paramiko except ImportError: paramiko = None if paramiko is None: raise exception.RequirementMissing(req='paramiko') ,import paramiko,16,1
openstack%2Frequirements~master~Ib82d11ba5d8fc4e5224089634096d7929dd17c97,openstack/requirements,master,Ib82d11ba5d8fc4e5224089634096d7929dd17c97,update constraint for python-cyborgclient to new release 2.2.0,MERGED,2023-07-06 08:38:20.000000000,2023-07-06 21:49:57.000000000,2023-07-06 21:49:01.000000000,"[{'_account_id': 14288}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-06 08:38:20.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/bc407bd0d2c732b679ce40621890ff6fd51e1ac0', 'message': 'update constraint for python-cyborgclient to new release 2.2.0\n\nmeta: version: 2.2.0\nmeta: diff-start: -\nmeta: series: bobcat\nmeta: branch: master\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: yes\nmeta: release:Author: Elod Illes <elod.illes@est.tech>\nmeta: release:Commit: Elod Illes <elod.illes@est.tech>\nmeta: release:Change-Id: If82d7a0224afbdd5542ac5149479e7dd89f56036\nmeta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>\nmeta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>\nmeta: release:Code-Review+1: Wenping Song <songwenping@inspur.com>\nmeta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>\nChange-Id: Ib82d11ba5d8fc4e5224089634096d7929dd17c97\n'}]",0,887768,bc407bd0d2c732b679ce40621890ff6fd51e1ac0,8,3,1,11131,,,0,"update constraint for python-cyborgclient to new release 2.2.0

meta: version: 2.2.0
meta: diff-start: -
meta: series: bobcat
meta: branch: master
meta: release-type: release
meta: pypi: yes
meta: first: yes
meta: release:Author: Elod Illes <elod.illes@est.tech>
meta: release:Commit: Elod Illes <elod.illes@est.tech>
meta: release:Change-Id: If82d7a0224afbdd5542ac5149479e7dd89f56036
meta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>
meta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>
meta: release:Code-Review+1: Wenping Song <songwenping@inspur.com>
meta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>
Change-Id: Ib82d11ba5d8fc4e5224089634096d7929dd17c97
",git fetch https://review.opendev.org/openstack/requirements refs/changes/68/887768/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,bc407bd0d2c732b679ce40621890ff6fd51e1ac0,new-release,python-cyborgclient===2.2.0,python-cyborgclient===2.1.0,1,1
openstack%2Fmanila~stable%2Fyoga~I1bb888a0b644f0b071816d275d464c4dd27125a7,openstack/manila,stable/yoga,I1bb888a0b644f0b071816d275d464c4dd27125a7,[NetApp] Fix non-disruptive migration cifs shares,NEW,2023-04-24 12:15:37.000000000,2023-07-06 21:38:53.000000000,,"[{'_account_id': 22348}, {'_account_id': 35049}]","[{'number': 1, 'created': '2023-04-24 12:15:37.000000000', 'files': ['manila/share/drivers/netapp/dataontap/cluster_mode/lib_base.py', 'manila/db/sqlalchemy/api.py', 'manila/tests/share/drivers/netapp/dataontap/client/test_client_cmode.py', 'manila/share/drivers/netapp/dataontap/protocols/cifs_cmode.py', 'manila/tests/share/drivers/netapp/dataontap/protocols/fakes.py', 'manila/tests/share/drivers/netapp/dataontap/protocols/test_cifs_cmode.py', 'releasenotes/notes/bug-1920937-fixed-cifs-share-migration-752fde9631fb077a.yaml', 'manila/share/drivers/netapp/dataontap/client/client_cmode.py'], 'web_link': 'https://opendev.org/openstack/manila/commit/e4d8857b543aef9d79632a64f34d0d1c25093349', 'message': '[NetApp] Fix non-disruptive migration cifs shares\n\nMigrate non-disruptive cifs share from different pools change the\nexport location. When the non-disruptive migration complete\nprocess is started a new share and export location is created. As\nresult, Manila finds a conflict between the old export location and\nthe new one.\n\nThis patch add a condition to skip export location creation when\na CIFS migration is in progress, also change the way that the export\nlocation is created. Instead of create the export path with share\nname, the new one is taken from the backend. The fix is only for\nZAPI API calls.\n\nChange-Id: I1bb888a0b644f0b071816d275d464c4dd27125a7\nCo-authored-by: Lucas Oliveira <lucasmoliveira059@gmail.com>\nCloses-bug: #1920937\n(cherry picked from commit 9ddddeeea3123c4af638e54c54a05deb3d59672a)\n'}]",1,881324,e4d8857b543aef9d79632a64f34d0d1c25093349,4,2,1,35002,,,0,"[NetApp] Fix non-disruptive migration cifs shares

Migrate non-disruptive cifs share from different pools change the
export location. When the non-disruptive migration complete
process is started a new share and export location is created. As
result, Manila finds a conflict between the old export location and
the new one.

This patch add a condition to skip export location creation when
a CIFS migration is in progress, also change the way that the export
location is created. Instead of create the export path with share
name, the new one is taken from the backend. The fix is only for
ZAPI API calls.

Change-Id: I1bb888a0b644f0b071816d275d464c4dd27125a7
Co-authored-by: Lucas Oliveira <lucasmoliveira059@gmail.com>
Closes-bug: #1920937
(cherry picked from commit 9ddddeeea3123c4af638e54c54a05deb3d59672a)
",git fetch https://review.opendev.org/openstack/manila refs/changes/24/881324/1 && git format-patch -1 --stdout FETCH_HEAD,"['manila/share/drivers/netapp/dataontap/cluster_mode/lib_base.py', 'manila/db/sqlalchemy/api.py', 'manila/tests/share/drivers/netapp/dataontap/client/test_client_cmode.py', 'manila/share/drivers/netapp/dataontap/protocols/cifs_cmode.py', 'manila/tests/share/drivers/netapp/dataontap/protocols/fakes.py', 'manila/tests/share/drivers/netapp/dataontap/protocols/test_cifs_cmode.py', 'releasenotes/notes/bug-1920937-fixed-cifs-share-migration-752fde9631fb077a.yaml', 'manila/share/drivers/netapp/dataontap/client/client_cmode.py']",8,e4d8857b543aef9d79632a64f34d0d1c25093349,bug/1920937-stable/2023.1-stable/2023.1-stable/zed-stable/yoga," def create_cifs_share(self, share_name, path): api_args = {'path': path, 'share-name': share_name}"," def create_cifs_share(self, share_name): share_path = '/%s' % share_name api_args = {'path': share_path, 'share-name': share_name}",74,24
openstack%2Fopenstacksdk~master~I5868129c49781a2e8b8a5a20c003e9e817f43f21,openstack/openstacksdk,master,I5868129c49781a2e8b8a5a20c003e9e817f43f21,Add share instance export location resource to shared file system,NEW,2023-02-09 20:20:00.000000000,2023-07-06 21:31:16.000000000,,"[{'_account_id': 15334}, {'_account_id': 16643}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-02-09 20:20:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/b6854edb4e3ade634daef0e7a44fb6ae72879440', 'message': '[WIP] Add share instance export location resource to shared file system\n\nChange-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21\n'}, {'number': 2, 'created': '2023-03-28 01:59:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/cc1154b580be80dee31aa46444148fbbc0b9344a', 'message': '[WIP] Add share instance export location resource to shared file system\n\nChange-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 3, 'created': '2023-03-28 16:48:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/8919ccd97b70fdf28e76fe444ced78fb2caef7af', 'message': '[WIP] Add share instance export location resource to shared file system\n\nChange-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 4, 'created': '2023-04-06 19:25:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/13186387427f945a60fa7da12edaa8d6a51179be', 'message': '[WIP] Add share instance export location resource to shared file system\n\nChange-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 5, 'created': '2023-04-18 02:34:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/bc26f3a7c9fba011c6bd6c9c5965da241deef781', 'message': 'Add share instance export location resource to shared file system\n\nChange-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 6, 'created': '2023-04-20 19:34:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/783a9f6c25a3be76ceab46008253f73661f92ce6', 'message': 'Add share instance export location resource to shared file system\n\nChange-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 7, 'created': '2023-04-25 17:55:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/9980b7dbfb6737ebe43a21c0f7889b8b8ca20003', 'message': 'Add share instance export location resource to shared file system\n\nChange-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 8, 'created': '2023-05-02 03:17:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/7b452c795d7ef4718c1a3cbdda680dfeac9adcdb', 'message': 'Add share instance export location resource to shared file system\n\nChange-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 9, 'created': '2023-05-02 03:19:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/31ab6ad3724956c5968cc9c8a4b71c1299f08c8e', 'message': 'Add share instance export location resource to shared file system\n\nChange-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 10, 'created': '2023-05-05 04:17:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/a89d8ff57041bffc88eb3da601563af8bb0a5d6d', 'message': 'Add share instance export location resource to shared file system\n\nChange-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 11, 'created': '2023-05-21 19:03:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/ad5993608f4a38f071a44371315d75a169729f35', 'message': 'Add share instance export location resource to shared file system\n\nChange-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 12, 'created': '2023-07-06 19:15:47.000000000', 'files': ['openstack/tests/unit/shared_file_system/v2/test_proxy.py', 'doc/source/user/proxies/shared_file_system.rst', 'doc/source/user/resources/shared_file_system/v2/share_instance_export_location.rst', 'examples/shared_file_system/share_instance_export_locations.py', 'doc/source/user/resources/shared_file_system/index.rst', 'openstack/shared_file_system/v2/_proxy.py', 'openstack/tests/unit/shared_file_system/v2/test_share_instance_export_location.py', 'openstack/tests/functional/shared_file_system/test_share_instance_export_location.py', 'openstack/shared_file_system/v2/share_instance_export_location.py', 'doc/source/user/guides/shared_file_system.rst'], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/c528e75ec6627384735faa9cda3898c76a92afa8', 'message': 'Add share instance export location resource to shared file system\n\nChange-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}]",50,873327,c528e75ec6627384735faa9cda3898c76a92afa8,38,3,12,32594,,,0,"Add share instance export location resource to shared file system

Change-Id: I5868129c49781a2e8b8a5a20c003e9e817f43f21
Co-Authored-By: Samuel Loegering <samloegering@icloud.com>
",git fetch https://review.opendev.org/openstack/openstacksdk refs/changes/27/873327/12 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/user/proxies/shared_file_system.rst', 'doc/source/user/resources/shared_file_system/v2/share_instance_export_location.rst', 'examples/shared_file_system/share_instance_export_locations.py', 'doc/source/user/resources/shared_file_system/index.rst', 'openstack/shared_file_system/v2/_proxy.py', 'openstack/tests/unit/shared_file_system/v2/test_share_instance_export_location.py', 'openstack/shared_file_system/v2/share_instance_export_location.py', 'openstack/tests/functional/shared_file_system/test_share_instance_export_location.py', 'doc/source/user/guides/shared_file_system.rst']",9,b6854edb4e3ade634daef0e7a44fb6ae72879440,manila, List Share Instace Export Locations ----------------------------------- Retrieve export locations for all the share instances for a given share id .. literalinclude:: ../examples/shared_file_system/share_instance_export_locations.py :pyobject: list_share_instance_export_locations Get Share Instance Export Location ---------------------------------- Get details for a particular export location of a share instance .. literalinclude:: ../examples/shared_file_system/share_instance_export_locations.py :pyobject: get_share_instance_export_location,,269,35
openstack%2Fopenstacksdk~master~I0a192a54c9247fbd9049782dbf8095c73db49512,openstack/openstacksdk,master,I0a192a54c9247fbd9049782dbf8095c73db49512,Add share replica resource to shared file system,NEW,2023-02-09 20:10:36.000000000,2023-07-06 21:08:07.000000000,,"[{'_account_id': 16643}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-02-09 20:10:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/78f6e2083d0291f0dfbbfe934b9a43a2ae46aa77', 'message': '[WIP] Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\n'}, {'number': 2, 'created': '2023-04-10 22:27:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/a942ed05acef51a2018a7d18982183a6a7c7f7b7', 'message': '[WIP] Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 3, 'created': '2023-04-11 18:31:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/91fdc5fbb685380fa937930cab60997660e5797f', 'message': '[WIP] Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 4, 'created': '2023-04-12 17:43:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/44dde2c534cb9caaace421e6a3e0408b1d0c7f24', 'message': '[WIP] Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 5, 'created': '2023-04-13 00:08:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/c1b02bb50d55084abf71c72fcfc816ac5872f4a4', 'message': '[WIP] Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 6, 'created': '2023-04-17 20:10:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/a408a506a3f431a02b8ceeba5abab9fbd460fe2f', 'message': 'Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 7, 'created': '2023-04-20 19:19:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/c572ac57def691a8086820da68f66eecc57d8277', 'message': 'Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 8, 'created': '2023-04-20 19:25:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/f8fa8147808c2abca20ff37a26a16f8d33d6c2ad', 'message': 'Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 9, 'created': '2023-04-22 23:23:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/dc69db601f413a9478bd408083bacddd47ab49ce', 'message': 'Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 10, 'created': '2023-04-23 00:57:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/a9b3f7105328f32563ac9bbae464ad1caf3f1c1b', 'message': 'Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 11, 'created': '2023-04-23 23:02:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/601f11806f1ad19541af6fa73298d43f52994390', 'message': 'Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 12, 'created': '2023-04-25 19:24:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/b35acc4b0fe7dec9d234e26bb9193a6041c505c8', 'message': 'Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 13, 'created': '2023-05-21 18:53:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/8846e0e17bb0c97273a5d3b24d19ffe69c5e07b8', 'message': 'Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}, {'number': 14, 'created': '2023-07-06 19:16:45.000000000', 'files': ['doc/source/user/resources/shared_file_system/v2/share_replica.rst', 'openstack/tests/unit/shared_file_system/v2/test_share_replica.py', 'openstack/tests/unit/shared_file_system/v2/test_proxy.py', 'doc/source/user/proxies/shared_file_system.rst', 'doc/source/user/resources/shared_file_system/index.rst', 'openstack/shared_file_system/v2/share_replica.py', 'examples/shared_file_system/share_replicas.py', 'openstack/shared_file_system/v2/_proxy.py', 'zuul.d/functional-jobs.yaml', 'openstack/tests/functional/shared_file_system/test_share_replica.py', 'doc/source/user/guides/shared_file_system.rst'], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/9162026de67549a080c9f467b3712fcd59f5ef55', 'message': 'Add share replica resource to shared file system\n\nChange-Id: I0a192a54c9247fbd9049782dbf8095c73db49512\nCo-Authored-By: Samuel Loegering <samloegering@icloud.com>\n'}]",99,873324,9162026de67549a080c9f467b3712fcd59f5ef55,38,2,14,32594,,,0,"Add share replica resource to shared file system

Change-Id: I0a192a54c9247fbd9049782dbf8095c73db49512
Co-Authored-By: Samuel Loegering <samloegering@icloud.com>
",git fetch https://review.opendev.org/openstack/openstacksdk refs/changes/24/873324/6 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/user/resources/shared_file_system/v2/share_replica.rst', 'openstack/tests/unit/shared_file_system/v2/test_share_replica.py', 'doc/source/user/proxies/shared_file_system.rst', 'doc/source/user/resources/shared_file_system/index.rst', 'openstack/shared_file_system/v2/share_replica.py', 'examples/shared_file_system/share_replicas.py', 'openstack/shared_file_system/v2/_proxy.py', 'doc/source/user/guides/shared_file_system.rst']",8,78f6e2083d0291f0dfbbfe934b9a43a2ae46aa77,manila, List Share replicas ------------------- Share replicas are the replicated copies of the existing share. You can use Share Replicas to sync data so that each share replica has an identical copy of the same share. Share replication can be used as a disaster recovery solution or as a load sharing mirroring solution. .. literalinclude:: ../examples/shared_file_system/share_replicas.py :pyobject: list_share_replicas Create Share Replica -------------------- Creates a share replica from attributes .. literalinclude:: ../examples/shared_file_system/share_replicas.py :pyobject: create_share_replica Get Share Replica ----------------- List deatils of a single share replica .. literalinclude:: ../examples/shared_file_system/share_replicas.py :pyobject: get_share_replica Delete Share Replica -------------------- Delete a share replica .. literalinclude:: ../examples/shared_file_system/share_replicas.py :pyobject: delete_share_replica Reset Status of Share Replica ----------------------------- Reset status of the share replica .. literalinclude:: ../examples/shared_file_system/share_replicas.py :pyobject: reset_status_share_replica Reset Replica State of Share Replica ------------------------------------ Reset replica_state of the share replica .. literalinclude:: ../examples/shared_file_system/share_replicas.py :pyobject: reset_replica_state_share_replica Force Delete Share Replica -------------------------- Force-delete share replica .. literalinclude:: ../examples/shared_file_system/share_replicas.py :pyobject: force_delete_share_replica Promote Share Replica --------------------- Promote share replica .. literalinclude:: ../examples/shared_file_system/share_replicas.py :pyobject: promote_share_replica Resync Share Replica -------------------- Resync share replica .. literalinclude:: ../examples/shared_file_system/share_replicas.py :pyobject: resync_share_replica,,448,1
openstack%2Foctavia~master~I90ddebabcd90f30fccc6e1751f7c188b35ef16c4,openstack/octavia,master,I90ddebabcd90f30fccc6e1751f7c188b35ef16c4,Fix test_driver_agent tests with newer octavia-lib,MERGED,2023-07-06 06:49:35.000000000,2023-07-06 20:18:32.000000000,2023-07-06 20:17:00.000000000,"[{'_account_id': 11628}, {'_account_id': 22348}, {'_account_id': 29244}]","[{'number': 1, 'created': '2023-07-06 06:49:35.000000000', 'files': ['octavia/tests/functional/api/drivers/driver_agent/test_driver_agent.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/133e74787d544fe27b699d40bf655838d4554c32', 'message': 'Fix test_driver_agent tests with newer octavia-lib\n\nThe tests in test_driver_agent compare recursively the dicts defined in\noctavia with the dicts returned by octavia-lib. But when a new attribute\nis added to octavia-lib, it breaks the tests in the -tips job until the\nnew attribute is included in octavia.\n\nTo mitigate this issue, the tests in octavia should only compare the\ndicts by using the keys that are known by octavia.\n\nChange-Id: I90ddebabcd90f30fccc6e1751f7c188b35ef16c4\n'}]",2,887751,133e74787d544fe27b699d40bf655838d4554c32,8,3,1,29244,,,0,"Fix test_driver_agent tests with newer octavia-lib

The tests in test_driver_agent compare recursively the dicts defined in
octavia with the dicts returned by octavia-lib. But when a new attribute
is added to octavia-lib, it breaks the tests in the -tips job until the
new attribute is included in octavia.

To mitigate this issue, the tests in octavia should only compare the
dicts by using the keys that are known by octavia.

Change-Id: I90ddebabcd90f30fccc6e1751f7c188b35ef16c4
",git fetch https://review.opendev.org/openstack/octavia refs/changes/51/887751/1 && git format-patch -1 --stdout FETCH_HEAD,['octavia/tests/functional/api/drivers/driver_agent/test_driver_agent.py'],1,133e74787d544fe27b699d40bf655838d4554c32,," def _compare_load_balancer_dicts(self, provider_lb_dict, result_dict): for key in (lib_consts.LOADBALANCER_ID, lib_consts.NAME, lib_consts.DESCRIPTION, lib_consts.PROJECT_ID, lib_consts.ADMIN_STATE_UP, lib_consts.VIP_ADDRESS, lib_consts.VIP_NETWORK_ID, lib_consts.VIP_SUBNET_ID, lib_consts.VIP_PORT_ID, lib_consts.VIP_QOS_POLICY_ID, lib_consts.FLAVOR, lib_consts.AVAILABILITY_ZONE, lib_consts.ADDITIONAL_VIPS): self.assertEqual(provider_lb_dict.get(key), result_dict.get(key)) provider_listener_dicts = provider_lb_dict[lib_consts.LISTENERS] result_listener_dicts = result_dict[lib_consts.LISTENERS] self.assertEqual(len(provider_listener_dicts), len(result_listener_dicts)) for listener_dicts in zip(provider_listener_dicts, result_listener_dicts): provider_listener_dict = listener_dicts[0] result_listener_dict = listener_dicts[1] self._compare_listener_dicts(provider_listener_dict, result_listener_dict) self.assertEqual(len(provider_lb_dict[lib_consts.POOLS]), len(result_dict[lib_consts.POOLS])) for pool_dicts in zip(provider_lb_dict[lib_consts.POOLS], result_dict[lib_consts.POOLS]): provider_pool_dict = pool_dicts[0] result_pool_dict = pool_dicts[1] self._compare_pool_dicts(provider_pool_dict, result_pool_dict) def _compare_listener_dicts(self, provider_listener_dict, result_listener_dict): for key in (lib_consts.LISTENER_ID, lib_consts.LOADBALANCER_ID, lib_consts.NAME, lib_consts.DESCRIPTION, lib_consts.PROJECT_ID, lib_consts.ADMIN_STATE_UP, lib_consts.PROTOCOL, lib_consts.PROTOCOL_PORT, lib_consts.CONNECTION_LIMIT, lib_consts.DEFAULT_POOL_ID, lib_consts.TIMEOUT_CLIENT_DATA, lib_consts.TIMEOUT_MEMBER_CONNECT, lib_consts.TIMEOUT_MEMBER_DATA, lib_consts.TIMEOUT_TCP_INSPECT, lib_consts.INSERT_HEADERS, lib_consts.ALLOWED_CIDRS, lib_consts.DEFAULT_TLS_CONTAINER_REF, lib_consts.DEFAULT_TLS_CONTAINER_DATA, lib_consts.SNI_CONTAINER_REFS, lib_consts.SNI_CONTAINER_DATA, lib_consts.CLIENT_CA_TLS_CONTAINER_REF, lib_consts.CLIENT_CA_TLS_CONTAINER_DATA, lib_consts.CLIENT_AUTHENTICATION, lib_consts.CLIENT_CRL_CONTAINER_REF, lib_consts.CLIENT_CRL_CONTAINER_DATA, lib_consts.TLS_CIPHERS, lib_consts.TLS_VERSIONS): self.assertEqual(provider_listener_dict.get(key), result_listener_dict.get(key)) provider_l7policy_dicts = provider_listener_dict.get( lib_consts.L7POLICIES) result_l7policy_dicts = result_listener_dict.get( lib_consts.L7POLICIES) self.assertEqual(len(provider_l7policy_dicts), len(result_l7policy_dicts)) for l7policy_dicts in zip(provider_l7policy_dicts, result_l7policy_dicts): provider_l7policy_dict = l7policy_dicts[0] result_l7policy_dict = l7policy_dicts[1] self._compare_l7policy_dicts(provider_l7policy_dict, result_l7policy_dict) def _compare_l7policy_dicts(self, provider_l7policy_dict, result_l7policy_dict): for key in (lib_consts.L7POLICY_ID, lib_consts.LISTENER_ID, lib_consts.NAME, lib_consts.DESCRIPTION, lib_consts.PROJECT_ID, lib_consts.ADMIN_STATE_UP, lib_consts.ACTION, lib_consts.POSITION, lib_consts.REDIRECT_POOL_ID, lib_consts.REDIRECT_URL, lib_consts.REDIRECT_PREFIX, lib_consts.REDIRECT_HTTP_CODE): self.assertEqual(provider_l7policy_dict.get(key), result_l7policy_dict.get(key)) provider_l7rule_dicts = provider_l7policy_dict.get(lib_consts.L7RULES) result_l7rule_dicts = result_l7policy_dict.get(lib_consts.L7RULES) if provider_l7rule_dicts or result_l7rule_dicts: self.assertIsNotNone(provider_l7rule_dicts) self.assertIsNotNone(result_l7rule_dicts) self.assertEqual(len(provider_l7rule_dicts), len(result_l7rule_dicts)) for l7rule_dicts in zip(provider_l7rule_dicts, result_l7rule_dicts): provider_l7rule_dict = l7rule_dicts[0] result_l7rule_dict = l7rule_dicts[1] self._compare_l7rule_dicts(provider_l7rule_dict, result_l7rule_dict) def _compare_l7rule_dicts(self, provider_l7rule_dict, result_l7rule_dict): for key in (lib_consts.L7RULE_ID, lib_consts.L7POLICY_ID, lib_consts.LISTENER_ID, lib_consts.NAME, lib_consts.DESCRIPTION, lib_consts.PROJECT_ID, lib_consts.ADMIN_STATE_UP, lib_consts.TYPE, lib_consts.COMPARE_TYPE, lib_consts.KEY, lib_consts.VALUE, lib_consts.INVERT): self.assertEqual(provider_l7rule_dict.get(key), result_l7rule_dict.get(key)) def _compare_pool_dicts(self, provider_pool_dict, result_pool_dict): for key in (lib_consts.POOL_ID, lib_consts.NAME, lib_consts.DESCRIPTION, lib_consts.PROJECT_ID, lib_consts.ADMIN_STATE_UP, lib_consts.LB_ALGORITHM, lib_consts.LOADBALANCER_ID, lib_consts.PROTOCOL, lib_consts.SESSION_PERSISTENCE, lib_consts.TLS_ENABLED, lib_consts.TLS_CONTAINER_REF, lib_consts.TLS_CONTAINER_DATA, lib_consts.CA_TLS_CONTAINER_REF, lib_consts.CA_TLS_CONTAINER_DATA, lib_consts.CRL_CONTAINER_REF, lib_consts.CRL_CONTAINER_DATA, lib_consts.TLS_CIPHERS, lib_consts.TLS_VERSIONS, lib_consts.ALPN_PROTOCOLS): self.assertEqual(provider_pool_dict.get(key), result_pool_dict.get(key)) provider_hm_dict = provider_pool_dict.get( lib_consts.HEALTHMONITOR) result_hm_dict = result_pool_dict.get( lib_consts.HEALTHMONITOR) if provider_hm_dict or result_hm_dict: self._compare_hm_dicts(provider_hm_dict, result_hm_dict) provider_member_dicts = provider_pool_dict.get( lib_consts.MEMBERS) result_member_dicts = result_pool_dict.get( lib_consts.MEMBERS) self.assertEqual(len(provider_member_dicts), len(result_member_dicts)) for member_dicts in zip(provider_member_dicts, result_member_dicts): provider_member_dict = member_dicts[0] result_member_dict = member_dicts[1] self._compare_member_dicts(provider_member_dict, result_member_dict) def _compare_hm_dicts(self, provider_hm_dict, result_hm_dict): for key in (lib_consts.HEALTHMONITOR_ID, lib_consts.POOL_ID, lib_consts.NAME, lib_consts.DESCRIPTION, lib_consts.PROJECT_ID, lib_consts.ADMIN_STATE_UP, lib_consts.TYPE, lib_consts.DELAY, lib_consts.TIMEOUT, lib_consts.MAX_RETRIES, lib_consts.MAX_RETRIES_DOWN, lib_consts.DOMAIN_NAME, lib_consts.EXPECTED_CODES, lib_consts.HTTP_METHOD, lib_consts.HTTP_VERSION, lib_consts.URL_PATH): self.assertEqual(provider_hm_dict.get(key), result_hm_dict.get(key)) def _compare_member_dicts(self, provider_member_dict, result_member_dict): for key in (lib_consts.MEMBER_ID, lib_consts.POOL_ID, lib_consts.NAME, lib_consts.DESCRIPTION, lib_consts.PROJECT_ID, lib_consts.ADMIN_STATE_UP, lib_consts.ADDRESS, lib_consts.PROTOCOL_PORT, lib_consts.MONITOR_ADDRESS, lib_consts.MONITOR_PORT, lib_consts.SUBNET_ID, lib_consts.WEIGHT, lib_consts.BACKUP): self.assertEqual(provider_member_dict.get(key), result_member_dict.get(key)) result_dict = result.to_dict(render_unsets=True, recurse=True) self._compare_load_balancer_dicts(self.provider_lb_dict, result_dict) result_dict = result.to_dict(render_unsets=True, recurse=True) self._compare_listener_dicts(self.provider_listener_dict, result_dict) result_dict = result.to_dict(render_unsets=True, recurse=True) self._compare_pool_dicts(self.provider_pool_dict, result_dict) result_dict = result.to_dict(render_unsets=True) self._compare_member_dicts(self.sample_data.provider_member1_dict, result_dict) result_dict = result.to_dict(render_unsets=True) self._compare_hm_dicts(self.sample_data.provider_hm1_dict, result_dict) result_dict = result.to_dict(render_unsets=True, recurse=True) self._compare_l7policy_dicts( self.sample_data.provider_l7policy1_dict, result_dict) result_dict = result.to_dict(render_unsets=True) self._compare_l7rule_dicts( self.sample_data.provider_l7rule1_dict, result_dict)"," self.assertEqual(self.provider_lb_dict, result.to_dict(render_unsets=True, recurse=True)) # We need to recurse here to pick up the SNI data self.assertEqual(self.provider_listener_dict, result.to_dict(render_unsets=True, recurse=True)) self.assertEqual(self.provider_pool_dict, result.to_dict(render_unsets=True, recurse=True)) self.assertEqual(self.sample_data.provider_member1_dict, result.to_dict(render_unsets=True)) self.assertEqual(self.sample_data.provider_hm1_dict, result.to_dict(render_unsets=True)) self.assertEqual(self.sample_data.provider_l7policy1_dict, result.to_dict(render_unsets=True, recurse=True)) self.assertEqual(self.sample_data.provider_l7rule1_dict, result.to_dict(render_unsets=True))",254,15
openstack%2Ftripleo-heat-templates~stable%2Ftrain~I9c762ab31626cb1d3201e09615fb2aa3ab33896d,openstack/tripleo-heat-templates,stable/train,I9c762ab31626cb1d3201e09615fb2aa3ab33896d,Uninstall leapp before running distro-sync,NEW,2023-07-05 14:54:11.000000000,2023-07-06 20:18:19.000000000,,"[{'_account_id': 11090}, {'_account_id': 22348}, {'_account_id': 22954}]","[{'number': 1, 'created': '2023-07-05 14:54:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/5d1829e68a7a1e62b051b38bdfb59b3814decbeb', 'message': 'Make sure leapp is protected before distro-sync\n\nMake sure leapp is protected before distro-sync\n\nChange-Id: I9c762ab31626cb1d3201e09615fb2aa3ab33896d\n'}, {'number': 2, 'created': '2023-07-05 14:57:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/e7eef94f87d7baa281f1f051c9dc79383ae5abb2', 'message': 'Make sure leapp is protected before distro-sync\n\nMake sure leapp is protected before distro-sync\n\nChange-Id: I9c762ab31626cb1d3201e09615fb2aa3ab33896d\n'}, {'number': 3, 'created': '2023-07-05 15:02:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/0063c0b82451875de93da572f31997e0f95c7dd4', 'message': ""Uninstall leapp before running distro-sync\n\nUninstall leapp before running distro-sync as it's no longer needed\nand will generate package conflicts in case leapp behavior changes\nand no longer excludes leapp from the yum/dnf exclusion list.\n\nChange-Id: I9c762ab31626cb1d3201e09615fb2aa3ab33896d\n""}, {'number': 4, 'created': '2023-07-05 20:17:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/424ae32af1405ac870dd0b600c02b3aea8fce841', 'message': ""Uninstall leapp before running distro-sync\n\nUninstall leapp before running distro-sync as it's no longer needed\nand will generate package conflicts in case leapp behavior changes\nand no longer excludes leapp from the yum/dnf exclusion list.\n\nChange-Id: I9c762ab31626cb1d3201e09615fb2aa3ab33896d\nResolves: rhbz#2219844\n""}, {'number': 5, 'created': '2023-07-05 20:46:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/785c7155efd52f1a3f1fa2cf5c941d816dfa8da5', 'message': ""Uninstall leapp before running distro-sync\n\nUninstall leapp before running distro-sync as it's no longer needed\nand will generate package conflicts in case leapp behavior changes\nand no longer excludes leapp from the yum/dnf exclusion list.\n\nChange-Id: I9c762ab31626cb1d3201e09615fb2aa3ab33896d\nResolves: rhbz#2219844\n""}, {'number': 6, 'created': '2023-07-06 16:07:14.000000000', 'files': ['deployment/tripleo-packages/tripleo-packages-baremetal-puppet.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/0d6039ad5f649d3c5bc6e5552347aad6ce854502', 'message': ""Uninstall leapp before running distro-sync\n\nUninstall leapp before running distro-sync as it's no longer needed\nand will generate package conflicts in case leapp behavior changes\nand no longer excludes leapp from the yum/dnf exclusion list.\n\nChange-Id: I9c762ab31626cb1d3201e09615fb2aa3ab33896d\nResolves: rhbz#2219844\n""}]",6,887716,0d6039ad5f649d3c5bc6e5552347aad6ce854502,15,3,6,7130,,,0,"Uninstall leapp before running distro-sync

Uninstall leapp before running distro-sync as it's no longer needed
and will generate package conflicts in case leapp behavior changes
and no longer excludes leapp from the yum/dnf exclusion list.

Change-Id: I9c762ab31626cb1d3201e09615fb2aa3ab33896d
Resolves: rhbz#2219844
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/16/887716/3 && git format-patch -1 --stdout FETCH_HEAD,['deployment/tripleo-packages/tripleo-packages-baremetal-puppet.yaml'],1,5d1829e68a7a1e62b051b38bdfb59b3814decbeb,leapp, - name: Add leapp packages from DNF exclusion shell: dnf config-manager --save --setopt exclude='leapp',,2,0
openstack%2Fpython-manilaclient~master~I05413e9e8b846d725d8fe021daf5d320857aac2f,openstack/python-manilaclient,master,I05413e9e8b846d725d8fe021daf5d320857aac2f,[OSC] Fix output format for osc share subnet create,NEW,2022-09-27 12:44:41.000000000,2023-07-06 20:03:24.000000000,,"[{'_account_id': 6413}, {'_account_id': 16643}, {'_account_id': 22348}, {'_account_id': 29632}, {'_account_id': 32594}]","[{'number': 1, 'created': '2022-09-27 12:44:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/fac1b3e724cd9167475c81d7d08090782238d816', 'message': '[OSC] Formats the output of Openstack share network subnet create check\n\nThis patch fixes the format output when creating share network subnet\nwith check flag.\nCloses-bug: #1989818\n\nChange-Id: I05413e9e8b846d725d8fe021daf5d320857aac2f\n'}, {'number': 2, 'created': '2022-09-29 08:18:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/2d358c50fef27f878716c7cad3168e729cdb0b10', 'message': '[OSC] Formats the output of Openstack share network subnet create check\n\nThis patch fixes the format output when creating share network subnet\nwith check flag.\nCloses-bug: #1989818\n\nChange-Id: I05413e9e8b846d725d8fe021daf5d320857aac2f\n'}, {'number': 3, 'created': '2022-10-05 11:31:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/7020f6ff59e36bf5d349c060b56ae5e14e5ffdc5', 'message': '[OSC] Formats the output of Openstack share network subnet create check\n\nThis patch fixes the format output when creating share network subnet\nwith check flag.\nCloses-bug: #1989818\n\nChange-Id: I05413e9e8b846d725d8fe021daf5d320857aac2f\n'}, {'number': 4, 'created': '2023-03-11 10:45:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/9659f2afbfb126ce081a46eb400b7bb430355c55', 'message': '[OSC] Fix output format for osc share subnet create\n\nThe format of the osc share network subnet create command\noutput has been updated to return a list of hosts instead\nof a dictionary, as we do for other commands in the client.\nWith this change, we are enhancing the consistency of the\nclient and the readability of the output.\n\nCloses-bug: #1989818\n\nChange-Id: I05413e9e8b846d725d8fe021daf5d320857aac2f\n'}, {'number': 5, 'created': '2023-03-11 11:06:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/9c63a5c7a17f5d0d58e8103008c01a42666a37e4', 'message': '[OSC] Fix output format for osc share subnet create\n\nThe format of the osc share network subnet create command\noutput has been updated to return a list of hosts instead\nof a dictionary, as we do for other commands in the client.\nWith this change, we are enhancing the consistency of the\nclient and the readability of the output.\n\nCloses-bug: #1989818\n\nChange-Id: I05413e9e8b846d725d8fe021daf5d320857aac2f\n'}, {'number': 6, 'created': '2023-03-22 12:16:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/ad13ddd741eb5540b69b2a1cafabfc0cc21371e1', 'message': '[OSC] Fix output format for osc share subnet create\n\nThe format of the osc share network subnet create command\noutput has been updated to return a list of hosts instead\nof a dictionary, as we do for other commands in the client.\nWith this change, we are enhancing the consistency of the\nclient and the readability of the output.\n\nCloses-bug: #1989818\n\nChange-Id: I05413e9e8b846d725d8fe021daf5d320857aac2f\n'}, {'number': 7, 'created': '2023-03-26 16:03:51.000000000', 'files': ['manilaclient/tests/functional/osc/test_share_network_subnets.py', 'manilaclient/osc/v2/share_network_subnets.py', 'releasenotes/notes/bug-1989818-fix-share-network-output-format-93d997f0f4a33fab.yaml'], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/799278e644ce3b9578f85ecae546772e4a928cec', 'message': '[OSC] Fix output format for osc share subnet create\n\nThe format of the osc share network subnet create command\noutput has been updated to return a list of hosts instead\nof a dictionary, as we do for other commands in the client.\nWith this change, we are enhancing the consistency of the\nclient and the readability of the output.\n\nCloses-bug: #1989818\n\nChange-Id: I05413e9e8b846d725d8fe021daf5d320857aac2f\n'}]",14,859432,799278e644ce3b9578f85ecae546772e4a928cec,20,5,7,35328,,,0,"[OSC] Fix output format for osc share subnet create

The format of the osc share network subnet create command
output has been updated to return a list of hosts instead
of a dictionary, as we do for other commands in the client.
With this change, we are enhancing the consistency of the
client and the readability of the output.

Closes-bug: #1989818

Change-Id: I05413e9e8b846d725d8fe021daf5d320857aac2f
",git fetch https://review.opendev.org/openstack/python-manilaclient refs/changes/32/859432/2 && git format-patch -1 --stdout FETCH_HEAD,['manilaclient/osc/v2/share_network_subnets.py'],1,fac1b3e724cd9167475c81d7d08090782238d816,bug/1989818,"from manilaclient.common import cliutils if subnet_data: if parsed_args.formater == 'table': for k,v in subnet_data.items(): if isinstance(v, dict): capabilities_list = [v] dict_values = cliutils.convert_dict_list_to_string( capabilities_list ) subnet_data[k] = dict_values",,10,0
openstack%2Fswift~master~I0e928bcb3810e391297300f4949024db3cf87d05,openstack/swift,master,I0e928bcb3810e391297300f4949024db3cf87d05,CI: test under py311,MERGED,2023-06-20 23:42:55.000000000,2023-07-06 20:01:15.000000000,2023-07-04 01:29:04.000000000,"[{'_account_id': 1179}, {'_account_id': 7233}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-20 23:42:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/68cc42cebff62bc55440a03efb4d56354f4c8fde', 'message': 'CI: test under py311\n\nChange-Id: I0e928bcb3810e391297300f4949024db3cf87d05\n'}, {'number': 2, 'created': '2023-06-21 01:16:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/a36f8cb9b3b878246f47f1d81ed15a5a38d928f0', 'message': 'CI: test under py311\n\nChange-Id: I0e928bcb3810e391297300f4949024db3cf87d05\n'}, {'number': 3, 'created': '2023-06-21 16:21:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/f1cf79aa1e2a8a543afa4b0250015fe3fc43adc4', 'message': 'CI: test under py311\n\nJammy only offers a py311 RC, so include the __slots__ hack to avoid\nthe segfault from https://github.com/python/cpython/issues/99886.\n\nFix up a test to work with the slotted connection.\n\nChange-Id: I0e928bcb3810e391297300f4949024db3cf87d05\n'}, {'number': 4, 'created': '2023-06-23 00:28:27.000000000', 'files': ['swift/common/db.py', '.zuul.yaml', 'test/unit/account/test_backend.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/f955e81043b1d04fe7c54c03eaec405c99968ae1', 'message': 'CI: test under py311\n\nJammy only offers a py311 RC, so include the __slots__ hack to avoid\nthe segfault from https://github.com/python/cpython/issues/99886.\n\nFix up a test to work with the slotted connection.\n\nChange-Id: I0e928bcb3810e391297300f4949024db3cf87d05\n'}]",4,886541,f955e81043b1d04fe7c54c03eaec405c99968ae1,22,3,4,15343,,,0,"CI: test under py311

Jammy only offers a py311 RC, so include the __slots__ hack to avoid
the segfault from https://github.com/python/cpython/issues/99886.

Fix up a test to work with the slotted connection.

Change-Id: I0e928bcb3810e391297300f4949024db3cf87d05
",git fetch https://review.opendev.org/openstack/swift refs/changes/41/886541/1 && git format-patch -1 --stdout FETCH_HEAD,"['swift/common/db.py', '.zuul.yaml']",2,68cc42cebff62bc55440a03efb4d56354f4c8fde,py311-support, name: swift-tox-py311 parent: swift-tox-base nodeset: ubuntu-jammy description: | Run unit-tests for swift under cPython version 3.11. Uses tox with the ``py311`` environment. It sets TMPDIR to an XFS mount point created via tools/test-setup.sh. vars: tox_envlist: py311 bindep_profile: test py311 python_version: '3.11' post-run: tools/playbooks/common/cover-post.yaml - job: - swift-tox-py311: irrelevant-files: *unittest-irrelevant-files - swift-tox-py311,,22,0
openstack%2Fneutron~master~I1943e6e0d7d8e255e95f93881cc3caec16ab67fe,openstack/neutron,master,I1943e6e0d7d8e255e95f93881cc3caec16ab67fe,[OVN] Prevent binding a virtual type port,MERGED,2023-05-08 16:09:21.000000000,2023-07-06 19:43:01.000000000,2023-07-06 19:41:38.000000000,"[{'_account_id': 1131}, {'_account_id': 4694}, {'_account_id': 5948}, {'_account_id': 8313}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 22348}, {'_account_id': 32586}, {'_account_id': 34271}, {'_account_id': 34451}]","[{'number': 1, 'created': '2023-05-08 16:09:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/87b7f2a7703a7815f229c3253529acc6319afc4d', 'message': '[WIP] == [OVN] Prevent binding a virtual type port\n\nA LSP is type=virtual when its IP address is used by other ports\nas allowed address. If a LSP is type=virtual, this port cannot be\nbound (that means cannot be used as a port for a virtual machine).\n\nCloses-Bug: #2018529\nChange-Id: I1943e6e0d7d8e255e95f93881cc3caec16ab67fe\n'}, {'number': 2, 'created': '2023-05-09 09:43:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3938ef30fa4460e61230d8abe46ab6ac5e1acd4b', 'message': '[WIP] == [OVN] Prevent binding a virtual type port\n\nA LSP is type=virtual when its IP address is used by other ports\nas allowed address. If a LSP is type=virtual, this port cannot be\nbound (that means cannot be used as a port for a virtual machine).\n\nCloses-Bug: #2018529\nChange-Id: I1943e6e0d7d8e255e95f93881cc3caec16ab67fe\n'}, {'number': 3, 'created': '2023-05-11 17:20:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/7de7bc036d6fe2b33d2fdc9e4bb9a13e36a1aa39', 'message': '[WIP] == [OVN] Prevent binding a virtual type port\n\nTODO: testing\n      reno\n      docuemntation\n\nA LSP is type=virtual when its IP address is used by other ports\nas allowed address. If a LSP is type=virtual, this port cannot be\nbound (that means cannot be used as a port for a virtual machine).\n\nCloses-Bug: #2018529\nChange-Id: I1943e6e0d7d8e255e95f93881cc3caec16ab67fe\n'}, {'number': 4, 'created': '2023-05-30 10:59:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/6d95bf2df3b31b4e0317225df4ea2b44beb5717d', 'message': '[OVN] Prevent binding a virtual type port\n\nA LSP is type=virtual when its IP address is used by other ports\nas allowed address. If a LSP is type=virtual, this port cannot be\nbound (that means cannot be used as a port for a virtual machine).\n\nCloses-Bug: #2018529\nChange-Id: I1943e6e0d7d8e255e95f93881cc3caec16ab67fe\n'}, {'number': 5, 'created': '2023-05-31 16:30:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/342fd5d7ef535c1e6e2b5f1265b82dbc15676f22', 'message': '[OVN] Prevent binding a virtual type port\n\nA LSP is type=virtual when its IP address is used by other ports\nas allowed address. If a LSP is type=virtual, this port cannot be\nbound (that means cannot be used as a port for a virtual machine).\n\nCloses-Bug: #2018529\nChange-Id: I1943e6e0d7d8e255e95f93881cc3caec16ab67fe\n'}, {'number': 6, 'created': '2023-06-02 15:48:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e71c7fd1a3d056fdf6dfafea51ddbfcb114c4016', 'message': '[OVN] Prevent binding a virtual type port\n\nA LSP is type=virtual when its IP address is used by other ports\nas allowed address. If a LSP is type=virtual, this port cannot be\nbound (that means cannot be used as a port for a virtual machine).\n\nCloses-Bug: #2018529\nChange-Id: I1943e6e0d7d8e255e95f93881cc3caec16ab67fe\n'}, {'number': 7, 'created': '2023-07-03 08:15:11.000000000', 'files': ['neutron/common/ovn/utils.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'releasenotes/notes/ovn-virtual-port-prevent-binding-50efba5521e8a28e.yaml', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/68ecae5ff9a364e41126cb338902f1a36fc9413f', 'message': '[OVN] Prevent binding a virtual type port\n\nA LSP is type=virtual when its IP address is used by other ports\nas allowed address. If a LSP is type=virtual, this port cannot be\nbound (that means cannot be used as a port for a virtual machine).\n\nCloses-Bug: #2018529\nChange-Id: I1943e6e0d7d8e255e95f93881cc3caec16ab67fe\n'}]",28,882588,68ecae5ff9a364e41126cb338902f1a36fc9413f,69,11,7,16688,,,0,"[OVN] Prevent binding a virtual type port

A LSP is type=virtual when its IP address is used by other ports
as allowed address. If a LSP is type=virtual, this port cannot be
bound (that means cannot be used as a port for a virtual machine).

Closes-Bug: #2018529
Change-Id: I1943e6e0d7d8e255e95f93881cc3caec16ab67fe
",git fetch https://review.opendev.org/openstack/neutron refs/changes/88/882588/4 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/common/ovn/utils.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py']",2,87b7f2a7703a7815f229c3253529acc6319afc4d,bug/2018529, ovn_utils,,21,0
openstack%2Fdesignate~stable%2Fzed~I04c104cfc9ee2f03d0c5adca3c80bbfff20afb70,openstack/designate,stable/zed,I04c104cfc9ee2f03d0c5adca3c80bbfff20afb70,Fix TsigKeyring issues with dnspython 2.x,MERGED,2023-07-01 07:34:36.000000000,2023-07-06 19:41:40.000000000,2023-07-06 19:40:32.000000000,"[{'_account_id': 11628}, {'_account_id': 13252}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-01 07:34:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/designate/commit/0b624558a42bdc158ff09b5a1b27c0fa2dbb485b', 'message': 'Fix TsigKeyring issues with dnspython 2.x\n\n- Fixed issues in TsigKeyring.\n- Fixed tsgi issues in mdns handler.\n- Fixed invalid secret used in tests.\n- Added additional test coverage.\n- Re-enabled previously broken test.\n\nCloses-Bug: #1982252\nChange-Id: I04c104cfc9ee2f03d0c5adca3c80bbfff20afb70\n(cherry picked from commit 38c591eaa1109e7ea4600563e505dbdbe8a59b37)\n'}, {'number': 2, 'created': '2023-07-01 16:04:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/designate/commit/0b6816711f5320e738c48c202761f40656c74fd4', 'message': 'Fix TsigKeyring issues with dnspython 2.x\n\n- Fixed issues in TsigKeyring.\n- Fixed tsgi issues in mdns handler.\n- Fixed invalid secret used in tests.\n- Added additional test coverage.\n- Re-enabled previously broken test.\n\nCloses-Bug: #1982252\nChange-Id: I04c104cfc9ee2f03d0c5adca3c80bbfff20afb70\n(cherry picked from commit 38c591eaa1109e7ea4600563e505dbdbe8a59b37)\n'}, {'number': 3, 'created': '2023-07-02 09:49:29.000000000', 'files': ['designate/tests/__init__.py', 'designate/mdns/handler.py', 'designate/dnsutils.py', 'designate/tests/test_dnsutils.py', 'designate/tests/test_mdns/test_handler.py'], 'web_link': 'https://opendev.org/openstack/designate/commit/a99367cff117268ec265a12d1f46fdf0376b40cc', 'message': 'Fix TsigKeyring issues with dnspython 2.x\n\n- Fixed issues in TsigKeyring.\n- Fixed tsgi issues in mdns handler.\n- Fixed invalid secret used in tests.\n- Added additional test coverage.\n- Re-enabled previously broken test.\n\nAdditionally modified unit test to provide a storage provider,\nas this does not exist in the next release.\n\nCloses-Bug: #1982252\nChange-Id: I04c104cfc9ee2f03d0c5adca3c80bbfff20afb70\n(cherry picked from commit 38c591eaa1109e7ea4600563e505dbdbe8a59b37)\n'}]",2,887451,a99367cff117268ec265a12d1f46fdf0376b40cc,14,3,3,13252,,,0,"Fix TsigKeyring issues with dnspython 2.x

- Fixed issues in TsigKeyring.
- Fixed tsgi issues in mdns handler.
- Fixed invalid secret used in tests.
- Added additional test coverage.
- Re-enabled previously broken test.

Additionally modified unit test to provide a storage provider,
as this does not exist in the next release.

Closes-Bug: #1982252
Change-Id: I04c104cfc9ee2f03d0c5adca3c80bbfff20afb70
(cherry picked from commit 38c591eaa1109e7ea4600563e505dbdbe8a59b37)
",git fetch https://review.opendev.org/openstack/designate refs/changes/51/887451/1 && git format-patch -1 --stdout FETCH_HEAD,"['designate/tests/__init__.py', 'designate/mdns/handler.py', 'designate/dnsutils.py', 'designate/tests/test_dnsutils.py', 'designate/tests/test_mdns/test_handler.py']",5,0b624558a42bdc158ff09b5a1b27c0fa2dbb485b,,"import dns.tsigkeyring request.use_tsig(dns.tsigkeyring.from_text( {'test-key-two': 'AnotherSecretKey'}) ) args = [request.keyname, request.keyring.secret, 300, request.id, request.tsig_error, b'', request.mac, request.keyalgorithm]","from unittest import expectedFailure @expectedFailure request.keyring = {request.keyname: ''} request.had_tsig = True args = [request.keyname, request.keyring[request.keyname], request.fudge, request.original_id, request.tsig_error, request.other_data, request.mac, request.keyalgorithm]",99,16
openstack%2Fneutron~stable%2Fxena~I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265,openstack/neutron,stable/xena,I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265,Don't allow deletion of the router ports without IP addresses,MERGED,2023-07-04 14:49:38.000000000,2023-07-06 19:37:44.000000000,2023-07-06 19:36:22.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 14:49:38.000000000', 'files': ['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/2417b76aad624003488c22781143ec55f7cd1a62', 'message': ""Don't allow deletion of the router ports without IP addresses\n\nThis patch effectively reverts old patch [1]. From now on it will be not\nallowed to directly remove router ports which don't have fixed IPs\nassigned. Such ports will be treated as any other ports connected to the\nrouters.\nOriginally [1] was introduced to allow cleanup of the router ports for\nwhich subnets were deleted. But now it's not needed anymore as we\nprevent deletion of subnet if there are any ports with IP allocated from\nthat subnet.\n\nCloses-bug: #2025056\n\n[1] https://review.opendev.org/c/openstack/neutron/+/20424\n\nChange-Id: I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265\n(cherry picked from commit 32d589f03ed0d2744fe15173ebacafd18fced8a9)\n""}]",0,887617,2417b76aad624003488c22781143ec55f7cd1a62,9,3,1,11975,,,0,"Don't allow deletion of the router ports without IP addresses

This patch effectively reverts old patch [1]. From now on it will be not
allowed to directly remove router ports which don't have fixed IPs
assigned. Such ports will be treated as any other ports connected to the
routers.
Originally [1] was introduced to allow cleanup of the router ports for
which subnets were deleted. But now it's not needed anymore as we
prevent deletion of subnet if there are any ports with IP allocated from
that subnet.

Closes-bug: #2025056

[1] https://review.opendev.org/c/openstack/neutron/+/20424

Change-Id: I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265
(cherry picked from commit 32d589f03ed0d2744fe15173ebacafd18fced8a9)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/17/887617/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py']",2,2417b76aad624003488c22781143ec55f7cd1a62,bug/2025056-stable/2023.1-stable/zed-stable/yoga-stable/xena," 'device_id': '44', 'id': 'f', } with testtools.ExpectedException(n_exc.ServicePortInUse): self.db.prevent_l3_port_deletion(mock.Mock(), None)"," 'id': 'f' } self.db.prevent_l3_port_deletion(None, None)",3,11
openstack%2Fpython-openstackclient~stable%2F2023.1~I716f6a1496fc552b32809c7eb744283f3a3cd5a4,openstack/python-openstackclient,stable/2023.1,I716f6a1496fc552b32809c7eb744283f3a3cd5a4,Fix pep issue in the network service provider,MERGED,2023-07-06 15:46:02.000000000,2023-07-06 19:37:36.000000000,2023-07-06 19:36:26.000000000,"[{'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 15:46:02.000000000', 'files': ['openstackclient/network/v2/network_service_provider.py'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/07c5a26fe56a6374fcbcfaa61f9c340f9c0fdc10', 'message': 'Fix pep issue in the network service provider\n\npep gods started complaining (correctfully) about spacing in the old\ncommand. Apply `black -l 79` on the file to make it looking nice and\npassing checks.\n\nChange-Id: I716f6a1496fc552b32809c7eb744283f3a3cd5a4\n(cherry picked from commit a675c61e469067e556a39e89cb1b06484122004d)\n'}]",1,887812,07c5a26fe56a6374fcbcfaa61f9c340f9c0fdc10,7,2,1,7973,,,0,"Fix pep issue in the network service provider

pep gods started complaining (correctfully) about spacing in the old
command. Apply `black -l 79` on the file to make it looking nice and
passing checks.

Change-Id: I716f6a1496fc552b32809c7eb744283f3a3cd5a4
(cherry picked from commit a675c61e469067e556a39e89cb1b06484122004d)
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/12/887812/1 && git format-patch -1 --stdout FETCH_HEAD,['openstackclient/network/v2/network_service_provider.py'],1,07c5a26fe56a6374fcbcfaa61f9c340f9c0fdc10,," ""service_type"", ""name"", ""is_default"", ""Service Type"", ""Name"", ""Default"", return ( column_headers, ( utils.get_item_properties( s, columns, ) for s in data ), )"," 'service_type', 'name', 'is_default', 'Service Type', 'Name', 'Default', return(column_headers, (utils.get_item_properties( s, columns, ) for s in data))",16,10
openstack%2Fpython-manilaclient~master~I9f6f5adbd7905a23b2fcfafc204e0477983bb630,openstack/python-manilaclient,master,I9f6f5adbd7905a23b2fcfafc204e0477983bb630,[OSC] Add OSC Functional Tests Snapshot Instances,NEW,2022-05-18 18:29:14.000000000,2023-07-06 19:33:30.000000000,,"[{'_account_id': 16643}, {'_account_id': 22348}, {'_account_id': 33648}]","[{'number': 1, 'created': '2022-05-18 18:29:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/9f8b7d64857ce821c39b6001531b7679a90fdff6', 'message': '[WIC] Add OSC Functional Tests\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 2, 'created': '2022-05-18 18:33:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/520c1ae8c993cfb682f60056bc7179fb9709743b', 'message': '[WIP][OSC] Add OSC Functional Tests\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 3, 'created': '2022-05-18 19:51:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/fa8f804a17a4a62dfef9cd77999fa8b170f72075', 'message': '[WIP][OSC] Add OSC Functional Tests\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 4, 'created': '2022-05-19 14:12:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/a5065a72573e08c33f078fd879005fae8a5d67f3', 'message': '[OSC] Add OSC Functional Tests\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 5, 'created': '2022-05-26 20:37:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/0734f6e0b0195fe9d60468384e75ed985dec3869', 'message': '[OSC] Add OSC Functional Tests Snapshot Instances\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 6, 'created': '2022-06-29 16:02:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/3c49372863b027c4991f7d32854e9090d7812144', 'message': '[OSC] Add OSC Functional Tests Snapshot Instances\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 7, 'created': '2022-07-01 17:32:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/9b1068249b7f9ec8e5cef4c62bf9e00640824ed4', 'message': '[OSC] Add OSC Functional Tests Snapshot Instances\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 8, 'created': '2022-07-13 16:44:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/5485b4039db4a346c60a2ed5fe37a60fe397502f', 'message': '[OSC] Add OSC Functional Tests Snapshot Instances\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 9, 'created': '2022-07-18 17:11:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/0b37c9a035fec09be2120bac8bfccbe941b39e5f', 'message': '[OSC] Add OSC Functional Tests Snapshot Instances\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 10, 'created': '2022-07-25 18:03:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/aa80ae2f17e4776d57f31aa9cdd56c334cba9f66', 'message': '[OSC] Add OSC Functional Tests Snapshot Instances\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 11, 'created': '2022-08-23 14:24:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/104035698a402f0c9d8f1c75c7f6bb27f4334380', 'message': '[OSC] Add OSC Functional Tests Snapshot Instances\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 12, 'created': '2022-08-23 19:03:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/67af247f0869fe763caf2655f414c8c82b0229f2', 'message': '[OSC] Add OSC Functional Tests Snapshot Instances\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 13, 'created': '2022-10-17 16:45:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/592b07f865262ed0fc0bcce29d98f12ad2fdf716', 'message': '[OSC] Add OSC Functional Tests Snapshot Instances\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 14, 'created': '2023-01-23 15:17:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/bb6d98100d653960c81db5a4b5f7c7e8998fee12', 'message': '[OSC] Add OSC Functional Tests Snapshot Instances\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}, {'number': 15, 'created': '2023-07-06 18:25:26.000000000', 'files': ['manilaclient/tests/functional/osc/test_share_snapshot_instances.py', 'manilaclient/tests/functional/osc/base.py'], 'web_link': 'https://opendev.org/openstack/python-manilaclient/commit/273bc724b4fcbd9887415278a7230d4f21068ff9', 'message': '[OSC] Add OSC Functional Tests Snapshot Instances\n\nAdds osc functional tests for share snapshot instances\nshow, list, and set\n\nPartially-implements: bp openstack-client-support\nChange-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630\n'}]",31,842425,273bc724b4fcbd9887415278a7230d4f21068ff9,44,3,15,32594,,,0,"[OSC] Add OSC Functional Tests Snapshot Instances

Adds osc functional tests for share snapshot instances
show, list, and set

Partially-implements: bp openstack-client-support
Change-Id: I9f6f5adbd7905a23b2fcfafc204e0477983bb630
",git fetch https://review.opendev.org/openstack/python-manilaclient refs/changes/25/842425/12 && git format-patch -1 --stdout FETCH_HEAD,"['manilaclient/tests/functional/osc/test_share_snapshot_instances.py', 'manilaclient/tests/functional/osc/base.py']",2,9f8b7d64857ce821c39b6001531b7679a90fdff6,osc-functional-tests," def create_snapshot(self, share, name=None, description=None, force=None, add_cleanup=True): name = name or data_utils.rand_name('autotest_snapshot_name') cmd = ('snapshot create ' '%(share)s %(name)s %(desc)s' % {'share': share, 'name': '--name %s' % name, 'desc': '--description %s' % description}) cmd = cmd +' --wait' if force: cmd = cmd + ' --force %s' % force snapshot_object = self.dict_result('share', cmd) self._wait_for_object_status( 'share snapshot', snapshot_object['id'], 'available') if add_cleanup: self.addCleanup( self.openstack, 'share snapshot delete %s' % snapshot_object['id'] ) return snapshot_object",,98,0
openstack%2Fproject-config~master~I7fc9c9bebb7385143c2f281d9a5df702777d79b2,openstack/project-config,master,I7fc9c9bebb7385143c2f281d9a5df702777d79b2,"Migrate ""os-ken"" project to Launchpad",MERGED,2023-07-05 16:39:45.000000000,2023-07-06 19:24:44.000000000,2023-07-05 17:00:49.000000000,"[{'_account_id': 4694}, {'_account_id': 5263}, {'_account_id': 5948}, {'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 16:39:45.000000000', 'files': ['gerrit/projects.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/37ad65c16fe4206552febe62ac76fa7c327587f8', 'message': 'Migrate ""os-ken"" project to Launchpad\n\nThe project ""os-ken"" will report bugs in the Neutron Launchpad project\nsite [1], with the title prexif ""[os-ken]"" and using the tag ""os-ken"".\nThat decission was taken in the last Neutron team meeting [2].\n\n[1]https://bugs.launchpad.net/neutron/\n[2]https://meetings.opendev.org/meetings/networking/2023/networking.2023-07-04-14.00.log.html#l-85\n\nChange-Id: I7fc9c9bebb7385143c2f281d9a5df702777d79b2\n'}]",0,887726,37ad65c16fe4206552febe62ac76fa7c327587f8,8,6,1,16688,,,0,"Migrate ""os-ken"" project to Launchpad

The project ""os-ken"" will report bugs in the Neutron Launchpad project
site [1], with the title prexif ""[os-ken]"" and using the tag ""os-ken"".
That decission was taken in the last Neutron team meeting [2].

[1]https://bugs.launchpad.net/neutron/
[2]https://meetings.opendev.org/meetings/networking/2023/networking.2023-07-04-14.00.log.html#l-85

Change-Id: I7fc9c9bebb7385143c2f281d9a5df702777d79b2
",git fetch https://review.opendev.org/openstack/project-config refs/changes/26/887726/1 && git format-patch -1 --stdout FETCH_HEAD,['gerrit/projects.yaml'],1,37ad65c16fe4206552febe62ac76fa7c327587f8,os-ken_launchpad,, use-storyboard: true,0,1
openstack%2Fcharm-manila~stable%2Fyoga~I557d02dcbbde91f3f12b243f43be3a062c5180ad,openstack/charm-manila,stable/yoga,I557d02dcbbde91f3f12b243f43be3a062c5180ad,DNM - test to see if stable/yoga gate is working,ABANDONED,2023-06-14 08:57:35.000000000,2023-07-06 19:22:56.000000000,,"[{'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-14 08:57:35.000000000', 'files': ['rebuild'], 'web_link': 'https://opendev.org/openstack/charm-manila/commit/17c3dc7c252c1de0c509626f3f5da25074c1243c', 'message': 'DNM - test to see if stable/yoga gate is working\n\nChange-Id: I557d02dcbbde91f3f12b243f43be3a062c5180ad\n'}]",0,886048,17c3dc7c252c1de0c509626f3f5da25074c1243c,5,2,1,20870,,,0,"DNM - test to see if stable/yoga gate is working

Change-Id: I557d02dcbbde91f3f12b243f43be3a062c5180ad
",git fetch https://review.opendev.org/openstack/charm-manila refs/changes/48/886048/1 && git format-patch -1 --stdout FETCH_HEAD,['rebuild'],1,17c3dc7c252c1de0c509626f3f5da25074c1243c,test-gate,260008f3-f00f-4def-8196-6c86cec8786c,fdb2c6cd-5f92-40c2-9af1-fecff8a72d87,1,1
openstack%2Fcharm-manila~stable%2Fzed~I92b34b68b8ea73deda6bdc67c3d5ccd1358a62ad,openstack/charm-manila,stable/zed,I92b34b68b8ea73deda6bdc67c3d5ccd1358a62ad,DNM - Test to see if gate is working,ABANDONED,2023-06-14 08:56:25.000000000,2023-07-06 19:22:43.000000000,,"[{'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-14 08:56:25.000000000', 'files': ['rebuild'], 'web_link': 'https://opendev.org/openstack/charm-manila/commit/d91b551caa2eef528ac66aea8144541a9a86c14e', 'message': 'DNM - Test to see if gate is working\n\nChange-Id: I92b34b68b8ea73deda6bdc67c3d5ccd1358a62ad\n'}]",0,886047,d91b551caa2eef528ac66aea8144541a9a86c14e,5,2,1,20870,,,0,"DNM - Test to see if gate is working

Change-Id: I92b34b68b8ea73deda6bdc67c3d5ccd1358a62ad
",git fetch https://review.opendev.org/openstack/charm-manila refs/changes/47/886047/1 && git format-patch -1 --stdout FETCH_HEAD,['rebuild'],1,d91b551caa2eef528ac66aea8144541a9a86c14e,test-gate,fa7fb5fd-c940-4bb0-87b8-9d85869aa57c,53cf1d5c-1178-11ec-8ac2-bb3a2551099e,1,1
openstack%2Fironic-python-agent-builder~master~I430ba8f86883b233b975f615e0e50b01e22c66e6,openstack/ironic-python-agent-builder,master,I430ba8f86883b233b975f615e0e50b01e22c66e6,Remove outdated install pyyaml with pip2,MERGED,2023-06-19 08:48:59.000000000,2023-07-06 19:22:04.000000000,2023-07-06 19:21:05.000000000,"[{'_account_id': 11655}, {'_account_id': 22348}, {'_account_id': 24828}, {'_account_id': 32761}]","[{'number': 1, 'created': '2023-06-19 08:48:59.000000000', 'files': ['roles/ipa-build-dib-image/tasks/install.yaml'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent-builder/commit/241d14cd2406a96fd83256e408b5106cf612d3b6', 'message': ""Remove outdated install pyyaml with pip2\n\nWe don't support python2 since a while\n\nChange-Id: I430ba8f86883b233b975f615e0e50b01e22c66e6\n""}]",0,886379,241d14cd2406a96fd83256e408b5106cf612d3b6,10,4,1,23851,,,0,"Remove outdated install pyyaml with pip2

We don't support python2 since a while

Change-Id: I430ba8f86883b233b975f615e0e50b01e22c66e6
",git fetch https://review.opendev.org/openstack/ironic-python-agent-builder refs/changes/79/886379/1 && git format-patch -1 --stdout FETCH_HEAD,['roles/ipa-build-dib-image/tasks/install.yaml'],1,241d14cd2406a96fd83256e408b5106cf612d3b6,remove-pip2-command,,"# NOTE(dtantsur): work around the issue in older DIB versions when some # elements try to use the default Python instead of the one DIB is using, # failing with ""No module named yaml"" - name: Install PyYAML in Python 2 pip: name: PyYAML extra_args: -c ""{{ ansible_user_dir }}/{{ zuul.projects['opendev.org/openstack/requirements'].src_dir }}/upper-constraints.txt"" executable: pip2 become: true ignore_errors: true",0,10
openstack%2Fswift~master~Ic1962975e5ff7025a86e18fc02f5de01c91aff73,openstack/swift,master,Ic1962975e5ff7025a86e18fc02f5de01c91aff73,updater: Redirect to root on client errors,NEW,2023-07-05 20:07:46.000000000,2023-07-06 19:17:55.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-05 20:07:46.000000000', 'files': ['swift/obj/updater.py', 'test/unit/obj/test_updater.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/2ac7d9dfcdf440f3b6120341002c65beb895cb8c', 'message': ""updater: Redirect to root on client errors\n\nWe've seen (rather old) updates get stuck because they're pointed at a\nshard whose DBs have been unlinked. This was likely done manually, as\npart of some overlapping-shard-ranges cleanup. Since there's no deleted\nDB to point us toward the new shard, the update hangs around retrying\nfor a full reclaim_age.\n\nNow, treat all 4xx errors as a (soft) redirect to the root. If we got\nany explicit redirects, trust those more -- but if all we've got are\n404s, we should really be going back to the root.\n\nChange-Id: Ic1962975e5ff7025a86e18fc02f5de01c91aff73\n""}]",3,887740,2ac7d9dfcdf440f3b6120341002c65beb895cb8c,5,1,1,15343,,,0,"updater: Redirect to root on client errors

We've seen (rather old) updates get stuck because they're pointed at a
shard whose DBs have been unlinked. This was likely done manually, as
part of some overlapping-shard-ranges cleanup. Since there's no deleted
DB to point us toward the new shard, the update hangs around retrying
for a full reclaim_age.

Now, treat all 4xx errors as a (soft) redirect to the root. If we got
any explicit redirects, trust those more -- but if all we've got are
404s, we should really be going back to the root.

Change-Id: Ic1962975e5ff7025a86e18fc02f5de01c91aff73
",git fetch https://review.opendev.org/openstack/swift refs/changes/40/887740/1 && git format-patch -1 --stdout FETCH_HEAD,"['swift/obj/updater.py', 'test/unit/obj/test_updater.py']",2,2ac7d9dfcdf440f3b6120341002c65beb895cb8c,," def test_obj_put_async_404_redirects_to_root(self): policies = list(POLICIES) random.shuffle(policies) # setup updater conf = { 'devices': self.devices_dir, 'mount_check': 'false', 'swift_dir': self.testdir, } daemon = object_updater.ObjectUpdater(conf, logger=self.logger) async_dir = os.path.join(self.sda1, get_async_dir(policies[0])) os.mkdir(async_dir) dfmanager = DiskFileManager(conf, daemon.logger) ts_obj = next(self.ts_iter) self._write_async_update(dfmanager, ts_obj, policies[0], container_path='shard/container') orig_async_path, orig_async_data = self._check_async_file(async_dir) # run once fake_responses = [ # only round of update attempts; going back to the root rewrites # the pickle (404, {}), (404, {}), (404, {}), ] fake_status_codes, fake_headers = zip(*fake_responses) with mocked_http_conn( *fake_status_codes, headers=fake_headers) as conn: with mock.patch('swift.obj.updater.dump_recon_cache'): daemon.run_once() self._check_update_requests(conn.requests[:3], ts_obj, policies[0]) self._check_update_requests(conn.requests[3:], ts_obj, policies[0]) self.assertEqual(['/sda1/0/shard/container/o'] * 3, [req['path'] for req in conn.requests]) self.assertEqual( {'redirects': 1, 'async_pendings': 1}, daemon.logger.get_increment_counts()) # async file now points to root async_path, async_data = self._check_async_file(async_dir) self.assertEqual(orig_async_path, async_path) expected = dict(orig_async_data, container_path=None, redirect_history=[]) self.assertEqual(expected, async_data) ",,68,5
openstack%2Fnova-specs~master~I66a17b0840be4a9340a41022001420884d9f59eb,openstack/nova-specs,master,I66a17b0840be4a9340a41022001420884d9f59eb,Propose tooling and docs for unified limits,MERGED,2023-06-27 01:52:06.000000000,2023-07-06 18:41:55.000000000,2023-07-06 18:40:17.000000000,"[{'_account_id': 7166}, {'_account_id': 11604}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-27 01:52:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/c1b640532f5934b107654d3c8e46919a02216ea0', 'message': 'Propose tooling and docs for unified limits\n\nRelated to blueprint unified-limits-nova-tool-and-docs\n\nChange-Id: I66a17b0840be4a9340a41022001420884d9f59eb\n'}, {'number': 2, 'created': '2023-06-27 22:23:50.000000000', 'files': ['specs/2023.2/approved/unified-limits-nova-tool-and-docs.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/15e09863f15271d485d05ab919f83dbfc6952db0', 'message': 'Propose tooling and docs for unified limits\n\nRelated to blueprint unified-limits-nova-tool-and-docs\n\nChange-Id: I66a17b0840be4a9340a41022001420884d9f59eb\n'}]",23,887014,15e09863f15271d485d05ab919f83dbfc6952db0,14,3,2,4690,,,0,"Propose tooling and docs for unified limits

Related to blueprint unified-limits-nova-tool-and-docs

Change-Id: I66a17b0840be4a9340a41022001420884d9f59eb
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/14/887014/2 && git format-patch -1 --stdout FETCH_HEAD,['specs/2023.2/approved/unified-limits-nova-tool-and-docs.rst'],1,c1b640532f5934b107654d3c8e46919a02216ea0,bp/unified-limits-nova-tool-and-docs,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode =================================== Tooling and Docs for Unified Limits =================================== https://blueprints.launchpad.net/nova/+spec/unified-limits-nova-tool-and-docs In the Yoga release support for Unified Limits was added in Nova as an experimental feature to get early feedback and fix issues that were found by operators trying it out. Now that a few releases have passed, we want to go ahead and formalize the unified limits quota driver by creating a tool to help operators copy their existing legacy quota limits from Nova to unified limits in Keystone, publish official documentation in the Nova quota documentation, and removing the note on the ``[quota]driver=nova.quota.UnifiedLimitsDriver`` config option indicating its experimental status. Problem description =================== Currently there is no documentation in the Nova docs about unified limits and there isn't any automated tool for generating unified limits in Keystone from existing Nova legacy quota limits. Use Cases --------- * As an operator, I would like to use a tool to automatically copy my existing legacy quota limits from Nova to unified limits in Keystone. * As an operator, I would like formal documentation for unified limits quotas to be available. Proposed change =============== We propose to create an automated tool, for example maybe ``nova-manage limits migrate_to_unified_limits`` that will read existing legacy quota limits from the Nova database and config options and create equivalent unified limits for them in Keystone using the Keystone REST API. It will be able to migrate both default limits and project-scoped limits. It will not migrate user-scoped limits as they are not supported by unified limits. We will add formal docs about unified limits to the Nova docs and remove the note on the ``[quota]driver`` config option about the ``nova.quota.UnifiedLimitsDriver`` being in a development state. Alternatives ------------ Operators can create unified limits using the ``openstack limit`` openstack client commands without a provided tool. Data model impact ----------------- None REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- End users will be able to read documentation about how quotas work with unified limits. Performance Impact ------------------ None Other deployer impact --------------------- Deployers will have the option of using the quota limit migration tool to copy existing legacy Nova quota limits into Keystone unified limits instead of using openstackclient commands or otherwise calling the Keystone REST API manually. Developer impact ---------------- None Upgrade impact -------------- None Implementation ============== Assignee(s) ----------- Primary assignee: melwitt Other contributors: None Feature Liaison --------------- Feature liaison: melwitt Work Items ---------- * Develop a ``nova-manage limits`` command to copy existing legacy Nova quota limits from the Nova database and config options to unified limits by calling the Keystone REST API * Write documentation for unified limits in Nova * Remove note from ``[quota]driver=nova.quota.UnifiedLimitsDriver`` about the driver being in a development state * Collaborate with Keystone team to remove the docs warning in https://docs.openstack.org/keystone/latest/admin/unified-limits.html about the unified limits API labeled as experimental Dependencies ============ * https://specs.openstack.org/openstack/nova-specs/specs/yoga/implemented/unified-limits-nova.html Testing ======= Unit and/or functional testing for the quota limit migrate tool wil be added. We can also test the quota limit migrate tool alongside the existing nova/tools/hooks/post_test_hook.sh unified limits coverage in the nova-next CI job. Documentation Impact ==================== Operators will be most affected by the addition of Nova unified limits documentation. The following docs will need to be updated: * https://docs.openstack.org/nova/latest/user/quotas.html * https://docs.openstack.org/nova/latest/admin/quotas.html * https://docs.openstack.org/nova/latest/cli/nova-manage.html References ========== * https://etherpad.opendev.org/p/nova-bobcat-ptg#L415 * https://docs.openstack.org/keystone/latest/admin/unified-limits.html History ======= .. list-table:: Revisions :header-rows: 1 * - Release Name - Description * - 2023.2 Bobcat - Introduced ",,182,0
openstack%2Fbifrost~master~I208182e65884d63548d78c68f676b899c562a2dc,openstack/bifrost,master,I208182e65884d63548d78c68f676b899c562a2dc,CI: Update cached cirros image to 0.5.3,MERGED,2023-06-12 12:57:47.000000000,2023-07-06 18:39:18.000000000,2023-07-06 18:38:23.000000000,"[{'_account_id': 10239}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-12 12:57:47.000000000', 'files': ['playbooks/test-bifrost.yaml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/5ee710cafb3d4ba772a134c95f936223c157d80f', 'message': ""CI: Update cached cirros image to 0.5.3\n\nBifrost CI is currently failing to fetch the cirros image from cache:\n\n    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'\n\nThis may be caused by the removal of cirros-0.5.1 images from cache in\nchange Ibada405e0c1183559f428c749d0e54d0a45a2223.\n\nSwitch to cirros version 0.5.3 image instead.\n\nChange-Id: I208182e65884d63548d78c68f676b899c562a2dc\n""}]",0,885874,5ee710cafb3d4ba772a134c95f936223c157d80f,10,2,1,15197,,,0,"CI: Update cached cirros image to 0.5.3

Bifrost CI is currently failing to fetch the cirros image from cache:

    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'

This may be caused by the removal of cirros-0.5.1 images from cache in
change Ibada405e0c1183559f428c749d0e54d0a45a2223.

Switch to cirros version 0.5.3 image instead.

Change-Id: I208182e65884d63548d78c68f676b899c562a2dc
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/74/885874/1 && git format-patch -1 --stdout FETCH_HEAD,['playbooks/test-bifrost.yaml'],1,5ee710cafb3d4ba772a134c95f936223c157d80f,, cirros_deploy_image_upstream_url: file:///opt/cache/files/cirros-0.5.3-x86_64-disk.img, cirros_deploy_image_upstream_url: file:///opt/cache/files/cirros-0.5.1-x86_64-disk.img,1,1
openstack%2Ftripleo-docs~master~I935982424e4b3ceaa614db585475e3e1aabccb69,openstack/tripleo-docs,master,I935982424e4b3ceaa614db585475e3e1aabccb69,Move the crush hierarchy example close to the option definition,MERGED,2023-07-04 15:12:46.000000000,2023-07-06 18:27:18.000000000,2023-07-06 18:25:52.000000000,"[{'_account_id': 18002}, {'_account_id': 22348}, {'_account_id': 25402}, {'_account_id': 34598}]","[{'number': 1, 'created': '2023-07-04 15:12:46.000000000', 'files': ['deploy-guide/source/features/deployed_ceph.rst'], 'web_link': 'https://opendev.org/openstack/tripleo-docs/commit/a8151e9563ccf158a90e72308cd6048d076a6f2f', 'message': 'Move the crush hierarchy example close to the option definition\n\nThe TLD section was added between the definition and the example\nof the same option, so fix the order.\n\nChange-Id: I935982424e4b3ceaa614db585475e3e1aabccb69\n'}]",1,887631,a8151e9563ccf158a90e72308cd6048d076a6f2f,10,4,1,10459,,,0,"Move the crush hierarchy example close to the option definition

The TLD section was added between the definition and the example
of the same option, so fix the order.

Change-Id: I935982424e4b3ceaa614db585475e3e1aabccb69
",git fetch https://review.opendev.org/openstack/tripleo-docs refs/changes/31/887631/1 && git format-patch -1 --stdout FETCH_HEAD,['deploy-guide/source/features/deployed_ceph.rst'],1,a8151e9563ccf158a90e72308cd6048d076a6f2f,fix-crush-hierarchy,"TLD option ---------- During ceph spec generation, if ``--tld`` is passed to `ceph_spec_bootstrap`_ ansible module, generated spec will have the hostnames appended with tld. This ``--tld`` option is available in `openstack overcloud ceph deploy` and `openstack overcloud ceph spec` commands. for example:: openstack overcloud ceph deploy \ --tld ""redhat.local"" During `openstack overcloud ceph deploy` , even the hostnames of all overcloud nodes are appended with ``--tld`` option, which makes it a Fully qualified Domain name (canonical name) suitable for TLS-e configuration. ","TLD option ---------- During ceph spec generation, if ``--tld`` is passed to `ceph_spec_bootstrap`_ ansible module, generated spec will have the hostnames appended with tld. This ``--tld`` option is available in `openstack overcloud ceph deploy` and `openstack overcloud ceph spec` commands. for example:: openstack overcloud ceph deploy \ --tld ""redhat.local"" During `openstack overcloud ceph deploy` , even the hostnames of all overcloud nodes are appended with ``--tld`` option, which makes it a Fully qualified Domain name (canonical name) suitable for TLS-e configuration. ",17,17
openstack%2Fneutron~stable%2Fxena~I89db15dd1b629bc963f3b63926391a4a02cbedf7,openstack/neutron,stable/xena,I89db15dd1b629bc963f3b63926391a4a02cbedf7,Ensure traffic is not centralized if DVR is enabled,MERGED,2023-07-05 05:32:57.000000000,2023-07-06 18:04:31.000000000,2023-07-06 18:03:02.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 05:32:57.000000000', 'files': ['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'releasenotes/notes/dvr-external-mac-934409413e515eb2.yaml', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/ba7768d7ec399b1fd6bd3c806ea2ad20ce5c8fe7', 'message': 'Ensure traffic is not centralized if DVR is enabled\n\nThere is no need to clear the external_mac if DVR is enabled, not\neven when the port is down. This patch ensures the external_mac is\nonly deleted when DVR is not enabled.\n\nWithout this patch, if a VM with a floating IP gets deleted, and\nDVR is enabled, during some time the traffic gets (wrongly)\ncentralized while it should not. And it is also generating more\nload on the OVN side unnecesarily.\n\nCloses-Bug: #2025264\n\nChange-Id: I89db15dd1b629bc963f3b63926391a4a02cbedf7\n(cherry picked from commit 0090572b93d96348778c724194c26a976a9f8757)\n'}]",2,887626,ba7768d7ec399b1fd6bd3c806ea2ad20ce5c8fe7,15,3,1,23567,,,0,"Ensure traffic is not centralized if DVR is enabled

There is no need to clear the external_mac if DVR is enabled, not
even when the port is down. This patch ensures the external_mac is
only deleted when DVR is not enabled.

Without this patch, if a VM with a floating IP gets deleted, and
DVR is enabled, during some time the traffic gets (wrongly)
centralized while it should not. And it is also generating more
load on the OVN side unnecesarily.

Closes-Bug: #2025264

Change-Id: I89db15dd1b629bc963f3b63926391a4a02cbedf7
(cherry picked from commit 0090572b93d96348778c724194c26a976a9f8757)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/26/887626/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'releasenotes/notes/dvr-external-mac-934409413e515eb2.yaml', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py']",3,ba7768d7ec399b1fd6bd3c806ea2ad20ce5c8fe7,," def _update_dnat_entry_if_needed(self, port_id): if ovn_conf.is_ovn_distributed_floating_ip(): mac = nat['external_ids'].get(ovn_const.OVN_FIP_EXT_MAC_KEY) if mac and nat['external_mac'] != mac: self._update_dnat_entry_if_needed(port_id)"," def _update_dnat_entry_if_needed(self, port_id, up=True): if up and ovn_conf.is_ovn_distributed_floating_ip(): mac = nat['external_ids'][ovn_const.OVN_FIP_EXT_MAC_KEY] if nat['external_mac'] != mac: self._update_dnat_entry_if_needed(port_id, False)",28,15
openstack%2Fneutron~stable%2Fyoga~I89db15dd1b629bc963f3b63926391a4a02cbedf7,openstack/neutron,stable/yoga,I89db15dd1b629bc963f3b63926391a4a02cbedf7,Ensure traffic is not centralized if DVR is enabled,MERGED,2023-07-05 05:32:40.000000000,2023-07-06 18:04:29.000000000,2023-07-06 18:02:58.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 05:32:40.000000000', 'files': ['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'releasenotes/notes/dvr-external-mac-934409413e515eb2.yaml', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/61d9e5972d30318650faf8f0ff88756000bdef6a', 'message': 'Ensure traffic is not centralized if DVR is enabled\n\nThere is no need to clear the external_mac if DVR is enabled, not\neven when the port is down. This patch ensures the external_mac is\nonly deleted when DVR is not enabled.\n\nWithout this patch, if a VM with a floating IP gets deleted, and\nDVR is enabled, during some time the traffic gets (wrongly)\ncentralized while it should not. And it is also generating more\nload on the OVN side unnecesarily.\n\nCloses-Bug: #2025264\n\nChange-Id: I89db15dd1b629bc963f3b63926391a4a02cbedf7\n(cherry picked from commit 0090572b93d96348778c724194c26a976a9f8757)\n'}]",1,887625,61d9e5972d30318650faf8f0ff88756000bdef6a,12,3,1,23567,,,0,"Ensure traffic is not centralized if DVR is enabled

There is no need to clear the external_mac if DVR is enabled, not
even when the port is down. This patch ensures the external_mac is
only deleted when DVR is not enabled.

Without this patch, if a VM with a floating IP gets deleted, and
DVR is enabled, during some time the traffic gets (wrongly)
centralized while it should not. And it is also generating more
load on the OVN side unnecesarily.

Closes-Bug: #2025264

Change-Id: I89db15dd1b629bc963f3b63926391a4a02cbedf7
(cherry picked from commit 0090572b93d96348778c724194c26a976a9f8757)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/25/887625/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'releasenotes/notes/dvr-external-mac-934409413e515eb2.yaml', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py']",3,61d9e5972d30318650faf8f0ff88756000bdef6a,," def _update_dnat_entry_if_needed(self, port_id): if ovn_conf.is_ovn_distributed_floating_ip(): mac = nat['external_ids'].get(ovn_const.OVN_FIP_EXT_MAC_KEY) if mac and nat['external_mac'] != mac: self._update_dnat_entry_if_needed(port_id)"," def _update_dnat_entry_if_needed(self, port_id, up=True): if up and ovn_conf.is_ovn_distributed_floating_ip(): mac = nat['external_ids'][ovn_const.OVN_FIP_EXT_MAC_KEY] if nat['external_mac'] != mac: self._update_dnat_entry_if_needed(port_id, False)",28,15
openstack%2Fneutron~stable%2Fzed~I89db15dd1b629bc963f3b63926391a4a02cbedf7,openstack/neutron,stable/zed,I89db15dd1b629bc963f3b63926391a4a02cbedf7,Ensure traffic is not centralized if DVR is enabled,MERGED,2023-07-05 05:32:25.000000000,2023-07-06 18:04:21.000000000,2023-07-06 18:02:54.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 05:32:25.000000000', 'files': ['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'releasenotes/notes/dvr-external-mac-934409413e515eb2.yaml', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/b9b819d7665ee72fd3fd86b1e08f2121451e6c94', 'message': 'Ensure traffic is not centralized if DVR is enabled\n\nThere is no need to clear the external_mac if DVR is enabled, not\neven when the port is down. This patch ensures the external_mac is\nonly deleted when DVR is not enabled.\n\nWithout this patch, if a VM with a floating IP gets deleted, and\nDVR is enabled, during some time the traffic gets (wrongly)\ncentralized while it should not. And it is also generating more\nload on the OVN side unnecesarily.\n\nCloses-Bug: #2025264\n\nChange-Id: I89db15dd1b629bc963f3b63926391a4a02cbedf7\n(cherry picked from commit 0090572b93d96348778c724194c26a976a9f8757)\n'}]",1,887624,b9b819d7665ee72fd3fd86b1e08f2121451e6c94,12,3,1,23567,,,0,"Ensure traffic is not centralized if DVR is enabled

There is no need to clear the external_mac if DVR is enabled, not
even when the port is down. This patch ensures the external_mac is
only deleted when DVR is not enabled.

Without this patch, if a VM with a floating IP gets deleted, and
DVR is enabled, during some time the traffic gets (wrongly)
centralized while it should not. And it is also generating more
load on the OVN side unnecesarily.

Closes-Bug: #2025264

Change-Id: I89db15dd1b629bc963f3b63926391a4a02cbedf7
(cherry picked from commit 0090572b93d96348778c724194c26a976a9f8757)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/24/887624/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'releasenotes/notes/dvr-external-mac-934409413e515eb2.yaml', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py']",3,b9b819d7665ee72fd3fd86b1e08f2121451e6c94,," def _update_dnat_entry_if_needed(self, port_id): if ovn_conf.is_ovn_distributed_floating_ip(): mac = nat['external_ids'].get(ovn_const.OVN_FIP_EXT_MAC_KEY) if mac and nat['external_mac'] != mac: self._update_dnat_entry_if_needed(port_id)"," def _update_dnat_entry_if_needed(self, port_id, up=True): if up and ovn_conf.is_ovn_distributed_floating_ip(): mac = nat['external_ids'][ovn_const.OVN_FIP_EXT_MAC_KEY] if nat['external_mac'] != mac: self._update_dnat_entry_if_needed(port_id, False)",28,15
openstack%2Fneutron~stable%2F2023.1~I89db15dd1b629bc963f3b63926391a4a02cbedf7,openstack/neutron,stable/2023.1,I89db15dd1b629bc963f3b63926391a4a02cbedf7,Ensure traffic is not centralized if DVR is enabled,MERGED,2023-07-05 05:31:35.000000000,2023-07-06 18:04:05.000000000,2023-07-06 18:02:50.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 05:31:35.000000000', 'files': ['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'releasenotes/notes/dvr-external-mac-934409413e515eb2.yaml', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/fc29d622675ac4a7672416e735230a5759a49f41', 'message': 'Ensure traffic is not centralized if DVR is enabled\n\nThere is no need to clear the external_mac if DVR is enabled, not\neven when the port is down. This patch ensures the external_mac is\nonly deleted when DVR is not enabled.\n\nWithout this patch, if a VM with a floating IP gets deleted, and\nDVR is enabled, during some time the traffic gets (wrongly)\ncentralized while it should not. And it is also generating more\nload on the OVN side unnecesarily.\n\nCloses-Bug: #2025264\n\nChange-Id: I89db15dd1b629bc963f3b63926391a4a02cbedf7\n(cherry picked from commit 0090572b93d96348778c724194c26a976a9f8757)\n'}]",3,887623,fc29d622675ac4a7672416e735230a5759a49f41,15,3,1,23567,,,0,"Ensure traffic is not centralized if DVR is enabled

There is no need to clear the external_mac if DVR is enabled, not
even when the port is down. This patch ensures the external_mac is
only deleted when DVR is not enabled.

Without this patch, if a VM with a floating IP gets deleted, and
DVR is enabled, during some time the traffic gets (wrongly)
centralized while it should not. And it is also generating more
load on the OVN side unnecesarily.

Closes-Bug: #2025264

Change-Id: I89db15dd1b629bc963f3b63926391a4a02cbedf7
(cherry picked from commit 0090572b93d96348778c724194c26a976a9f8757)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/23/887623/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'releasenotes/notes/dvr-external-mac-934409413e515eb2.yaml', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py']",3,fc29d622675ac4a7672416e735230a5759a49f41,," def _update_dnat_entry_if_needed(self, port_id): if ovn_conf.is_ovn_distributed_floating_ip(): mac = nat['external_ids'].get(ovn_const.OVN_FIP_EXT_MAC_KEY) if mac and nat['external_mac'] != mac: self._update_dnat_entry_if_needed(port_id)"," def _update_dnat_entry_if_needed(self, port_id, up=True): if up and ovn_conf.is_ovn_distributed_floating_ip(): mac = nat['external_ids'][ovn_const.OVN_FIP_EXT_MAC_KEY] if nat['external_mac'] != mac: self._update_dnat_entry_if_needed(port_id, False)",28,15
openstack%2Fopenstacksdk~master~I69522bf820e5cf4546ccf99da2c7373218785d9c,openstack/openstacksdk,master,I69522bf820e5cf4546ccf99da2c7373218785d9c,volume: Add missing attributes to Extension,MERGED,2023-06-02 13:35:16.000000000,2023-07-06 18:03:59.000000000,2023-07-06 18:02:47.000000000,"[{'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 27900}]","[{'number': 1, 'created': '2023-06-02 13:35:16.000000000', 'files': ['openstack/block_storage/v3/extension.py', 'openstack/tests/functional/block_storage/v3/test_extension.py', 'openstack/tests/unit/block_storage/v3/test_extension.py'], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/0adf8f5601001b3b768d62d787b730f8fd531202', 'message': ""volume: Add missing attributes to Extension\n\nWe also rename 'updated' to 'updated_at' to match the 'Extension'\nobjects provided by other services.\n\nChange-Id: I69522bf820e5cf4546ccf99da2c7373218785d9c\n""}]",2,885132,0adf8f5601001b3b768d62d787b730f8fd531202,14,3,1,15334,,,0,"volume: Add missing attributes to Extension

We also rename 'updated' to 'updated_at' to match the 'Extension'
objects provided by other services.

Change-Id: I69522bf820e5cf4546ccf99da2c7373218785d9c
",git fetch https://review.opendev.org/openstack/openstacksdk refs/changes/32/885132/1 && git format-patch -1 --stdout FETCH_HEAD,"['openstack/block_storage/v3/extension.py', 'openstack/tests/functional/block_storage/v3/test_extension.py', 'openstack/tests/unit/block_storage/v3/test_extension.py']",3,0adf8f5601001b3b768d62d787b730f8fd531202,volume-gaps," self.assertEqual(EXTENSION['links'], extension_resource.links) self.assertEqual(EXTENSION['name'], extension_resource.name) self.assertEqual(EXTENSION['updated'], extension_resource.updated_at)"," self.assertEqual(EXTENSION['updated'], extension_resource.updated)",9,5
openstack%2Fopenstack-ansible-os_nova~master~I56aee80180804b8a3e3316cffc6fa8115513b8f1,openstack/openstack-ansible-os_nova,master,I56aee80180804b8a3e3316cffc6fa8115513b8f1,Apply always tag to nova_virt_detect.yml,MERGED,2023-06-06 05:35:49.000000000,2023-07-06 17:46:42.000000000,2023-07-06 17:45:36.000000000,"[{'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 28619}]","[{'number': 1, 'created': '2023-06-06 05:35:49.000000000', 'files': ['tasks/main.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_nova/commit/c90a5c2b927b955aaa82d78e65548423b24f2ecc', 'message': ""Apply always tag to nova_virt_detect.yml\n\nRunning nova playbook with tag limit may lead to an error:\n\nThe conditional check 'nova_virt_type != 'ironic'' failed. The error\nwas: error while evaluating conditional (nova_virt_type != 'ironic'):\n'nova_virt_type' is undefined\\n\\nThe error appears to be in\n'/etc/ansible/roles/os_nova/tasks/main.yml': line 289, column 3, but\nmay be elsewhere in the file depending on the exact syntax problem.\n\nIt can be easily fixed by applying always tag to tasks from\nnova_virt_detect.yml\n\nChange-Id: I56aee80180804b8a3e3316cffc6fa8115513b8f1\n""}]",0,885337,c90a5c2b927b955aaa82d78e65548423b24f2ecc,8,3,1,32666,,,0,"Apply always tag to nova_virt_detect.yml

Running nova playbook with tag limit may lead to an error:

The conditional check 'nova_virt_type != 'ironic'' failed. The error
was: error while evaluating conditional (nova_virt_type != 'ironic'):
'nova_virt_type' is undefined\n\nThe error appears to be in
'/etc/ansible/roles/os_nova/tasks/main.yml': line 289, column 3, but
may be elsewhere in the file depending on the exact syntax problem.

It can be easily fixed by applying always tag to tasks from
nova_virt_detect.yml

Change-Id: I56aee80180804b8a3e3316cffc6fa8115513b8f1
",git fetch https://review.opendev.org/openstack/openstack-ansible-os_nova refs/changes/37/885337/1 && git format-patch -1 --stdout FETCH_HEAD,['tasks/main.yml'],1,c90a5c2b927b955aaa82d78e65548423b24f2ecc,tls-backend, args: apply: tags: - always,,4,0
openstack%2Fironic~master~Ie0a5075451742736ceb71f7e446118e5a2d7284e,openstack/ironic,master,Ie0a5075451742736ceb71f7e446118e5a2d7284e,SQLAlchemy: LOG the connection event action,ABANDONED,2023-07-06 13:39:37.000000000,2023-07-06 17:44:03.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-06 13:39:37.000000000', 'files': ['ironic/db/sqlalchemy/__init__.py'], 'web_link': 'https://opendev.org/openstack/ironic/commit/e48b8ec6c597c1c8d03e3ab66b5947ffd7422afa', 'message': 'SQLAlchemy: LOG the connection event action\n\nChange-Id: Ie0a5075451742736ceb71f7e446118e5a2d7284e\n'}]",0,887848,e48b8ec6c597c1c8d03e3ab66b5947ffd7422afa,5,1,1,11655,,,0,"SQLAlchemy: LOG the connection event action

Change-Id: Ie0a5075451742736ceb71f7e446118e5a2d7284e
",git fetch https://review.opendev.org/openstack/ironic refs/changes/48/887848/1 && git format-patch -1 --stdout FETCH_HEAD,['ironic/db/sqlalchemy/__init__.py'],1,e48b8ec6c597c1c8d03e3ab66b5947ffd7422afa,,from oslo_log import logLOG = log.getLogger(__name__) LOG.debug('Initializing SQLite connection to utilize the write-ahead' 'journal mode of operationl.'),,5,0
openstack%2Fcharm-guide~master~I4eacc16879ee0bf789175e7ac2d4462645adce5b,openstack/charm-guide,master,I4eacc16879ee0bf789175e7ac2d4462645adce5b,Add documentation for TGT to LIO migration,NEW,2023-04-27 13:08:05.000000000,2023-07-06 16:25:41.000000000,,"[{'_account_id': 2424}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-04-27 13:08:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-guide/commit/b6e8a370b4bc41a3d491b22457455648f0a9ab33', 'message': 'Add documentation for TGT to LIO migration\n\nChange-Id: I4eacc16879ee0bf789175e7ac2d4462645adce5b\n'}, {'number': 2, 'created': '2023-04-27 13:22:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-guide/commit/ebfc01a9aad3dafa97492c135b505a4716de886c', 'message': 'Add documentation for TGT to LIO migration\n\nChange-Id: I4eacc16879ee0bf789175e7ac2d4462645adce5b\n'}, {'number': 3, 'created': '2023-07-06 14:31:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-guide/commit/5247a3d90dd21cd6a43261d438dbad64ae5278d6', 'message': 'Add documentation for TGT to LIO migration\n\nChange-Id: I4eacc16879ee0bf789175e7ac2d4462645adce5b\n'}, {'number': 4, 'created': '2023-07-06 16:17:50.000000000', 'files': ['doc/source/project/issues-and-procedures.rst', 'doc/source/project/procedures/cinder-lvm-tgt-to-lio-migration.rst'], 'web_link': 'https://opendev.org/openstack/charm-guide/commit/7ed5d80645eb8a9fcba03ebe754bd3c856547926', 'message': 'Add documentation for TGT to LIO migration\n\nChange-Id: I4eacc16879ee0bf789175e7ac2d4462645adce5b\n'}]",2,881724,7ed5d80645eb8a9fcba03ebe754bd3c856547926,10,2,4,10058,,,0,"Add documentation for TGT to LIO migration

Change-Id: I4eacc16879ee0bf789175e7ac2d4462645adce5b
",git fetch https://review.opendev.org/openstack/charm-guide refs/changes/24/881724/4 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/project/issues-and-procedures.rst', 'doc/source/project/procedures/cinder-lvm-tgt-to-lio-migration.rst']",2,b6e8a370b4bc41a3d491b22457455648f0a9ab33,fix-target-helper,"================================================ Target helper migration: TGT to LIO (cinder-lvm) ================================================ Starting with the Wallaby release, the 'lioadm' library (python3-rtslib-fb) is the default target helper installed with the cinder-common package. However, Cinder's default value is still set to 'tgt', the old helper. To resolve this, either manually install the 'tgt' package or configure the charm to use 'lioadm'. The latter is recommended, as new charm deployments will use the new helper by default without manual user intervention. For deployments upgrading from Victoria or earlier, perform the following steps to use the new helper: 1 - Upgrade cinder and cinder-lvm charms to the latest version from the Wallaby channel. 2 - Explicitly set the target_helper to tgtadm for all cinder-lvm backends .. code-block:: none juju config cinder-lvm config-flags='target_helper=tgtadm' 3 - Execute the OpenStack upgrade transitioning from Victoria to Wallaby: .. code-block:: none juju config cinder openstack-origin=cloud:focal-wallaby 4 - Create a new cinder-lvm application, as in the example bellow, with the same configuration as the previous cinder-lvm, but adding the new target helper config. .. important:: The new application must have a different name than the previous one, as you need a way to differentiate new and old backends. The alias option can also be used to differentiate the backends, but it is not mandatory. The new application can have the same volume-group name as the previous one. .. code-block:: yaml applications: cinder-lvm-lio: charm: cinder-lvm channel: wallaby/stable options: alias: stsstack-lio block-device: /var/lib/lvm_pool0.img|20G volume-group: cinder-volumes-stsstack config-flags: target_helper=lioadm relations: - [ cinder, cinder-lvm-lio ] Install the new application: .. code-block:: none juju export-bundle > current-juju.yaml juju deploy ./current-juju.yaml --overlay ./cinder-lvm-lio.yaml 5 - Disable old tgtadm backends .. code-block:: none cinder service-disable juju-348a1c-focal-ussuri-lvm-3-0@LVM-stsstack \ cinder-volume 6 - Optionally, migrate volumes to the new backend and remove the old backend/application. .. code-block:: none openstack volume migrate --host \ juju-8f5d46-focal-ussuri-lvm-graylog-0@LVM-stsstack-lio v1 ",,80,0
openstack%2Fnova-specs~master~Idb188abde2781827a35c354db34be66b100251e1,openstack/nova-specs,master,Idb188abde2781827a35c354db34be66b100251e1,SR-IOV NIC device tracking in Placement,NEW,2023-05-28 05:01:00.000000000,2023-07-06 16:24:47.000000000,,"[{'_account_id': 7166}, {'_account_id': 8313}, {'_account_id': 9708}, {'_account_id': 11604}, {'_account_id': 11975}, {'_account_id': 15554}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-05-28 05:01:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/8dbd287c0de07cd082f11fe603ccaf1e87a14e77', 'message': 'SR-IOV NIC device tracking in Placement\n\nThis change introduces a spec to describe modeling of\nSR-IOV NIC device using Placement.\n\nBlueprint: track-sriov-nics-in-placement\nChange-Id: Idb188abde2781827a35c354db34be66b100251e1\n'}, {'number': 2, 'created': '2023-05-29 03:50:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/8a928a40e11c3131d4fd9c3e71dc958014e2408b', 'message': 'SR-IOV NIC device tracking in Placement\n\nThis change introduces a spec to describe modeling of\nSR-IOV NIC device using Placement.\n\nBlueprint: track-sriov-nics-in-placement\nChange-Id: Idb188abde2781827a35c354db34be66b100251e1\n'}, {'number': 3, 'created': '2023-05-30 04:55:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/afee2f95c91a9dbff3823637ba6c37754cb6bdd9', 'message': 'SR-IOV NIC device tracking in Placement\n\nThis change introduces a spec to describe modeling of\nSR-IOV NIC device using Placement.\n\nBlueprint: track-sriov-nics-in-placement\nChange-Id: Idb188abde2781827a35c354db34be66b100251e1\n'}, {'number': 4, 'created': '2023-06-09 05:28:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/96ad7c927dfe031169962236439242a73ab80a5c', 'message': 'SR-IOV NIC device tracking in Placement\n\nThis change introduces a spec to describe modeling of\nSR-IOV NIC device using Placement.\n\nBlueprint: track-sriov-nics-in-placement\nChange-Id: Idb188abde2781827a35c354db34be66b100251e1\n'}, {'number': 5, 'created': '2023-06-14 08:25:39.000000000', 'files': ['specs/2023.2/approved/track-sriov-nics-in-placement.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/5f3d549cfbc23b7490a94c326d6ea29cebfd867c', 'message': 'SR-IOV NIC device tracking in Placement\n\nThis change introduces a spec to describe modeling of\nSR-IOV NIC device using Placement.\n\nBlueprint: track-sriov-nics-in-placement\nChange-Id: Idb188abde2781827a35c354db34be66b100251e1\n'}]",44,884569,5f3d549cfbc23b7490a94c326d6ea29cebfd867c,22,8,5,35507,,,0,"SR-IOV NIC device tracking in Placement

This change introduces a spec to describe modeling of
SR-IOV NIC device using Placement.

Blueprint: track-sriov-nics-in-placement
Change-Id: Idb188abde2781827a35c354db34be66b100251e1
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/69/884569/5 && git format-patch -1 --stdout FETCH_HEAD,['specs/2023.2/approved/track-sriov-nics-in-placement.rst'],1,8dbd287c0de07cd082f11fe603ccaf1e87a14e77,bp/track-sriov-nics-in-placement,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ================================= SR-IOV NICs Tracking In Placement ================================= https://blueprints.launchpad.net/nova/+spec/track-sriov-nics-in-placement In the zed and 2023.1 (antelope) releases support was added for tracking PCI devices that did not contain the physical_network tag in placement. This enables generic PCI devices that are consumed via flavor based PCI passthough to be tracked in placement. PCI devices that are consomed via Neutron port however are not tracked in placement. This spec aims to address that gap and enable tracking of Neutron manged PCI devices. Problem description =================== Nova has supported generic stateless PCI passthrough for many releases using a dedicated PCI tracker in conjunction with a ``PciPassthroughFilter`` scheduler post filter. The PCI tracker is responsible for tracking which PCI devices are available, claimed, and allocated, the capabilities of the device, its consumer when claimed or allocated as well as the type of PCI device and location. The ``PciPassthroughFilter`` is responsible for ensuring that devices, requested by the VM, exist on a host during scheduling. These PCI requests come from two sources: flavor-based PCI requests that are generated using the ``pci_passthrough:alias`` `flavor extra specs`_ and Neutron based PCI requests generated from SR-IOV backed Neutron ports. .. _`flavor extra specs`: https://docs.openstack.org/nova/latest/configuration/extra-specs.html#pci_passthrough:alias Currently Nova has the capability to model the availablity of flavor managed PCI devices in placement but lack the same capability for devices consumed via Neutron ports. All instance requests or VM with SR-IOV, VDPA, hardware offloaded OVS or DPU ports rely on the ``PciPassthroughFilter`` to select hosts. While the current approach to SR-IOV nics tracking works there are some limitations in the current design and there is room for optimization. .. rubric:: Limitations * The current implementation is functionally slow. * While Nova today tracks the capabilities of network interfaces in the ``extra_info`` field of the ``pci_devices`` table and the ``PciPassthroughFilter`` could match on those capabilities there is no user-facing way to express a request for an SR-IOV Neutron port with a specific network capability e.g. TSO. .. rubric:: Optimizations * Use placement to track SR-IOV nics. Use Cases --------- - As an operator, I want to use Placement API to track the used SR-IOV resources for Neutron managed ports. - As an operator, I want to schedule my VMs to the correct PF/VF even if multiple device choices are available on the host. - As an operator, I want to associate quotas with SR-IOV Neutron ports. .. note:: Device quotas would require unified limits to be implemented. Implementing quotas is out of the scope of this spec beyond enabling the use case by modeling PCI devices in Placement. This spec will also only focus on Neutron SR-IOV ports. Proposed change =============== PCI device_spec configuration ----------------------------- There is list of Neutron port ``vnic_type`` (e.g. ``direct``, ``direct-physical``, ``vdpa`` etc.) where port needs to be backed by VF or PF PCI devices. In a simple case, when a port only requires a PCI device but does not require any other resources (e.g. bandwidth) then Nova needs to create placement request groups for each Neutron port by extending the prefilter introduced in generic PCI in placement implementation. Compared to the PCI alias case, in SR-IOV nics case, neither the name of resource class nor the vendor ID, product ID pair is known at scheduling time. Therefore, the prefilter does not know what resource class needs to be requested in the placement request group. To resolve this, PCI devices that are intended to be used for Neutron based SR-IOV, hardware offloaded OVS or VDPA should not use ``resource_class`` tag in the ``[PCI]device_spec``. Instead Nova will use standard resource classes to model these resources. The resource classes are explained in later section. Today Nova allows consuming type-PCI or type-VF for ``direct`` ports. This is mostly there due to historical reasons and it should be cleaned up. Modeling SR-IOV devices in Placement ------------------------------------ PCI device modeling in Placement is already implemented. Each PCI device of type ``type-PCI`` and ``type-PF`` will be modeled as a Placement resource provider (RP) with the name ``<hypervisor_hostname>_<pci_address>``. The hypervisor_hostname prefix will be the same string as the name of the root RP. The pci_address part of the name will be the full PCI address in the same format of ``DDDD:BB:AA.FF``. Each SR-IOV NIC device RP will have an inventory of resource class and traits derived by Nova based on device categorization explained below: * A device in the ``device_spec`` will be consumable only via PCI alias if it does not have ``physical_network`` tag attached. * A device that has ``physical_network`` tag attached will be considered a network device and it will be modelled as ``PCI_NETDEV`` resource. * A device that has ``physical_network`` tag and also has the capability to provide VFs will have a trait ``HW_NIC_SRIOV`` but still use the ``PCI_NETDEV`` resource class. * A device that has ``physical_network`` tag and is a VF will be modelled as a ``SRIOV_NET_VF`` resource. * A device that has physical_network and is a VF with a vdpa device will be modelled as a ``VDPA_NETDEV``. The actual implementation of this will reuse the existing logic to determine the device_type and to determine which resource class to use rather than implementing this in the ``device_spec`` parsing. PCI placement tracking will only take effect if the ``[pci]report_in_placement`` config option is set to ``True``. Every Neutron ``vnic_type`` can be mapped to one single resource class by Nova. The following ``vnic_type`` -> resource class mapping is suggested: * ``direct-physical`` -> ``PCI_NETDEV`` * ``direct``, ``macvtap``, ``virtio-forwarder``, ``remote-managed`` -> ``SRIOV_NET_VF`` * ``vdpa`` -> ``VDPA_NETDEV`` Nova will use these resource classes to report device inventories to Placement. Then the prefilter can translate the ``vnic_type`` of the ports to request the specific resource class during scheduling. Another speciality of Neutron-based SR-IOV is that the devices listed in the ``device_spec`` always have a ``physical_network`` tag. This information needs to be reported as a trait to the PF RP in Placement. Also, the port's requested physnet needs to be included in the Placement request group by the prefilter. If a SR-IOV device is matching a ``device_spec`` entry with a ``physical_network`` tag then an inventory of 1 is reported of the ``resource_class`` derived by Nova. There is a more complex case when the Neutron port not only requests a PCI device but also requests additional resources (e.g. bandwidth) via the port ``resource_request`` attribute. Supporting this is currently out of scope of this spec but is intended to be suported in the future. Nova will detect and refuse to boot an instance with an SR-IOV type port that contains addtional resouce requests. This will be done by returning a code 409 until support for this is added. Attaching SR-IOV ports with additional resources will also be detected and rejected. Neutron SR-IOV ports with QoS (out of scope) -------------------------------------------- When a Neutron port requests additional resources, Nova generates Placement request groups from the ``resource_request`` and as in the simple case will generate a request group from the PCI request. The resource request of these groups of a Neutron port needs to be correlated to ensure that a port gets the PCI device and the bandwidth from the same physical device. However, today the bandwidth is modeled under the Neutron RP subtree while PCI devices will be modeled right under the root RP. So the two RPs to allocate from are not within the same subtree. (Note that Placement always fulfills a named request group from a single RP but allows correlating such request groups within the same subtree.) We have multiple options here: * Create a scheduler filter that removes allocation candidates where these request groups are fulfilled from different physical devices. * Report the bandwidth and the PCI device resource on the same RP. This breaks the clear ownership of a single RP as the bandwidth is reported by the Neutron agent while the PCI device is reported by Nova. * Move the two RPs (bandwidth and PCI dev) into the same subtree. This needs an agreement between Nova and Neutron devs where to move the RPs and needs an extra reshape to implement the move. * Enhance Placement to allow sharing of resources between RPs within the same RP tree. By that, we could make the bandwidth RP a sharing RP that shares resources with the PCI device RP representing the physical device. To enable forward progress with the minimum of dependencies and incremental progress, the preferred short term solution is to enhance the existing pci_passthough_filter to remove allocation candidates where these request groups are fulfilled from different physical devices or add a new scheduler filter that removes allocation candidates. Requesting PCI devices ---------------------- Nova will continue using the ``InstancePCIRequest`` to track the requested SR-IOV NIC devices for a VM. VM lifecycle operations ----------------------- The initial scheduling is very similar to the later scheduling done due to move operations. So, the existing implementation can be reused. Also, the current logic that switches the source node Placement allocation to be held by the migration UUID can be reused. Attaching and detaching PCI devices will continue to be supported via Neutron SR-IOV ports. Alternatives ------------ * We could keep using the legacy tracking with all its good and bad properties. * We could have Nova create the resource providers for the SR-IOV devices under the Neutron SR-IOV NIC agent resource provider. However Nova does not know which network backend is in use and this would create a start up dependency loop between Nova and Neutron. Neutron need the comptue RP to be created before it can create the SR-IOV NIC agent RP and Nova would need the agent RP to finish reporting the pci devices. * We could defer the Nova support for tracking SR-IOV devices in placement until we reshape how banditwith is tracked or we develop a new placement featutre to corralate the relevent RPs. Data model impact ----------------- ``InstancePCIRequest`` object will be extended to include the required and forbidden traits and the resource class generated by Nova. REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- None Performance Impact ------------------ In general, this is expected to improve the scheduling performance but should have no runtime performance impact on guests. The introduction of new ``RequestGroup`` objects will make the computation of the placement query slightly longer and the resulting execution time may increase for instances with SR-IOV NIC requests but should have no effect for instances without such requests. This added complexity is expected to be offset the result of the offloading of the filtering to Placement and the removal of reschedules due to racing for the last PCI device on a host, the overall performance is expected to improve. Other deployer impact --------------------- To utilize the new feature the operator will have to define two new config options. One to enable the placement scheduling logic and a second to enable the reporting of the PCI devices to Placement. Developer impact ---------------- None Upgrade impact -------------- The new Placement based PCI tracking will be disabled by default. Deployments already using PCI devices can freely upgrade to the new Nova version without any impact. At this state the PCI device management will be done by the ``PciPassthroughFilter`` in the scheduler and the PCI claim in the PCI device tracker in the compute service same as in the previous version of Nova. Then after the upgrade the new PCI device tracking can be enabled in two phases. First the PCI inventory reporting needs to be enabled by ``[pci]report_to_placement`` on each compute host. During the startup of the nova-compute service with ``[pci]report_to_placement = True`` config the service will do the reshape of the provider tree and start reporting PCI device inventory to Placement. Nova compute will also heal the PCI allocation of the existing instances in Placement. This healing will be done for new instances with PCI requests until a future release where the prefilter enabled by default. This is needed to keep the resource usage in sync in Placement even if the instance scheduling is done without the prefilter requesting PCI allocations in Placement. .. note:: Once ``[pci]report_to_placement`` is enabled for a compute host it cannot be disabled any more. Second, after every compute has been configured to report PCI inventories to Placement the scheduling logic needs to be enabled in the nova-scheduler configuration via the ``[filter_scheduler]pci_in_placement`` configuration option. Implementation ============== Assignee(s) ----------- Primary assignee: kpawar Feature Liaison --------------- Feature liaison: balazs-gibizer Work Items ---------- * translate ``InstancePCIRequest`` objects to RequestGroup objects in the RequestSpec * support adding resource class and required traits to SR-IOV NIC requests. * filter and reject SR-IOV NIC requests with resources both during VM creation and SR-IOV port attachment. Dependencies ============ The unified limits feature exists in an opt-in, experimental state and will allow defining limits for the new PCI resources if enabled. Testing ======= As this is a PCI passthrough related feature it cannot be tested in upstream tempest. Testing will be primarily done via the extensive unit and functional test suites that exists for instances with PCI devices. Documentation Impact ==================== The PCI passthrough doc will have to be rewritten to document the new ``resource_class`` and ``trait`` tags for SR-IOV NIC devices. References ========== * _`CPU resource tracking spec`: https://specs.openstack.org/openstack/nova-specs/specs/train/implemented/cpu-resources.html * _`Unified Limits Integration in Nova`: https://specs.openstack.org/openstack/nova-specs/specs/ussuri/approved/unified-limits-nova.html * _`Support virtual GPU resources`: https://specs.openstack.org/openstack/nova-specs/specs/queens/implemented/add-support-for-vgpu.html * _`PCI device tracking in Placement`: https://specs.openstack.org/openstack/nova-specs/specs/2023.1/implemented/pci-device-tracking-in-placement.html History ======= .. list-table:: Revisions :header-rows: 1 * - Release Name - Description * - 2023.2 Bobcat - Introduced ",,401,0
openstack%2Foslo.cache~master~Ib3e3028d967c166d21b60cf4cb7c9d5dc82a8fe7,openstack/oslo.cache,master,Ib3e3028d967c166d21b60cf4cb7c9d5dc82a8fe7,Do not mark hosts as alive when they are all dead,NEW,2022-01-14 13:58:56.000000000,2023-07-06 15:42:23.000000000,,"[{'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 31245}, {'_account_id': 32666}]","[{'number': 1, 'created': '2022-01-14 13:58:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.cache/commit/fcf960cc9c0f571d1a04b2f61767c4aee595295b', 'message': ""Do not mark hosts as alive when they are all dead\n\nThis feature is supposed to check for alive servers more frequently when they are all down.\nUnfortunately, it does more harm than good.\n\nI have made several tests with keystone and here is what I saw when all memcached backends were unreachable:\n1. memcached services were stopped but their hosts were alive(keystone received 'connection refused') - everything was working fine\n2. memcached backends and their hosts were completely unreachable(keystone received 'no route to host') - the whole keystone stopped responding\n\nKeystone should be working fine even if all memcached backends are down. In this case it wasn't.\nAfer I deleted the discussed functionality from the code, keystone was finally able to handle failure of all memcached backends.\nI believe this issue is not only limited to keystone, I just used it as an example.\n\nChange-Id: Ib3e3028d967c166d21b60cf4cb7c9d5dc82a8fe7\n""}, {'number': 2, 'created': '2022-08-08 09:31:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.cache/commit/f50d6c200c23935b66108133f462eaedbcdf8f78', 'message': ""Do not mark hosts as alive when they are all dead\n\nThis feature is supposed to check for alive servers more frequently when they are all down.\nUnfortunately, it does more harm than good.\n\nI have made several tests with keystone and here is what I saw when all memcached backends were unreachable:\n1. memcached services were stopped but their hosts were alive(keystone received 'connection refused') - everything was working fine\n2. memcached backends and their hosts were completely unreachable(keystone received 'no route to host') - the whole keystone stopped responding\n\nKeystone should be working fine even if all memcached backends are down. In this case it wasn't.\nAfer I deleted the discussed functionality from the code, keystone was finally able to handle failure of all memcached backends.\nI believe this issue is not only limited to keystone, I just used it as an example.\n\nChange-Id: Ib3e3028d967c166d21b60cf4cb7c9d5dc82a8fe7\n""}, {'number': 3, 'created': '2022-08-08 09:34:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.cache/commit/db9916610e5f7f84463d8123f830affd4f8d0617', 'message': ""Do not mark hosts as alive when they are all dead\n\nThis feature is supposed to check for alive servers more frequently when they are all down.\nUnfortunately, it does more harm than good.\n\nI have made several tests with keystone and here is what I saw when all memcached backends were unreachable:\n1. memcached services were stopped but their hosts were alive(keystone received 'connection refused') - everything was working fine\n2. memcached backends and their hosts were completely unreachable(keystone received 'no route to host') - the whole keystone stopped responding\n\nKeystone should be working fine even if all memcached backends are down. In this case it wasn't.\nAfter I deleted the discussed functionality from the code, keystone was finally able to handle failure of all memcached backends.\nI believe this issue is not only limited to keystone, I just used it as an example.\n\nChange-Id: Ib3e3028d967c166d21b60cf4cb7c9d5dc82a8fe7\n""}, {'number': 4, 'created': '2023-02-17 14:49:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/oslo.cache/commit/357df19cc1e88f5db7d235366c725265709b0c0d', 'message': ""Do not mark hosts as alive when they are all dead\n\nThis feature is supposed to check for alive servers more frequently when they are all down.\nUnfortunately, it does more harm than good.\n\nI have made several tests with keystone and here is what I saw when all memcached backends were unreachable:\n1. memcached services were stopped but their hosts were alive(keystone received 'connection refused') - everything was working fine\n2. memcached backends and their hosts were completely unreachable(keystone received 'no route to host') - the whole keystone stopped responding\n\nKeystone should be working fine even if all memcached backends are down. In this case it wasn't.\nAfter I deleted the discussed functionality from the code, keystone was finally able to handle failure of all memcached backends.\nI believe this issue is not only limited to keystone, I just used it as an example.\n\nChange-Id: Ib3e3028d967c166d21b60cf4cb7c9d5dc82a8fe7\n""}, {'number': 5, 'created': '2023-02-27 13:41:38.000000000', 'files': ['oslo_cache/_memcache_pool.py'], 'web_link': 'https://opendev.org/openstack/oslo.cache/commit/e34fbe29250a328377f44b67167e12de2da72b12', 'message': ""Do not mark hosts as alive when they are all dead\n\nThis feature is supposed to check for alive servers more frequently when they are all down.\nUnfortunately, it does more harm than good.\n\nI have made several tests with keystone and here is what I saw when all memcached backends were unreachable:\n1. memcached services were stopped but their hosts were alive(keystone received 'connection refused') - everything was working fine\n2. memcached backends and their hosts were completely unreachable(keystone received 'no route to host') - the whole keystone stopped responding\n\nKeystone should be working fine even if all memcached backends are down. In this case it wasn't.\nAfter I deleted the discussed functionality from the code, keystone was finally able to handle failure of all memcached backends.\nI believe this issue is not only limited to keystone, I just used it as an example.\n\nChange-Id: Ib3e3028d967c166d21b60cf4cb7c9d5dc82a8fe7\n""}]",5,824716,e34fbe29250a328377f44b67167e12de2da72b12,20,4,5,32666,,,0,"Do not mark hosts as alive when they are all dead

This feature is supposed to check for alive servers more frequently when they are all down.
Unfortunately, it does more harm than good.

I have made several tests with keystone and here is what I saw when all memcached backends were unreachable:
1. memcached services were stopped but their hosts were alive(keystone received 'connection refused') - everything was working fine
2. memcached backends and their hosts were completely unreachable(keystone received 'no route to host') - the whole keystone stopped responding

Keystone should be working fine even if all memcached backends are down. In this case it wasn't.
After I deleted the discussed functionality from the code, keystone was finally able to handle failure of all memcached backends.
I believe this issue is not only limited to keystone, I just used it as an example.

Change-Id: Ib3e3028d967c166d21b60cf4cb7c9d5dc82a8fe7
",git fetch https://review.opendev.org/openstack/oslo.cache refs/changes/16/824716/1 && git format-patch -1 --stdout FETCH_HEAD,['oslo_cache/_memcache_pool.py'],1,fcf960cc9c0f571d1a04b2f61767c4aee595295b,,," # If all hosts are dead we should forget that they're dead. This # way we won't get completely shut off until dead_retry seconds # pass, but will be checking servers as frequent as we can (over # way smaller socket_timeout) if all(deaduntil > now for deaduntil in self._hosts_deaduntil): self._debug_logger('All hosts are dead. Marking them as live.') self._hosts_deaduntil[:] = [0] * len(self._hosts_deaduntil)",0,7
openstack%2Fnova-specs~master~Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902,openstack/nova-specs,master,Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902,Adds cleanup to remove dangling volumes,MERGED,2023-03-28 10:21:28.000000000,2023-07-06 15:33:49.000000000,2023-07-06 15:32:33.000000000,"[{'_account_id': 4393}, {'_account_id': 4690}, {'_account_id': 7166}, {'_account_id': 9535}, {'_account_id': 9708}, {'_account_id': 11604}, {'_account_id': 11655}, {'_account_id': 16207}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-03-28 10:21:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/aa9691f34752ca6f25879f0513efbdc1823ea796', 'message': 'Add cleanup flag spec to remove dangling volumes\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 2, 'created': '2023-03-28 10:45:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/5195e23c43fa781847dff06b294ffb4f2e27b820', 'message': 'Add cleanup flag spec to remove dangling volumes\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 3, 'created': '2023-03-29 06:38:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/c117b9c579185f154bf2a84f14a5b2220e0c78cf', 'message': 'Add cleanup flag to remove dangling volumes\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 4, 'created': '2023-03-29 06:52:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/ca3bc5e30e8a29e2fe327f67345fc91c8e1df9dd', 'message': 'Add cleanup flag to remove dangling volumes\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 5, 'created': '2023-03-29 06:55:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/d6eb12da3104a40afc6fc5ad17028dbf6bba37e2', 'message': 'Add cleanup flag to remove dangling volumes\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 6, 'created': '2023-03-31 10:55:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/f39b630c356135e619562205ebcd3dc1118fa3cb', 'message': 'Add cleanup flag to remove dangling volumes\n\nBlueprint: https://blueprints.launchpad.net/nova/+spec/nova-manage-cleanup-dangling-volume-attachments\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 7, 'created': '2023-03-31 11:39:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/d446e080a4199c3105ba39f764998993b98e0644', 'message': 'Add cleanup flag to remove dangling volumes\n\nBlueprint: https://blueprints.launchpad.net/nova/+spec/nova-manage-cleanup-dangling-volume-attachments\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 8, 'created': '2023-03-31 13:08:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/377947a6c998a366c57d8d6fc132d210a63e69cb', 'message': 'Add cleanup flag to remove dangling volumes\n\nBlueprint: https://blueprints.launchpad.net/nova/+spec/nova-manage-cleanup-dangling-volume-attachments\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 9, 'created': '2023-04-03 06:20:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/cc020388757c6b6a09fd723520b6b8cef051461b', 'message': 'Add cleanup flag to remove dangling volumes\n\nBlueprint: https://blueprints.launchpad.net/nova/+spec/nova-manage-cleanup-dangling-volume-attachments\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 10, 'created': '2023-04-10 06:37:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/228f40f2fa188f94c5e43b2fccda623c42eea74d', 'message': 'Add cleanup flag to remove dangling volumes\n\nBlueprint: https://blueprints.launchpad.net/nova/+spec/nova-manage-cleanup-dangling-volume-attachments\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 11, 'created': '2023-05-15 11:18:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/2e1c161885a91de6eeb3eaadece591794a540d55', 'message': 'Adds cleanup to remove dangling volumes\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 12, 'created': '2023-05-16 09:27:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/befeb2b945cd58663433afb9b0cf50ba54c64c30', 'message': 'Adds cleanup to remove dangling volumes\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 13, 'created': '2023-06-07 08:14:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/a50d1a21b081a7abfa719a34567902747d053fcd', 'message': 'Adds cleanup to remove dangling volumes\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 14, 'created': '2023-06-21 11:37:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/1e1a3a926291df99de93cc7f73e6c8a3f4a583d3', 'message': 'Adds cleanup to remove dangling volumes\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}, {'number': 15, 'created': '2023-06-30 09:45:31.000000000', 'files': ['specs/2023.2/approved/cleanup-dangling-volume-attachments.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/871aa3e4779be7deca9cde9d2a9bf40a5c56d010', 'message': 'Adds cleanup to remove dangling volumes\n\nChange-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902\n'}]",99,878757,871aa3e4779be7deca9cde9d2a9bf40a5c56d010,63,9,15,34860,,,0,"Adds cleanup to remove dangling volumes

Change-Id: Ic7a456ecb59dd4498444f953a6bcb7f63ee3c902
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/57/878757/8 && git format-patch -1 --stdout FETCH_HEAD,['specs/2023.2/approved/nova-manage-cleanup-dangling-volume-attachments.rst'],1,aa9691f34752ca6f25879f0513efbdc1823ea796,dangling-volumes,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode =================================================== Clean Up dangling volumes using nova-manage command =================================================== Launchpad blueprint: https://blueprints.launchpad.net/nova/+spec/nova-manage-cleanup-dangling-volume-attachments Find out if there is any dangling/unattached volume in nova database and remove it from nova database. Problem description =================== In case after some volume operation, volume get deattached from instance but nova did not get notified and thinks volume is still attached to instance. This can lead to different issues which required volume details from block device mapping table, such as live miration and resizing of instance. Delete volume attachment-id using cinder 1 - There is a volume attached to an instance 2 - Delete volume attachement 3 - Verify using cinder api volume is not attached to instance 4 - Verify from nova api volume would be listed as attached to instance 5 - Verify in nova block device mapping table volume would be listed as attached to instance Use Cases --------- - As a operator, I want to remove all dangling volumes safely my instance had. So this can not affect other volume related operations. Proposed change =============== Soft delete block device mapping. Verify using nova.cinder.API.check_attached, if volume is not attached at cinder side, soft delete at nova side. Check attach volume raises, an InvalidVolume exception if volume is not in ""in-use"" status. if exception get caught perform soft-delete by calling bdm.destroy, this will update bdm table as deleted=ID. We already have a nova-mange utility with volume refresh option, add a new flag --clean-orphans to find all dangling volumes and then remove them from nova block device mapping table. .. code-block:: shell nova-manage volume_attachment refresh --clean-orphans pre-requisite: server should be shut-off, else later no other volume will be attached to it and same volume can not be attached to other instance. Problem if we use current refresh functionality user need to pass volume-id as well. So, we can have a new functionality, cleanup volumes in same class VolumeAttachmentCommands. Might be issue: .. code-block:: shell if we cleanup the volume and then later we attached same again now in bdm there are 2 entries. one deleted and another not deleted. | ec891162-8b65-4726-a16a-3eecf6257435 | 1ce5fc3a-0c28-4c0a-94c0-26cd0746e7f9 | ccb720ea-5d64-465c-9b15-b2b7a3b55cac | volume | volume | 097e3bf7-9697-40e6-8fdc-f2eca291e16e | NULL | 19 | | 0bcdfef7-6af5-44ee-bf21-4955607f5239 | 1ce5fc3a-0c28-4c0a-94c0-26cd0746e7f9 | ccb720ea-5d64-465c-9b15-b2b7a3b55cac | volume | volume | 68ef99d4-859b-4dca-b6e4-d2e59db532f0 | NULL | 0 | is this Okay ? Alternatives ------------ 1 - For each instance in bdm table, check if volume exists is attached if not remove it. In this solution instance uuid is not required, so we can have this as a cron job as well. Data model impact ----------------- nova block device mapping table will get updated on every operation, but no model changes. REST API impact --------------- N/A Security impact --------------- N/A Notifications impact -------------------- N/A Other end user impact --------------------- N/A Performance Impact ------------------ N/A Other deployer impact --------------------- N/A Developer impact ---------------- N/A Upgrade impact -------------- N/A Implementation ============== Assignee(s) ----------- Primary assignee: auniyal Feature Liaison --------------- Feature liaison: auniyal Work Items ---------- - Create a flag for nova-manage volume_attachment refresh functionality. - List all volumes instance instance block device mapping has. - Check volume status from cinder api, for volumes where source and destination type is volume. - if such volumes status is not in-use in cinder volume DB, update nova block device mapping DB to remove attachment from instance. Dependencies ============ N/A Testing ======= - Delete volume without deattaching from server - Delete server volume attachment-id when server is not shut-off, and run cleanup of dangling volumes. Documentation Impact ==================== Documentation will be updated References ========== N/A History ======= .. list-table:: Revisions :header-rows: 1 * - Release Name - Description * - 2023.2 Bobcat - Introduced ",,202,0
openstack%2Fneutron~stable%2Fvictoria~I99fc9c3fcbf39c4f7a79d5ca936f16605dace1dd,openstack/neutron,stable/victoria,I99fc9c3fcbf39c4f7a79d5ca936f16605dace1dd,"[stable-only] Disable ""neutron-tempest-plugin-scenario-ovn-wallaby""",ABANDONED,2023-07-06 10:17:09.000000000,2023-07-06 15:32:45.000000000,,"[{'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 10:17:09.000000000', 'files': ['zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/neutron/commit/f8d8bae419e4d8736623995cc8a20d567fd91e79', 'message': '[stable-only] Disable ""neutron-tempest-plugin-scenario-ovn-wallaby""\n\nUntil the git clone issue is fixed (solved in newer versions in\ndevstack project), this CI job must be disabled.\n\nRelated-Bug: #2025486\nChange-Id: I99fc9c3fcbf39c4f7a79d5ca936f16605dace1dd\n'}]",0,887789,f8d8bae419e4d8736623995cc8a20d567fd91e79,4,2,1,16688,,,0,"[stable-only] Disable ""neutron-tempest-plugin-scenario-ovn-wallaby""

Until the git clone issue is fixed (solved in newer versions in
devstack project), this CI job must be disabled.

Related-Bug: #2025486
Change-Id: I99fc9c3fcbf39c4f7a79d5ca936f16605dace1dd
",git fetch https://review.opendev.org/openstack/neutron refs/changes/89/887789/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/project.yaml'],1,f8d8bae419e4d8736623995cc8a20d567fd91e79,bug/2025486, - neutron-tempest-plugin-api-victoria - neutron-tempest-plugin-scenario-linuxbridge-victoria - neutron-tempest-plugin-scenario-openvswitch-victoria - neutron-tempest-plugin-scenario-openvswitch-iptables_hybrid-victoria # NOTE(ralonsoh): disabled until LP#2025486 is fixed. #- neutron-tempest-plugin-scenario-ovn-victoria - neutron-tempest-plugin-designate-scenario-victoria - neutron-tempest-plugin-api-victoria, - neutron-tempest-plugin-jobs-victoria,8,1
openstack%2Fneutron~stable%2Fwallaby~I61451d483d72001c1aac8ab7b0af1d5b07bc1a3d,openstack/neutron,stable/wallaby,I61451d483d72001c1aac8ab7b0af1d5b07bc1a3d,"[stable-only] Disable ""neutron-tempest-plugin-scenario-ovn-wallaby""",ABANDONED,2023-07-06 10:14:56.000000000,2023-07-06 15:32:43.000000000,,"[{'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 10:14:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/1422c112b43d73e8a0ad02116ad5078ac54a9ea2', 'message': '[stable-only] Disable ""neutron-tempest-plugin-scenario-ovn-wallaby""\n\nUntil the git clone issue is fixed (solved in newer versions in\ndevstack project), this CI job must be disabled.\n\nRelated-Bug: #2025486\nChange-Id: I61451d483d72001c1aac8ab7b0af1d5b07bc1a3d\n'}, {'number': 2, 'created': '2023-07-06 10:16:03.000000000', 'files': ['zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/neutron/commit/290b00e0f0e5e4aff9f21ae8c6c335baec52bc64', 'message': '[stable-only] Disable ""neutron-tempest-plugin-scenario-ovn-wallaby""\n\nUntil the git clone issue is fixed (solved in newer versions in\ndevstack project), this CI job must be disabled.\n\nRelated-Bug: #2025486\nChange-Id: I61451d483d72001c1aac8ab7b0af1d5b07bc1a3d\n'}]",0,887788,290b00e0f0e5e4aff9f21ae8c6c335baec52bc64,5,2,2,16688,,,0,"[stable-only] Disable ""neutron-tempest-plugin-scenario-ovn-wallaby""

Until the git clone issue is fixed (solved in newer versions in
devstack project), this CI job must be disabled.

Related-Bug: #2025486
Change-Id: I61451d483d72001c1aac8ab7b0af1d5b07bc1a3d
",git fetch https://review.opendev.org/openstack/neutron refs/changes/88/887788/2 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/project.yaml'],1,1422c112b43d73e8a0ad02116ad5078ac54a9ea2,bug/2025486, - neutron-tempest-plugin-api-wallaby - neutron-tempest-plugin-scenario-linuxbridge-wallaby - neutron-tempest-plugin-scenario-openvswitch-wallaby - neutron-tempest-plugin-scenario-openvswitch-iptables_hybrid-wallaby # NOTE(ralonsoh): disables until LP#2025486 is fixed. #- neutron-tempest-plugin-scenario-ovn-wallaby - neutron-tempest-plugin-designate-scenario-wallaby - neutron-tempest-plugin-api-wallaby, - neutron-tempest-plugin-jobs-wallaby,8,1
openstack%2Fcharms.openstack~stable%2Fwallaby~I76abbd29ca910fe4c4d62da09e2d2dd3b5c798a6,openstack/charms.openstack,stable/wallaby,I76abbd29ca910fe4c4d62da09e2d2dd3b5c798a6,Skip version caching for subordinate charms,NEW,2023-04-25 20:10:57.000000000,2023-07-06 14:47:36.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-04-25 20:10:57.000000000', 'files': ['unit_tests/charms_openstack/charm/test_defaults.py', 'charms_openstack/charm/defaults.py'], 'web_link': 'https://opendev.org/openstack/charms.openstack/commit/e101bdc7e355c4740c7c1855974ba7815167caa6', 'message': 'Skip version caching for subordinate charms\n\nCaching the charm OpenStack version will cause undesired behaviors\nif the charm is a subordinate. The charm local cache is firstly\npopulated during the charm install. If the charm is a subordinate,\nthe version will remain the same regardless of future OpenStack\nupgrades because today, the hook that updates the cache to the new\nversion in only called for principal charms.\n\nRelated-bug: #1949074\nChange-Id: I76abbd29ca910fe4c4d62da09e2d2dd3b5c798a6\n(cherry picked from commit fd042afdf186ab419ee35ea9678735895c62b723)\n(cherry picked from commit 3b81431f4e4060384c443c10488fba1d2a10b733)\n(cherry picked from commit fae7fb232bcab9434685fc40c71a99ef57e7f330)\n(cherry picked from commit afbe6e76c1cefd30f64bb67682b25b354513e02d)\n'}]",1,881506,e101bdc7e355c4740c7c1855974ba7815167caa6,6,1,1,10058,,,0,"Skip version caching for subordinate charms

Caching the charm OpenStack version will cause undesired behaviors
if the charm is a subordinate. The charm local cache is firstly
populated during the charm install. If the charm is a subordinate,
the version will remain the same regardless of future OpenStack
upgrades because today, the hook that updates the cache to the new
version in only called for principal charms.

Related-bug: #1949074
Change-Id: I76abbd29ca910fe4c4d62da09e2d2dd3b5c798a6
(cherry picked from commit fd042afdf186ab419ee35ea9678735895c62b723)
(cherry picked from commit 3b81431f4e4060384c443c10488fba1d2a10b733)
(cherry picked from commit fae7fb232bcab9434685fc40c71a99ef57e7f330)
(cherry picked from commit afbe6e76c1cefd30f64bb67682b25b354513e02d)
",git fetch https://review.opendev.org/openstack/charms.openstack refs/changes/06/881506/1 && git format-patch -1 --stdout FETCH_HEAD,"['unit_tests/charms_openstack/charm/test_defaults.py', 'charms_openstack/charm/defaults.py']",2,e101bdc7e355c4740c7c1855974ba7815167caa6,fix-target-helper,"import charmhelpers.core.hookenv as hookenv release_version = None # Using the cached OpenStack version will cause undesired behaviors # if the charm is a subordinate. The charm local cache is firstly # populated during the charm install and after that, only during the # openstack upgrades. If the charm is a subordinate, the version will # always remain the same. if not hookenv.is_subordinate(): release_version = unitdata.kv().get(OPENSTACK_RELEASE_KEY, None) # Skip caching the release if the charm is a subordinate. if not hookenv.is_subordinate(): unitdata.kv().set(OPENSTACK_RELEASE_KEY, release_version) "," release_version = unitdata.kv().get(OPENSTACK_RELEASE_KEY, None) unitdata.kv().set(OPENSTACK_RELEASE_KEY, release_version)",29,2
openstack%2Fneutron~stable%2Fwallaby~If3487f1a9522438e2b6f3ebb30036a9360a2cb05,openstack/neutron,stable/wallaby,If3487f1a9522438e2b6f3ebb30036a9360a2cb05,[DNM] wallaby ovn job fix,ABANDONED,2023-07-06 10:45:06.000000000,2023-07-06 14:21:13.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-06 10:45:06.000000000', 'files': ['zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/neutron/commit/3ddb55e9fb8268f85c6375693f462223a9f16101', 'message': '[DNM] wallaby ovn job fix\n\nDepends-On: https://review.opendev.org/c/openstack/devstack/+/887790\nChange-Id: If3487f1a9522438e2b6f3ebb30036a9360a2cb05\n'}]",0,887791,3ddb55e9fb8268f85c6375693f462223a9f16101,3,1,1,13861,,,0,"[DNM] wallaby ovn job fix

Depends-On: https://review.opendev.org/c/openstack/devstack/+/887790
Change-Id: If3487f1a9522438e2b6f3ebb30036a9360a2cb05
",git fetch https://review.opendev.org/openstack/neutron refs/changes/91/887791/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/project.yaml'],1,3ddb55e9fb8268f85c6375693f462223a9f16101,bug/1987832, - neutron-tempest-plugin-scenario-ovn-wallaby, templates: - neutron-tempest-plugin-jobs-wallaby - openstack-cover-jobs - openstack-python3-wallaby-jobs - openstack-python3-wallaby-jobs-arm64 - publish-openstack-docs-pti - periodic-stable-jobs - check-requirements - release-notes-jobs-python3 - neutron-experimental-jobs - neutron-periodic-jobs - neutron-tox-override-jobs - neutron-functional-with-uwsgi - neutron-fullstack-with-uwsgi - neutron-rally-task - neutron-grenade-multinode - neutron-grenade-dvr-multinode: # TODO(slaweq): make that job voting when bug # https://bugs.launchpad.net/neutron/+bug/1920778 # will be fixed voting: false - neutron-tempest-multinode-full-py3 - neutron-tempest-dvr-ha-multinode-full - neutron-tempest-slow-py3 - neutron-tempest-ipv6-only - neutron-ovn-tempest-ovs-release - neutron-ovn-tempest-ovs-release-ipv6-only # TODO(slaweq): add this job again to the check queue when it will be # working fine on python 3 #- networking-midonet-tempest-aio-ml2-centos-7: # voting: false - neutron-ovn-rally-task: voting: false - neutron-ovn-tempest-slow gate: jobs: - neutron-functional-with-uwsgi - neutron-fullstack-with-uwsgi - neutron-tempest-multinode-full-py3 - neutron-grenade-multinode # TODO(slaweq): make that job gating when bug # https://bugs.launchpad.net/neutron/+bug/1920778 # will be fixed # - neutron-grenade-dvr-multinode - neutron-tempest-slow-py3 - neutron-tempest-ipv6-only - neutron-ovn-tempest-ovs-release - neutron-ovn-tempest-ovs-release-ipv6-only - neutron-ovn-tempest-slow #- neutron-ovn-rally-task #- neutron-ovn-tripleo-ci-centos-8-containers-multinode,1,51
openstack%2Fkeystone~master~I906cb8f7b76833c880a40c1aa0584fe7ab93cb7a,openstack/keystone,master,I906cb8f7b76833c880a40c1aa0584fe7ab93cb7a,sql: Fix incorrect columns,MERGED,2023-04-06 10:48:29.000000000,2023-07-06 14:16:15.000000000,2023-07-06 14:14:19.000000000,"[{'_account_id': 7414}, {'_account_id': 14250}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-04-06 10:48:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/20908ff4f5ff8f7fc92f911d2b9b24202fb6b7b6', 'message': 'sql: Fix incorrect columns\n\nIn these instances, we take the migrations to be the ""official"" version\n- since they\'re stricter in almost all cases - updating the models to\nsuit.\n\nChange-Id: I906cb8f7b76833c880a40c1aa0584fe7ab93cb7a\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}, {'number': 2, 'created': '2023-06-27 09:58:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/3219cc1632808ac5af3bfc85f5b73fe66b0899ec', 'message': 'sql: Fix incorrect columns\n\nIn these instances, we take the migrations to be the ""official"" version\n- since they\'re stricter in almost all cases - updating the models to\nsuit.\n\nChange-Id: I906cb8f7b76833c880a40c1aa0584fe7ab93cb7a\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}, {'number': 3, 'created': '2023-07-03 11:12:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/e05e3929d162182aaaa99c11fcf36f179ec6ffe2', 'message': 'sql: Fix incorrect columns\n\nIn these instances, we take the migrations to be the ""official"" version\n- since they\'re stricter in almost all cases - updating the models to\nsuit.\n\nThis change highlights a slight issue in our use of a config option in\nour database schema, which we shouldn\'t really do. A TODO is left to\naddress this later. We can also remove a now-unnecessary TODO from our\ninitial migration related to the same issue: we have our own tooling for\nmigrations that *does* load and register config options so there is no\nlonger an issue here.\n\nChange-Id: I906cb8f7b76833c880a40c1aa0584fe7ab93cb7a\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}, {'number': 4, 'created': '2023-07-03 11:33:09.000000000', 'files': ['keystone/common/sql/migrations/versions/27e647c0fad4_initial_version.py', 'keystone/common/sql/migrations/env.py', 'keystone/credential/backends/sql.py', 'keystone/identity/backends/sql_model.py', 'keystone/federation/backends/sql.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/2bf70a10a2a7e0ef31f790c21d9cb6b2643c186d', 'message': 'sql: Fix incorrect columns\n\nIn these instances, we take the migrations to be the ""official"" version\n- since they\'re stricter in almost all cases - updating the models to\nsuit.\n\nThis change highlights a slight issue in our use of a config option in\nour database schema, which we shouldn\'t really do. A TODO is left to\naddress this later. We can also remove a now-unnecessary TODO from our\ninitial migration related to the same issue: we have our own tooling for\nmigrations that *does* load and register config options so there is no\nlonger an issue here.\n\nChange-Id: I906cb8f7b76833c880a40c1aa0584fe7ab93cb7a\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}]",1,879734,2bf70a10a2a7e0ef31f790c21d9cb6b2643c186d,18,4,4,15334,,,0,"sql: Fix incorrect columns

In these instances, we take the migrations to be the ""official"" version
- since they're stricter in almost all cases - updating the models to
suit.

This change highlights a slight issue in our use of a config option in
our database schema, which we shouldn't really do. A TODO is left to
address this later. We can also remove a now-unnecessary TODO from our
initial migration related to the same issue: we have our own tooling for
migrations that *does* load and register config options so there is no
longer an issue here.

Change-Id: I906cb8f7b76833c880a40c1aa0584fe7ab93cb7a
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/keystone refs/changes/34/879734/1 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/common/sql/migrations/env.py', 'keystone/credential/backends/sql.py', 'keystone/identity/backends/sql_model.py', 'keystone/federation/backends/sql.py']",4,20908ff4f5ff8f7fc92f911d2b9b24202fb6b7b6,sqlalchemy-20,"import keystone.confCONF = keystone.conf.CONF relay_state_prefix = sql.Column( sql.String(256), nullable=False, server_default=CONF.saml.relay_state_prefix, )"," relay_state_prefix = sql.Column(sql.String(256), nullable=False)",38,28
openstack%2Fneutron~stable%2F2023.1~I04f120d3ccd11b18ae08cb10b19a7cb5a0e8983d,openstack/neutron,stable/2023.1,I04f120d3ccd11b18ae08cb10b19a7cb5a0e8983d,Disable pool recycle in tests,MERGED,2023-07-04 12:54:49.000000000,2023-07-06 13:58:54.000000000,2023-07-06 13:56:22.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 12:54:49.000000000', 'files': ['neutron/tests/base.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/cd8035306f3195aab541d52bfd453223d35d9172', 'message': ""Disable pool recycle in tests\n\nThe default for connection_recycle_time is\none hour. If any test using StaticSqlFixture\nruns after 1 hour it fails as connection get's\nrecycled.\n\nWith sqlite memory db if there is a connection\ndisconnect or reconnect db get's wiped off.\nThis patch disables the pool recycle so tests\ncan run fine even in slow environments.\n\nCloses-Bug: #2024674\nChange-Id: I04f120d3ccd11b18ae08cb10b19a7cb5a0e8983d\n(cherry picked from commit 576c468b711a94a1e6a7d5c65841b6a042dab855)\n""}]",1,887610,cd8035306f3195aab541d52bfd453223d35d9172,9,3,1,13861,,,0,"Disable pool recycle in tests

The default for connection_recycle_time is
one hour. If any test using StaticSqlFixture
runs after 1 hour it fails as connection get's
recycled.

With sqlite memory db if there is a connection
disconnect or reconnect db get's wiped off.
This patch disables the pool recycle so tests
can run fine even in slow environments.

Closes-Bug: #2024674
Change-Id: I04f120d3ccd11b18ae08cb10b19a7cb5a0e8983d
(cherry picked from commit 576c468b711a94a1e6a7d5c65841b6a042dab855)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/10/887610/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/base.py'],1,cd8035306f3195aab541d52bfd453223d35d9172,bug/2024674-stable/2023.1," # NOTE(ykarel): Disable pool recycle as tables are dropped with sqlite # memory db with connection close or reconnect cfg.CONF.set_override('connection_recycle_time', -1, group='database') ",,4,0
openstack%2Fneutron~stable%2Fzed~I04f120d3ccd11b18ae08cb10b19a7cb5a0e8983d,openstack/neutron,stable/zed,I04f120d3ccd11b18ae08cb10b19a7cb5a0e8983d,Disable pool recycle in tests,MERGED,2023-07-04 12:55:19.000000000,2023-07-06 13:58:45.000000000,2023-07-06 13:56:26.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 12:55:19.000000000', 'files': ['neutron/tests/base.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/a03a60e89deefc12153c31229e00a77a58525f45', 'message': ""Disable pool recycle in tests\n\nThe default for connection_recycle_time is\none hour. If any test using StaticSqlFixture\nruns after 1 hour it fails as connection get's\nrecycled.\n\nWith sqlite memory db if there is a connection\ndisconnect or reconnect db get's wiped off.\nThis patch disables the pool recycle so tests\ncan run fine even in slow environments.\n\nCloses-Bug: #2024674\nChange-Id: I04f120d3ccd11b18ae08cb10b19a7cb5a0e8983d\n(cherry picked from commit 576c468b711a94a1e6a7d5c65841b6a042dab855)\n""}]",0,887611,a03a60e89deefc12153c31229e00a77a58525f45,9,3,1,13861,,,0,"Disable pool recycle in tests

The default for connection_recycle_time is
one hour. If any test using StaticSqlFixture
runs after 1 hour it fails as connection get's
recycled.

With sqlite memory db if there is a connection
disconnect or reconnect db get's wiped off.
This patch disables the pool recycle so tests
can run fine even in slow environments.

Closes-Bug: #2024674
Change-Id: I04f120d3ccd11b18ae08cb10b19a7cb5a0e8983d
(cherry picked from commit 576c468b711a94a1e6a7d5c65841b6a042dab855)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/11/887611/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/tests/base.py'],1,a03a60e89deefc12153c31229e00a77a58525f45,bug/2024674-stable/2023.1-stable/zed," # NOTE(ykarel): Disable pool recycle as tables are dropped with sqlite # memory db with connection close or reconnect cfg.CONF.set_override('connection_recycle_time', -1, group='database') ",,4,0
openstack%2Fcinder-tempest-plugin~master~Ic999af2e2f0c2429363f611f4bb113581b83bacf,openstack/cinder-tempest-plugin,master,Ic999af2e2f0c2429363f611f4bb113581b83bacf,Add test for create volume from backup,NEW,2023-06-29 15:14:20.000000000,2023-07-06 13:58:18.000000000,,"[{'_account_id': 20813}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 15:14:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/38c062811318638ac4669f5604eb97ba30056b37', 'message': 'Add test for create volume from backup\n\nRelated-Bug: #2025277\nChange-Id: Ic999af2e2f0c2429363f611f4bb113581b83bacf\n'}, {'number': 2, 'created': '2023-06-29 18:06:45.000000000', 'files': ['cinder_tempest_plugin/api/volume/test_volume_backup.py'], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/bf94b3eb62f5e8430ab1543b61905017bad462f5', 'message': 'Add test for create volume from backup\n\nRelated-Bug: #2025277\nChange-Id: Ic999af2e2f0c2429363f611f4bb113581b83bacf\n'}]",2,887294,bf94b3eb62f5e8430ab1543b61905017bad462f5,5,2,2,4523,,,0,"Add test for create volume from backup

Related-Bug: #2025277
Change-Id: Ic999af2e2f0c2429363f611f4bb113581b83bacf
",git fetch https://review.opendev.org/openstack/cinder-tempest-plugin refs/changes/94/887294/2 && git format-patch -1 --stdout FETCH_HEAD,['cinder_tempest_plugin/api/volume/test_volume_backup.py'],1,38c062811318638ac4669f5604eb97ba30056b37,," class VolumesBackupsTest347(VolumesBackupTest): min_microversion = '3.47' @decorators.idempotent_id('a685788f-caa9-4d7d-b109-243d835d921a') def test_backup_create_and_create_volume_from_it(self): """"""Test backup create and create vol from backup."""""" src_vol = self.create_volume() backup = self.create_backup(volume_id=src_vol['id']) dest_vol = self.create_volume(backup_id=backup['id']) waiters.wait_for_volume_resource_status( self.volumes_client, dest_vol['id'], 'available')",,17,0
openstack%2Fneutron~stable%2Fxena~I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35,openstack/neutron,stable/xena,I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35,Delete sg rule which remote is the deleted sg,MERGED,2023-07-04 12:26:18.000000000,2023-07-06 13:57:50.000000000,2023-07-06 13:56:17.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 12:26:18.000000000', 'files': ['neutron/tests/unit/api/rpc/handlers/test_securitygroups_rpc.py', 'neutron/api/rpc/handlers/securitygroups_rpc.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/a8b8902b11bc7843c4950f88b549f00048d95b4f', 'message': 'Delete sg rule which remote is the deleted sg\n\nBased on bug #2008712 if we have a security-group which\nis the remote group of a 2nd security-group, the backend\nnever deletes the rule of the 2nd group which\nremote_group_id is the original security-group.\nBy AFTER_DELETE event for each rule that has the\nsecurity_group_id as remote_group_id, we can make the\nmech drivers do their work and delete these rules in the\nbackend.\n\nOne version of this fix was merged:\nhttps://review.opendev.org/q/I207ecf7954b06507e03cb16b502ceb6e2807e0e7\nand reverted due to #2019449:\nhttps://review.opendev.org/q/I077fe87435f61bd29d5c1efc979c2adebca26181\n\nThis patch is based on\nhttps://review.opendev.org/c/openstack/neutron/+/876716/1\n\nCloses-Bug: #2008712\nRelated-Bug: #2019449\nChange-Id: I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35\n(cherry picked from commit 67a0b0728788207cee27adb586880fabb8da6f25)\n'}]",0,887467,a8b8902b11bc7843c4950f88b549f00048d95b4f,9,3,1,8313,,,0,"Delete sg rule which remote is the deleted sg

Based on bug #2008712 if we have a security-group which
is the remote group of a 2nd security-group, the backend
never deletes the rule of the 2nd group which
remote_group_id is the original security-group.
By AFTER_DELETE event for each rule that has the
security_group_id as remote_group_id, we can make the
mech drivers do their work and delete these rules in the
backend.

One version of this fix was merged:
https://review.opendev.org/q/I207ecf7954b06507e03cb16b502ceb6e2807e0e7
and reverted due to #2019449:
https://review.opendev.org/q/I077fe87435f61bd29d5c1efc979c2adebca26181

This patch is based on
https://review.opendev.org/c/openstack/neutron/+/876716/1

Closes-Bug: #2008712
Related-Bug: #2019449
Change-Id: I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35
(cherry picked from commit 67a0b0728788207cee27adb586880fabb8da6f25)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/67/887467/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/api/rpc/handlers/test_securitygroups_rpc.py', 'neutron/api/rpc/handlers/securitygroups_rpc.py']",2,a8b8902b11bc7843c4950f88b549f00048d95b4f,bug/2008712-stable/2023.1-stable/zed-stable/yoga-stable/xena," # If there's a rule which remote is the deleted sg, remove that also. rules = self.rcache.match_resources_with_func( 'SecurityGroupRule', lambda sg_rule: sg_rule.remote_group_id == existing.id) for rule in rules: self.rcache.record_resource_delete(context, 'SecurityGroupRule', rule.id)",,56,1
openstack%2Fneutron~stable%2Fyoga~I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35,openstack/neutron,stable/yoga,I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35,Delete sg rule which remote is the deleted sg,MERGED,2023-07-04 12:25:21.000000000,2023-07-06 13:57:49.000000000,2023-07-06 13:56:13.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 12:25:21.000000000', 'files': ['neutron/tests/unit/api/rpc/handlers/test_securitygroups_rpc.py', 'neutron/api/rpc/handlers/securitygroups_rpc.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/78b7cf82dd84ea19c6a147c8715961f7c37ce448', 'message': 'Delete sg rule which remote is the deleted sg\n\nBased on bug #2008712 if we have a security-group which\nis the remote group of a 2nd security-group, the backend\nnever deletes the rule of the 2nd group which\nremote_group_id is the original security-group.\nBy AFTER_DELETE event for each rule that has the\nsecurity_group_id as remote_group_id, we can make the\nmech drivers do their work and delete these rules in the\nbackend.\n\nOne version of this fix was merged:\nhttps://review.opendev.org/q/I207ecf7954b06507e03cb16b502ceb6e2807e0e7\nand reverted due to #2019449:\nhttps://review.opendev.org/q/I077fe87435f61bd29d5c1efc979c2adebca26181\n\nThis patch is based on\nhttps://review.opendev.org/c/openstack/neutron/+/876716/1\n\nCloses-Bug: #2008712\nRelated-Bug: #2019449\nChange-Id: I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35\n(cherry picked from commit 67a0b0728788207cee27adb586880fabb8da6f25)\n'}]",0,887466,78b7cf82dd84ea19c6a147c8715961f7c37ce448,9,3,1,8313,,,0,"Delete sg rule which remote is the deleted sg

Based on bug #2008712 if we have a security-group which
is the remote group of a 2nd security-group, the backend
never deletes the rule of the 2nd group which
remote_group_id is the original security-group.
By AFTER_DELETE event for each rule that has the
security_group_id as remote_group_id, we can make the
mech drivers do their work and delete these rules in the
backend.

One version of this fix was merged:
https://review.opendev.org/q/I207ecf7954b06507e03cb16b502ceb6e2807e0e7
and reverted due to #2019449:
https://review.opendev.org/q/I077fe87435f61bd29d5c1efc979c2adebca26181

This patch is based on
https://review.opendev.org/c/openstack/neutron/+/876716/1

Closes-Bug: #2008712
Related-Bug: #2019449
Change-Id: I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35
(cherry picked from commit 67a0b0728788207cee27adb586880fabb8da6f25)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/66/887466/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/api/rpc/handlers/test_securitygroups_rpc.py', 'neutron/api/rpc/handlers/securitygroups_rpc.py']",2,78b7cf82dd84ea19c6a147c8715961f7c37ce448,bug/2008712-stable/2023.1-stable/zed-stable/yoga," # If there's a rule which remote is the deleted sg, remove that also. rules = self.rcache.match_resources_with_func( 'SecurityGroupRule', lambda sg_rule: sg_rule.remote_group_id == existing.id) for rule in rules: self.rcache.record_resource_delete(context, 'SecurityGroupRule', rule.id)",,56,1
openstack%2Fneutron~stable%2Fzed~I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35,openstack/neutron,stable/zed,I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35,Delete sg rule which remote is the deleted sg,MERGED,2023-07-04 12:25:00.000000000,2023-07-06 13:57:47.000000000,2023-07-06 13:56:09.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 12:25:00.000000000', 'files': ['neutron/tests/unit/api/rpc/handlers/test_securitygroups_rpc.py', 'neutron/api/rpc/handlers/securitygroups_rpc.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/4d09a6f7de00fe0db11af860d0da32719c8dd168', 'message': 'Delete sg rule which remote is the deleted sg\n\nBased on bug #2008712 if we have a security-group which\nis the remote group of a 2nd security-group, the backend\nnever deletes the rule of the 2nd group which\nremote_group_id is the original security-group.\nBy AFTER_DELETE event for each rule that has the\nsecurity_group_id as remote_group_id, we can make the\nmech drivers do their work and delete these rules in the\nbackend.\n\nOne version of this fix was merged:\nhttps://review.opendev.org/q/I207ecf7954b06507e03cb16b502ceb6e2807e0e7\nand reverted due to #2019449:\nhttps://review.opendev.org/q/I077fe87435f61bd29d5c1efc979c2adebca26181\n\nThis patch is based on\nhttps://review.opendev.org/c/openstack/neutron/+/876716/1\n\nCloses-Bug: #2008712\nRelated-Bug: #2019449\nChange-Id: I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35\n(cherry picked from commit 67a0b0728788207cee27adb586880fabb8da6f25)\n'}]",1,887465,4d09a6f7de00fe0db11af860d0da32719c8dd168,11,3,1,8313,,,0,"Delete sg rule which remote is the deleted sg

Based on bug #2008712 if we have a security-group which
is the remote group of a 2nd security-group, the backend
never deletes the rule of the 2nd group which
remote_group_id is the original security-group.
By AFTER_DELETE event for each rule that has the
security_group_id as remote_group_id, we can make the
mech drivers do their work and delete these rules in the
backend.

One version of this fix was merged:
https://review.opendev.org/q/I207ecf7954b06507e03cb16b502ceb6e2807e0e7
and reverted due to #2019449:
https://review.opendev.org/q/I077fe87435f61bd29d5c1efc979c2adebca26181

This patch is based on
https://review.opendev.org/c/openstack/neutron/+/876716/1

Closes-Bug: #2008712
Related-Bug: #2019449
Change-Id: I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35
(cherry picked from commit 67a0b0728788207cee27adb586880fabb8da6f25)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/65/887465/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/api/rpc/handlers/test_securitygroups_rpc.py', 'neutron/api/rpc/handlers/securitygroups_rpc.py']",2,4d09a6f7de00fe0db11af860d0da32719c8dd168,bug/2008712-stable/2023.1-stable/zed," # If there's a rule which remote is the deleted sg, remove that also. rules = self.rcache.match_resources_with_func( 'SecurityGroupRule', lambda sg_rule: sg_rule.remote_group_id == existing.id) for rule in rules: self.rcache.record_resource_delete(context, 'SecurityGroupRule', rule.id)",,56,1
openstack%2Fneutron~stable%2F2023.1~I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35,openstack/neutron,stable/2023.1,I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35,Delete sg rule which remote is the deleted sg,MERGED,2023-07-04 12:23:36.000000000,2023-07-06 13:57:47.000000000,2023-07-06 13:56:05.000000000,"[{'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 12:23:36.000000000', 'files': ['neutron/tests/unit/api/rpc/handlers/test_securitygroups_rpc.py', 'neutron/api/rpc/handlers/securitygroups_rpc.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/287f4231a0b4b0e7b3fdaa0fcd415d6a4aeb2930', 'message': 'Delete sg rule which remote is the deleted sg\n\nBased on bug #2008712 if we have a security-group which\nis the remote group of a 2nd security-group, the backend\nnever deletes the rule of the 2nd group which\nremote_group_id is the original security-group.\nBy AFTER_DELETE event for each rule that has the\nsecurity_group_id as remote_group_id, we can make the\nmech drivers do their work and delete these rules in the\nbackend.\n\nOne version of this fix was merged:\nhttps://review.opendev.org/q/I207ecf7954b06507e03cb16b502ceb6e2807e0e7\nand reverted due to #2019449:\nhttps://review.opendev.org/q/I077fe87435f61bd29d5c1efc979c2adebca26181\n\nThis patch is based on\nhttps://review.opendev.org/c/openstack/neutron/+/876716/1\n\nCloses-Bug: #2008712\nRelated-Bug: #2019449\nChange-Id: I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35\n(cherry picked from commit 67a0b0728788207cee27adb586880fabb8da6f25)\n'}]",1,887464,287f4231a0b4b0e7b3fdaa0fcd415d6a4aeb2930,9,3,1,8313,,,0,"Delete sg rule which remote is the deleted sg

Based on bug #2008712 if we have a security-group which
is the remote group of a 2nd security-group, the backend
never deletes the rule of the 2nd group which
remote_group_id is the original security-group.
By AFTER_DELETE event for each rule that has the
security_group_id as remote_group_id, we can make the
mech drivers do their work and delete these rules in the
backend.

One version of this fix was merged:
https://review.opendev.org/q/I207ecf7954b06507e03cb16b502ceb6e2807e0e7
and reverted due to #2019449:
https://review.opendev.org/q/I077fe87435f61bd29d5c1efc979c2adebca26181

This patch is based on
https://review.opendev.org/c/openstack/neutron/+/876716/1

Closes-Bug: #2008712
Related-Bug: #2019449
Change-Id: I9e8ddfa26c5402fefd573b0e2ea5f3a57983ca35
(cherry picked from commit 67a0b0728788207cee27adb586880fabb8da6f25)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/64/887464/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/tests/unit/api/rpc/handlers/test_securitygroups_rpc.py', 'neutron/api/rpc/handlers/securitygroups_rpc.py']",2,287f4231a0b4b0e7b3fdaa0fcd415d6a4aeb2930,bug/2008712-stable/2023.1," # If there's a rule which remote is the deleted sg, remove that also. rules = self.rcache.match_resources_with_func( 'SecurityGroupRule', lambda sg_rule: sg_rule.remote_group_id == existing.id) for rule in rules: self.rcache.record_resource_delete(context, 'SecurityGroupRule', rule.id)",,56,1
openstack%2Fmanila-specs~master~Icdc909c43459730c8e35a677779d842e36dc7b1b,openstack/manila-specs,master,Icdc909c43459730c8e35a677779d842e36dc7b1b,Allow locking shares against deletion,MERGED,2023-04-29 01:58:47.000000000,2023-07-06 13:41:14.000000000,2023-07-06 13:40:12.000000000,"[{'_account_id': 11604}, {'_account_id': 16207}, {'_account_id': 22348}, {'_account_id': 29632}, {'_account_id': 30407}, {'_account_id': 35677}]","[{'number': 1, 'created': '2023-04-29 01:58:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/d469d5e4e2242618435add2e01ed7bd91b5a6f7e', 'message': 'Allow locking shares against deletion\n\nA proposal to introduce ""resource locks"" that\ncan be placed by project users against project\nresources and specific resource actions that\nthey intend to prevent. In the 2023.2 Bobcat\ncycle, the share deletion resource locks will\nbe implemented.\n\nPartially-Implements: bp allow-locking-shares-against-deletion\nAPIImpact\n\nChange-Id: Icdc909c43459730c8e35a677779d842e36dc7b1b\nSigned-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>\n'}, {'number': 2, 'created': '2023-06-14 08:34:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/db6ae3f55811a8a338d974077a36a1b5bfa8dab3', 'message': 'Allow locking shares against deletion\n\nA proposal to introduce ""resource locks"" that\ncan be placed by project users against project\nresources and specific resource actions that\nthey intend to prevent. In the 2023.2 Bobcat\ncycle, the share deletion resource locks will\nbe implemented.\n\nPartially-Implements: bp allow-locking-shares-against-deletion\nAPIImpact\n\nChange-Id: Icdc909c43459730c8e35a677779d842e36dc7b1b\nSigned-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>\n'}, {'number': 3, 'created': '2023-06-30 07:12:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/415e52dacbcf6ee4bc3e5332f0fbbf000605acec', 'message': 'Allow locking shares against deletion\n\nA proposal to introduce ""resource locks"" that\ncan be placed by project users against project\nresources and specific resource actions that\nthey intend to prevent. In the 2023.2 Bobcat\ncycle, the share deletion resource locks will\nbe implemented.\n\nPartially-Implements: bp allow-locking-shares-against-deletion\nAPIImpact\n\nChange-Id: Icdc909c43459730c8e35a677779d842e36dc7b1b\nSigned-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>\n'}, {'number': 4, 'created': '2023-07-05 14:45:17.000000000', 'files': ['specs/bobcat/allow-locking-shares-against-deletion.rst', 'doc/source/index.rst'], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/194021de433f609fb3e0867ca72ef84cf521b8f2', 'message': 'Allow locking shares against deletion\n\nA proposal to introduce ""resource locks"" that\ncan be placed by project users against project\nresources and specific resource actions that\nthey intend to prevent. In the 2023.2 Bobcat\ncycle, the share deletion resource locks will\nbe implemented.\n\nPartially-Implements: bp allow-locking-shares-against-deletion\nAPIImpact\n\nChange-Id: Icdc909c43459730c8e35a677779d842e36dc7b1b\nSigned-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>\n'}]",80,881894,194021de433f609fb3e0867ca72ef84cf521b8f2,33,6,4,16643,,,0,"Allow locking shares against deletion

A proposal to introduce ""resource locks"" that
can be placed by project users against project
resources and specific resource actions that
they intend to prevent. In the 2023.2 Bobcat
cycle, the share deletion resource locks will
be implemented.

Partially-Implements: bp allow-locking-shares-against-deletion
APIImpact

Change-Id: Icdc909c43459730c8e35a677779d842e36dc7b1b
Signed-off-by: Goutham Pacha Ravi <gouthampravi@gmail.com>
",git fetch https://review.opendev.org/openstack/manila-specs refs/changes/94/881894/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/index.rst', 'specs/bobcat/allow-locking-shares-against-deletion.rst']",2,d469d5e4e2242618435add2e01ed7bd91b5a6f7e,bp/allow-locking-shares-against-deletion,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ===================================== Allow locking shares against deletion ===================================== Include the URL of your launchpad blueprint: https://blueprints.launchpad.net/manila/+spec/allow-locking-shares-against-deletion The default RBAC permits a non-reader project user to create and delete shares under the project's namespace. Deletion of shares can be dangerous, and we expect that users exercise caution before initiating the action. The Shared File Systems (Manila) API ensures that some pre-conditions are met prior to proceeding with the deletion. A desirable pre-condition would be to check if the share is actively being used by a client workload. Such a check however is not straight forward to perform as it cannot reliably be implemented in a consistent manner across all Network Attached Storage (NAS) protocols or storage system back ends that Manila supports. In other words, Manila does not know who and how many clients have mounted a share, or if data is actively being read or written into the share. So there is a need for a safety mechanism to prevent unintentional consequences. This specification proposes a new pre-condition, one that allows any non-reader project user to create a deletion lock against a share in the project's namespace. The deletion lock can be removed by the same user, or by a privileged user. Problem description =================== A shared file system is served by a network file server and it allows several simultaneous clients to connect, read and write to it. In OpenStack, Manila shares are collectively owned by the project users that the share belongs to. A user in the project can delete a share that some other user is actively using, and Manila API provides no way to indicate or coordinate communication prior to this deletion. Further, as part of the protocol, NAS clients are hardened to survive minor network interruptions and server side failures within a degree of tolerance. If the server goes unresponsive for a while, the client can wait, filling up its write cache or retrying its reads until they succeed, or until the tolerance expires. In the most common scenario, the client can be instructed to wait indefinitely (""hard mounts""). So, an extended server failure can be catastrophic to the client. If a file system that is mounted is deleted on the server, the client typically exercises the same waiting behavior and can go unresponsive in the process. Use Cases ========= The most recent use case is with the OpenStack Compute feature that allows users to mount their shares to virtual machines via VirtIOFS `[1]`_. With this feature, a compute host can plumb a mounted network filesystem to one or more guests. If the share is deleted while it is mounted, the compute host would be compromised. This would disable all virtual machines on the host, not just the virtual machine that was using the share via VirtIOFS. So while the Compute service orchestrates the mount, a user's action of deleting the backing share can bring down the shared infrastructure. Proposed change =============== Users will have the ability to lock any share in the project. Multiple locks can be placed on the share. A share cannot be deleted unless all locks have been removed. Only the user that placed the lock, or the administrator user can remove a given lock. If a user attempts to lock a share that is previously locked by them, the API will not present an error. The lock record can be updated with a new lock reason or a different lock action. The implementation of this feature will include generalizations for future extensibility. The lock API will accept a resource ID, resource type, and a resource action that must be locked. In the 2023.2 Bobcat release cycle, we will only be implementing deletion locks for shares. Alternatives ------------ Shares could have an ""in-use"" state that could prevent adverse manipulation. The presence of access rules can allow a share to transition to this ""in-use"" status. The problem with this approach is that users could drain access rules prior to deleting the share. This provides a two-step deletion ensuring that the action is deliberate. However, in the use case above, it wouldn't protect OpenStack Compute service resources from losing the share gracelessly. Data model impact ----------------- A new table will be introduced:: +-----------------+---------------+----------+----------+ | Field | Type | Nullable | Default | +-----------------+---------------+----------+----------+ | ID | varchar(36) | NO | NULL | | USER_ID | varchar(36) | YES | NULL | | PROJECT_ID | varchar(36) | YES | NULL | | RESOURCE_ACTION | varchar(255) | YES | 'delete' | | RESOURCE_TYPE | varchar(255) | YES | NULL | | RESOURCE_ID | varchar(36) | NO | NULL | | LOCK_REASON | varchar(1023) | YES | NULL | | DELETED | tinyint(1) | YES | NULL | | CREATED_AT | datetime(6) | YES | NULL | | DELETED_AT | datetime(6) | YES | NULL | | UPDATED_AT | datetime(6) | YES | NULL | +-----------------+---------------+----------+----------+ The table will assist storing lock records and will be manipulated as locks are created, updated and removed. A database migration will create this table with no initial data. REST API impact --------------- The APIs using resource lock endpoints and methods will only be available in a new API micro version. However, if resource locks exist, they cannot be circumvented by using an older API micro version to perform the action that they are preventing. **Create a resource lock on a particular action**:: POST /v2/resource-locks Normal http response code(s): - 204 - Lock created successfully Expected http error code(s): - 401 - Unauthorized; user has not authenticated - 400 - Bad Request - 400 - Unrecognized action on resource - 400 - Unrecognized resource (no such resource in project namespace) - 403 - Forbidden; user is forbidden by policy - 404 - API does not exist in micro version - 406 - API version not supported Request example:: { 'resource_lock': { 'resource_action': 'delete', 'resource_type': 'share', 'resource_id': 'a448e0d2-7501-4b99-a447-1b89e3961e39', 'lock_reason': 'share is used by audit team' } } Response example:: { 'resource_lock': { 'id': 'be0871e8-742e-4c19-8567-7016fa0e2235', 'user_id': 'cec1dd3e297b45348228f4fc3f5dba38', 'project_id': '2e47ac4e2cf04a5b8b8509de8177d65d', 'resource_action': 'delete', 'resource_type': 'share', 'resource_id': 'a448e0d2-7501-4b99-a447-1b89e3961e39', 'lock_reason': 'share is used by audit team', 'created_at': '2023-04-28T09:49:58-05:00', 'updated_at': None } } **Update a resource lock**:: PUT /v2/resource-locks/{id} Updatable fields include ""resource_action"" and ""lock_reason"". ""lock_reason"" can be nullified on update. Only the user that created the lock or a user with ""admin"" role will be allowed to update a lock per default RBAC policy. Normal http response code(s): - 200 - Lock updated successfully Expected http error code(s): - 401 - Unauthorized; user has not authenticated - 400 - Bad Request - 400 - Unrecognized action on resource - 400 - Unrecognized resource (no such resource in project namespace) - 403 - Forbidden; user is forbidden by policy - 404 - API does not exist in micro version - 404 - lock does not exist in project namespace - 406 - API version not supported Request example:: { 'resource_lock': { 'id': 'be0871e8-742e-4c19-8567-7016fa0e2235', 'lock_reason': 'share will be used by audit team until 2024' } } Response example:: { 'resource_lock': { 'id': 'be0871e8-742e-4c19-8567-7016fa0e2235', 'user_id': 'cec1dd3e297b45348228f4fc3f5dba38', 'project_id': '2e47ac4e2cf04a5b8b8509de8177d65d', 'resource_action': 'delete', 'resource_type': 'share', 'resource_id': 'a448e0d2-7501-4b99-a447-1b89e3961e39', 'lock_reason': 'share will be used by audit team until 2024', 'created_at': '2023-04-28T09:49:58.231919', 'updated_at': '2023-04-28T20:01:13.12106' } } **Delete a resource lock for a particular action**:: DELETE /v2/resource-locks/{id} Only the user that created the lock or a user with ""admin"" role will be allowed to delete a lock per default RBAC policy. Normal http response code(s): - 204 - Lock deleted successfully Expected http error code(s): - 401 - Unauthorized; user has not authenticated - 400 - Bad Request - 403 - Forbidden; user is forbidden by policy - 404 - API does not exist in microversion - 404 - lock does not exist in project namespace - 406 - API version not supported Request and response do not contain any data **List resource locks**:: GET /v2/resource-locks?{queries} Queries will allow filtering with exact and inexact (""created_since"", ""created_before"") attributes. Querying with ""project_id"" or ""all_projects"" will only be allowed for a user with ""admin"" role per default RBAC policy. Normal http response code(s): - 200 - List of locks in project namespace Expected http error code(s): - 401 - Unauthorized; user has not authenticated - 403 - Forbidden; user is forbidden by policy - 404 - API does not exist in microversion - 406 - API version not supported Response example:: { 'resource_locks': [ { 'id': 'be0871e8-742e-4c19-8567-7016fa0e2235', 'user_id': 'cec1dd3e297b45348228f4fc3f5dba38', 'project_id': '2e47ac4e2cf04a5b8b8509de8177d65d', 'resource_action': 'delete', 'resource_type': 'share', 'resource_id': 'a448e0d2-7501-4b99-a447-1b89e3961e39', 'lock_reason': 'share will be used by audit team until 2024' }, { 'id': '4945b04e-cdda-4308-9cfd-1483e7f9dd8c', 'user_id': '80b789450540431db23575b333059ca8', 'project_id': '2e47ac4e2cf04a5b8b8509de8177d65d', 'resource_action': 'shrink', 'resource_type': 'share', 'resource_id': '4227fbd2-7f55-4ff4-9239-2cfc700d9fdf', 'lock_reason': 'space is reserved for in place snapshots' } ] } **Show lock**:: GET /v2/resource-locks/{id} Normal http response code(s): - 200 - Details of a lock in the project namespace Expected http error code(s): - 401 - Unauthorized; user has not authenticated - 403 - Forbidden; user is forbidden by policy - 404 - API does not exist in micro version - 404 - lock does not exist in project namespace - 406 - API version not supported Response example:: { 'resource_lock': { 'id': 'be0871e8-742e-4c19-8567-7016fa0e2235', 'user_id': 'cec1dd3e297b45348228f4fc3f5dba38', 'project_id': '2e47ac4e2cf04a5b8b8509de8177d65d', 'resource_action': 'delete', 'resource_type': 'share', 'resource_id': 'a448e0d2-7501-4b99-a447-1b89e3961e39', 'lock_reason': 'share will be used by audit team until 2024', 'created_at': '2023-04-28T09:49:58.231919', 'updated_at': '2023-04-28T20:01:13.12106' } } **Deleting a share that has locks**:: DELETE /v2/shares/{id} Normal http response code(s): - 202 - No locks exist and all other pre-conditions allow, accepted Expected http error code(s): - 401 - Unauthorized; user has not authenticated - 403 - Forbidden; user is forbidden by policy - 404 - API does not exist in microversion - 404 - share does not exist in project namespace - 409 - share deletion precondition failed, perhaps there's a lock - 406 - API version not supported **New RBAC policies will be introduced:** .. code-block:: python """"""Policy defaults that are used in specific policies below:"""""" RULE_ADMIN = ""role:admin"" PROJECT_MEMBER = ""rule:project-member"" PROJECT_READER = ""rule:project-reader"" PROJECT_OWNER_USER = ""rule:project-owner-user"" ADMIN_OR_PROJECT_MEMBER = f'({RULE_ADMIN}) or ({PROJECT_MEMBER})' ADMIN_OR_PROJECT_READER = f'({RULE_ADMIN}) or ({PROJECT_READER})' ADMIN_OR_PROJECT_OWNER_USER = f'({RULE_ADMIN}) or ({PROJECT_OWNER_USER})' rules = [ policy.RuleDefault( name='project-member', check_str='role:member and ' 'project_id:%(project_id)s', description='Project scoped Member', scope_types=['project']), policy.RuleDefault( name='project-reader', check_str='role:reader and ' 'project_id:%(project_id)s', description='Project scoped Reader', scope_types=['project']), policy.RuleDefault( name='project-owner-user', check_str='role:member and ' 'project_id:%(project_id)s and ' 'user_id:%(user_id)s', description='Project scoped Member who owns a resource', scope_types=['project']), ] * Create a lock .. code-block:: python policy.DocumentedRuleDefault( name= 'resource_locks:create', check_str=ADMIN_OR_PROJECT_MEMBER, scope_types=['project'], description=""Create a resource lock."", operations=[ { 'method': 'POST', 'path': '/resource-locks', }, ], ) * Update a lock .. code-block:: python policy.DocumentedRuleDefault( name= 'resource_locks:update', check_str=ADMIN_OR_PROJECT_OWNER_USER, scope_types=['project'], description=""Update a resource lock."", operations=[ { 'method': 'PUT', 'path': '/resource-locks/{id}', }, ], ) * Delete a lock .. code-block:: python policy.DocumentedRuleDefault( name= 'resource_locks:delete', check_str=ADMIN_OR_PROJECT_OWNER_USER, scope_types=['project'], description=""Delete a resource lock."", operations=[ { 'method': 'DELETE', 'path': '/resource-locks/{id}', }, ], ) * List locks .. code-block:: python policy.DocumentedRuleDefault( name= 'resource_locks:index', check_str=ADMIN_OR_PROJECT_READER, scope_types=['project'], description=""List all resource locks."", operations=[ { 'method': 'GET', 'path': '/resource-locks?{queries}', }, ], ) * List locks with project queries .. code-block:: python policy.DocumentedRuleDefault( name= 'resource_locks:get_all_projects', check_str=ADMIN, scope_types=['project'], description=""Create a resource lock."", operations=[ { 'method': 'GET', 'path': '/resource-locks?all_projects=1&project_id={project_id}', }, ], ) * Get lock .. code-block:: python policy.DocumentedRuleDefault( name= 'resource_locks:get', check_str=ADMIN_OR_PROJECT_READER, scope_types=['project'], description=""Get details about a resource lock."", operations=[ { 'method': 'GET', 'path': '/resource-locks/{id}', }, ], ) Driver impact ------------- None. This is an API only feature. Security impact --------------- Default RBAC policies will allow users with ""admin"" role to create, view or delete user locks. The ""admin"" role is presumed to be given to the operator user. If a lock must be created on behalf of the user by a service or an application, it is advised that the service or application is configured with a user that has the ""service"" role and not ""admin"". No further security impact, positive or negative is noted. Notifications impact -------------------- ""lock.create"" and ""lock.delete"" notification events will be emitted for the respective actions. Other end user impact --------------------- User Interface improvements will be introduced in OpenStackClient (``python-manilaclient`` plugin) and the OpenStack Dashboard (``manila-ui`` plugin). The OpenStackClient addition will be accompanied by ``manilaclient`` and ``openstacksdk`` interfaces: * Create a resource lock: .. code-block:: bash openstack share lock create <resource_id> <resource_action> \ [--resource-type <resource_type>] \ [--reason <lock_reason>}] * Update a resource lock: .. code-block:: bash openstack share lock update <id> \ [--resource-action <resource_action>] \ [--reason <lock_reason>}] * Delete a resource lock: .. code-block:: bash openstack share lock delete <id> * List resource locks: .. code-block:: bash openstack share lock list * Show a resource lock: .. code-block:: bash openstack share lock show <id> Performance Impact ------------------ As we're introducing a new pre-condition on share deletion, the share delete API will suffer performance degradation due to the additional lookup. It's not possible to avoid this lookup even when locks are not used in the environment. We'll optimize the query by using appropriate indices. In the future, as more resources and resource actions use this approach, we will be impacting the existing performance of these APIs. It's a trade-off for the feature functionality. Other deployer impact --------------------- None. Developer impact ---------------- Consider allowing locks via this interface when defining or manipulating actions. Implementation ============== Assignee(s) ----------- Primary assignee: gouthamr Work Items ---------- - Manila API changes - support in manilaclient, openstackclient, manila UI - support in openstacksdk - e2e tests with manila-tempest-plugin - API Reference, user and administrator documentation Dependencies ============ * This feature doesn't depend on work elsewhere, but, the VirtIOFS integration effort in Nova requires this feature. Testing ======= New tests will be added to create locks, list locks, show locks, delete locks. Test cases will cover use of multiple locks and involve validation of request and response schema and codes. RBAC policies will also be tested via tempest. Documentation Impact ==================== API Reference will be updated alongside the API changes. User and administrator documentation will follow alongside the UX changes in respective repositories. References ========== _`[1]` `VirtIOFS Specification <https://specs.openstack .org/openstack/nova-specs/specs/2023.2/approved/libvirt-virtiofs-attach-manila-shares.html>`_ [2] `2023.2 Bobcat PTG Discussion <https://etherpad.opendev .org/p/nova-bobcat-ptg#72>`_ ",,633,2
openstack%2Fcharm-vault~master~I240ebb4bd14932a6bf95f41da3f2cd7776742266,openstack/charm-vault,master,I240ebb4bd14932a6bf95f41da3f2cd7776742266,Improve snap channel refresh mechanism,MERGED,2023-04-19 15:12:51.000000000,2023-07-06 13:08:24.000000000,2023-07-06 13:08:24.000000000,"[{'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-04-19 15:12:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-vault/commit/883c241a5350622be13d01629fb8905027019abb', 'message': 'Blocked unit if vault snap channel changed\n\n- blocked unit, when snap channel changed\n- add refresh-snap-channel action to proceed snap refresh\n\nChange-Id: I240ebb4bd14932a6bf95f41da3f2cd7776742266\n'}, {'number': 2, 'created': '2023-04-20 07:50:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-vault/commit/449acc644a2752f6f0491dce8d6d48558a9798b4', 'message': 'Blocked unit if vault snap channel changed\n\n- blocked unit, when snap channel changed\n- add refresh-snap-channel action to proceed snap refresh\n\nRelated-Bug: 2007587\nChange-Id: I240ebb4bd14932a6bf95f41da3f2cd7776742266\n'}, {'number': 3, 'created': '2023-06-14 11:00:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-vault/commit/556c11fe40c5e3366c4c7924eb77add00f3711b0', 'message': 'Improve snap channel refresh mechanism\n\n- stop vault.service before refresing it\n- added a warning note that changing the channel config option will\n  cause the vault to be sealed\n\nRelated-Bug: 2007587\nChange-Id: I240ebb4bd14932a6bf95f41da3f2cd7776742266\n'}, {'number': 4, 'created': '2023-06-22 15:13:43.000000000', 'files': ['src/config.yaml', 'unit_tests/test_reactive_vault_handlers.py', 'src/reactive/vault_handlers.py'], 'web_link': 'https://opendev.org/openstack/charm-vault/commit/9e927889d0e29de919816c315b2c6f5643f53049', 'message': 'Improve snap channel refresh mechanism\n\n- stop vault.service before refresing it\n- added a warning note that changing the channel config option will\n  cause the vault to be sealed\n\nRelated-Bug: 2007587\nChange-Id: I240ebb4bd14932a6bf95f41da3f2cd7776742266\n'}]",12,880858,9e927889d0e29de919816c315b2c6f5643f53049,24,3,4,32363,,,0,"Improve snap channel refresh mechanism

- stop vault.service before refresing it
- added a warning note that changing the channel config option will
  cause the vault to be sealed

Related-Bug: 2007587
Change-Id: I240ebb4bd14932a6bf95f41da3f2cd7776742266
",git fetch https://review.opendev.org/openstack/charm-vault refs/changes/58/880858/4 && git format-patch -1 --stdout FETCH_HEAD,"['src/README.md', 'src/config.yaml', 'unit_tests/test_reactive_vault_handlers.py', 'src/actions.yaml', 'src/actions/refresh-snap-channel', 'src/actions/actions.py', 'src/reactive/vault_handlers.py']",7,883c241a5350622be13d01629fb8905027019abb,lp2007587," if snap.get_installed_channel(""vault"") != channel: set_flag(""snap.channel.refresh"") if is_flag_set(""snap.channel.refresh""): status_set( ""blocked"", ""The snap channel must be refreshed manually to {} channel with "" ""the refresh-snap-channel action."".format(config(""channel"")), ) return"," service_restart, snap.refresh('vault', channel=channel) if vault.can_restart(): log(""Restarting vault"", level=DEBUG) service_restart('vault') if config('totally-unsecure-auto-unlock'): vault.prepare_vault()",56,26
openstack%2Fcinder~master~I06fb186ce14121d00f82da456ca381f9cbc9484a,openstack/cinder,master,I06fb186ce14121d00f82da456ca381f9cbc9484a,Tatlin unified driver - rename tatlin_api object,ABANDONED,2023-07-04 08:35:06.000000000,2023-07-06 13:00:07.000000000,,[],"[{'number': 1, 'created': '2023-07-04 08:35:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/ebed875f4f0926d55e2d35df2849f168975595ba', 'message': 'Tatlin unified driver - rename tatlin_api object\n\nIn Tatlin Unified driver client object is incorrectly called\ntatlin_api. TatlinAPI is a different type and does not have\nTatlin*Client methods. Renamed to tatlin_client.\n\nPartially Implements: blueprint yadro-tatlin-unified-refactoring\nChange-Id: I10135877889a17c6ae6274f8be1b992294bf65d3\n\nChange-Id: I06fb186ce14121d00f82da456ca381f9cbc9484a\n'}, {'number': 2, 'created': '2023-07-04 08:50:16.000000000', 'files': ['cinder/volume/drivers/yadro/tatlin_iscsi.py', 'cinder/tests/unit/volume/drivers/yadro/test_tatlin_common.py', 'cinder/volume/drivers/yadro/tatlin_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/253bb25a37a5e306089fe2aa426f610ebf8c81b6', 'message': 'Tatlin unified driver - rename tatlin_api object\n\nIn Tatlin Unified driver client object is incorrectly called\ntatlin_api. TatlinAPI is a different type and does not have\nTatlin*Client methods. Renamed to tatlin_client.\n\nPartially Implements: blueprint yadro-tatlin-unified-refactoring\nChange-Id: I10135877889a17c6ae6274f8be1b992294bf65d3\n\nChange-Id: I06fb186ce14121d00f82da456ca381f9cbc9484a\n'}]",0,887578,253bb25a37a5e306089fe2aa426f610ebf8c81b6,11,0,2,13671,,,0,"Tatlin unified driver - rename tatlin_api object

In Tatlin Unified driver client object is incorrectly called
tatlin_api. TatlinAPI is a different type and does not have
Tatlin*Client methods. Renamed to tatlin_client.

Partially Implements: blueprint yadro-tatlin-unified-refactoring
Change-Id: I10135877889a17c6ae6274f8be1b992294bf65d3

Change-Id: I06fb186ce14121d00f82da456ca381f9cbc9484a
",git fetch https://review.opendev.org/openstack/cinder refs/changes/78/887578/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/yadro/tatlin_iscsi.py', 'cinder/tests/unit/volume/drivers/yadro/test_tatlin_common.py', 'cinder/volume/drivers/yadro/tatlin_common.py']",3,ebed875f4f0926d55e2d35df2849f168975595ba,," self.tatlin_client = self._get_tatlin_client() self.tatlin_client.get_resource_count(self.pool_id) LOG.debug('Current pool %(pool)s has %(pool_res)s resourses. ' if pool_res_count >= self._max_pool_resource_count: message = _('Create volume failed. ' 'Too many resources per pool: ' % pool_res_count) if cluster_res_count >= self.MAX_ALLOWED_RESOURCES: message = _('Create volume failed. ' 'Too many resources per cluster: ' % cluster_res_count) self.tatlin_client.create_volume(volume.name_id, name, size, self.pool_id, lbaFormat=self._lba_format) if self.tatlin_client.is_volume_ready(volume.name_id): if (self.tatlin_client.get_volume_status(volume.name_id) == 'online'): if not self.tatlin_client.is_volume_exists(volume.name_id): self.tatlin_client.delete_volume(volume.name_id) if not self.tatlin_client.is_volume_exists(volume.name_id): if self.tatlin_client.is_volume_exists(volume.name_id): self.tatlin_client.extend_volume(volume.name_id, size) self.tatlin_client.update_qos( pool_stat = self.tatlin_client.get_pool_detail(self.pool_id) sys_stat = self.tatlin_client.get_sys_statistic() self.tatlin_client.get_resource_count(self.pool_id) result = self.tatlin_client.get_volume_info(source_name) result = self.tatlin_client.get_volume_info(source_name) self.tatlin_client.add_vol_to_host(volume.name_id, host_id) self.tatlin_client.remove_vol_from_host(volume.name_id, host_id) cur_ports = self.tatlin_client.get_resource_ports_array(volume_id) result = self.tatlin_client.get_resource_mapping() self.tatlin_client.export_volume(volume.name_id, ports) self._pool_id = self.tatlin_client.get_pool_id_by_name("," self.tatlin_api = self._get_tatlin_client() self.tatlin_api.get_resource_count(self.pool_id) LOG.debug('Current pool %(pool)s has %(pool_res)s res.' if pool_res_count > 255: message = _('TatlinVolumeDriver create volume failed. ' 'Too many resources per pool created') if cluster_res_count + 1 > self.MAX_ALLOWED_RESOURCES: message = _('TatlinVolumeDriver create volume failed. ' 'Too many resources per cluster created') self.tatlin_api.create_volume(volume.name_id, name, size, self.pool_id, lbaFormat=self._lba_format) if self.tatlin_api.is_volume_ready(volume.name_id): if self.tatlin_api.get_volume_status(volume.name_id) == 'online': if not self.tatlin_api.is_volume_exists(volume.name_id): self.tatlin_api.delete_volume(volume.name_id) if not self.tatlin_api.is_volume_exists(volume.name_id): if self.tatlin_api.is_volume_exists(volume.name_id): self.tatlin_api.extend_volume(volume.name_id, size) self.tatlin_api.update_qos( pool_stat = self.tatlin_api.get_pool_detail(self.pool_id) sys_stat = self.tatlin_api.get_sys_statistic() self.tatlin_api.get_resource_count(self.pool_id) result = self.tatlin_api.get_volume_info(source_name) result = self.tatlin_api.get_volume_info(source_name) self.tatlin_api.add_vol_to_host(volume.name_id, host_id) self.tatlin_api.remove_vol_from_host(volume.name_id, host_id) cur_ports = self.tatlin_api.get_resource_ports_array(volume_id) result = self.tatlin_api.get_resource_mapping() self.tatlin_api.export_volume(volume.name_id, ports) self._pool_id = self.tatlin_api.get_pool_id_by_name(",47,46
openstack%2Fbifrost~stable%2F2023.1~I208182e65884d63548d78c68f676b899c562a2dc,openstack/bifrost,stable/2023.1,I208182e65884d63548d78c68f676b899c562a2dc,CI: Update cached cirros image to 0.5.3,MERGED,2023-07-06 09:49:34.000000000,2023-07-06 12:52:17.000000000,2023-07-06 12:51:15.000000000,"[{'_account_id': 22348}, {'_account_id': 23851}]","[{'number': 1, 'created': '2023-07-06 09:49:34.000000000', 'files': ['playbooks/test-bifrost.yaml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/eef2efb704985c86d313591024890f205ebda2d6', 'message': ""CI: Update cached cirros image to 0.5.3\n\nBifrost CI is currently failing to fetch the cirros image from cache:\n\n    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'\n\nThis may be caused by the removal of cirros-0.5.1 images from cache in\nchange Ibada405e0c1183559f428c749d0e54d0a45a2223.\n\nSwitch to cirros version 0.5.3 image instead.\n\nChange-Id: I208182e65884d63548d78c68f676b899c562a2dc\n""}]",0,887783,eef2efb704985c86d313591024890f205ebda2d6,7,2,1,10239,,,0,"CI: Update cached cirros image to 0.5.3

Bifrost CI is currently failing to fetch the cirros image from cache:

    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'

This may be caused by the removal of cirros-0.5.1 images from cache in
change Ibada405e0c1183559f428c749d0e54d0a45a2223.

Switch to cirros version 0.5.3 image instead.

Change-Id: I208182e65884d63548d78c68f676b899c562a2dc
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/83/887783/1 && git format-patch -1 --stdout FETCH_HEAD,['playbooks/test-bifrost.yaml'],1,eef2efb704985c86d313591024890f205ebda2d6,, cirros_deploy_image_upstream_url: file:///opt/cache/files/cirros-0.5.3-x86_64-disk.img, cirros_deploy_image_upstream_url: file:///opt/cache/files/cirros-0.5.1-x86_64-disk.img,1,1
openstack%2Foctavia~stable%2F2023.1~I3706fd5e12e17be37edce974563c6806d4f09709,openstack/octavia,stable/2023.1,I3706fd5e12e17be37edce974563c6806d4f09709,allowed_cidr validation for additional_vips,NEW,2023-04-15 20:30:36.000000000,2023-07-06 12:50:42.000000000,,"[{'_account_id': 22348}, {'_account_id': 29244}]","[{'number': 1, 'created': '2023-04-15 20:30:36.000000000', 'files': ['octavia/tests/unit/network/drivers/neutron/test_allowed_address_pairs.py', 'octavia/network/drivers/neutron/allowed_address_pairs.py', 'releasenotes/notes/allowed_cidr-validation-for-additional_vips-175c32824cc7ee95.yaml', 'octavia/tests/functional/api/v2/test_listener.py', 'octavia/api/v2/controllers/listener.py'], 'web_link': 'https://opendev.org/openstack/octavia/commit/5783c6767f682af25b2e4e8da560af98a3109913', 'message': 'allowed_cidr validation for additional_vips\n\nThe validation for the allowed_cidr parameter did not take into account\nthe IP version of additional vIPs. The parameter was rejected if the IP\nversion did not match the IP version of the primary vIP. As additional\nvIPs can have a different IP version from the primary vIP all vIPs must\nbe checked during validation.\n\nChange-Id: I3706fd5e12e17be37edce974563c6806d4f09709\n(cherry picked from commit 60f579b64a8f603e5a67860a90cd413a0e9ca381)\n'}]",0,880503,5783c6767f682af25b2e4e8da560af98a3109913,3,2,1,11290,,,0,"allowed_cidr validation for additional_vips

The validation for the allowed_cidr parameter did not take into account
the IP version of additional vIPs. The parameter was rejected if the IP
version did not match the IP version of the primary vIP. As additional
vIPs can have a different IP version from the primary vIP all vIPs must
be checked during validation.

Change-Id: I3706fd5e12e17be37edce974563c6806d4f09709
(cherry picked from commit 60f579b64a8f603e5a67860a90cd413a0e9ca381)
",git fetch https://review.opendev.org/openstack/octavia refs/changes/03/880503/1 && git format-patch -1 --stdout FETCH_HEAD,"['octavia/tests/unit/network/drivers/neutron/test_allowed_address_pairs.py', 'octavia/network/drivers/neutron/allowed_address_pairs.py', 'releasenotes/notes/allowed_cidr-validation-for-additional_vips-175c32824cc7ee95.yaml', 'octavia/tests/functional/api/v2/test_listener.py', 'octavia/api/v2/controllers/listener.py']",5,5783c6767f682af25b2e4e8da560af98a3109913,multi-vip-allowed-cidr-stable/2023.1," def _validate_cidr_compatible_with_vip(self, vips, allowed_cidrs): for cidr in allowed_cidrs: for vip in vips: # Check if CIDR IP version matches VIP IP version if (common_utils.is_cidr_ipv6(cidr) == common_utils.is_ipv6(vip)): break else: msg = _(""CIDR %(cidr)s IP version incompatible with all VIPs "" ""%(vips)s IP version."") detail=msg % {'cidr': cidr, 'vips': vips}) lb_db = self.repositories.load_balancer.get( lock_session, id=lb_id) vip_addresses = [lb_db.vip.ip_address] vip_addresses.extend([vip.ip_address for vip in lb_db.additional_vips]) self._validate_cidr_compatible_with_vip(vip_addresses, allowed_cidrs) vip_addresses = [db_listener.load_balancer.vip.ip_address] vip_addresses.extend( [vip.ip_address for vip in db_listener.load_balancer.additional_vips] ) vip_addresses, listener.allowed_cidrs)"," def _validate_cidr_compatible_with_vip(self, vip, allowed_cidrs): for cidr in allowed_cidrs: # Check if CIDR IP version matches VIP IP version if common_utils.is_cidr_ipv6(cidr) != common_utils.is_ipv6(vip): msg = _(""CIDR %(cidr)s IP version incompatible with VIP "" ""%(vip)s IP version."") detail=msg % {'cidr': cidr, 'vip': vip}) vip_db = self.repositories.vip.get( lock_session, load_balancer_id=lb_id) vip_address = vip_db.ip_address self._validate_cidr_compatible_with_vip(vip_address, allowed_cidrs) vip_address = db_listener.load_balancer.vip.ip_address vip_address, listener.allowed_cidrs)",107,37
openstack%2Fnetworking-odl~stable%2Fussuri~I5a099d5934872b29274944f4941c8348f7d90160,openstack/networking-odl,stable/ussuri,I5a099d5934872b29274944f4941c8348f7d90160,[stable-only] Remove the periodic stable jobs,ABANDONED,2023-06-27 15:53:56.000000000,2023-07-06 12:45:26.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-06-27 15:53:56.000000000', 'files': ['.zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/networking-odl/commit/df20889a8126d50d17ae6fd41fcf02154550f4fb', 'message': '[stable-only] Remove the periodic stable jobs\n\nThis project has been deprecated and there is no need to consume\nCI resources periodically.\n\nChange-Id: I5a099d5934872b29274944f4941c8348f7d90160\n'}]",2,887076,df20889a8126d50d17ae6fd41fcf02154550f4fb,5,1,1,16688,,,0,"[stable-only] Remove the periodic stable jobs

This project has been deprecated and there is no need to consume
CI resources periodically.

Change-Id: I5a099d5934872b29274944f4941c8348f7d90160
",git fetch https://review.opendev.org/openstack/networking-odl refs/changes/76/887076/1 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.d/project.yaml'],1,df20889a8126d50d17ae6fd41fcf02154550f4fb,remove_periodic_jobs,, - periodic-stable-jobs-neutron,0,1
openstack%2Fnova-specs~master~Ie796abd247a9231d1e126a589f03a81696960b26,openstack/nova-specs,master,Ie796abd247a9231d1e126a589f03a81696960b26,Re-propose spec for ephemeral storage encryption,MERGED,2023-06-26 23:00:10.000000000,2023-07-06 12:41:09.000000000,2023-07-06 12:39:56.000000000,"[{'_account_id': 7166}, {'_account_id': 11604}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-26 23:00:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/b2a3c3495a3a188b79f17c8173ea1b24d5ef7bdd', 'message': 'Re-propose spec for ephemeral storage encryption\n\nPreviously-approved: 2023.1, Zed, Yoga, Xena, Wallaby\n\nThe spec has been updated to reflect a change needed in the\nimplementation to support snapshot and shelve for ephemeral encrypted\ninstances. The encryption secret is needed in order to boot a new\ninstance from a snapshot of an ephemeral encrypted instance and to\nunshelve an ephemeral encrypted instance. So, the spec is updated to\npropose an additional flavor extra spec or image property to keep the\nencryption secret UUID from the key manager.\n\nRelated to blueprint ephemeral-storage-encryption\n\nChange-Id: Ie796abd247a9231d1e126a589f03a81696960b26\n'}, {'number': 2, 'created': '2023-06-27 09:15:29.000000000', 'files': ['specs/2023.2/approved/ephemeral-storage-encryption.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/ea0cf9579f16c93d23c4e7210a6bc3d8ec87792d', 'message': 'Re-propose spec for ephemeral storage encryption\n\nPreviously-approved: 2023.1, Zed, Yoga, Xena, Wallaby\n\nThe spec has been updated to reflect a change needed in the\nimplementation to support snapshot and shelve for ephemeral encrypted\ninstances. The encryption secret is needed in order to boot a new\ninstance from a snapshot of an ephemeral encrypted instance and to\nunshelve an ephemeral encrypted instance. So, the spec is updated to\npropose an additional flavor extra spec or image property to keep the\nencryption secret UUID from the key manager.\n\nRelated to blueprint ephemeral-storage-encryption\n\n\nChange-Id: Ie796abd247a9231d1e126a589f03a81696960b26\n'}]",10,887011,ea0cf9579f16c93d23c4e7210a6bc3d8ec87792d,16,3,2,4690,,,0,"Re-propose spec for ephemeral storage encryption

Previously-approved: 2023.1, Zed, Yoga, Xena, Wallaby

The spec has been updated to reflect a change needed in the
implementation to support snapshot and shelve for ephemeral encrypted
instances. The encryption secret is needed in order to boot a new
instance from a snapshot of an ephemeral encrypted instance and to
unshelve an ephemeral encrypted instance. So, the spec is updated to
propose an additional flavor extra spec or image property to keep the
encryption secret UUID from the key manager.

Related to blueprint ephemeral-storage-encryption


Change-Id: Ie796abd247a9231d1e126a589f03a81696960b26
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/11/887011/2 && git format-patch -1 --stdout FETCH_HEAD,['specs/2023.2/approved/ephemeral-storage-encryption.rst'],1,b2a3c3495a3a188b79f17c8173ea1b24d5ef7bdd,bp/ephemeral-storage-encryption,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ====================================================== Flavour and Image defined ephemeral storage encryption ====================================================== https://blueprints.launchpad.net/nova/+spec/ephemeral-storage-encryption This spec outlines a new approach to ephemeral storage encryption in Nova allowing users to select how their ephemeral storage is encrypted at rest through the use of flavors with specific extra specs or images with specific properties. The aim being to bring the ephemeral storage encryption experience within Nova in line with the block storage encryption implementation provided by Cinder where user selectable `encrypted volume types`_ are available. .. note:: This spec will only cover the high level changes to the API and compute layers, implementation within specific virt drivers is left for separate specs. Problem description =================== At present the only in-tree ephemeral storage encryption support is provided by the libvirt virt driver when using the lvm imagebackend. The current implementation provides basic operator controlled and configured host specific support for ephemeral disk encryption at rest where all instances on a given compute are forced to use encrypted ephemeral storage using the dm-crypt ``PLAIN`` encryption format. This is not ideal and makes ephemeral storage encryption completely opaque to the end user as opposed to the block storage encryption support provided by Cinder where users are able to opt-in to using admin defined encrypted volume types to ensure their storage is encrypted at rest. Additionally the current implementation uses a single symmetric key to encrypt all ephemeral storage associated with the instance. As the ``PLAIN`` encryption format is used there is no way to rotate this key in-place. Use Cases --------- * As a user I want to request that all of my ephemeral storage is encrypted at rest through the selection of a specific flavor or image. * As a user I want to be able to pick how my ephemeral storage is encrypted at rest through the selection of a specific flavor or image. * As an admin/operator I want to either enforce ephemeral encryption per flavor or per image. * As an admin/operator I want to provide sane choices to my end users regarding how their ephemeral storage is encrypted at rest. * As a virt driver maintainer/developer I want to indicate that my driver supports ephemeral storage encryption using a specific encryption format. * As a virt driver maintainer/developer I want to provide sane default encryption format and options for users looking to encrypt their ephemeral storage at rest. I want these associated with the encrypted storage until it is deleted. Proposed change =============== To enable this new flavor extra specs, image properties and host configurables will be introduced. These will control when and how ephemeral storage encryption at rest is enabled for an instance. .. note:: The following ``hw_ephemeral_encryption`` image properties do not relate to if an image is encrypted at rest within the Glance service. They only relate to how ephemeral storage will be encrypted at rest when used by a provisioned instance within Nova. Separate image properties have been documented in the `Glance image encryption`_ and `Cinder image encryption`_ specs to cover how images can be encrypted at rest within Glance. Allow ephemeral encryption to be configured by flavor, image or config ---------------------------------------------------------------------- To enable ephemeral encryption per instance the following boolean based flavor extra spec and image property will be introduced: * ``hw:ephemeral_encryption`` * ``hw_ephemeral_encryption`` The above will enable ephemeral storage encryption for an instance but does not control the encryption format used or the associated options. For this the following flavor extra specs, image properties and configurables will be introduced. The encryption format used will be controlled by the following flavor extra specs and image properties: * ``hw:ephemeral_encryption_format`` * ``hw_ephemeral_encryption_format`` When neither of the above are provided but ephemeral encryption is still requested an additional host configurable will be used to provide a default format per compute, this will initially default to ``luks``: * ``[ephemeral_storage_encryption]/default_format`` This could lead to requests against different clouds resulting in a different ephemeral encryption format being used but as this is transparent to the end user from within the instance it shouldn't have any real impact. The format will be provided as a string that maps to a ``BlockDeviceEncryptionFormatTypeField`` oslo.versionedobjects field value: * ``plain`` for the plain dm-crypt format * ``luks`` for the LUKSv1 format To enable snapshot and shelve of instances using ephemeral encryption, the UUID of the encryption security stored in the key manager for the resultant image will be kept with the image as a flavor extra spec or image property: * ``hw:ephemeral_encryption_secret_uuid`` * ``hw_ephemeral_encryption_secret_uuid`` The secret UUID is needed when creating an instance from an ephemeral encrypted snapshot or when unshelving an ephemeral encrypted instance. BlockDeviceMapping changes -------------------------- The ``BlockDeviceMapping`` object will be extended to include the following fields encapsulating some of the above information per ephemeral disk within the instance: ``encrypted`` A simple boolean to indicate if the block device is encrypted. This will initially only be populated when ephemeral encryption is used but could easily be used for encrypted volumes as well in the future. ``encryption_secret_uuid`` As the name suggests this will contain the UUID of the associated encryption secret for the disk. The type of secret used here will be specific to the encryption format and virt driver used, it should not be assumed that this will always been an symmetric key as is currently the case with all encrypted volumes provided by Cinder. For example, for ``luks`` based ephemeral storage this secret will be a ``passphrase``. ``encryption_format`` A new ``BlockDeviceEncryptionFormatType`` enum and associated ``BlockDeviceEncryptionFormatTypeField`` field listing the encryption format. The available options being kept in line with the constants currently provided by os-brick and potentially merged in the future if both can share these types and fields somehow. ``encryption_options`` A simple unversioned dict of strings containing encryption options specific to the virt driver implementation, underlying hypervisor and format being used. .. note:: The ``encryption_options`` field will be unused and not exposed to end users initially because of the security and upgrade implications around it. For the first pass, sensible defaults for the cipher algorithm, cipher mode, and initialization vector generator algorithm will be hard-coded instead. Encryption options could be exposed to end users in the future when a proper design which addresses security and handles all upgrade scenarios is developed. Populate ephemeral encryption BlockDeviceMapping attributes during build ------------------------------------------------------------------------ When launching an instance with ephemeral encryption requested via either the image or flavor the ``BlockDeviceMapping.encrypted`` attribute will be set to ``True`` for each ``BlockDeviceMapping`` record with a ``destination_type`` value of ``local``. This will happen after the original API BDM dicts have been transformed into objects within the Compute API but before scheduling the instance(s). The ``encryption_format`` attribute will also take its' value from the image or flavor if provided. Any differences or conflicts between the image and flavor for this will raise a ``409 Conflict`` error being raised by the API. Use ``COMPUTE_EPHEMERAL_ENCRYPTION`` compatibility traits --------------------------------------------------------- A ``COMPUTE_EPHEMERAL_ENCRYPTION`` compute compatibility trait was introduced during `Wallaby`__ and will be reported by virt drivers to indicate overall support for ephemeral storage encryption using this new approach. This trait will always be used by pre-filter outlined in the following section when ephemeral encryption has been requested, regardless of any format being specified in the request, allowing the compute that eventually handles the request to select a format it supports using the ``[ephemeral_storage_encryption]/default_format`` configurable. .. __: https://review.opendev.org/c/openstack/os-traits/+/759878 ``COMPUTE_EPHEMERAL_ENCRYPTION_$FORMAT`` compute compatibility traits were also added to os-traits during Wallaby and will be reported by virt drivers to indicate support for specific ephemeral storage encryption formats. For example: * ``COMPUTE_EPHEMERAL_ENCRYPTION_LUKS`` * ``COMPUTE_EPHEMERAL_ENCRYPTION_LUKSV2`` * ``COMPUTE_EPHEMERAL_ENCRYPTION_PLAIN`` These traits will only be used alongside the ``COMPUTE_EPHEMERAL_ENCRYPTION`` trait when the ``hw_ephemeral_encryption_format`` image property or ``hw:ephemeral_encryption_format`` extra spec have been provided in the initial request. Introduce an ephemeral encryption request pre-filter ---------------------------------------------------- A new pre-filter will be introduced that adds the above traits as required to the request spec when the aforementioned image properties or flavor extra specs are provided. As outlined above this will always include the ``COMPUTE_EPHEMERAL_ENCRYPTION`` trait when ephemeral encryption has been requested and may optionally include one of the format specific traits if a format is included in the request. Expose ephemeral encryption attributes via block_device_info ------------------------------------------------------------ Once the ``BlockDeviceMapping`` objects have been updated and the instance scheduled to a compute the objects are transformed once again into a ``block_device_info`` dict understood by the virt layer that at present contains the following: ``root_device_name`` The root device path used by the instance. ``ephemerals`` A list of ``DriverEphemeralBlockDevice`` dict objects detailing the ephemeral disks attached to the instance. Note this does not include the initial image based disk used by the instance that is classified as an ephemeral disk in terms of the ephemeral encryption feature. ``block_device_mapping`` A list of ``DriverVol*BlockDevice`` dict objects detailing the volume based disks attached to the instance. ``swap`` An optional ``DriverSwapBlockDevice`` dict object detailing the swap device. For example: .. code-block:: json { ""root_device_name"": ""/dev/vda"", ""ephemerals"": [ { ""guest_format"": null, ""device_name"": ""/dev/vdb"", ""device_type"": ""disk"", ""size"": 1, ""disk_bus"": ""virtio"" } ], ""block_device_mapping"": [], ""swap"": { ""swap_size"": 1, ""device_name"": ""/dev/vdc"", ""disk_bus"": ""virtio"" } } As noted above ``block_device_info`` does not provide a complete overview of the storage associated with an instance. In order for it to be useful in the context of ephemeral storage encryption we would need to extend the dict to always include information relating to local image based disks. As such a new ``DriverImageBlockDevice`` dict class will be introduced covering image based block devices and provided to the virt layer via an additional ``image`` key within the ``block_device_info`` dict when the instance uses such a disk. As with the other ``Driver*BlockDevice`` dict classes this will proxy access to the underlying ``BlockDeviceMapping`` object allowing the virt layer to lookup the previously listed ``encrypted`` and ``encryption_*`` attributes. While outside the scope of this spec the above highlights a huge amount of complexity and technical debt still residing in the codebase around how storage configurations are handled between the different layers. In the long term we should plan to remove ``block_device_info`` and replace it with direct access to ``BlockDeviceMapping`` based objects ensuring the entire configuration is always exposed to the virt layer. Report that a disk is encrypted at rest through the metadata API ---------------------------------------------------------------- Extend the metadata API so that users can confirm that their ephemeral storage is encrypted at rest through the metadata API, accessible from within their instance. .. code-block:: json { ""devices"": [ { ""type"": ""nic"", ""bus"": ""pci"", ""address"": ""0000:00:02.0"", ""mac"": ""00:11:22:33:44:55"", ""tags"": [""trusted""] }, { ""type"": ""disk"", ""bus"": ""virtio"", ""address"": ""0:0"", ""serial"": ""12352423"", ""path"": ""/dev/vda"", ""encrypted"": ""True"" }, { ""type"": ""disk"", ""bus"": ""ide"", ""address"": ""0:0"", ""serial"": ""disk-vol-2352423"", ""path"": ""/dev/sda"", ""tags"": [""baz""] } ] } This should also be extended to cover disks provided by encrypted volumes but this is obviously out of scope for this implementation. Block resize between flavors with different hw:ephemeral_encryption settings ---------------------------------------------------------------------------- Ephemeral data is expected to persist through a resize and as such any resize between flavors that differed in their configuration of ephemeral encryption (one enabled, another disabled or formats etc) would cause us to convert this data in place. This isn't trivial and so for this initial implementation resizing between flavors that differ will be blocked. Provide a migration path from the legacy implementation ------------------------------------------------------- New ``nova-manage`` and ``nova-status`` commands will be introduced to migrate any instances using the legacy libvirt virt driver implementation ahead of the removal of this in a future release. The ``nova-manage`` command will ensure that any existing instances with ``ephemeral_key_uuid`` set will have their associated ``BlockDeviceMapping`` records updated to reference said secret key, the ``plain`` encryption format and configured options on the host before clearing ``ephemeral_key_uuid``. Additionally the libvirt virt driver will also attempt to migrate instances with ``ephemeral_key_uuid`` set during spawn. This should allow at least some of the instances to be moved during the W release ahead of X. The ``nova-status`` command will simply report on the existence of any instances with ``ephemeral_key_uuid`` set that do not have the corresponding ``BlockDeviceMapping`` attributes enabled etc. Deprecate the now legacy implementation --------------------------------------- The legacy implementation within the libvirt virt driver will be deprecated for removal in a future release once the ability to migrate is in place. Alternatives ------------ Continue to use the transparent host configurables and expand support to other encryption formats such as ``LUKS``. Data model impact ----------------- See above for the various flavor extra spec, image property, ``BlockDeviceMapping`` and ``DriverBlockDevice`` object changes. REST API impact --------------- * Flavor extra specs and image property validation will be introduced for the any ephemeral encryption provided options. * Attempts to resize between flavors that differ in their ephemeral encryption options will be rejected. * Attempts to rebuild between images that differ in their ephemeral encryption options will be allowed. * The metadata API will be changed to allow users to determine if their ephemeral storage is encrypted as discussed above. Security impact --------------- This should hopefully be positive given the unique secret per disk and user visible choice regarding how their ephemeral storage is encrypted at rest. Additionally this should allow additional virt drivers to support ephemeral storage encryption while also allowing the libvirt virt driver to increase coverage of the feature across more imagebackends such as qcow2 and rbd. .. note:: Internal base images stored locally in Nova will not be encrypted at rest. Notifications impact -------------------- N/A Other end user impact --------------------- Users will now need to opt-in to ephemeral storage encryption being used by their instances through their choice of image or flavors. Performance Impact ------------------ The additional pre-filter will add a small amount of overhead when scheduling instances but this should fail fast if ephemeral encryption is not requested through the image or flavor. The performance impact of increased use of ephemeral storage encryption by instances is left to be discussed in the virt driver specific specs as this will vary between hypervisors. Other deployer impact --------------------- N/A Developer impact ---------------- Virt driver developers will be able to indicate support for specific ephemeral storage encryption formats using the newly introduced compute compatibility traits. Upgrade impact -------------- The compute traits should ensure that requests to schedule instances using ephemeral storage encryption with mixed computes (N-1 and N) will work during a rolling upgrade. As discussed earlier in the spec future upgrades will need to provide a path for existing ephemeral storage encryption users to migrate from the legacy implementation. This should be trivial but may require an additional grenade based job in CI during the W cycle to prove out the migration path. Implementation ============== Assignee(s) ----------- Primary assignee: melwitt Other contributors: lyarwood Feature Liaison --------------- Feature liaison: melwitt Work Items ---------- * Introduce ``hw_ephemeral_encryption*`` image properties and ``hw:ephemeral_encryption`` flavor extra specs. * Introduce a new ``encrypted``. ``encryption_secret_uuid``, ``encryption_format`` and ``encryption_options`` attributes to the BlockDeviceMapping Object. * Wire up the new ``BlockDeviceMapping`` object attributes through the ``Driver*BlockDevice`` layer and ``block_device_info`` dict. * Report ephemeral storage encryption through the metadata API. * Introduce new ``nova-manage`` and ``nova-status`` commands to allow existing users to migrate to this new implementation. This should however be blocked outside of testing until a virt driver implementation is landed. * Validate all of the above in functional tests ahead of any virt driver implementation landing. Dependencies ============ None Testing ======= At present without a virt driver implementation this will be tested entirely within our unit and functional test suites. Once a virt driver implementation is available additional integration tests in Tempest and whitebox tests can be written. Testing of the migration path from the legacy implementation will require an additional grenade job but this will require the libvirt virt driver implementation to be completed first. Documentation Impact ==================== * The new host configurables, flavor extra specs and image properties should be documented. * New user documentation should be written covering the overall use of the feature from a Nova point of view. * Reference documentation around `BlockDeviceMapping` objects etc should be updated to make note of the new encryption attributes. References ========== .. _`Glance image encryption`: https://specs.openstack.org/openstack/glance-specs/specs/victoria/approved/glance/image-encryption.html .. _`Cinder image encryption`: https://specs.openstack.org/openstack/cinder-specs/specs/wallaby/image-encryption.html .. _`encrypted volume types`: https://docs.openstack.org/cinder/latest/configuration/block-storage/volume-encryption.html#create-an-encrypted-volume-type .. _`libvirt virt driver`: https://libvirt.org/formatstorageencryption.html#StorageEncryptionLuks History ======= Optional section intended to be used each time the spec is updated to describe new design, API or any database schema updated. Useful to let reader understand what's happened along the time. .. list-table:: Revisions :header-rows: 1 * - Release Name - Description * - Wallaby - Introduced * - Xena - Reproposed * - Yoga - Reproposed * - Zed - Reproposed * - 2023.1 Antelope - Reproposed * - 2023.2 Bobcat - Reproposed ",,560,0
openstack%2Fneutron~stable%2Fwallaby~Iab77c64c03fd0d44ff7a3fc1c556d85a8c480bb9,openstack/neutron,stable/wallaby,Iab77c64c03fd0d44ff7a3fc1c556d85a8c480bb9,Return 409 Conflict to tenant user deleting port attached to FIP,MERGED,2023-06-16 15:07:58.000000000,2023-07-06 12:38:38.000000000,2023-07-06 12:37:18.000000000,"[{'_account_id': 8313}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-16 15:07:58.000000000', 'files': ['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/0180d797e26415ac112610692f42b2dc9dc6dcb6', 'message': 'Return 409 Conflict to tenant user deleting port attached to FIP\n\nWhen a tenant user try to delete a port that has attached a FIP by\nan admin user is getting a 500 ServerError.\n\nThis patch improves the error to 409 Conflict doing some additionals\nchecks on the delete_port method.\n\nNew exception has been included locally, but will be removed as soon\nneutron-lib bumps to a newer release.\n\nConflicts:\n      neutron/db/l3_db.py\n\nCloses-Bug: 2017680\nChange-Id: Iab77c64c03fd0d44ff7a3fc1c556d85a8c480bb9\n(cherry picked from commit 9f6f6d5082b4341529144e992d5293675146ae88)\n'}]",1,886277,0180d797e26415ac112610692f42b2dc9dc6dcb6,9,3,1,34451,,,0,"Return 409 Conflict to tenant user deleting port attached to FIP

When a tenant user try to delete a port that has attached a FIP by
an admin user is getting a 500 ServerError.

This patch improves the error to 409 Conflict doing some additionals
checks on the delete_port method.

New exception has been included locally, but will be removed as soon
neutron-lib bumps to a newer release.

Conflicts:
      neutron/db/l3_db.py

Closes-Bug: 2017680
Change-Id: Iab77c64c03fd0d44ff7a3fc1c556d85a8c480bb9
(cherry picked from commit 9f6f6d5082b4341529144e992d5293675146ae88)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/77/886277/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py']",2,0180d797e26415ac112610692f42b2dc9dc6dcb6,wallaby," @mock.patch.object(l3_obj.FloatingIP, 'objects_exist') @mock.patch.object(l3_obj.FloatingIP, 'get_objects') def test_disassociate_floatingips_conflict_by_fip_attached(self, get_objects, objects_exist): context_tenant = context.Context('tenant', 'tenant', is_admin=False) objects_exist.return_value = True get_objects.side_effect = [ [], [{'id': 'floating_ip1', 'port_id': 'port_id'}]] self.assertRaises(l3_db.FipAssociated, self.db.disassociate_floatingips, context_tenant, 'port_id') objects_exist.assert_called_once_with( mock.ANY, fixed_port_id='port_id') expected_calls = [ mock.call(context_tenant, fixed_port_id='port_id'), mock.call(mock.ANY, fixed_port_id='port_id')] get_objects.assert_has_calls(expected_calls) ",,47,0
openstack%2Fneutron~stable%2Fxena~Iab77c64c03fd0d44ff7a3fc1c556d85a8c480bb9,openstack/neutron,stable/xena,Iab77c64c03fd0d44ff7a3fc1c556d85a8c480bb9,Return 409 Conflict to tenant user deleting port attached to FIP,MERGED,2023-06-16 15:05:54.000000000,2023-07-06 12:38:33.000000000,2023-07-06 12:37:14.000000000,"[{'_account_id': 8313}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-16 15:05:54.000000000', 'files': ['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/3e8716e7f5db0955807daadc1c31f6957ae429a6', 'message': 'Return 409 Conflict to tenant user deleting port attached to FIP\n\nWhen a tenant user try to delete a port that has attached a FIP by\nan admin user is getting a 500 ServerError.\n\nThis patch improves the error to 409 Conflict doing some additionals\nchecks on the delete_port method.\n\nNew exception has been included locally, but will be removed as soon\nneutron-lib bumps to a newer release.\n\nCloses-Bug: 2017680\nChange-Id: Iab77c64c03fd0d44ff7a3fc1c556d85a8c480bb9\n(cherry picked from commit 9f6f6d5082b4341529144e992d5293675146ae88)\n'}]",2,886273,3e8716e7f5db0955807daadc1c31f6957ae429a6,12,3,1,34451,,,0,"Return 409 Conflict to tenant user deleting port attached to FIP

When a tenant user try to delete a port that has attached a FIP by
an admin user is getting a 500 ServerError.

This patch improves the error to 409 Conflict doing some additionals
checks on the delete_port method.

New exception has been included locally, but will be removed as soon
neutron-lib bumps to a newer release.

Closes-Bug: 2017680
Change-Id: Iab77c64c03fd0d44ff7a3fc1c556d85a8c480bb9
(cherry picked from commit 9f6f6d5082b4341529144e992d5293675146ae88)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/73/886273/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py']",2,3e8716e7f5db0955807daadc1c31f6957ae429a6,xena," @mock.patch.object(l3_obj.FloatingIP, 'objects_exist') @mock.patch.object(l3_obj.FloatingIP, 'get_objects') def test_disassociate_floatingips_conflict_by_fip_attached(self, get_objects, objects_exist): context_tenant = context.Context('tenant', 'tenant', is_admin=False) objects_exist.return_value = True get_objects.side_effect = [ [], [{'id': 'floating_ip1', 'port_id': 'port_id'}]] self.assertRaises(l3_db.FipAssociated, self.db.disassociate_floatingips, context_tenant, 'port_id') objects_exist.assert_called_once_with( mock.ANY, fixed_port_id='port_id') expected_calls = [ mock.call(context_tenant, fixed_port_id='port_id'), mock.call(mock.ANY, fixed_port_id='port_id')] get_objects.assert_has_calls(expected_calls) ",,44,1
openstack%2Fproject-config~master~Ia2c1d54dc9eec429a6b20b5f6cbdcadec46435c2,openstack/project-config,master,Ia2c1d54dc9eec429a6b20b5f6cbdcadec46435c2,Add OpenStack K8S charms,MERGED,2023-07-04 13:31:42.000000000,2023-07-06 12:30:54.000000000,2023-07-06 12:19:08.000000000,"[{'_account_id': 935}, {'_account_id': 5263}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 13:31:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/project-config/commit/329278e98498fbf4d582650581d782e201437665', 'message': 'Add OpenStack K8S charms\n\nSetup and import OpenStack K8S Charms for deployment of\nheat and barbican hosted in Kubernetes.\n\nRequired-By:\nChange-Id: Ia2c1d54dc9eec429a6b20b5f6cbdcadec46435c2\n'}, {'number': 2, 'created': '2023-07-04 13:32:33.000000000', 'files': ['zuul/main.yaml', 'gerrit/projects.yaml'], 'web_link': 'https://opendev.org/openstack/project-config/commit/766bda25d0e56613d8950ee59c7e635ead8d1a5f', 'message': 'Add OpenStack K8S charms\n\nSetup and import OpenStack K8S Charms for deployment of\nheat and barbican hosted in Kubernetes.\n\nRequired-By: I218144cbe0fef51f3e28cb902c81783138f60905\nChange-Id: Ia2c1d54dc9eec429a6b20b5f6cbdcadec46435c2\n'}]",1,887602,766bda25d0e56613d8950ee59c7e635ead8d1a5f,10,3,2,12549,,,0,"Add OpenStack K8S charms

Setup and import OpenStack K8S Charms for deployment of
heat and barbican hosted in Kubernetes.

Required-By: I218144cbe0fef51f3e28cb902c81783138f60905
Change-Id: Ia2c1d54dc9eec429a6b20b5f6cbdcadec46435c2
",git fetch https://review.opendev.org/openstack/project-config refs/changes/02/887602/2 && git format-patch -1 --stdout FETCH_HEAD,"['gerrit/projects.yaml', 'zuul/main.yaml']",2,329278e98498fbf4d582650581d782e201437665,sunbeam-heat-barbican, - openstack/charm-barbican-k8s - openstack/charm-heat-k8s,,16,0
openstack%2Fneutron~master~Ib9ec45d643c6162c526cd5a02db270094b575e34,openstack/neutron,master,Ib9ec45d643c6162c526cd5a02db270094b575e34,[OVN][L3] Optimize FIP update operation,MERGED,2023-06-26 10:05:40.000000000,2023-07-06 12:29:31.000000000,2023-07-06 12:28:05.000000000,"[{'_account_id': 1131}, {'_account_id': 5948}, {'_account_id': 6773}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 15554}, {'_account_id': 22348}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-06-26 10:05:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/897b09d64113e21862593c524b7205efdf749912', 'message': '[DNM][WIP] == [OVN][FIP] Optimize FIP update\n\nChange-Id: Ib9ec45d643c6162c526cd5a02db270094b575e34\n'}, {'number': 2, 'created': '2023-06-26 10:06:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/01936332479e9dc0b796610649dd4b518cab9f54', 'message': '[DNM][WIP] == [OVN][FIP] Optimize FIP update\n\nChange-Id: Ib9ec45d643c6162c526cd5a02db270094b575e34\n'}, {'number': 3, 'created': '2023-06-27 13:30:31.000000000', 'files': ['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py', 'neutron/services/ovn_l3/plugin.py', 'neutron/tests/unit/services/ovn_l3/test_plugin.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/7b85f9c244cde25d2dad81f51a0c96a429ddc488', 'message': '[OVN][L3] Optimize FIP update operation\n\nIf the floating IP updates only the QoS policy, the method now\nskips the OVN NAT rules update and updates only the QoS policy.\nThat avoids the OVN NAT rules deletion and creation and the\n``FIPAddDeleteEvent`` event that deletes the MAC binding entries\nfor an active floating IP, causing a disruption.\n\nCloses-Bug: #2025144\n\nChange-Id: Ib9ec45d643c6162c526cd5a02db270094b575e34\n'}]",1,886974,7b85f9c244cde25d2dad81f51a0c96a429ddc488,21,8,3,16688,,,0,"[OVN][L3] Optimize FIP update operation

If the floating IP updates only the QoS policy, the method now
skips the OVN NAT rules update and updates only the QoS policy.
That avoids the OVN NAT rules deletion and creation and the
``FIPAddDeleteEvent`` event that deletes the MAC binding entries
for an active floating IP, causing a disruption.

Closes-Bug: #2025144

Change-Id: Ib9ec45d643c6162c526cd5a02db270094b575e34
",git fetch https://review.opendev.org/openstack/neutron refs/changes/74/886974/3 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py', 'neutron/services/ovn_l3/plugin.py']",2,897b09d64113e21862593c524b7205efdf749912,bug/2025144," self._ovn_client.update_floatingip(context, fip, floatingip)"," self._ovn_client.update_floatingip(context, fip)",16,11
openstack%2Fnova-specs~master~Iea9c017a4f104370e75a3f1e5070c8efa01e30b6,openstack/nova-specs,master,Iea9c017a4f104370e75a3f1e5070c8efa01e30b6,"Re-propose ""Policy service role spec""",MERGED,2023-04-28 18:26:47.000000000,2023-07-06 12:13:54.000000000,2023-07-06 12:12:16.000000000,"[{'_account_id': 7166}, {'_account_id': 11604}, {'_account_id': 22348}, {'_account_id': 32761}]","[{'number': 1, 'created': '2023-04-28 18:26:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/278b0eff1e3cd3052be57729a3c2523bfa133163', 'message': 'Re-propose ""Policy service role spec""\n\nThis spec is to add service role to nova service-to-service\nAPIs policies.\n\nPartial implement blueprint policy-service-role-default\n\nAPIImpact\n\nChange-Id: Iea9c017a4f104370e75a3f1e5070c8efa01e30b6\n'}, {'number': 2, 'created': '2023-06-27 04:33:23.000000000', 'files': ['specs/2023.2/approved/policy-service-role-default.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/9036a12bc64ac9a369906e9cb461d0267411aee3', 'message': 'Re-propose ""Policy service role spec""\n\nThis spec is to add service role to nova service-to-service\nAPIs policies.\n\nPartial implement blueprint policy-service-role-default\n\nAPIImpact\n\nChange-Id: Iea9c017a4f104370e75a3f1e5070c8efa01e30b6\n'}]",2,881880,9036a12bc64ac9a369906e9cb461d0267411aee3,12,4,2,8556,,,0,"Re-propose ""Policy service role spec""

This spec is to add service role to nova service-to-service
APIs policies.

Partial implement blueprint policy-service-role-default

APIImpact

Change-Id: Iea9c017a4f104370e75a3f1e5070c8efa01e30b6
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/80/881880/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/2023.2/approved/policy-service-role-default.rst'],1,278b0eff1e3cd3052be57729a3c2523bfa133163,bp/policy-service-role-default,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode =========================== Policy Service Role Default =========================== https://blueprints.launchpad.net/nova/+spec/policy-service-role-default Ideally all internal service-to-service APIs should not be accessible by admin or end user by default. From policy defaults it should be clear which APIs are supposed to be used by admin or end user and which is for internal service-to-service APIs communication. Problem description =================== Currently, internal service-to-service communication APIs have their default policy as either admin or project roles which means operators need to assign the admin or project roles to their service users. That service user having admin or project role access is poor security practice as they can perform admin or project level operations. Another problem is that APIs which are meant to only be used by internal services are able to be called by regular users and human admins. Requiring (and allowing only) a service role for these APIs help avoid intentional and accidental abuse. Use Cases --------- As an operator I want to keep ``service`` role user to access service-to-service APIs with least privilege. Proposed change =============== We need to make sure all the policy rules for internal service-to-service APIs are default to ``service`` role only. Example: .. code-block:: python policy.DocumentedRuleDefault( name='os_compute_api:os-server-external-events:create', check_str='role:service', scope_types=['project'] ) Keystone's ``service`` role is kept outside of the existing role hierarchy that includes ``admin``, ``member``, and ``reader``. Keeping the ``service`` role outside the current hierarchy ensures we're following the principle of least privilege for service accounts. We need to make all the service-to-service APIs which are *only* suitable for services default to ``service`` role only. But we might have some cases where APIs are both intended for service usage, as well as admin (any other user role) usage. For such policy rules we need to default them to ``service`` as well as ``admin`` (or any other user role) role. For example, 'role:admin or role:service' As Nova have dropped the system scope implementation, service-to-service communication with ``service`` role will be done with project scope token (which is currently done in devstack setup). Below APIs policy will be default to ``service`` role: * os_compute_api:os-assisted-volume-snapshots:create * os_compute_api:os-assisted-volume-snapshots:delete * os_compute_api:os-volumes-attachments:swap * os_compute_api:os-server-external-events:create Alternatives ------------ Keep the service-to-service APIs default same as it is and expect operators to take care of the ``service`` role users access permissions by overriding it in the policy.yaml. Data model impact ----------------- None REST API impact --------------- Below APIs policy will be default to ``service`` role: * os_compute_api:os-assisted-volume-snapshots:create * os_compute_api:os-assisted-volume-snapshots:delete * os_compute_api:os-volumes-attachments:swap * os_compute_api:os-server-external-events:create Security impact --------------- Easier to understand service-to-service APIs policy and restricting them to least privilege. Notifications impact -------------------- None Other end user impact --------------------- None Performance Impact ------------------ None Other deployer impact --------------------- If service-to-service APIs are used by the admin or end user then make sure to override the required permission in policy.yaml because by default they will be accessed by the ``service`` role user only. Developer impact ---------------- New APIs must add policies that follow the new pattern. Upgrade impact -------------- If service-to-service APIs are used by the admin or end user then make sure to override the required permission in policy.yaml because by default they will be accessed by the ``service`` role user only. If deployment overrides these policies then, they need to start considering the new default policy rules. Implementation ============== Assignee(s) ----------- Primary assignee: gmann Feature Liaison --------------- Feature liaison: dansmith Work Items ---------- * Modify the service-to-service APIs defaults * Modify policy rule unit tests Dependencies ============ None Testing ======= Modify or add the policy unit tests. Add a job enabling the new defaults and run the tempest tests to make sure existing service-service APIs communication work fine. If needed modify the token used by services as per the new defaults. Documentation Impact ==================== API Reference should be updated to add all the service-service APIs under separate section and mention about ``service`` role as their default. References ========== History ======= .. list-table:: Revisions :header-rows: 1 * - Release Name - Description * - 2023.1 - Introduced * - 2023.2 - Re-proposed ",,195,0
openstack%2Freleases~master~Id489e2567b8c38d1defc9229087e1d25ccb59a59,openstack/releases,master,Id489e2567b8c38d1defc9229087e1d25ccb59a59,[neutron] Release the 1.0.0.0b1 version of ovn-bgp-agent,MERGED,2023-06-30 14:58:16.000000000,2023-07-06 11:16:30.000000000,2023-07-06 08:50:48.000000000,"[{'_account_id': 308}, {'_account_id': 6773}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 23567}]","[{'number': 1, 'created': '2023-06-30 14:58:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/0f3550cdb2b63c7dab6ac25f982540b2730c9fb1', 'message': '[neutron] Release the first version of ovn-bgp-agent\n\nThis is the first release of the project ""ovn-bgp-agent"".\n\nChange-Id: Id489e2567b8c38d1defc9229087e1d25ccb59a59\n'}, {'number': 2, 'created': '2023-06-30 15:03:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/3c47fe29eaef8b0c108f0f431586ed99c74b8305', 'message': '[neutron] Release the 0.5.0 version of ovn-bgp-agent\n\nThis is the first release of the project ""ovn-bgp-agent"" in the\nOpenStack community. However the project had four previous releases\nthat are documented in [1].\n\n[1]https://pypi.org/project/ovn-bgp-agent/\n\nChange-Id: Id489e2567b8c38d1defc9229087e1d25ccb59a59\n'}, {'number': 3, 'created': '2023-06-30 15:41:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/1c3f4961004b3d3cdc257d1e047ae39b55aaec73', 'message': '[neutron] Release the 1.0.0.0rc1 version of ovn-bgp-agent\n\nThis is the first release of the project ""ovn-bgp-agent"" in the\nOpenStack community. However the project had four previous releases\nthat are documented in [1].\n\n[1]https://pypi.org/project/ovn-bgp-agent/\n\nChange-Id: Id489e2567b8c38d1defc9229087e1d25ccb59a59\n'}, {'number': 4, 'created': '2023-06-30 16:31:04.000000000', 'files': ['deliverables/bobcat/ovn-bgp-agent.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/1f7d482c13e6fec4f0de674a0123c5a43029d105', 'message': '[neutron] Release the 1.0.0.0b1 version of ovn-bgp-agent\n\nThis is the first release of the project ""ovn-bgp-agent"" in the\nOpenStack community. However the project had four previous releases\nthat are documented in [1].\n\n[1]https://pypi.org/project/ovn-bgp-agent/\n\nChange-Id: Id489e2567b8c38d1defc9229087e1d25ccb59a59\n'}]",14,887385,1f7d482c13e6fec4f0de674a0123c5a43029d105,35,7,4,16688,,,0,"[neutron] Release the 1.0.0.0b1 version of ovn-bgp-agent

This is the first release of the project ""ovn-bgp-agent"" in the
OpenStack community. However the project had four previous releases
that are documented in [1].

[1]https://pypi.org/project/ovn-bgp-agent/

Change-Id: Id489e2567b8c38d1defc9229087e1d25ccb59a59
",git fetch https://review.opendev.org/openstack/releases refs/changes/85/887385/4 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/ovn-bgp-agent.yaml'],1,0f3550cdb2b63c7dab6ac25f982540b2730c9fb1,ovn-bgp-agent_0.5.0,--- launchpad: ovn-bgp-agent team: neutron type: other release-model: cycle-with-rc repository-settings: openstack/ovn-bgp-agent: {} releases: - version: 0.1.0 projects: - repo: openstack/ovn-bgp-agent hash: c930d525635d70fa3a675de7f7a36f9171aaaf77 ,,12,0
openstack%2Fdevstack~master~I6aacac94f9697088338b3d2f99d8eaa22c2be67b,openstack/devstack,master,I6aacac94f9697088338b3d2f99d8eaa22c2be67b,Add 10 second buffer for uwsgi service stop,MERGED,2023-06-16 09:01:37.000000000,2023-07-06 11:15:43.000000000,2023-07-06 11:14:43.000000000,"[{'_account_id': 8556}, {'_account_id': 13252}, {'_account_id': 22348}, {'_account_id': 22873}]","[{'number': 1, 'created': '2023-06-16 09:01:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/devstack/commit/758eff0a858866cac41f38201411df33c09f2fae', 'message': 'Add 10 second buffer for uwsgi service stop\n\nDefault for systemd TimeoutStopSec is 90 seconds\nand that is same for graceful shutdown of uwsgi\nservice(WORKER_TIMEOUT).\n\nDue to the Related-Bug graceful stop attempt\nfails and there is no room for force shutdown.\nThis patch adds 10 seconds buffer so service\nstop is successful.\n\nCloses-Bug: #2020643\nRelated-Bug: #2015065\nChange-Id: I6aacac94f9697088338b3d2f99d8eaa22c2be67b\n'}, {'number': 2, 'created': '2023-06-21 12:47:37.000000000', 'files': ['stackrc'], 'web_link': 'https://opendev.org/openstack/devstack/commit/7288df34f8513caf6f3985c75855feb572f6b004', 'message': 'Add 10 second buffer for uwsgi service stop\n\nDefault for systemd TimeoutStopSec is 90 seconds\nand that is same for default graceful shutdown of\nuwsgi service(WORKER_TIMEOUT).\n\nDue to the Related-Bug graceful stop attempt\nfails and there is no room for force shutdown.\nThis patch reduces default for WORKER_TIMEOUT by\n10 seconds so there is a buffer to force stop the\nservice.\n\nCloses-Bug: #2020643\nRelated-Bug: #2015065\nChange-Id: I6aacac94f9697088338b3d2f99d8eaa22c2be67b\n'}]",13,886250,7288df34f8513caf6f3985c75855feb572f6b004,37,4,2,13861,,,0,"Add 10 second buffer for uwsgi service stop

Default for systemd TimeoutStopSec is 90 seconds
and that is same for default graceful shutdown of
uwsgi service(WORKER_TIMEOUT).

Due to the Related-Bug graceful stop attempt
fails and there is no room for force shutdown.
This patch reduces default for WORKER_TIMEOUT by
10 seconds so there is a buffer to force stop the
service.

Closes-Bug: #2020643
Related-Bug: #2015065
Change-Id: I6aacac94f9697088338b3d2f99d8eaa22c2be67b
",git fetch https://review.opendev.org/openstack/devstack refs/changes/50/886250/1 && git format-patch -1 --stdout FETCH_HEAD,['functions-common'],1,758eff0a858866cac41f38201411df33c09f2fae,bug/2020643," iniset -sudo $unitfile ""Service"" ""TimeoutStopSec"" ""$((${WORKER_TIMEOUT} + 10))""",,1,0
openstack%2Freleases~master~I8261f08aaf2432c8bdd3406eb29bf498b41874e7,openstack/releases,master,I8261f08aaf2432c8bdd3406eb29bf498b41874e7,Bobcat-2 release for oslo.middleware,ABANDONED,2023-07-03 09:48:22.000000000,2023-07-06 11:09:06.000000000,,"[{'_account_id': 22348}, {'_account_id': 28522}, {'_account_id': 31245}]","[{'number': 1, 'created': '2023-07-03 09:48:22.000000000', 'files': ['deliverables/bobcat/oslo.middleware.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/e4aa509a28df81c32959b68d7a5f54e660c90fe4', 'message': 'Bobcat-2 release for oslo.middleware\n\nThis is the Bobcat-2 milestone release for oslo.middleware.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: I8261f08aaf2432c8bdd3406eb29bf498b41874e7\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",2,887490,e4aa509a28df81c32959b68d7a5f54e660c90fe4,5,3,1,17685,,,0,"Bobcat-2 release for oslo.middleware

This is the Bobcat-2 milestone release for oslo.middleware.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: I8261f08aaf2432c8bdd3406eb29bf498b41874e7
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/90/887490/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/oslo.middleware.yaml'],1,e4aa509a28df81c32959b68d7a5f54e660c90fe4,bobcat-milestone-2,releases: - version: 5.2.0 projects: - repo: openstack/oslo.middleware hash: 17531fe654805542364090cf9b66784dab1318ac,,5,0
openstack%2Freleases~master~Ic6c386303901da633b7f62e2ab807db6910d50a6,openstack/releases,master,Ic6c386303901da633b7f62e2ab807db6910d50a6,Bobcat-2 release for oslo.metrics,ABANDONED,2023-07-03 09:47:40.000000000,2023-07-06 11:07:12.000000000,,"[{'_account_id': 22348}, {'_account_id': 28522}, {'_account_id': 31245}]","[{'number': 1, 'created': '2023-07-03 09:47:40.000000000', 'files': ['deliverables/bobcat/oslo.metrics.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/132aba5fae9d82fd0c1ab63be89710574fd453b8', 'message': 'Bobcat-2 release for oslo.metrics\n\nThis is the Bobcat-2 milestone release for oslo.metrics.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: Ic6c386303901da633b7f62e2ab807db6910d50a6\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",2,887489,132aba5fae9d82fd0c1ab63be89710574fd453b8,5,3,1,17685,,,0,"Bobcat-2 release for oslo.metrics

This is the Bobcat-2 milestone release for oslo.metrics.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: Ic6c386303901da633b7f62e2ab807db6910d50a6
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/89/887489/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/oslo.metrics.yaml'],1,132aba5fae9d82fd0c1ab63be89710574fd453b8,bobcat-milestone-2,releases: - version: 0.7.0 projects: - repo: openstack/oslo.metrics hash: 5b478403278e69ca9fd822cb117c3e2aa49a6cda,,5,0
openstack%2Freleases~master~Ifc70061c5ad8199d4f57fe033a3b1a626532cfed,openstack/releases,master,Ifc70061c5ad8199d4f57fe033a3b1a626532cfed,Bobcat-2 release for oslo.config,ABANDONED,2023-07-03 09:44:22.000000000,2023-07-06 11:04:02.000000000,,"[{'_account_id': 22348}, {'_account_id': 28522}, {'_account_id': 31245}]","[{'number': 1, 'created': '2023-07-03 09:44:22.000000000', 'files': ['deliverables/bobcat/oslo.config.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/47b9e374a561e0d027b7ca96543d9b3ac9037192', 'message': 'Bobcat-2 release for oslo.config\n\nThis is the Bobcat-2 milestone release for oslo.config.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: Ifc70061c5ad8199d4f57fe033a3b1a626532cfed\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",2,887488,47b9e374a561e0d027b7ca96543d9b3ac9037192,5,3,1,17685,,,0,"Bobcat-2 release for oslo.config

This is the Bobcat-2 milestone release for oslo.config.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: Ifc70061c5ad8199d4f57fe033a3b1a626532cfed
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/88/887488/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/oslo.config.yaml'],1,47b9e374a561e0d027b7ca96543d9b3ac9037192,bobcat-milestone-2,releases: - version: 9.2.0 projects: - repo: openstack/oslo.config hash: 28187da6d7b8eae39208e7cefbbc7da32ec3d3be,,5,0
openstack%2Fcinder~master~I6a33e4c85057bdf79530f9c9f24d867c49931310,openstack/cinder,master,I6a33e4c85057bdf79530f9c9f24d867c49931310,Fix Infinidat driver to inherit compression,NEW,2023-04-27 16:04:14.000000000,2023-07-06 11:00:08.000000000,,"[{'_account_id': 5314}, {'_account_id': 22348}, {'_account_id': 29122}, {'_account_id': 30615}]","[{'number': 1, 'created': '2023-04-27 16:04:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6ab18cd07a008d6dbf0b3d8948e2fdf346e516a8', 'message': 'Fix Infinidat driver to inherit compression\n\n* Fixed Infinidat driver to inherit compression setting by default\n  for all newly created volumes. Admin can set `True` or `False`\n  for the `infinidat_use_compression` option in the driver section\n  of cinder.conf to explicitly enable or disable compression setting\n  for all newly created volumes.\n\n* Removed support for pre-v3.0 InfiniBox systems that do not support\n  compression. These versions are end of life and no longer supported\n  for a long time\n\nCloses-bug: #2017815\nSigned-off-by: Alexander Deiter <adeiter@infinidat.com>\nChange-Id: I6a33e4c85057bdf79530f9c9f24d867c49931310\n'}, {'number': 2, 'created': '2023-05-04 09:09:30.000000000', 'files': ['doc/source/configuration/block-storage/drivers/infinidat-volume-driver.rst', 'releasenotes/notes/bug-2017815-infinidat-fix-compression-setting-04eaf71933d55912.yaml', 'cinder/tests/unit/volume/drivers/test_infinidat.py', 'cinder/volume/drivers/infinidat.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/026dd9d34a3fa9bfc912cb350953aef64a6864a5', 'message': 'Fix Infinidat driver to inherit compression\n\n* Fixed Infinidat driver to inherit compression setting by default for\n  all newly created volumes. Admin can set ``True`` or ``False`` for\n  the ``infinidat_use_compression`` option in the driver section of\n  cinder.conf to explicitly enable or disable compression setting for\n  all newly created volumes. Or leave this option unset (commented out)\n  for all created volumes to inherit their compression setting from\n  their parent pool at creation time. The default value is unset.\n\n* Removed support for pre-v3.0 InfiniBox systems that do not support\n  compression. These versions are end of life and no longer supported\n  for a long time\n\nCloses-bug: #2017815\nSigned-off-by: Alexander Deiter <adeiter@infinidat.com>\nChange-Id: I6a33e4c85057bdf79530f9c9f24d867c49931310\n'}]",17,881188,026dd9d34a3fa9bfc912cb350953aef64a6864a5,59,4,2,35075,,,0,"Fix Infinidat driver to inherit compression

* Fixed Infinidat driver to inherit compression setting by default for
  all newly created volumes. Admin can set ``True`` or ``False`` for
  the ``infinidat_use_compression`` option in the driver section of
  cinder.conf to explicitly enable or disable compression setting for
  all newly created volumes. Or leave this option unset (commented out)
  for all created volumes to inherit their compression setting from
  their parent pool at creation time. The default value is unset.

* Removed support for pre-v3.0 InfiniBox systems that do not support
  compression. These versions are end of life and no longer supported
  for a long time

Closes-bug: #2017815
Signed-off-by: Alexander Deiter <adeiter@infinidat.com>
Change-Id: I6a33e4c85057bdf79530f9c9f24d867c49931310
",git fetch https://review.opendev.org/openstack/cinder refs/changes/88/881188/2 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/configuration/block-storage/drivers/infinidat-volume-driver.rst', 'releasenotes/notes/bug-2017815-infinidat-fix-compression-setting-04eaf71933d55912.yaml', 'cinder/tests/unit/volume/drivers/test_infinidat.py', 'cinder/volume/drivers/infinidat.py']",4,6ab18cd07a008d6dbf0b3d8948e2fdf346e516a8,bug/2017815," help='Specifies whether to explicitly enable or disable ' 'compression for newly created volumes. By default, ' 'compression for all created volumes is inherited ' 'from its parent pool at creation time.'), compression_enabled = self.configuration.infinidat_use_compression if compression_enabled is not None: create_kwargs[""compression_enabled""] = compression_enabled"," default=False, help='Specifies whether to turn on compression for newly ' 'created volumes.'), if (self.configuration.infinidat_use_compression and not self._system.compat.has_compression()): # InfiniBox systems support compression only from v3.0 and up msg = _('InfiniBox system does not support volume compression.\n' 'Compression is available on InfiniBox 3.0 onward.\n' 'Please disable volume compression by setting ' 'infinidat_use_compression to False in the Cinder ' 'configuration file.') raise exception.VolumeDriverException(message=msg) if self._system.compat.has_compression(): create_kwargs[""compression_enabled""] = ( self.configuration.infinidat_use_compression)",32,23
openstack%2Frequirements~master~I957e82319bf11f0dc7d30c618d3c7803e833979f,openstack/requirements,master,I957e82319bf11f0dc7d30c618d3c7803e833979f,update constraint for tosca-parser to new release 2.9.0,NEW,2023-07-06 09:09:15.000000000,2023-07-06 10:58:28.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-06 09:09:15.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/e23192240641d79c937264de99f6e34b5b262cd0', 'message': 'update constraint for tosca-parser to new release 2.9.0\n\nmeta: version: 2.9.0\nmeta: diff-start: -\nmeta: series: bobcat\nmeta: branch: master\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: yes\nmeta: release:Author: Elod Illes <elod.illes@est.tech>\nmeta: release:Commit: Elod Illes <elod.illes@est.tech>\nmeta: release:Change-Id: I45bcf5d20445972177e1df7dca3eb53a64b7f932\nmeta: release:Workflow+1: Thierry Carrez <thierry@openstack.org>\nmeta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>\nmeta: release:Code-Review+1: Yasufumi Ogawa <yasufum.o@gmail.com>\nmeta: release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nChange-Id: I957e82319bf11f0dc7d30c618d3c7803e833979f\n'}]",0,887778,e23192240641d79c937264de99f6e34b5b262cd0,2,1,1,11131,,,0,"update constraint for tosca-parser to new release 2.9.0

meta: version: 2.9.0
meta: diff-start: -
meta: series: bobcat
meta: branch: master
meta: release-type: release
meta: pypi: yes
meta: first: yes
meta: release:Author: Elod Illes <elod.illes@est.tech>
meta: release:Commit: Elod Illes <elod.illes@est.tech>
meta: release:Change-Id: I45bcf5d20445972177e1df7dca3eb53a64b7f932
meta: release:Workflow+1: Thierry Carrez <thierry@openstack.org>
meta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>
meta: release:Code-Review+1: Yasufumi Ogawa <yasufum.o@gmail.com>
meta: release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
Change-Id: I957e82319bf11f0dc7d30c618d3c7803e833979f
",git fetch https://review.opendev.org/openstack/requirements refs/changes/78/887778/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,e23192240641d79c937264de99f6e34b5b262cd0,new-release,tosca-parser===2.9.0,tosca-parser===2.8.0,1,1
openstack%2Fmonasca-api~master~Iea5016022cc717d0984d57efa1d2e502389a3e45,openstack/monasca-api,master,Iea5016022cc717d0984d57efa1d2e502389a3e45,DNM: dummy change to test gate health Change-Id: I0658e32038501c845e0186b54e776723fdd0e918,NEW,2023-07-06 09:45:26.000000000,2023-07-06 10:49:49.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-06 09:45:26.000000000', 'files': ['docker/health_check.py'], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/d8c85fc983fce7e73fd89675d8cd2f54dc83dc31', 'message': 'DNM: dummy change to test gate health\nChange-Id: I0658e32038501c845e0186b54e776723fdd0e918\n\nChange-Id: Iea5016022cc717d0984d57efa1d2e502389a3e45\n'}]",0,887782,d8c85fc983fce7e73fd89675d8cd2f54dc83dc31,2,1,1,35988,,,0,"DNM: dummy change to test gate health
Change-Id: I0658e32038501c845e0186b54e776723fdd0e918

Change-Id: Iea5016022cc717d0984d57efa1d2e502389a3e45
",git fetch https://review.opendev.org/openstack/monasca-api refs/changes/82/887782/1 && git format-patch -1 --stdout FETCH_HEAD,['docker/health_check.py'],1,d8c85fc983fce7e73fd89675d8cd2f54dc83dc31,,# dummy test change if __name__ == '__main__': main(), if __name__ == '__main__': main() ,2,2
openstack%2Fpython-glanceclient~master~I96283b493c60de69f4598fb4470fba5fb117d749,openstack/python-glanceclient,master,I96283b493c60de69f4598fb4470fba5fb117d749,Release notes for 4.4.0,MERGED,2023-07-06 09:51:08.000000000,2023-07-06 10:45:42.000000000,2023-07-06 10:44:37.000000000,"[{'_account_id': 8122}, {'_account_id': 9303}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 09:51:08.000000000', 'files': ['releasenotes/notes/4.4.0_Release-a3c89184f345e5a2.yaml'], 'web_link': 'https://opendev.org/openstack/python-glanceclient/commit/62e6fc8270a3e76e6b21cf2e6385f3c4bfe56cad', 'message': 'Release notes for 4.4.0\n\nChange-Id: I96283b493c60de69f4598fb4470fba5fb117d749\n'}]",0,887784,62e6fc8270a3e76e6b21cf2e6385f3c4bfe56cad,7,3,1,19138,,,0,"Release notes for 4.4.0

Change-Id: I96283b493c60de69f4598fb4470fba5fb117d749
",git fetch https://review.opendev.org/openstack/python-glanceclient refs/changes/84/887784/1 && git format-patch -1 --stdout FETCH_HEAD,['releasenotes/notes/4.4.0_Release-a3c89184f345e5a2.yaml'],1,62e6fc8270a3e76e6b21cf2e6385f3c4bfe56cad,4.4.0-reno,--- fixes: - | Bug 2012442_: import image with glance-download return 400 - | Bug 1934626_: glanceclient has no support to add type while creating md-property for namespace .. _2012442: https://code.launchpad.net/bugs/2012442 .. _1934626: https://code.launchpad.net/bugs/1934626 ,,9,0
openstack%2Fneutron-tempest-plugin~master~I16182fde9c4be47beacb09c5e23eb91ea909146f,openstack/neutron-tempest-plugin,master,I16182fde9c4be47beacb09c5e23eb91ea909146f,"Revert ""Fix wallaby/victoria neutron-tempest-plugin-ovn job""",ABANDONED,2023-07-06 10:05:00.000000000,2023-07-06 10:32:49.000000000,,"[{'_account_id': 6773}, {'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 13861}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 10:05:00.000000000', 'files': ['zuul.d/wallaby_jobs.yaml', 'zuul.d/victoria_jobs.yaml'], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/5fea42635c1a74a015c2853a3dabdbb2072b2366', 'message': 'Revert ""Fix wallaby/victoria neutron-tempest-plugin-ovn job""\n\nThis reverts commit 24b5edd88c8eb889e05dfccd72ccba136661dcb9.\n\nReason for revert: Wallaby CI is pinned to 1.8.0 and Victoria\nto 1.6.0. Because we can\'t update these versions (the CI is\nnot taking this patch), we should find another way to solve the\ngit clone issue.\n\nChange-Id: I16182fde9c4be47beacb09c5e23eb91ea909146f\n'}]",2,887684,5fea42635c1a74a015c2853a3dabdbb2072b2366,5,5,1,16688,,,0,"Revert ""Fix wallaby/victoria neutron-tempest-plugin-ovn job""

This reverts commit 24b5edd88c8eb889e05dfccd72ccba136661dcb9.

Reason for revert: Wallaby CI is pinned to 1.8.0 and Victoria
to 1.6.0. Because we can't update these versions (the CI is
not taking this patch), we should find another way to solve the
git clone issue.

Change-Id: I16182fde9c4be47beacb09c5e23eb91ea909146f
",git fetch https://review.opendev.org/openstack/neutron-tempest-plugin refs/changes/84/887684/1 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/wallaby_jobs.yaml', 'zuul.d/victoria_jobs.yaml']",2,5fea42635c1a74a015c2853a3dabdbb2072b2366,fix-wallaby,," # TODO(lucasagomes): Remove the OVN_BRANCH and OVS_BRANCH config # once https://review.opendev.org/c/openstack/devstack/+/887185 # is merged OVN_BRANCH: ""branch-22.03"" OVS_BRANCH: ""branch-3.0""",0,10
openstack%2Fovn-octavia-provider~master~Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd,openstack/ovn-octavia-provider,master,Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd,Ensure DVR is restablished on member on cascade deletion,MERGED,2023-06-23 16:24:13.000000000,2023-07-06 10:18:50.000000000,2023-07-06 10:17:39.000000000,"[{'_account_id': 6773}, {'_account_id': 22348}, {'_account_id': 23567}, {'_account_id': 34451}]","[{'number': 1, 'created': '2023-06-23 16:24:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ovn-octavia-provider/commit/ec8c49bdf5f4ab7d647779ef5cc777afffa56b2a', 'message': '[WIP] Ensure DVR is restablished on member on cascade deletion\n\nTraffic to member, if they have FIPs gets centralized when they\nare part of a loadbalancer. However, when the loadbalancer gets\ndeleted, the traffic should be distributed again (if DVR was\nenabled). To do that this patch also considers the cascade deletion\n\nChange-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd\n'}, {'number': 2, 'created': '2023-06-26 09:54:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ovn-octavia-provider/commit/c350ed9ed458544038e4cbb230cf5b29b386b3ac', 'message': '[WIP] Ensure DVR is restablished on member on cascade deletion\n\nTraffic to member, if they have FIPs gets centralized when they\nare part of a loadbalancer. However, when the loadbalancer gets\ndeleted, the traffic should be distributed again (if DVR was\nenabled). To do that this patch also considers the cascade deletion\n\nChange-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd\n'}, {'number': 3, 'created': '2023-06-30 17:09:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ovn-octavia-provider/commit/abffcc728ca9a4ca446b73b6a132aa15ffc1c249', 'message': 'Ensure DVR is restablished on member on cascade deletion\n\nTraffic to member, if they have FIPs gets centralized when they\nare part of a loadbalancer. However, when the loadbalancer gets\ndeleted, the traffic should be distributed again (if DVR was\nenabled). To do that this patch also considers the cascade deletion\n\nChange-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd\n'}, {'number': 4, 'created': '2023-07-03 10:18:53.000000000', 'files': ['ovn_octavia_provider/tests/unit/test_driver.py', 'ovn_octavia_provider/helper.py', 'ovn_octavia_provider/driver.py', 'ovn_octavia_provider/tests/unit/test_helper.py'], 'web_link': 'https://opendev.org/openstack/ovn-octavia-provider/commit/20997b185f21aac8959d7517b4ffb7bc44c1b76a', 'message': 'Ensure DVR is restablished on member on cascade deletion\n\nTraffic to member, if they have FIPs gets centralized when they\nare part of a loadbalancer. However, when the loadbalancer gets\ndeleted, the traffic should be distributed again (if DVR was\nenabled). To do that this patch also considers the cascade deletion\n\nCloses-Bug: #2025637\nChange-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd\n'}]",4,886611,20997b185f21aac8959d7517b4ffb7bc44c1b76a,19,4,4,23567,,,0,"Ensure DVR is restablished on member on cascade deletion

Traffic to member, if they have FIPs gets centralized when they
are part of a loadbalancer. However, when the loadbalancer gets
deleted, the traffic should be distributed again (if DVR was
enabled). To do that this patch also considers the cascade deletion

Closes-Bug: #2025637
Change-Id: Ie4b44c9f15fc9e33a68f9aacd766590b974c63fd
",git fetch https://review.opendev.org/openstack/ovn-octavia-provider refs/changes/11/886611/3 && git format-patch -1 --stdout FETCH_HEAD,['ovn_octavia_provider/helper.py'],1,ec8c49bdf5f4ab7d647779ef5cc777afffa56b2a,," member_ip = mem_info.split('_')[2].split("":"")[0] member_info = { 'id': mem_info.split('_')[1], 'address': member_ip, 'pool_id': pool_id, 'subnet_id': mem_info.split('_')[3], 'action': ovn_const.REQ_INFO_MEMBER_DELETED} self.handle_member_dvr(member_info) ",,9,0
openstack%2Fneutron~stable%2Fwallaby~I41ffffeca433faab2244ff3d1876ca078ce5ebfb,openstack/neutron,stable/wallaby,I41ffffeca433faab2244ff3d1876ca078ce5ebfb,Load FIP information during initialize not init,MERGED,2023-07-03 11:29:54.000000000,2023-07-06 10:16:36.000000000,2023-07-06 10:14:59.000000000,"[{'_account_id': 1131}, {'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 11:29:54.000000000', 'files': ['neutron/agent/l3/dvr_local_router.py', 'neutron/tests/unit/agent/l3/test_dvr_local_router.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/5d3da0fd318b523aeccbf3b9572659cefcba05bb', 'message': ""Load FIP information during initialize not init\n\nDvrLocalRouter._load_used_fip_information() is called during the class\ninit however in some cases it tries to access a network namespace which\nhasn't yet been created. This results in NetworkNamespaceNotFound.\n\nThis change ensures that we instead create any FIP priority rules after\nthe network namespace has been created by calling\n_load_used_fip_information() from the initialize function rather than\nin the class instantiation.\n\nCloses-Bug: #2025129\nChange-Id: I41ffffeca433faab2244ff3d1876ca078ce5ebfb\n(cherry picked from commit c8c74f12e048e7858eee332883dbe7c1dc1d0f0c)\n""}]",0,887511,5d3da0fd318b523aeccbf3b9572659cefcba05bb,9,5,1,16688,,,0,"Load FIP information during initialize not init

DvrLocalRouter._load_used_fip_information() is called during the class
init however in some cases it tries to access a network namespace which
hasn't yet been created. This results in NetworkNamespaceNotFound.

This change ensures that we instead create any FIP priority rules after
the network namespace has been created by calling
_load_used_fip_information() from the initialize function rather than
in the class instantiation.

Closes-Bug: #2025129
Change-Id: I41ffffeca433faab2244ff3d1876ca078ce5ebfb
(cherry picked from commit c8c74f12e048e7858eee332883dbe7c1dc1d0f0c)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/11/887511/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/agent/l3/dvr_local_router.py', 'neutron/tests/unit/agent/l3/test_dvr_local_router.py']",2,5d3da0fd318b523aeccbf3b9572659cefcba05bb,bug/2025129," @mock.patch.object(router_info.RouterInfo, 'initialize') def test_initialize_dvr_local_router(self, super_initialize): ri = self._create_router() self.mock_load_fip.assert_not_called() ri.initialize(self.process_monitor) super_initialize.assert_called_once_with(self.process_monitor) self.mock_load_fip.assert_called_once() ",,12,0
openstack%2Frequirements~master~I769d749fe8d50d817a34be611a526831ee9ae181,openstack/requirements,master,I769d749fe8d50d817a34be611a526831ee9ae181,update constraint for python-neutronclient to new release 11.0.0,NEW,2023-07-06 08:47:02.000000000,2023-07-06 10:10:59.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-06 08:47:02.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/bfa7790891de3b20b69645a45038b6596fa2b38e', 'message': 'update constraint for python-neutronclient to new release 11.0.0\n\nmeta: version: 11.0.0\nmeta: diff-start: -\nmeta: series: bobcat\nmeta: branch: master\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: no\nmeta: release:Author: Elod Illes <elod.illes@est.tech>\nmeta: release:Commit: Elod Illes <elod.illes@est.tech>\nmeta: release:Change-Id: Iedc29697b241eaf003d1a20db1d88c1b6f8075d2\nmeta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>\nmeta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>\nmeta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>\nmeta: release:Code-Review+1: Rodolfo Alonso <ralonsoh@redhat.com>\nmeta: release:Code-Review+1: Slawek Kaplonski <skaplons@redhat.com>\nmeta: release:Code-Review+1: likui <likui@yovole.com>\nChange-Id: I769d749fe8d50d817a34be611a526831ee9ae181\n'}]",0,887771,bfa7790891de3b20b69645a45038b6596fa2b38e,2,1,1,11131,,,0,"update constraint for python-neutronclient to new release 11.0.0

meta: version: 11.0.0
meta: diff-start: -
meta: series: bobcat
meta: branch: master
meta: release-type: release
meta: pypi: yes
meta: first: no
meta: release:Author: Elod Illes <elod.illes@est.tech>
meta: release:Commit: Elod Illes <elod.illes@est.tech>
meta: release:Change-Id: Iedc29697b241eaf003d1a20db1d88c1b6f8075d2
meta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>
meta: release:Workflow+1: Hervé Beraud <herveberaud.pro@gmail.com>
meta: release:Code-Review+2: Hervé Beraud <herveberaud.pro@gmail.com>
meta: release:Code-Review+1: Rodolfo Alonso <ralonsoh@redhat.com>
meta: release:Code-Review+1: Slawek Kaplonski <skaplons@redhat.com>
meta: release:Code-Review+1: likui <likui@yovole.com>
Change-Id: I769d749fe8d50d817a34be611a526831ee9ae181
",git fetch https://review.opendev.org/openstack/requirements refs/changes/71/887771/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,bfa7790891de3b20b69645a45038b6596fa2b38e,new-release,python-neutronclient===11.0.0,python-neutronclient===10.0.0,1,1
openstack%2Fnova~master~Iab294811172c7d3d723f716f154f0b57e9760223,openstack/nova,master,Iab294811172c7d3d723f716f154f0b57e9760223,WIP: Added api for healthcheck,ABANDONED,2023-05-09 13:51:28.000000000,2023-07-06 10:09:28.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-05-09 13:51:28.000000000', 'files': ['nova/api/openstack/compute/routes.py', 'nova/api/openstack/compute/health.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/05ab03585ce00df47a3878fcad72a71c8fd170c7', 'message': 'WIP: Added api for healthcheck\n\nChange-Id: Iab294811172c7d3d723f716f154f0b57e9760223\n'}]",0,882718,05ab03585ce00df47a3878fcad72a71c8fd170c7,7,1,1,34860,,,0,"WIP: Added api for healthcheck

Change-Id: Iab294811172c7d3d723f716f154f0b57e9760223
",git fetch https://review.opendev.org/openstack/nova refs/changes/18/882718/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/api/openstack/compute/routes.py', 'nova/api/openstack/compute/health.py']",2,05ab03585ce00df47a3878fcad72a71c8fd170c7,nova-healthcheck-rfe,"from oslo_log import log as logging from nova.api.openstack import wsgi from nova.api.openstack.compute import services from nova.compute import api as compute from nova import servicegroup LOG = logging.getLogger(__name__) class HealthController(wsgi.Controller): def __init__(self): super(HealthController, self).__init__() self.host_api = compute.HostAPI() self.servicegroup_api = servicegroup.API() def _get_service_detail(self, svc, additional_fields, req, cell_down_support=False): alive = self.servicegroup_api.service_is_up(svc) state = (alive and ""pass"") or ""fail"" service_detail = {'binary': svc['binary'], 'host': svc['host'], 'state': state, } return service_detail def _get_services(self, context): services = [ s for s in self.host_api.service_get_all(context, set_zones=True, all_cells=True, cell_down_support=True) if s.topic ] service_detail = {} for svc in services: alive = self.servicegroup_api.service_is_up(svc) state = (alive and ""pass"") or ""fail"" service_detail.update({ svc['uuid']:{ 'binary': svc['binary'], 'host': svc['host'], 'state': state, } }) return service_detail @wsgi.expected_errors(()) def index(self, req): """"""Return a list of all running services. Filter by host & service name """""" context = req.environ['nova.context'] LOG.info(""~~~""*100) services = self._get_services(context) LOG.info(""~~~""*100) return services",,75,0
openstack%2Fcharm-mysql-innodb-cluster~stable%2Fjammy~Id6a6337004586699c7f943530fa7a32deae26db0,openstack/charm-mysql-innodb-cluster,stable/jammy,Id6a6337004586699c7f943530fa7a32deae26db0,Prometheus mysqld exporter,NEW,2023-02-28 00:27:03.000000000,2023-07-06 10:08:56.000000000,,"[{'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-02-28 00:27:03.000000000', 'files': ['src/tests/bundles/focal.yaml', 'unit_tests/test_lib_charm_openstack_mysql_innodb_cluster.py', 'unit_tests/__init__.py', 'src/lib/charm/openstack/mysql_innodb_cluster.py', 'src/metadata.yaml', 'src/tests/bundles/focal-full.yaml', 'unit_tests/test_mysql_innodb_cluster_handlers.py', 'src/tests/tests.yaml', 'src/layer.yaml', 'src/config.yaml', 'unit_tests/test_prometheus_mysql_exporter_handlers.py', 'src/reactive/prometheus_mysql_exporter_handlers.py', 'src/tests/bundles/jammy.yaml', 'src/reactive/mysql_innodb_cluster_handlers.py'], 'web_link': 'https://opendev.org/openstack/charm-mysql-innodb-cluster/commit/29aa24710f15a6fb9cbb9f38fbec23d46f91d95a', 'message': ""Prometheus mysqld exporter\n\nInstall mysqld-exporter with snap. It's a prometheus exporter for mysql,\nwhich provide the http interface for prometheus to pull the metrics.\nIt should only install & setup configuration when relation connect to prometheus,\nand disable when relation remove.\nThe charm also need to create read only user for the exporter to get the\nmetrics.\n\nhttps://github.com/prometheus/mysqld_exporter\n\nChange-Id: Id6a6337004586699c7f943530fa7a32deae26db0\nfunc-test-pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/859\n(cherry picked from commit 85a3deee22a57f562ca1d88843279ecee3658abc)\n""}]",2,875590,29aa24710f15a6fb9cbb9f38fbec23d46f91d95a,9,3,1,5112,,,0,"Prometheus mysqld exporter

Install mysqld-exporter with snap. It's a prometheus exporter for mysql,
which provide the http interface for prometheus to pull the metrics.
It should only install & setup configuration when relation connect to prometheus,
and disable when relation remove.
The charm also need to create read only user for the exporter to get the
metrics.

https://github.com/prometheus/mysqld_exporter

Change-Id: Id6a6337004586699c7f943530fa7a32deae26db0
func-test-pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/859
(cherry picked from commit 85a3deee22a57f562ca1d88843279ecee3658abc)
",git fetch https://review.opendev.org/openstack/charm-mysql-innodb-cluster refs/changes/90/875590/1 && git format-patch -1 --stdout FETCH_HEAD,"['src/tests/bundles/focal.yaml', 'unit_tests/test_lib_charm_openstack_mysql_innodb_cluster.py', 'unit_tests/__init__.py', 'src/lib/charm/openstack/mysql_innodb_cluster.py', 'src/metadata.yaml', 'src/tests/bundles/focal-full.yaml', 'unit_tests/test_mysql_innodb_cluster_handlers.py', 'src/tests/tests.yaml', 'src/layer.yaml', 'src/config.yaml', 'unit_tests/test_prometheus_mysql_exporter_handlers.py', 'src/reactive/prometheus_mysql_exporter_handlers.py', 'src/tests/bundles/jammy.yaml', 'src/reactive/mysql_innodb_cluster_handlers.py']",14,29aa24710f15a6fb9cbb9f38fbec23d46f91d95a,feat/852064-stable/jammy,"from .prometheus_mysql_exporter_handlers import ( create_remote_prometheus_exporter_user ) if not instance.create_user( instance.cluster_password, ""all"", ): if not instance.create_user( unit.received['cluster-password'], ""all""): if reactive.endpoint_from_flag(""prometheus.available""): create_remote_prometheus_exporter_user() if not instance.create_user( db_monitor.relation_ip, username, password, ""read_only"""," if not instance.create_cluster_user( instance.cluster_password): if not instance.create_cluster_user( unit.received['cluster-password']): if not instance.create_cluster_user( db_monitor.relation_ip, username, password, True",652,62
openstack%2Fneutron-tempest-plugin~master~I18c81ae4a9b5a50e2db25302ab99883248b473a1,openstack/neutron-tempest-plugin,master,I18c81ae4a9b5a50e2db25302ab99883248b473a1,Fix wallaby/victoria neutron-tempest-plugin-ovn job,MERGED,2023-07-05 10:35:04.000000000,2023-07-06 10:05:00.000000000,2023-07-05 14:42:22.000000000,"[{'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 13861}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 10:35:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/5a52177c5e87a55aba138835ec0574b1ddd61d6e', 'message': ""Fix wallaby neutron-tempest-plugin-ovn job\n\nThis could be fixed by merging\nhttps://review.opendev.org/c/openstack/devstack/+/887184 but, talking to\ndevstack maintainers upstream this patch does introduce a regression:\n\n<frickler> the problem with that patch is that it introduces a regression\nbecause GIT_DEPTH no longer works and until we have a fix for that,\nI'm hesitating to do more backports of it\n\nSo, I think we could workaround this by using the name of the branches\nwith those commits included instead of the commit hash, that would work\nwith the git clone function from devstack.\n\nChange-Id: I18c81ae4a9b5a50e2db25302ab99883248b473a1\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n""}, {'number': 2, 'created': '2023-07-05 10:35:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/09e94103286c76576980f4041a1a748aa46dc887', 'message': ""Fix wallaby neutron-tempest-plugin-ovn job\n\nThis could be fixed by merging\nhttps://review.opendev.org/c/openstack/devstack/+/887184 but, talking to\ndevstack maintainers upstream this patch does introduce a regression:\n\n<frickler> the problem with that patch is that it introduces a regression\nbecause GIT_DEPTH no longer works and until we have a fix for that,\nI'm hesitating to do more backports of it\n\nSo, I think we could workaround this by using the name of the branches\nwith those commits included instead of the commit hash, that would work\nwith the git clone function from devstack.\n\nChange-Id: I18c81ae4a9b5a50e2db25302ab99883248b473a1\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n""}, {'number': 3, 'created': '2023-07-05 11:10:47.000000000', 'files': ['zuul.d/wallaby_jobs.yaml', 'zuul.d/victoria_jobs.yaml'], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/24b5edd88c8eb889e05dfccd72ccba136661dcb9', 'message': ""Fix wallaby/victoria neutron-tempest-plugin-ovn job\n\nThis could be fixed by merging\nhttps://review.opendev.org/c/openstack/devstack/+/887184 and\nhttps://review.opendev.org/c/openstack/devstack/+/887185 but, talking\nto devstack maintainers upstream this patch does introduce a regression:\n\n<frickler> the problem with that patch is that it introduces a regression\nbecause GIT_DEPTH no longer works and until we have a fix for that,\nI'm hesitating to do more backports of it\n\nSo, I think we could workaround this by using the name of the branches\nwith those commits included instead of the commit hash, that would work\nwith the git clone function from devstack.\n\nChange-Id: I18c81ae4a9b5a50e2db25302ab99883248b473a1\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n""}]",8,887666,24b5edd88c8eb889e05dfccd72ccba136661dcb9,18,5,3,6773,,,0,"Fix wallaby/victoria neutron-tempest-plugin-ovn job

This could be fixed by merging
https://review.opendev.org/c/openstack/devstack/+/887184 and
https://review.opendev.org/c/openstack/devstack/+/887185 but, talking
to devstack maintainers upstream this patch does introduce a regression:

<frickler> the problem with that patch is that it introduces a regression
because GIT_DEPTH no longer works and until we have a fix for that,
I'm hesitating to do more backports of it

So, I think we could workaround this by using the name of the branches
with those commits included instead of the commit hash, that would work
with the git clone function from devstack.

Change-Id: I18c81ae4a9b5a50e2db25302ab99883248b473a1
Signed-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>
",git fetch https://review.opendev.org/openstack/neutron-tempest-plugin refs/changes/66/887666/2 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/wallaby_jobs.yaml'],1,5a52177c5e87a55aba138835ec0574b1ddd61d6e,fix-wallaby," devstack_localrc: # TODO(lucasagomes): Remove the OVN_BRANCH and OVS_BRANCH config # once https://review.opendev.org/c/openstack/devstack/+/887184 # is merged OVN_BRANCH: ""branch-22.03"" OVS_BRANCH: ""branch-3.0""",,6,0
openstack%2Fneutron~stable%2Fxena~I41ffffeca433faab2244ff3d1876ca078ce5ebfb,openstack/neutron,stable/xena,I41ffffeca433faab2244ff3d1876ca078ce5ebfb,Load FIP information during initialize not init,MERGED,2023-07-03 11:29:42.000000000,2023-07-06 10:00:11.000000000,2023-07-06 09:58:47.000000000,"[{'_account_id': 1131}, {'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 11:29:42.000000000', 'files': ['neutron/agent/l3/dvr_local_router.py', 'neutron/tests/unit/agent/l3/test_dvr_local_router.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/7d595ded10790c9a3694019eba73a8e886f12071', 'message': ""Load FIP information during initialize not init\n\nDvrLocalRouter._load_used_fip_information() is called during the class\ninit however in some cases it tries to access a network namespace which\nhasn't yet been created. This results in NetworkNamespaceNotFound.\n\nThis change ensures that we instead create any FIP priority rules after\nthe network namespace has been created by calling\n_load_used_fip_information() from the initialize function rather than\nin the class instantiation.\n\nCloses-Bug: #2025129\nChange-Id: I41ffffeca433faab2244ff3d1876ca078ce5ebfb\n(cherry picked from commit c8c74f12e048e7858eee332883dbe7c1dc1d0f0c)\n""}]",3,887510,7d595ded10790c9a3694019eba73a8e886f12071,18,5,1,16688,,,0,"Load FIP information during initialize not init

DvrLocalRouter._load_used_fip_information() is called during the class
init however in some cases it tries to access a network namespace which
hasn't yet been created. This results in NetworkNamespaceNotFound.

This change ensures that we instead create any FIP priority rules after
the network namespace has been created by calling
_load_used_fip_information() from the initialize function rather than
in the class instantiation.

Closes-Bug: #2025129
Change-Id: I41ffffeca433faab2244ff3d1876ca078ce5ebfb
(cherry picked from commit c8c74f12e048e7858eee332883dbe7c1dc1d0f0c)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/10/887510/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/agent/l3/dvr_local_router.py', 'neutron/tests/unit/agent/l3/test_dvr_local_router.py']",2,7d595ded10790c9a3694019eba73a8e886f12071,bug/2025129," @mock.patch.object(router_info.RouterInfo, 'initialize') def test_initialize_dvr_local_router(self, super_initialize): ri = self._create_router() self.mock_load_fip.assert_not_called() ri.initialize(self.process_monitor) super_initialize.assert_called_once_with(self.process_monitor) self.mock_load_fip.assert_called_once() ",,12,0
openstack%2Freleases~master~If65debbef4ae6527f7a811ace99cf69e4b0d5339,openstack/releases,master,If65debbef4ae6527f7a811ace99cf69e4b0d5339,[zed] release oslo.messaging 14.0.1,MERGED,2023-07-06 07:55:13.000000000,2023-07-06 09:54:09.000000000,2023-07-06 09:54:09.000000000,"[{'_account_id': 308}, {'_account_id': 17685}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 07:55:13.000000000', 'files': ['deliverables/zed/oslo.messaging.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/2f95c4d58da64f789975566dab4e90f88677aeed', 'message': '[zed] release oslo.messaging 14.0.1\n\nChange-Id: If65debbef4ae6527f7a811ace99cf69e4b0d5339\n'}]",2,887767,2f95c4d58da64f789975566dab4e90f88677aeed,8,3,1,28522,,,0,"[zed] release oslo.messaging 14.0.1

Change-Id: If65debbef4ae6527f7a811ace99cf69e4b0d5339
",git fetch https://review.opendev.org/openstack/releases refs/changes/67/887767/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/zed/oslo.messaging.yaml'],1,2f95c4d58da64f789975566dab4e90f88677aeed,oslo-zed, - version: 14.0.1 projects: - repo: openstack/oslo.messaging hash: fa3195a3459cae3f4e9be43f114ee2d5eb7a60f1,,4,0
openstack%2Ftempest~master~I01b6a8eb5b0e319205834d0d36b21e3c15e8c9d6,openstack/tempest,master,I01b6a8eb5b0e319205834d0d36b21e3c15e8c9d6,"Mark ""test_live_migration_with_trunk"" as unstable",MERGED,2023-06-20 13:56:03.000000000,2023-07-06 09:40:54.000000000,2023-06-22 01:25:20.000000000,"[{'_account_id': 7166}, {'_account_id': 8313}, {'_account_id': 8556}, {'_account_id': 9708}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-20 13:56:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/b974a086b2c6c606d9885d41ce165a73886d8518', 'message': 'Mark ""test_live_migration_with_trunk"" as unstable\n\nMost probably due to a new OVN version, the subports status of a trunk\nport  are always DOWN. We are investigating this issue right now. In\norder to unblock the Nova CI, this test is marked as unstable\ntemporarily.\n\nRelated-Bug: #2024160\nChange-Id: I01b6a8eb5b0e319205834d0d36b21e3c15e8c9d6\n'}, {'number': 2, 'created': '2023-06-20 19:05:22.000000000', 'files': ['tempest/api/compute/admin/test_live_migration.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/0c953a7c100a8eced9158402106eec99f64a5378', 'message': 'Mark ""test_live_migration_with_trunk"" as unstable\n\nMost probably due to a new OVN version, the subports status of a trunk\nport  are always DOWN. We are investigating this issue right now. In\norder to unblock the Nova CI, this test is marked as unstable\ntemporarily.\n\nRelated-Bug: #2024160\nChange-Id: I01b6a8eb5b0e319205834d0d36b21e3c15e8c9d6\n'}]",9,886496,0c953a7c100a8eced9158402106eec99f64a5378,25,5,2,16688,,,0,"Mark ""test_live_migration_with_trunk"" as unstable

Most probably due to a new OVN version, the subports status of a trunk
port  are always DOWN. We are investigating this issue right now. In
order to unblock the Nova CI, this test is marked as unstable
temporarily.

Related-Bug: #2024160
Change-Id: I01b6a8eb5b0e319205834d0d36b21e3c15e8c9d6
",git fetch https://review.opendev.org/openstack/tempest refs/changes/96/886496/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/compute/admin/test_live_migration.py'],1,b974a086b2c6c606d9885d41ce165a73886d8518,bug/2024160, @decorators.unstable_test('bug 2024160'),,1,0
openstack%2Freleases~master~Ice1fa6e981ca88240f6bdc7ce47f823854d1da40,openstack/releases,master,Ice1fa6e981ca88240f6bdc7ce47f823854d1da40,[yoga] release oslo.messaging 12.13.1,MERGED,2023-07-06 07:53:18.000000000,2023-07-06 09:38:59.000000000,2023-07-06 09:38:59.000000000,"[{'_account_id': 308}, {'_account_id': 17685}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-06 07:53:18.000000000', 'files': ['deliverables/yoga/oslo.messaging.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/e183654753d3af4efe61e159c022fe8a02472e29', 'message': '[yoga] release oslo.messaging 12.13.1\n\nChange-Id: Ice1fa6e981ca88240f6bdc7ce47f823854d1da40\n'}]",2,887766,e183654753d3af4efe61e159c022fe8a02472e29,8,3,1,28522,,,0,"[yoga] release oslo.messaging 12.13.1

Change-Id: Ice1fa6e981ca88240f6bdc7ce47f823854d1da40
",git fetch https://review.opendev.org/openstack/releases refs/changes/66/887766/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/yoga/oslo.messaging.yaml'],1,e183654753d3af4efe61e159c022fe8a02472e29,oslo-yoga, - version: 12.13.1 projects: - repo: openstack/oslo.messaging hash: f20a905ea6f41399c1723f8f1cbd0bc1097b8672,,4,0
openstack%2Fmonasca-api~master~I0658e32038501c845e0186b54e776723fdd0e918,openstack/monasca-api,master,I0658e32038501c845e0186b54e776723fdd0e918,DNM: dummy change to test gate health Change-Id: I0658e32038501c845e0186b54e776723fdd0e918,NEW,2023-07-06 09:01:06.000000000,2023-07-06 09:38:44.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-06 09:01:06.000000000', 'files': ['docker/health_check.py'], 'web_link': 'https://opendev.org/openstack/monasca-api/commit/dab1e8535f7d631a041f2442898a6e95444cdcbb', 'message': 'DNM: dummy change to test gate health\nChange-Id: I0658e32038501c845e0186b54e776723fdd0e918\n'}]",0,887775,dab1e8535f7d631a041f2442898a6e95444cdcbb,3,1,1,35988,,,0,"DNM: dummy change to test gate health
Change-Id: I0658e32038501c845e0186b54e776723fdd0e918
",git fetch https://review.opendev.org/openstack/monasca-api refs/changes/75/887775/1 && git format-patch -1 --stdout FETCH_HEAD,['docker/health_check.py'],1,dab1e8535f7d631a041f2442898a6e95444cdcbb,, # dummy test change,,2,0
openstack%2Fmasakari~master~I44b1db763d3a0ed9eb4c007278c8f131db9b57dd,openstack/masakari,master,I44b1db763d3a0ed9eb4c007278c8f131db9b57dd,log when set host on maintenance,MERGED,2023-06-29 10:55:22.000000000,2023-07-06 09:19:36.000000000,2023-07-06 09:18:38.000000000,"[{'_account_id': 22348}, {'_account_id': 30623}]","[{'number': 1, 'created': '2023-06-29 10:55:22.000000000', 'files': ['masakari/engine/manager.py'], 'web_link': 'https://opendev.org/openstack/masakari/commit/bad1f2fe6e5b4cc1f04c8723d9aba8c4cfffb164', 'message': 'log when set host on maintenance\n\nChange-Id: I44b1db763d3a0ed9eb4c007278c8f131db9b57dd\n'}]",0,887264,bad1f2fe6e5b4cc1f04c8723d9aba8c4cfffb164,7,2,1,30623,,,0,"log when set host on maintenance

Change-Id: I44b1db763d3a0ed9eb4c007278c8f131db9b57dd
",git fetch https://review.opendev.org/openstack/masakari refs/changes/64/887264/1 && git format-patch -1 --stdout FETCH_HEAD,['masakari/engine/manager.py'],1,bad1f2fe6e5b4cc1f04c8723d9aba8c4cfffb164,," LOG.info(""Set host %s on maintenance."", host_name)",,1,0
openstack%2Fopenstack-ansible-rabbitmq_server~master~I99683a031f935b579d38ae457c484c9a150344c6,openstack/openstack-ansible-rabbitmq_server,master,I99683a031f935b579d38ae457c484c9a150344c6,Use wildcards to specify rabbit/erlang versions,MERGED,2023-07-04 10:58:44.000000000,2023-07-06 09:15:42.000000000,2023-07-05 17:21:27.000000000,"[{'_account_id': 16011}, {'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 28619}, {'_account_id': 32666}]","[{'number': 1, 'created': '2023-07-04 10:58:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-rabbitmq_server/commit/dc5eba187a11681e22c183574571d9b54f953754', 'message': 'Use wildcards to specify rabbit/erlang versions\n\nFor a long time we were struggling with disappearing packages on\npackagecloud/cloudsmith repos.\nWe were told to use {ppa1,yum1}.novemberain.com because packages should\nnot disappear from there[1].\nUnfortunately it just happened causing Rocky jobs to fail with error\nmessage: ""No package erlang-25.3.2-1.el9.x86_64 available.""\n\nBecause we had this issue for a long time and we have not found any\nproper solution so far, I think the best we can do is to use wildcards\nin a version definitions.\n\nAdditionally, I don\'t think that pinning to specific patch version\n(number afer second dot) is really needed because according to [2],\nrabbitmq 3.11.X should always work with erlang 25.3.Y.\nBased on that information, it is enough to pin:\nrabbitmq-server==3.11.* and erlang==25.3.*\n\n[1] https://github.com/rabbitmq/rabbitmq-server/discussions/8386#discussioncomment-6022021\n[2] https://www.rabbitmq.com/which-erlang.html#compatibility-matrix\n\nRequired-By: https://review.opendev.org/c/openstack/openstack-ansible-os_rally/+/887528\nChange-Id: I99683a031f935b579d38ae457c484c9a150344c6\n'}, {'number': 2, 'created': '2023-07-04 16:13:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-rabbitmq_server/commit/710b1ffb4f93a6a3599c6a73e271c7828e223fe4', 'message': 'Use wildcards to specify rabbit/erlang versions\n\nFor a long time we were struggling with disappearing packages on\npackagecloud/cloudsmith repos.\nWe were told to use {ppa1,yum1}.novemberain.com because packages should\nnot disappear from there[1].\nUnfortunately it just happened causing Rocky jobs to fail with error\nmessage: ""No package erlang-25.3.2-1.el9.x86_64 available.""\n\nBecause we had this issue for a long time and we have not found any\nproper solution so far, I think the best we can do is to use wildcards for\nversion definitions.\nWildcards are used only for build numbers(number after 3th dot) if they are\npresent. It should minimize a chance to install incompatible erlang and rabbitmq\nversions.\n\n[1] https://github.com/rabbitmq/rabbitmq-server/discussions/8386#discussioncomment-6022021\n\nRequired-By: https://review.opendev.org/c/openstack/openstack-ansible-os_rally/+/887528\nChange-Id: I99683a031f935b579d38ae457c484c9a150344c6\n'}, {'number': 3, 'created': '2023-07-04 16:47:31.000000000', 'files': ['vars/redhat.yml', 'vars/debian.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-rabbitmq_server/commit/953ceccb0b374f15e055abc4830d5de2e9a2c2aa', 'message': 'Use wildcards to specify rabbit/erlang versions\n\nFor a long time we were struggling with disappearing packages on\npackagecloud/cloudsmith repos.\nWe were told to use {ppa1,yum1}.novemberain.com because packages should\nnot disappear from there[1].\nUnfortunately it just happened causing Rocky jobs to fail with error\nmessage: ""No package erlang-25.3.2-1.el9.x86_64 available.""\n\nBecause we had this issue for a long time and we have not found any\nproper solution so far, I think the best we can do is to use wildcards\nfor version definitions.\nWildcards are used only for build numbers(number after 3th dot) if they\nare present. It should minimize a chance to install incompatible erlang\nand rabbitmq versions.\n\n[1] https://github.com/rabbitmq/rabbitmq-server/discussions/8386#discussioncomment-6022021\n\nRequired-By: https://review.opendev.org/c/openstack/openstack-ansible-os_rally/+/887528\nChange-Id: I99683a031f935b579d38ae457c484c9a150344c6\n'}]",4,887592,953ceccb0b374f15e055abc4830d5de2e9a2c2aa,17,5,3,32666,,,0,"Use wildcards to specify rabbit/erlang versions

For a long time we were struggling with disappearing packages on
packagecloud/cloudsmith repos.
We were told to use {ppa1,yum1}.novemberain.com because packages should
not disappear from there[1].
Unfortunately it just happened causing Rocky jobs to fail with error
message: ""No package erlang-25.3.2-1.el9.x86_64 available.""

Because we had this issue for a long time and we have not found any
proper solution so far, I think the best we can do is to use wildcards
for version definitions.
Wildcards are used only for build numbers(number after 3th dot) if they
are present. It should minimize a chance to install incompatible erlang
and rabbitmq versions.

[1] https://github.com/rabbitmq/rabbitmq-server/discussions/8386#discussioncomment-6022021

Required-By: https://review.opendev.org/c/openstack/openstack-ansible-os_rally/+/887528
Change-Id: I99683a031f935b579d38ae457c484c9a150344c6
",git fetch https://review.opendev.org/openstack/openstack-ansible-rabbitmq_server refs/changes/92/887592/3 && git format-patch -1 --stdout FETCH_HEAD,"['vars/redhat.yml', 'vars/debian.yml']",2,dc5eba187a11681e22c183574571d9b54f953754,,"_rabbitmq_package_version: ""3.11.*""_rabbitmq_erlang_version_spec: ""{{ (rabbitmq_install_method == 'external_repo') | ternary('1:25.3.*', '1:22.*') }}""","_rabbitmq_package_version: ""3.11.17-1""_rabbitmq_erlang_version_spec: ""{{ (rabbitmq_install_method == 'external_repo') | ternary('1:25.3.2.1-1', '1:22.*') }}""",4,4
openstack%2Fovn-bgp-agent~master~I9abf987004370d0c3ec6a152bb28ab450a9af2c2,openstack/ovn-bgp-agent,master,I9abf987004370d0c3ec6a152bb28ab450a9af2c2,Fix issue with virtual ports not being exposed on time,MERGED,2023-05-19 05:43:45.000000000,2023-07-06 09:14:50.000000000,2023-07-06 09:13:49.000000000,"[{'_account_id': 6773}, {'_account_id': 22348}, {'_account_id': 23567}]","[{'number': 1, 'created': '2023-05-19 05:43:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ovn-bgp-agent/commit/b7361f9238c9021223f4500bc15a09f0a2c5cd60', 'message': 'Fix issue with virtual ports not being exposed on time\n\nThe requested-chassis information is only added by neutron\nfor """" port types. The virtual ports chassis is decided by OVN\nnot by neutron, and neutron only update that information in the\nperiodic maintenance task.\n\nAs part of [1] information about the virtual port chassis is being\nadded to the external_ids. This patch is adapting the NB DB driver\nto consume this new source of information and being able to expose\nthose ports when OVN associates them to a node.\n\nDepends-On: https://review.opendev.org/c/openstack/neutron/+/882705\n\nCloses-Bug: #2020157\n\n[1] https://review.opendev.org/c/openstack/neutron/+/882705\n\nChange-Id: I9abf987004370d0c3ec6a152bb28ab450a9af2c2\n'}, {'number': 2, 'created': '2023-05-19 07:19:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ovn-bgp-agent/commit/011b4669ea6584ad414db1f9fcae2a6b9f76ffc6', 'message': 'Fix issue with virtual ports not being exposed on time\n\nThe requested-chassis information is only added by neutron\nfor """" port types. The virtual ports chassis is decided by OVN\nnot by neutron, and neutron only update that information in the\nperiodic maintenance task.\n\nAs part of [1] information about the virtual port chassis is being\nadded to the external_ids. This patch is adapting the NB DB driver\nto consume this new source of information and being able to expose\nthose ports when OVN associates them to a node.\n\nDepends-On: https://review.opendev.org/c/openstack/neutron/+/882705\n\nCloses-Bug: #2020157\n\n[1] https://review.opendev.org/c/openstack/neutron/+/882705\n\nChange-Id: I9abf987004370d0c3ec6a152bb28ab450a9af2c2\n'}, {'number': 3, 'created': '2023-05-19 07:24:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ovn-bgp-agent/commit/78a6e6a9909e58d9c5aaee630c709d6c43c08d25', 'message': 'Fix issue with virtual ports not being exposed on time\n\nThe requested-chassis information is only added by neutron\nfor """" port types. The virtual ports chassis is decided by OVN\nnot by neutron, and neutron only update that information in the\nperiodic maintenance task.\n\nAs part of [1] information about the virtual port chassis is being\nadded to the external_ids. This patch is adapting the NB DB driver\nto consume this new source of information and being able to expose\nthose ports when OVN associates them to a node.\n\nDepends-On: https://review.opendev.org/c/openstack/neutron/+/882705\n\nCloses-Bug: #2020157\n\n[1] https://review.opendev.org/c/openstack/neutron/+/882705\n\nChange-Id: I9abf987004370d0c3ec6a152bb28ab450a9af2c2\n'}, {'number': 4, 'created': '2023-07-05 08:16:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ovn-bgp-agent/commit/d8e734a4223e7113e73d58320a8aa0f092c10dfc', 'message': 'Fix issue with virtual ports not being exposed on time\n\nThe requested-chassis information is only added by neutron\nfor """" port types. The virtual ports chassis is decided by OVN\nnot by neutron, and neutron only update that information in the\nperiodic maintenance task.\n\nAs part of [1] information about the virtual port chassis is being\nadded to the external_ids. This patch is adapting the NB DB driver\nto consume this new source of information and being able to expose\nthose ports when OVN associates them to a node.\n\nDepends-On: https://review.opendev.org/c/openstack/neutron/+/882705\n\nCloses-Bug: #2020157\n\n[1] https://review.opendev.org/c/openstack/neutron/+/882705\n\nChange-Id: I9abf987004370d0c3ec6a152bb28ab450a9af2c2\n'}, {'number': 5, 'created': '2023-07-05 10:41:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ovn-bgp-agent/commit/c4bca172233eafec68458acdf86343df27906597', 'message': 'Fix issue with virtual ports not being exposed on time\n\nThe requested-chassis information is only added by neutron\nfor """" port types. The virtual ports chassis is decided by OVN\nnot by neutron, and neutron only update that information in the\nperiodic maintenance task.\n\nAs part of [1] information about the virtual port chassis is being\nadded to the external_ids. This patch is adapting the NB DB driver\nto consume this new source of information and being able to expose\nthose ports when OVN associates them to a node.\n\nDepends-On: https://review.opendev.org/c/openstack/neutron/+/882705\n\nCloses-Bug: #2020157\n\n[1] https://review.opendev.org/c/openstack/neutron/+/882705\n\nChange-Id: I9abf987004370d0c3ec6a152bb28ab450a9af2c2\n'}, {'number': 6, 'created': '2023-07-05 11:56:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ovn-bgp-agent/commit/315732c7e385d0f6ca28c3fbb1e159052f0d4b98', 'message': 'Fix issue with virtual ports not being exposed on time\n\nThe requested-chassis information is only added by neutron\nfor """" port types. The virtual ports chassis is decided by OVN\nnot by neutron, and neutron only update that information in the\nperiodic maintenance task.\n\nAs part of [1] information about the virtual port chassis is being\nadded to the external_ids, as well as for """" ports. This patch is\nadapting the NB DB driver to consume this new source of information\nand being able to expose those ports when OVN associates them to a\nnode.\n\nDepends-On: https://review.opendev.org/c/openstack/neutron/+/882705\n\nCloses-Bug: #2020157\n\n[1] https://review.opendev.org/c/openstack/neutron/+/882705\n\nChange-Id: I9abf987004370d0c3ec6a152bb28ab450a9af2c2\n'}, {'number': 7, 'created': '2023-07-05 13:59:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ovn-bgp-agent/commit/f38a327794c3e065a171f6f6bdbfa1eec15a8fb4', 'message': 'Fix issue with virtual ports not being exposed on time\n\nThe requested-chassis information is only added by neutron\nfor """" port types. The virtual ports chassis is decided by OVN\nnot by neutron, and neutron only update that information in the\nperiodic maintenance task.\n\nAs part of [1] information about the virtual port chassis is being\nadded to the external_ids, as well as for """" ports. This patch is\nadapting the NB DB driver to consume this new source of information\nand being able to expose those ports when OVN associates them to a\nnode.\n\nDepends-On: https://review.opendev.org/c/openstack/neutron/+/882705\n\nCloses-Bug: #2020157\n\n[1] https://review.opendev.org/c/openstack/neutron/+/882705\n\nChange-Id: I9abf987004370d0c3ec6a152bb28ab450a9af2c2\n'}, {'number': 8, 'created': '2023-07-05 14:25:09.000000000', 'files': ['ovn_bgp_agent/drivers/openstack/utils/ovn.py', 'ovn_bgp_agent/drivers/openstack/watchers/base_watcher.py', 'ovn_bgp_agent/tests/unit/drivers/openstack/watchers/test_nb_bgp_watcher.py', 'ovn_bgp_agent/tests/unit/drivers/openstack/utils/test_ovn.py', 'ovn_bgp_agent/constants.py', 'ovn_bgp_agent/drivers/openstack/watchers/nb_bgp_watcher.py'], 'web_link': 'https://opendev.org/openstack/ovn-bgp-agent/commit/eaf217d89da5184ff660accda76a8fe1de5a16d4', 'message': 'Fix issue with virtual ports not being exposed on time\n\nThe requested-chassis information is only added by neutron\nfor """" port types. The virtual ports chassis is decided by OVN\nnot by neutron, and neutron only update that information in the\nperiodic maintenance task.\n\nAs part of [1] information about the virtual port chassis is being\nadded to the external_ids, as well as for """" ports. This patch is\nadapting the NB DB driver to consume this new source of information\nand being able to expose those ports when OVN associates them to a\nnode.\n\nDepends-On: https://review.opendev.org/c/openstack/neutron/+/882705\n\nCloses-Bug: #2020157\n\n[1] https://review.opendev.org/c/openstack/neutron/+/882705\n\nChange-Id: I9abf987004370d0c3ec6a152bb28ab450a9af2c2\n'}]",4,883187,eaf217d89da5184ff660accda76a8fe1de5a16d4,26,3,8,23567,,,0,"Fix issue with virtual ports not being exposed on time

The requested-chassis information is only added by neutron
for """" port types. The virtual ports chassis is decided by OVN
not by neutron, and neutron only update that information in the
periodic maintenance task.

As part of [1] information about the virtual port chassis is being
added to the external_ids, as well as for """" ports. This patch is
adapting the NB DB driver to consume this new source of information
and being able to expose those ports when OVN associates them to a
node.

Depends-On: https://review.opendev.org/c/openstack/neutron/+/882705

Closes-Bug: #2020157

[1] https://review.opendev.org/c/openstack/neutron/+/882705

Change-Id: I9abf987004370d0c3ec6a152bb28ab450a9af2c2
",git fetch https://review.opendev.org/openstack/ovn-bgp-agent refs/changes/87/883187/8 && git format-patch -1 --stdout FETCH_HEAD,"['ovn_bgp_agent/drivers/openstack/utils/ovn.py', 'ovn_bgp_agent/drivers/openstack/watchers/base_watcher.py', 'ovn_bgp_agent/constants.py', 'ovn_bgp_agent/drivers/openstack/watchers/nb_bgp_watcher.py']",4,b7361f9238c9021223f4500bc15a09f0a2c5cd60,," current_chassis = self._get_chassis(row) old_chassis = self._get_chassis(old) current_chassis = self._get_chassis(row) if event == self.ROW_DELETE: return current_chassis == self.agent.chassis and row.up if hasattr(old, 'options') or hasattr(old, 'external_ids'): # check chassis change old_chassis = self._get_chassis(old) if old_chassis != self.agent.chassis: return False if not current_chassis or current_chassis != old_chassis: return True current_chassis = self._get_chassis(row) if row.type == constants.OVN_VM_VIF_PORT_TYPE: if hasattr(old, 'options'): # check chassis change old_chassis = self._get_chassis(old) if not old_chassis or current_chassis != old_chassis: return True if hasattr(old, 'external_ids'): # check fips addition old_port_fip = old.external_ids.get( constants.OVN_FIP_EXT_ID_KEY) if not old_port_fip or current_port_fip != old_port_fip: return True elif row.type == constants.OVN_VIRTUAL_VIF_PORT_TYPE: if hasattr(old, 'external_ids'): old_chassis = self._get_chassis(old) old_port_fip = old.external_ids.get( constants.OVN_FIP_EXT_ID_KEY) if (current_chassis != old_chassis or current_port_fip != old_port_fip): return True current_chassis = self._get_chassis(row) if row.type == constants.OVN_VM_VIF_PORT_TYPE: if hasattr(old, 'options'): # check chassis change old_chassis = self._get_chassis(old) if (not old_chassis or old_chassis != self.agent.chassis): return False if current_chassis != old_chassis and current_port_fip: return True # There was no change in chassis, so only progress if the # chassis matches if current_chassis != self.agent.chassis: return False if hasattr(old, 'external_ids'): # check fips deletion old_port_fip = old.external_ids.get( constants.OVN_FIP_EXT_ID_KEY) if not old_port_fip: return False if old_port_fip != current_port_fip: return True elif row.type == constants.OVN_VIRTUAL_VIF_PORT_TYPE: if hasattr(old, 'external_ids'): # check chassis change old_chassis = self._get_chassis(old) if (not old_chassis or old_chassis != self.agent.chassis): return False if current_chassis != old_chassis and current_port_fip: return True # check fips deletion old_port_fip = old.external_ids.get( constants.OVN_FIP_EXT_ID_KEY) if not old_port_fip: return False if old_port_fip != current_port_fip: return True"," current_chassis = row.options.get(constants.OVN_REQUESTED_CHASSIS) old_chassis = old.options.get(constants.OVN_REQUESTED_CHASSIS) current_chassis = row.options.get(constants.OVN_REQUESTED_CHASSIS) if event == self.ROW_DELETE: return current_chassis == self.agent.chassis if hasattr(old, 'options'): # check chassis change old_chassis = old.options.get(constants.OVN_REQUESTED_CHASSIS) if old_chassis != self.agent.chassis: return False if not current_chassis or current_chassis != old_chassis: return True current_chassis = row.options.get(constants.OVN_REQUESTED_CHASSIS) if hasattr(old, 'options'): # check chassis change old_chassis = old.options.get(constants.OVN_REQUESTED_CHASSIS) if not old_chassis or current_chassis != old_chassis: return True if hasattr(old, 'external_ids'): # check fips addition old_port_fip = old.external_ids.get( constants.OVN_FIP_EXT_ID_KEY) if not old_port_fip or current_port_fip != old_port_fip: return True current_chassis = row.options.get(constants.OVN_REQUESTED_CHASSIS) if hasattr(old, 'options'): # check chassis change old_chassis = old.options.get(constants.OVN_REQUESTED_CHASSIS) if (not old_chassis or old_chassis != self.agent.chassis): return False if current_chassis != old_chassis and current_port_fip: return True # There was no change in chassis, so only progress if the # chassis matches if current_chassis != self.agent.chassis: return False if hasattr(old, 'external_ids'): # check fips deletion old_port_fip = old.external_ids.get( constants.OVN_FIP_EXT_ID_KEY) if not old_port_fip: return False if old_port_fip != current_port_fip: return True",86,45
openstack%2Fkeystone~master~I98a68caa5493839f8dea007308bd031693115848,openstack/keystone,master,I98a68caa5493839f8dea007308bd031693115848,WIP sql: Don't rely on globals to configure enginefacades,ABANDONED,2022-06-17 13:38:33.000000000,2023-07-06 09:05:58.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2022-06-17 13:38:33.000000000', 'files': ['keystone/common/sql/core.py', 'keystone/cmd/cli.py', 'keystone/common/sql/upgrades.py', 'keystone/tests/unit/identity/backends/test_sql.py', 'keystone/tests/unit/test_sql_banned_operations.py', 'keystone/tests/unit/test_sql_upgrade.py', 'keystone/tests/unit/ksfixtures/database.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/9033ab3ae915ae110d81b3275a31fcce457d4678', 'message': ""WIP sql: Don't rely on globals to configure enginefacades\n\nI can't figure out what test is causing a conflict with the update\nmigration tests in the alembic migration series but the root cause is\nthe use of global enginefacades. We can't remove the global enginefacade\nsince those things are supposed to be global, but we can rely on monkey\npatching rather than a single enginefacade that we need to patch.\n\nWIP because I've clearly missed something as we're seeing Foreign Key\nConstraint failures when creating projects. I would assume this is\nbecause the parent project isn't being created.\n\nChange-Id: I98a68caa5493839f8dea007308bd031693115848\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n""}]",1,846368,9033ab3ae915ae110d81b3275a31fcce457d4678,4,1,1,15334,,,0,"WIP sql: Don't rely on globals to configure enginefacades

I can't figure out what test is causing a conflict with the update
migration tests in the alembic migration series but the root cause is
the use of global enginefacades. We can't remove the global enginefacade
since those things are supposed to be global, but we can rely on monkey
patching rather than a single enginefacade that we need to patch.

WIP because I've clearly missed something as we're seeing Foreign Key
Constraint failures when creating projects. I would assume this is
because the parent project isn't being created.

Change-Id: I98a68caa5493839f8dea007308bd031693115848
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/keystone refs/changes/68/846368/1 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/common/sql/core.py', 'keystone/cmd/cli.py', 'keystone/common/sql/upgrades.py', 'keystone/tests/unit/identity/backends/test_sql.py', 'keystone/tests/unit/test_sql_banned_operations.py', 'keystone/tests/unit/ksfixtures/database.py', 'keystone/tests/unit/test_sql_upgrade.py']",7,9033ab3ae915ae110d81b3275a31fcce457d4678,remove-sqlalchemy-migrate," self.engine = engine self.engine, self.repo_path, self.min_version, self.engine, self.repo_name, target_repo_version=version, database.initialize_sql_session( self.engine.url, enforce_sqlite_fks=False, ) upgrades.get_db_version(self.engine, 'expand'), upgrades.get_db_version(self.engine, 'data_migration'), upgrades.get_db_version(self.engine, 'contract'), status = checker.check_db_sync_status(self.engine) status = checker.check_db_sync_status(self.engine) status = checker.check_db_sync_status(self.engine) status = checker.check_db_sync_status(self.engine) status = checker.check_db_sync_status(self.engine)"," engine, self.repo_path, self.min_version, self.repo_name, target_repo_version=version, self.sessionmaker = enginefacade.writer.get_sessionmaker() database.initialize_sql_session(self.engine.url, enforce_sqlite_fks=False) # Override keystone's context manager to be oslo.db's global context # manager. sql.core._TESTING_USE_GLOBAL_CONTEXT_MANAGER = True self.addCleanup(setattr, sql.core, '_TESTING_USE_GLOBAL_CONTEXT_MANAGER', False) self.addCleanup(sql.cleanup) upgrades.get_db_version('expand'), upgrades.get_db_version('data_migration'), upgrades.get_db_version('contract'), status = checker.check_db_sync_status() status = checker.check_db_sync_status() status = checker.check_db_sync_status() status = checker.check_db_sync_status() status = checker.check_db_sync_status()",103,127
openstack%2Freleases~master~I2ce9460d630d9af0aed04588d33e666ad4f30a75,openstack/releases,master,I2ce9460d630d9af0aed04588d33e666ad4f30a75,Release neutron 21.1.2 and 22.0.2,MERGED,2023-06-28 06:30:21.000000000,2023-07-06 08:50:51.000000000,2023-07-06 08:50:51.000000000,"[{'_account_id': 308}, {'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 17685}, {'_account_id': 21798}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-06-28 06:30:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/a8308bca43cd686f812434287fd802d0a2c0d462', 'message': 'Release neutron 21.2.0 with important bug fix\n\nThis is required for fix release for bug/886680\n(https://review.opendev.org/c/openstack/neutron/+/886680)\n\nChange-Id: I2ce9460d630d9af0aed04588d33e666ad4f30a75\n'}, {'number': 2, 'created': '2023-06-28 06:59:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/d1f6bd6d2e886a4a16c6c8e384bb5993b4a38c93', 'message': 'Release neutron 21.2.0 and 22.1.0\n\nThis is required for fix release for bug/886680\n(https://review.opendev.org/c/openstack/neutron/+/886680)\n\nChange-Id: I2ce9460d630d9af0aed04588d33e666ad4f30a75\n'}, {'number': 3, 'created': '2023-06-28 14:31:42.000000000', 'files': ['deliverables/zed/neutron.yaml', 'deliverables/antelope/neutron.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/3d35def94b658a5fecade412398ae9caf8a1c7bd', 'message': 'Release neutron 21.1.2 and 22.0.2\n\nThis is required for fix release for bug/886680\n(https://review.opendev.org/c/openstack/neutron/+/886680)\n\nChange-Id: I2ce9460d630d9af0aed04588d33e666ad4f30a75\n'}]",7,887138,3d35def94b658a5fecade412398ae9caf8a1c7bd,20,8,3,12404,,,0,"Release neutron 21.1.2 and 22.0.2

This is required for fix release for bug/886680
(https://review.opendev.org/c/openstack/neutron/+/886680)

Change-Id: I2ce9460d630d9af0aed04588d33e666ad4f30a75
",git fetch https://review.opendev.org/openstack/releases refs/changes/38/887138/2 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/zed/neutron.yaml'],1,a8308bca43cd686f812434287fd802d0a2c0d462,release-neutron, - version: 21.2.0 projects: - repo: openstack/neutron hash: d539460ab59e13e7ac9eced80e871b5055912dc3,,4,0
openstack%2Freleases~master~I62874663f891d0536042af7411048e0205ae635b,openstack/releases,master,I62874663f891d0536042af7411048e0205ae635b,Bobcat-2 release for oslo.utils,MERGED,2023-07-03 09:55:08.000000000,2023-07-06 08:50:46.000000000,2023-07-06 08:50:46.000000000,"[{'_account_id': 308}, {'_account_id': 1131}, {'_account_id': 22348}, {'_account_id': 28522}, {'_account_id': 31245}]","[{'number': 1, 'created': '2023-07-03 09:55:08.000000000', 'files': ['deliverables/bobcat/oslo.utils.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/c16a9595cae7bc8a321cdbc173e2e8324eebe081', 'message': 'Bobcat-2 release for oslo.utils\n\nThis is the Bobcat-2 milestone release for oslo.utils.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: I62874663f891d0536042af7411048e0205ae635b\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",0,887491,c16a9595cae7bc8a321cdbc173e2e8324eebe081,9,5,1,17685,,,0,"Bobcat-2 release for oslo.utils

This is the Bobcat-2 milestone release for oslo.utils.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: I62874663f891d0536042af7411048e0205ae635b
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/91/887491/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/oslo.utils.yaml'],1,c16a9595cae7bc8a321cdbc173e2e8324eebe081,bobcat-milestone-2,releases: - version: 6.2.0 projects: - repo: openstack/oslo.utils hash: 8115085dac49b005b623a74339eddc2bd9e096ce,,5,0
openstack%2Freleases~master~I45bcf5d20445972177e1df7dca3eb53a64b7f932,openstack/releases,master,I45bcf5d20445972177e1df7dca3eb53a64b7f932,Bobcat-2 release for tosca-parser,MERGED,2023-07-03 10:20:05.000000000,2023-07-06 08:50:44.000000000,2023-07-06 08:50:44.000000000,"[{'_account_id': 308}, {'_account_id': 22348}, {'_account_id': 25701}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-03 10:20:05.000000000', 'files': ['deliverables/bobcat/tosca-parser.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/3edc18f15a3e53b12c3680d831df7a981585bb1b', 'message': 'Bobcat-2 release for tosca-parser\n\nThis is the Bobcat-2 milestone release for tosca-parser.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: I45bcf5d20445972177e1df7dca3eb53a64b7f932\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",0,887501,3edc18f15a3e53b12c3680d831df7a981585bb1b,9,4,1,17685,,,0,"Bobcat-2 release for tosca-parser

This is the Bobcat-2 milestone release for tosca-parser.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: I45bcf5d20445972177e1df7dca3eb53a64b7f932
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/01/887501/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/tosca-parser.yaml'],1,3edc18f15a3e53b12c3680d831df7a981585bb1b,bobcat-milestone-2,releases: - version: 2.9.0 projects: - repo: openstack/tosca-parser hash: 6c919b777ddb1d1ffa0758bc25d60d57eedb5d59,,5,0
openstack%2Freleases~master~I49a0d84f09177dfba153bd1ec62c698c77bae4ea,openstack/releases,master,I49a0d84f09177dfba153bd1ec62c698c77bae4ea,Bobcat-2 release for python-ironicclient,MERGED,2023-07-03 10:03:55.000000000,2023-07-06 08:50:42.000000000,2023-07-06 08:50:42.000000000,"[{'_account_id': 308}, {'_account_id': 10342}, {'_account_id': 15519}, {'_account_id': 22348}, {'_account_id': 23851}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-03 10:03:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/ba8ab05b24da13c34977373c3846b7f874bce126', 'message': 'Bobcat-2 release for python-ironicclient\n\nThis is the Bobcat-2 milestone release for python-ironicclient.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: I49a0d84f09177dfba153bd1ec62c698c77bae4ea\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}, {'number': 2, 'created': '2023-07-05 17:37:01.000000000', 'files': ['deliverables/bobcat/python-ironicclient.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/c10ebeefebbb59c7c0ad65cdbbf303057d930fab', 'message': 'Bobcat-2 release for python-ironicclient\n\nThis is the Bobcat-2 milestone release for python-ironicclient.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: I49a0d84f09177dfba153bd1ec62c698c77bae4ea\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",2,887497,c10ebeefebbb59c7c0ad65cdbbf303057d930fab,14,6,2,17685,,,0,"Bobcat-2 release for python-ironicclient

This is the Bobcat-2 milestone release for python-ironicclient.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: I49a0d84f09177dfba153bd1ec62c698c77bae4ea
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/97/887497/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/python-ironicclient.yaml'],1,ba8ab05b24da13c34977373c3846b7f874bce126,bobcat-milestone-2, - version: 5.3.0 projects: - repo: openstack/python-ironicclient hash: dd8e146a4cec8d0f2d2ab3fc40a19e5d3384edcb,,4,0
openstack%2Ftripleo-common~stable%2Ftrain~I04f6ac171b10af7a294819d6248eac641090cc49,openstack/tripleo-common,stable/train,I04f6ac171b10af7a294819d6248eac641090cc49,Only modify the container manifest if the mediaType has changed.,MERGED,2023-07-05 16:42:45.000000000,2023-07-06 08:50:41.000000000,2023-07-06 08:49:44.000000000,"[{'_account_id': 9816}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 16:42:45.000000000', 'files': ['tripleo_common/image/image_uploader.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/464aad0ab84ef8f795905d7a10349593931703ed', 'message': 'Only modify the container manifest if the mediaType has changed.\n\nOnly modify the manifest if the mediaType has actually changed.  This is a\npartial fix for https://bugzilla.redhat.com/show_bug.cgi?id=2213672 where the\njson pretty print with indent=3 causes the SHA256 of the manifest to change\nsince the manifest is not pretty printed the same way in the source registry.\n\nWhen the SHA256 changes, the above bug is triggered since the code does\nnot account for the fact that the httpd type-map file also needs to be\nupdated to use the changed SHA256.\n\nThis will need to be backported to stable/train as well.\n\nChange-Id: I04f6ac171b10af7a294819d6248eac641090cc49\nSigned-off-by: James Slagle <jslagle@redhat.com>\n(cherry picked from commit b0962d2ba09fbb4da33daa328e6a50cac5e3ba05)\n'}]",0,887676,464aad0ab84ef8f795905d7a10349593931703ed,7,2,1,7144,,,0,"Only modify the container manifest if the mediaType has changed.

Only modify the manifest if the mediaType has actually changed.  This is a
partial fix for https://bugzilla.redhat.com/show_bug.cgi?id=2213672 where the
json pretty print with indent=3 causes the SHA256 of the manifest to change
since the manifest is not pretty printed the same way in the source registry.

When the SHA256 changes, the above bug is triggered since the code does
not account for the fact that the httpd type-map file also needs to be
updated to use the changed SHA256.

This will need to be backported to stable/train as well.

Change-Id: I04f6ac171b10af7a294819d6248eac641090cc49
Signed-off-by: James Slagle <jslagle@redhat.com>
(cherry picked from commit b0962d2ba09fbb4da33daa328e6a50cac5e3ba05)
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/76/887676/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_common/image/image_uploader.py'],1,464aad0ab84ef8f795905d7a10349593931703ed,," new_manifest_type = manifest_type set_config_MediaType = False new_layers = [] new_manifest_type = MEDIA_MANIFEST_V2 set_config_MediaType = True new_manifest_type = MEDIA_MANIFEST_V2 elif manifest_type == MEDIA_OCI_INDEX_V1: new_manifest_type = MEDIA_MANIFEST_V2_LIST # Only modify the manifest if the mediaType has actually changed. # This is a partial fix for # https://bugzilla.redhat.com/show_bug.cgi?id=2213672 # where the json pretty print with indent=3 causes the SHA256 of # the manifest to change since the manifest is not pretty printed # the same way in the source registry. if (manifest_type != new_manifest_type or set_config_MediaType or new_layers): manifest['mediaType'] = new_manifest_type manifest_str = json.dumps(manifest, indent=3) new_manifest_type, (image, new_manifest_type, manifest_url)) 'Content-Type': new_manifest_type"," manifest_type = MEDIA_MANIFEST_V2 new_layers = [] manifest_type = MEDIA_MANIFEST_V2 elif manifest_type == MEDIA_OCI_INDEX_V1: manifest_type = MEDIA_MANIFEST_V2_LIST manifest['mediaType'] = manifest_type manifest_str = json.dumps(manifest, indent=3) manifest_type, (image, manifest_type, manifest_url)) 'Content-Type': manifest_type",22,9
openstack%2Freleases~master~Ib46fa1fa2700c2384f57597d8828b184758be614,openstack/releases,master,Ib46fa1fa2700c2384f57597d8828b184758be614,Recommend to use format-yaml for schedule yaml,NEW,2023-07-03 13:54:52.000000000,2023-07-06 08:36:44.000000000,,"[{'_account_id': 308}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 13:54:52.000000000', 'files': ['doc/source/reference/process.rst'], 'web_link': 'https://opendev.org/openstack/releases/commit/0678047ec527946efe8f15ec1715e5d5cc74e5ac', 'message': 'Recommend to use format-yaml for schedule yaml\n\nIn order to check consistency before the patch gets merged.\n\nChange-Id: Ib46fa1fa2700c2384f57597d8828b184758be614\n'}]",0,887519,0678047ec527946efe8f15ec1715e5d5cc74e5ac,3,2,1,17685,,,0,"Recommend to use format-yaml for schedule yaml

In order to check consistency before the patch gets merged.

Change-Id: Ib46fa1fa2700c2384f57597d8828b184758be614
",git fetch https://review.opendev.org/openstack/releases refs/changes/19/887519/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/reference/process.rst'],1,0678047ec527946efe8f15ec1715e5d5cc74e5ac,format-yaml, .. note:: Use ``format-yaml doc/source/$series/schedule.yaml`` before pushing the patch to gerrit to check consistency. ,,5,0
openstack%2Freleases~master~Iedc29697b241eaf003d1a20db1d88c1b6f8075d2,openstack/releases,master,Iedc29697b241eaf003d1a20db1d88c1b6f8075d2,Bobcat-2 release for python-neutronclient,MERGED,2023-07-03 10:11:33.000000000,2023-07-06 08:30:16.000000000,2023-07-06 08:30:16.000000000,"[{'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 28522}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-07-03 10:11:33.000000000', 'files': ['deliverables/bobcat/python-neutronclient.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/b3e3e3bc3ecf2c726f6f6153498a4872f95e4540', 'message': 'Bobcat-2 release for python-neutronclient\n\nThis is the Bobcat-2 milestone release for python-neutronclient.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: Iedc29697b241eaf003d1a20db1d88c1b6f8075d2\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",1,887499,b3e3e3bc3ecf2c726f6f6153498a4872f95e4540,11,6,1,17685,,,0,"Bobcat-2 release for python-neutronclient

This is the Bobcat-2 milestone release for python-neutronclient.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: Iedc29697b241eaf003d1a20db1d88c1b6f8075d2
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/99/887499/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/python-neutronclient.yaml'],1,b3e3e3bc3ecf2c726f6f6153498a4872f95e4540,bobcat-milestone-2, - version: 11.0.0 projects: - repo: openstack/python-neutronclient hash: d4976152401907b94d50a1ec3ad473fbe7c32dad,,4,0
openstack%2Freleases~master~I42f7f568e413cb42d22ea7158d87d27dd20e2fb6,openstack/releases,master,I42f7f568e413cb42d22ea7158d87d27dd20e2fb6,Bobcat-2 release for ovsdbapp,MERGED,2023-07-03 09:56:49.000000000,2023-07-06 08:30:14.000000000,2023-07-06 08:30:14.000000000,"[{'_account_id': 5756}, {'_account_id': 6773}, {'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 28522}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-07-03 09:56:49.000000000', 'files': ['deliverables/bobcat/ovsdbapp.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/50685947088b37849f6021ab3f2b086db4bec228', 'message': 'Bobcat-2 release for ovsdbapp\n\nThis is the Bobcat-2 milestone release for ovsdbapp.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: I42f7f568e413cb42d22ea7158d87d27dd20e2fb6\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",1,887492,50685947088b37849f6021ab3f2b086db4bec228,13,9,1,17685,,,0,"Bobcat-2 release for ovsdbapp

This is the Bobcat-2 milestone release for ovsdbapp.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: I42f7f568e413cb42d22ea7158d87d27dd20e2fb6
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/92/887492/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/ovsdbapp.yaml'],1,50685947088b37849f6021ab3f2b086db4bec228,bobcat-milestone-2, - version: 2.4.0 projects: - repo: openstack/ovsdbapp hash: d542e5cee1c41703c0edb3e206c36c1392a57028,,4,0
openstack%2Freleases~master~I65447a5b4897567c9524e28a049919a2b73ff635,openstack/releases,master,I65447a5b4897567c9524e28a049919a2b73ff635,Bobcat-2 release for python-troveclient,MERGED,2023-07-03 10:14:18.000000000,2023-07-06 08:30:11.000000000,2023-07-06 08:30:11.000000000,"[{'_account_id': 6732}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 26285}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-03 10:14:18.000000000', 'files': ['deliverables/bobcat/python-troveclient.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/caf258893c187ba266760c213d0f1be5a53945f3', 'message': 'Bobcat-2 release for python-troveclient\n\nThis is the Bobcat-2 milestone release for python-troveclient.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: I65447a5b4897567c9524e28a049919a2b73ff635\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",1,887500,caf258893c187ba266760c213d0f1be5a53945f3,9,5,1,17685,,,0,"Bobcat-2 release for python-troveclient

This is the Bobcat-2 milestone release for python-troveclient.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: I65447a5b4897567c9524e28a049919a2b73ff635
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/00/887500/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/python-troveclient.yaml'],1,caf258893c187ba266760c213d0f1be5a53945f3,bobcat-milestone-2,releases: - version: 8.2.0 projects: - repo: openstack/python-troveclient hash: b739fe40ad3720c00dc290ef5c2354169bb54865,,5,0
openstack%2Freleases~master~I82779805f32939fb4fdf404875a41361501d2a78,openstack/releases,master,I82779805f32939fb4fdf404875a41361501d2a78,Bobcat-2 release for octavia-lib,MERGED,2023-07-03 09:36:15.000000000,2023-07-06 08:30:09.000000000,2023-07-06 08:30:09.000000000,"[{'_account_id': 6469}, {'_account_id': 11628}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 28522}, {'_account_id': 29244}]","[{'number': 1, 'created': '2023-07-03 09:36:15.000000000', 'files': ['deliverables/bobcat/octavia-lib.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/c3799f79dcc13a45683ee59768392c7c563c178c', 'message': 'Bobcat-2 release for octavia-lib\n\nThis is the Bobcat-2 milestone release for octavia-lib.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: I82779805f32939fb4fdf404875a41361501d2a78\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",3,887485,c3799f79dcc13a45683ee59768392c7c563c178c,9,6,1,17685,,,0,"Bobcat-2 release for octavia-lib

This is the Bobcat-2 milestone release for octavia-lib.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: I82779805f32939fb4fdf404875a41361501d2a78
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/85/887485/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/octavia-lib.yaml'],1,c3799f79dcc13a45683ee59768392c7c563c178c,bobcat-milestone-2,releases: - version: 3.3.0 projects: - repo: openstack/octavia-lib hash: bad19004b09dd6daf1b6db0a6f96545790a08336,,5,0
openstack%2Freleases~master~If82d7a0224afbdd5542ac5149479e7dd89f56036,openstack/releases,master,If82d7a0224afbdd5542ac5149479e7dd89f56036,Bobcat-2 release for python-cyborgclient,MERGED,2023-07-03 10:00:32.000000000,2023-07-06 08:28:13.000000000,2023-07-06 08:28:13.000000000,"[{'_account_id': 13629}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 26458}, {'_account_id': 28522}, {'_account_id': 31412}]","[{'number': 1, 'created': '2023-07-03 10:00:32.000000000', 'files': ['deliverables/bobcat/python-cyborgclient.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/49acd2f24bfa1db189cd5ed119b04edee1441019', 'message': 'Bobcat-2 release for python-cyborgclient\n\nThis is the Bobcat-2 milestone release for python-cyborgclient.\nClients and libraries following the cycle-with-intermediary release\nmodel are encouraged to release each milestone to make sure updates\nare made available.\n\nIf the team is happy with the current release hash, please +1 this\npatch and we will process the release. If you need to get some\nimportant changes merged first, leave a -1 and update the patch with\na new commit hash to use once it is ready.\n\nIf a release is definitely not wanted at this time, please -1 and\nleave a comment explaining that and we can abandon the patch.\n\nAny patches with no response will be assumed to be OK and will be\nprocessed after July 6th.\n\nChange-Id: If82d7a0224afbdd5542ac5149479e7dd89f56036\nSigned-off-by: Elod Illes <elod.illes@est.tech>\n'}]",1,887494,49acd2f24bfa1db189cd5ed119b04edee1441019,9,6,1,17685,,,0,"Bobcat-2 release for python-cyborgclient

This is the Bobcat-2 milestone release for python-cyborgclient.
Clients and libraries following the cycle-with-intermediary release
model are encouraged to release each milestone to make sure updates
are made available.

If the team is happy with the current release hash, please +1 this
patch and we will process the release. If you need to get some
important changes merged first, leave a -1 and update the patch with
a new commit hash to use once it is ready.

If a release is definitely not wanted at this time, please -1 and
leave a comment explaining that and we can abandon the patch.

Any patches with no response will be assumed to be OK and will be
processed after July 6th.

Change-Id: If82d7a0224afbdd5542ac5149479e7dd89f56036
Signed-off-by: Elod Illes <elod.illes@est.tech>
",git fetch https://review.opendev.org/openstack/releases refs/changes/94/887494/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/python-cyborgclient.yaml'],1,49acd2f24bfa1db189cd5ed119b04edee1441019,bobcat-milestone-2,releases: - version: 2.2.0 projects: - repo: openstack/python-cyborgclient hash: 8a1428b88ad66caa0c5e3fed8e008ff297cd83c9,,5,0
openstack%2Fvenus-tempest-plugin~master~I81e9c935ac52816dda5d87784f8a727380c1ce2b,openstack/venus-tempest-plugin,master,I81e9c935ac52816dda5d87784f8a727380c1ce2b,add venus-tempest job,NEW,2023-03-15 09:14:36.000000000,2023-07-06 08:06:11.000000000,,"[{'_account_id': 22348}, {'_account_id': 26458}, {'_account_id': 27565}, {'_account_id': 30455}, {'_account_id': 31412}]","[{'number': 1, 'created': '2023-03-15 09:14:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/venus-tempest-plugin/commit/e286553baa7a38bcdbe63d99dbcd4aa190a0dd5e', 'message': 'add venus-tempest job\n\nthis patch:\n1、added venus-tempest job to venus\n\nChange-Id: I81e9c935ac52816dda5d87784f8a727380c1ce2b\n'}, {'number': 2, 'created': '2023-03-15 09:19:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/venus-tempest-plugin/commit/17ab221303298325ac61edfa04ab1aee7f587468', 'message': 'add venus-tempest job\n\nthis patch:\n1、added venus-tempest job to venus\n\nChange-Id: I81e9c935ac52816dda5d87784f8a727380c1ce2b\n'}, {'number': 3, 'created': '2023-03-15 09:41:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/venus-tempest-plugin/commit/9bc766f4278d9b12ed5c3b1b37bd9849e72ae8c8', 'message': 'add venus-tempest job\n\nthis patch:\n1、added venus-tempest job to venus\n\nChange-Id: I81e9c935ac52816dda5d87784f8a727380c1ce2b\n'}, {'number': 4, 'created': '2023-03-15 10:10:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/venus-tempest-plugin/commit/b148cc20655b65515b3154da1e7113a1bee19952', 'message': 'add venus-tempest job\n\nthis patch:\n1、added venus-tempest job to venus\n\nChange-Id: I81e9c935ac52816dda5d87784f8a727380c1ce2b\n'}, {'number': 5, 'created': '2023-03-15 10:50:33.000000000', 'files': ['.zuul.yaml', 'venus_tempest_plugin/tests/api/base.py'], 'web_link': 'https://opendev.org/openstack/venus-tempest-plugin/commit/068f30f3fc119dea520871ab33b543ddd18a939f', 'message': 'add venus-tempest job\n\nthis patch:\n1、added venus-tempest job to venus\n\nChange-Id: I81e9c935ac52816dda5d87784f8a727380c1ce2b\n'}]",6,877159,068f30f3fc119dea520871ab33b543ddd18a939f,12,5,5,30409,,,0,"add venus-tempest job

this patch:
1、added venus-tempest job to venus

Change-Id: I81e9c935ac52816dda5d87784f8a727380c1ce2b
",git fetch https://review.opendev.org/openstack/venus-tempest-plugin refs/changes/59/877159/5 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.yaml'],1,e286553baa7a38bcdbe63d99dbcd4aa190a0dd5e,master3, - openstack-python-jobs - check-requirements - release-notes-jobs-python3 - publish-openstack-docs-pti check: jobs: - venus-tempest gate: jobs: - venus-tempest, - noop-jobs,10,1
openstack%2Freleases~master~Id20e2a481e7bc408e74d4ca98a96be0aedb3895a,openstack/releases,master,Id20e2a481e7bc408e74d4ca98a96be0aedb3895a,release oslo.utils 6.2.0,ABANDONED,2023-07-06 07:49:26.000000000,2023-07-06 08:05:46.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-06 07:49:26.000000000', 'files': ['deliverables/bobcat/oslo.utils.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/847e3bac362557385531f572216d80fb283ae762', 'message': 'release oslo.utils 6.2.0\n\nChange-Id: Id20e2a481e7bc408e74d4ca98a96be0aedb3895a\n'}]",0,887763,847e3bac362557385531f572216d80fb283ae762,4,1,1,28522,,,0,"release oslo.utils 6.2.0

Change-Id: Id20e2a481e7bc408e74d4ca98a96be0aedb3895a
",git fetch https://review.opendev.org/openstack/releases refs/changes/63/887763/1 && git format-patch -1 --stdout FETCH_HEAD,['deliverables/bobcat/oslo.utils.yaml'],1,847e3bac362557385531f572216d80fb283ae762,oslo-bobcat,releases: - version: 6.2.0 projects: - repo: openstack/oslo.utils hash: 8115085dac49b005b623a74339eddc2bd9e096ce,,5,0
openstack%2Fcinder~master~Idd202063fe95857f8f459f69d490cded1597a75e,openstack/cinder,master,Idd202063fe95857f8f459f69d490cded1597a75e,Allow Volume Migration with additional extra specs to enter driver code,NEW,2023-01-17 12:16:29.000000000,2023-07-06 08:00:08.000000000,,"[{'_account_id': 9535}, {'_account_id': 20813}, {'_account_id': 22348}, {'_account_id': 30615}, {'_account_id': 34489}, {'_account_id': 35075}]","[{'number': 1, 'created': '2023-01-17 12:16:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/dad6422176ceaddf9f40fa97fdaf4e5d23359fb7', 'message': 'Allow Volume Migration with additional extra specs to enter driver code\n\nMigration of volume with various extra specs other\nthan the volume_backend_name,RESKEY:availability_zones still needs to be\nmigrated from driver specific code. The patch will allow\nthe flow to enter driver code along with additional extra specs.\n\nChange-Id: Idd202063fe95857f8f459f69d490cded1597a75e\n'}, {'number': 2, 'created': '2023-01-19 13:09:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9687820884c6cf1c2244522b218e8c7961f8c4bd', 'message': 'Allow Volume Migration with additional extra specs to enter driver code\n\nMigration of volume with various extra specs other\nthan the volume_backend_name,RESKEY:availability_zones still needs to be\nmigrated from driver specific code. This patch will allow\nthe flow to enter driver code along with additional extra specs.\n\nChange-Id: Idd202063fe95857f8f459f69d490cded1597a75e\n'}, {'number': 3, 'created': '2023-01-20 09:38:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/f1663a546b51db4bded828921f7a5ed206c70f45', 'message': 'Allow Volume Migration with additional extra specs to enter driver code\n\nMigration of volume with various extra specs other\nthan the volume_backend_name,RESKEY:availability_zones still needs to be\nmigrated from driver specific code. This patch will allow\nthe flow to enter driver code along with additional extra specs.\n\nCloses- Bug : https://bugs.launchpad.net/cinder/+bug/2001619\n\nChange-Id: Idd202063fe95857f8f459f69d490cded1597a75e\n'}, {'number': 4, 'created': '2023-01-23 11:26:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1008a6a1460b2b273003b716209212104a38d36a', 'message': 'Allow Volume Migration with additional extra specs to enter driver code\n\nMigration of volume with various extra specs other\nthan the volume_backend_name,RESKEY:availability_zones still needs to be\nmigrated from driver specific code. This patch will allow\nthe flow to enter driver code along with additional extra specs.\n\nCloses- Bug : https://bugs.launchpad.net/cinder/+bug/2001619\n\nChange-Id: Idd202063fe95857f8f459f69d490cded1597a75e\n'}, {'number': 5, 'created': '2023-02-09 16:27:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b91fd4440ba624abba58fa153e2dbe4ade1b5808', 'message': 'Allow Volume Migration with additional extra specs to enter driver code\n\nMigration of volume with various extra specs other\nthan the volume_backend_name,RESKEY:availability_zones still needs to be\nmigrated from driver specific code. This patch will allow\nthe flow to enter driver code along with additional extra specs.\n\nCloses-Bug : #2001619\n\nChange-Id: Idd202063fe95857f8f459f69d490cded1597a75e\n'}, {'number': 6, 'created': '2023-02-10 00:43:49.000000000', 'files': ['cinder/volume/manager.py', 'cinder/volume/driver.py', 'releasenotes/notes/bug-2001619-to-check-extra-specs-from-driver-for-retype-vol-migration-1f7bc9967dfd2287.yaml', 'cinder/tests/unit/volume/test_volume_migration.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/93b47e857c8a44300cf4c5be6e9959840cd8b32d', 'message': 'Allow Volume Migration with additional extra specs to enter driver code\n\nMigration of volume with various extra specs other\nthan the volume_backend_name,RESKEY:availability_zones still needs to be\nmigrated from driver specific code. This patch will allow\nthe flow to enter driver code along with additional extra specs.\n\nCloses-Bug: #2001619\nChange-Id: Idd202063fe95857f8f459f69d490cded1597a75e\n'}]",21,869999,93b47e857c8a44300cf4c5be6e9959840cd8b32d,124,6,6,35679,,,0,"Allow Volume Migration with additional extra specs to enter driver code

Migration of volume with various extra specs other
than the volume_backend_name,RESKEY:availability_zones still needs to be
migrated from driver specific code. This patch will allow
the flow to enter driver code along with additional extra specs.

Closes-Bug: #2001619
Change-Id: Idd202063fe95857f8f459f69d490cded1597a75e
",git fetch https://review.opendev.org/openstack/cinder refs/changes/99/869999/6 && git format-patch -1 --stdout FETCH_HEAD,['cinder/volume/manager.py'],1,dad6422176ceaddf9f40fa97fdaf4e5d23359fb7,bug/2001619," # Migration of volume with various extra specs(other than the # volume_backend_name,RESKEY:availability_zones)still needs to be # migrated from driver specific code. The below condition will allow # the flow to enter driver code along with additional extra specs. if 'volume_migration_enabled' in extra_specs: return True",,6,0
openstack%2Fcyborg-specs~master~I0c95399489163f692c059fdb271dce71149dc9c9,openstack/cyborg-specs,master,I0c95399489163f692c059fdb271dce71149dc9c9,repropose attribute api support spec,NEW,2023-06-07 12:13:40.000000000,2023-07-06 07:58:54.000000000,,"[{'_account_id': 22348}, {'_account_id': 26458}]","[{'number': 1, 'created': '2023-06-07 12:13:40.000000000', 'files': ['specs/2023.2/approved/attribute-api-support.rst'], 'web_link': 'https://opendev.org/openstack/cyborg-specs/commit/7fe220ac79ee6e35db9282b843fb66a907497b82', 'message': 'repropose attribute api support spec\n\nChange-Id: I0c95399489163f692c059fdb271dce71149dc9c9\n'}]",4,885434,7fe220ac79ee6e35db9282b843fb66a907497b82,4,2,1,31412,,,0,"repropose attribute api support spec

Change-Id: I0c95399489163f692c059fdb271dce71149dc9c9
",git fetch https://review.opendev.org/openstack/cyborg-specs refs/changes/34/885434/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/2023.2/approved/attribute-api-support.rst'],1,7fe220ac79ee6e35db9282b843fb66a907497b82,bp/support_attribute_api,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ===================== Support attribute API ===================== This spec adds a new group of APIs to manage the lifecycle of accelerator's attributes. Problem description =================== Attribute is designed for describing customized information of an accelerator. Now they are generated by drivers, users can not add/delete/update them, it's not applicable to our scenarios now. Use Cases --------- An admin or operator needs a group of APIs to manage his accelerator's attributes. Here are some useful scenarios: * For a NIC accelerator, we need to add a phys_net attribute, it's should be created by deployer or other components. * For some Function Volatile Accelerators, we can create the Function name as an attribute. * Also for some information, such as Function_UUID is machine readable. Proposed change =============== None Alternatives ------------ None Data model impact ----------------- * Add attribute object to deployable object. REST API impact --------------- URL: ``/v2/deployable/{uuid}/attribute`` METHOD: ``GET`` List all attributes of specified deployable. Normal response code (200) and body:: { ""attributes"":[{ ""key"":""key1"", ""value"":""value1"", ""uuid"":""uuid1"" } ] } Error response code and body: * 401 (Unauthorized): Unauthorized * 403 (Forbidden): RBAC check failed * No response body URL: ``/v2/deployable/{uuid}/attribute/{uuid_or_key}`` METHOD: ``GET`` GET specified attribute of specified deployable. Query Parameters: None Normal response code (200) and body:: { ""attribute"": { ""key"":""key1"", ""value"":""value1"", ""uuid"":""uuid1"", ""created_at"":""2020-05-28T03:03:20"", ""updated_at"":""2020-05-28T03:03:20"" } } Error response code and body: * 401 (Unauthorized): Unauthorized * 403 (Forbidden): RBAC check failed * 404 (NotFound): No deployable of that UUID or no attribute of that UUID exists * No response body URL: ``/v2/deployable/{uuid}/attribute`` METHOD: ``POST`` Create one or more deployable attribute(s). Request body:: [ { ""key"": ""key1"", ""value"": ""value1"" }, { ""key"": ""key2"", ""value"": ""value2"" }, ... ] Normal response code and body: * 204 (No content) * No response body Error response code: * 401 (Unauthorized): Unauthorized * 403 (Forbidden): RBAC check failed * 409 (Conflict): Bad input or key is not unique Error response body:: {""error"": ""error-string""} URL: ``/v2/deployable/{uuid}/attribute/{uuid_or_key}`` METHOD: ``DELETE`` Delete an exist deployable attribute. Query Parameters: None Normal response code and body: * 204 (No content) * No response body Error response code: * 401 (Unauthorized): Unauthorized * 403 (Forbidden): RBAC check failed * 404 (NotFound): No deployable of that UUID or no attribute of that UUID exists Error response body:: {""error"": ""error-string""} URL: ``/v2/deployable/{uuid}/attribute`` METHOD: ``DELETE`` Delete all attributes of a deployable. Query Parameters: None Normal response code and body: * 204 (No content) * No response body Error response code: * 401 (Unauthorized): Unauthorized * 403 (Forbidden): RBAC check failed Error response body:: {""error"": ""error-string""} URL: ``/v2/deployable/{uuid}/attribute/{uuid_or_key}`` METHOD: ``PUT`` Update an exist deployable attribute. Query Parameters: None Request body (Value of deployable attribute):: {""value"": ""value1""} Normal response code and body: * 204 (No content) * No response body Error response code and body: * 401 (Unauthorized): Unauthorized * 403 (Forbidden): RBAC check failed * 404 (NotFound): No deployable of that UUID or no attribute of that UUID exists Error response body:: {""error"": ""error-string""} Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- * Change Cyborg Attribute table. Performance Impact ------------------ None Other deployer impact --------------------- None Developer impact ---------------- * If the user want to use these feature, they should upgrade their Cyborg * project to latest to support these changes. Implementation ============== Assignee(s) ----------- Primary assignee: hejunli Work Items ---------- * Change Cyborg REST APIs. * Change Cyborg Attribute table. * Change Cyborg deployable object. * Change cyborgclient to support Attribute management action. * Add related tests. Dependencies ============ None Testing ======= Appropriate unit and functional tests should be added. Documentation Impact ==================== * Need a documentation to record microversion history. * Need a documentaiton to explain api usage. References ========== None History ======= .. list-table:: Revisions :header-rows: 1 * - Release Name - Description * - Antelope - Introduced ",,301,0
openstack%2Fcharms.openstack~stable%2F21.10~I491ca9f482a00b7ca3fa44aa8c26ef73559c178f,openstack/charms.openstack,stable/21.10,I491ca9f482a00b7ca3fa44aa8c26ef73559c178f,Use unittest.mock instead of mock,ABANDONED,2023-03-04 05:41:43.000000000,2023-07-06 07:20:02.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-03-04 05:41:43.000000000', 'files': ['unit_tests/charms_openstack/plugins/test_adapters.py', 'test-requirements.txt', 'charms_openstack/test_mocks.py', 'unit_tests/charms_openstack/charm/test_core.py', 'unit_tests/__init__.py', 'unit_tests/charms_openstack/charm/test_defaults.py', 'unit_tests/charms_openstack/charm/utils.py', 'unit_tests/utils.py', 'unit_tests/test_charms_openstack_bus.py', 'unit_tests/test_charms_openstack_devices_pci.py', 'unit_tests/charms_openstack/plugins/test_classes.py', 'unit_tests/test_charms_openstack_adapters.py', 'charms_openstack/test_utils.py', 'unit_tests/charms_openstack/charm/test_classes.py'], 'web_link': 'https://opendev.org/openstack/charms.openstack/commit/5e3300338ae6e4450432794709041f11f460ad32', 'message': 'Use unittest.mock instead of mock\n\nThe mock third party library was needed for mock support in py2\nruntimes. Since we now only support py36 and later, we can use the\nstandard lib unittest.mock module instead.\n\nChange-Id: I491ca9f482a00b7ca3fa44aa8c26ef73559c178f\n(cherry picked from commit 2812f9f6649d809e80bc66bee4a12509fef6bb0d)\n'}]",0,876455,5e3300338ae6e4450432794709041f11f460ad32,3,1,1,5112,,,0,"Use unittest.mock instead of mock

The mock third party library was needed for mock support in py2
runtimes. Since we now only support py36 and later, we can use the
standard lib unittest.mock module instead.

Change-Id: I491ca9f482a00b7ca3fa44aa8c26ef73559c178f
(cherry picked from commit 2812f9f6649d809e80bc66bee4a12509fef6bb0d)
",git fetch https://review.opendev.org/openstack/charms.openstack refs/changes/55/876455/1 && git format-patch -1 --stdout FETCH_HEAD,"['unit_tests/charms_openstack/plugins/test_adapters.py', 'test-requirements.txt', 'charms_openstack/test_mocks.py', 'unit_tests/charms_openstack/charm/test_core.py', 'unit_tests/__init__.py', 'unit_tests/charms_openstack/charm/test_defaults.py', 'unit_tests/charms_openstack/charm/utils.py', 'unit_tests/utils.py', 'unit_tests/test_charms_openstack_bus.py', 'unit_tests/test_charms_openstack_devices_pci.py', 'unit_tests/charms_openstack/plugins/test_classes.py', 'unit_tests/test_charms_openstack_adapters.py', 'charms_openstack/test_utils.py', 'unit_tests/charms_openstack/charm/test_classes.py']",14,5e3300338ae6e4450432794709041f11f460ad32,drop_mock-stable/21.10,from unittest import mock,import mock,13,14
openstack%2Fcinder~master~Ida8b41850bc6dfa8b000f72b71001badb3bb36ea,openstack/cinder,master,Ida8b41850bc6dfa8b000f72b71001badb3bb36ea,Volume State Update Failed After Backup Completed,NEW,2022-11-30 07:55:48.000000000,2023-07-06 06:00:07.000000000,,"[{'_account_id': 22348}, {'_account_id': 30615}]","[{'number': 1, 'created': '2022-11-30 07:55:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/40694eb9729d8b0ccddebf920f8150fdfcb2547a', 'message': ""Generally, after the backup is created, the state of the volume will be reset to the state before the backup. However, if we create a backup of a volume with status 'in-use'(attached to an instance). When backup creating, the instance get deleted, status will be set to in-use by error, but not reset to available.\n\nWhen an instance is deleted, the volume will be detached, and the volume status will be set to available. After that, the volume backup is completed, and the status of the volume is reset to in-use. The bug is caused by this.\n\nThis patch fix this by rechecking attachment status before resetting volume status.\nCloses-Bug: #1996039\n\nChange-Id: Ida8b41850bc6dfa8b000f72b71001badb3bb36ea\n""}, {'number': 2, 'created': '2022-11-30 07:58:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/228fca2c0d6d6d905ffc20f6b415f272c8319fee', 'message': ""Volume State Update Failed After Backup Completed\n\nGenerally, after the backup is created, the state of the volume will be reset to the state before the backup. However, if we create a backup of a volume with status 'in-use'(attached to an instance). When backup creating, the instance get deleted, status will be set to in-use by error, but not reset to available.\n\nWhen an instance is deleted, the volume will be detached, and the volume status will be set to available. After that, the volume backup is completed, and the status of the volume is reset to in-use. The bug is caused by this.\n\nThis patch fix this by rechecking attachment status before resetting volume status.\nCloses-Bug: #1996039\n\nChange-Id: Ida8b41850bc6dfa8b000f72b71001badb3bb36ea\n""}, {'number': 3, 'created': '2022-11-30 08:49:33.000000000', 'files': ['cinder/backup/manager.py', 'cinder/tests/unit/backup/test_backup.py', 'cinder/db/sqlalchemy/api.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/2acbb9286b913a1225f28fe3d877515d5ae60e63', 'message': ""Volume State Update Failed After Backup Completed\n\nGenerally, after the backup is created, the state of the volume will be reset to the state before the backup. However, if we create a backup of a volume with status 'in-use'(attached to an instance). When backup creating, the instance get deleted, status will be set to in-use by error, but not reset to available.\n\nWhen an instance is deleted, the volume will be detached, and the volume status will be set to available. After that, the volume backup is completed, and the status of the volume is reset to in-use. The bug is caused by this.\n\nThis patch fix this by rechecking attachment status before resetting volume status.\nCloses-Bug: #1996039\n\nChange-Id: Ida8b41850bc6dfa8b000f72b71001badb3bb36ea\n""}]",1,866106,2acbb9286b913a1225f28fe3d877515d5ae60e63,28,2,3,35495,,,0,"Volume State Update Failed After Backup Completed

Generally, after the backup is created, the state of the volume will be reset to the state before the backup. However, if we create a backup of a volume with status 'in-use'(attached to an instance). When backup creating, the instance get deleted, status will be set to in-use by error, but not reset to available.

When an instance is deleted, the volume will be detached, and the volume status will be set to available. After that, the volume backup is completed, and the status of the volume is reset to in-use. The bug is caused by this.

This patch fix this by rechecking attachment status before resetting volume status.
Closes-Bug: #1996039

Change-Id: Ida8b41850bc6dfa8b000f72b71001badb3bb36ea
",git fetch https://review.opendev.org/openstack/cinder refs/changes/06/866106/2 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/backup/manager.py', 'cinder/db/sqlalchemy/api.py']",2,40694eb9729d8b0ccddebf920f8150fdfcb2547a,," # Hide volume status update to available on volume migration or upload # or backup, as status is updated later on those flows. or volume.status == 'backing-up'"," # Hide volume status update to available on volume migration or upload, # as status is updated later on those flows.",7,2
openstack%2Fdiskimage-builder~master~I105d5440ee138ad3b751d8b5b7ed4a22863de9a9,openstack/diskimage-builder,master,I105d5440ee138ad3b751d8b5b7ed4a22863de9a9,Create a wildcard InfiniBand connection profile for IB interfaces,ABANDONED,2023-04-16 11:20:07.000000000,2023-07-06 05:46:08.000000000,,"[{'_account_id': 4571}, {'_account_id': 7118}, {'_account_id': 11655}, {'_account_id': 12171}, {'_account_id': 22348}, {'_account_id': 24245}, {'_account_id': 35001}]","[{'number': 1, 'created': '2023-04-16 11:20:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/37d98a71f763fb4ecd2ac7f9f0c2edff2157b38c', 'message': ""Create a wildcard InfiniBand connection profile for IB interfaces\n\nCurrently, NetworkManager can't automatically create default\nconnection profiles for InfiniBand interfaces.\n\nSo, as a workaround, we are installing\nNetworkManager-system-connections-infiniband.nmconnection\nto NetworkManager to create a wildcard InfiniBand connection profile.\n\nThe content of NetworkManager-system-connections-infiniband.nmconnection\nis generated by running this command:\n`nmcli --offline connection add type infiniband connection.multi-connect multiple`\n\nChange-Id: I105d5440ee138ad3b751d8b5b7ed4a22863de9a9\n""}, {'number': 2, 'created': '2023-04-19 07:51:50.000000000', 'files': ['diskimage_builder/elements/dhcp-all-interfaces/README.rst', 'releasenotes/notes/bug-2016965-be1dafbe1ee03647.yaml', 'diskimage_builder/elements/dhcp-all-interfaces/install.d/NetworkManager-system-connections-infiniband.nmconnection', 'diskimage_builder/elements/dhcp-all-interfaces/install.d/50-dhcp-all-interfaces'], 'web_link': 'https://opendev.org/openstack/diskimage-builder/commit/0720af0bd645ea4082942bb576741b2eff59e69a', 'message': ""Create a wildcard InfiniBand connection profile for IB interfaces\n\nCurrently, NetworkManager can't automatically create default\nconnection profiles for InfiniBand interfaces.\n\nSo, as a workaround, we are installing\nNetworkManager-system-connections-infiniband.nmconnection\nto NetworkManager to create a wildcard InfiniBand connection profile.\n\nThe content of NetworkManager-system-connections-infiniband.nmconnection\nis generated by running this command:\n`nmcli --offline connection add type infiniband connection.multi-connect multiple`\n\nCloses-Bug: #2016965\nChange-Id: I105d5440ee138ad3b751d8b5b7ed4a22863de9a9\n""}]",6,880567,0720af0bd645ea4082942bb576741b2eff59e69a,20,7,2,25241,,,0,"Create a wildcard InfiniBand connection profile for IB interfaces

Currently, NetworkManager can't automatically create default
connection profiles for InfiniBand interfaces.

So, as a workaround, we are installing
NetworkManager-system-connections-infiniband.nmconnection
to NetworkManager to create a wildcard InfiniBand connection profile.

The content of NetworkManager-system-connections-infiniband.nmconnection
is generated by running this command:
`nmcli --offline connection add type infiniband connection.multi-connect multiple`

Closes-Bug: #2016965
Change-Id: I105d5440ee138ad3b751d8b5b7ed4a22863de9a9
",git fetch https://review.opendev.org/openstack/diskimage-builder refs/changes/67/880567/2 && git format-patch -1 --stdout FETCH_HEAD,"['diskimage_builder/elements/dhcp-all-interfaces/install.d/NetworkManager-system-connections-infiniband.nmconnection', 'diskimage_builder/elements/dhcp-all-interfaces/install.d/50-dhcp-all-interfaces']",2,37d98a71f763fb4ecd2ac7f9f0c2edff2157b38c,," # Because NetworkManager can't automatically create default connection # profiles for InfiniBand interfaces, we are installing # NetworkManager-system-connections-infiniband.nmconnection to # NetworkManager to create a wildcard InfiniBand connection profile install -D -g root -o root -m 0600 ${SCRIPTDIR}/NetworkManager-system-connections-infiniband.nmconnection /etc/NetworkManager/system-connections/infiniband.nmconnection ",,24,0
openstack%2Fcinder~master~I0943e0078e1409eba1f89935239e37b92d0d2725,openstack/cinder,master,I0943e0078e1409eba1f89935239e37b92d0d2725,Fujitsu Driver: Modify extend volume,NEW,2022-08-05 06:29:45.000000000,2023-07-06 03:00:08.000000000,,"[{'_account_id': 22348}, {'_account_id': 30615}]","[{'number': 1, 'created': '2022-08-05 06:29:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6c76889904853b441005da05e097692a0babc07c', 'message': 'Fujitsu Driver: Modify extend volume\n\nThe logic of volume extension has been modified.\n\nWhen extending volumes, diffrent processes will be performed according to the type of pool.\n\nChange-Id: I0943e0078e1409eba1f89935239e37b92d0d2725\n'}, {'number': 2, 'created': '2022-11-21 01:08:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c91d4470ab313c346a981a8c2a8fe883d416d158', 'message': 'Fujitsu Driver: Modify extend volume\n\nThe logic of volume extension has been modified.\n\nWhen extending volumes, diffrent processes will be performed according to the type of pool.\n\nChange-Id: I0943e0078e1409eba1f89935239e37b92d0d2725\n'}, {'number': 3, 'created': '2022-11-22 05:47:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/8e6e6c461c972745b5956dbbd11f3d176cdac64a', 'message': 'Fujitsu Driver: Modify extend volume\n\nThe logic of volume extension has been modified.\n\nWhen extending volumes, diffrent processes will be performed according to the type of pool.\n\nChange-Id: I0943e0078e1409eba1f89935239e37b92d0d2725\n'}, {'number': 4, 'created': '2022-12-15 08:54:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/ef2ca74fc55d594bcc8095ac976cbea0b78d2b7f', 'message': 'Fujitsu Driver: Modify extend volume\n\nThe logic of volume extension has been modified.\n\nWhen extending volumes, diffrent processes will be performed according to the type of pool.\n\nChange-Id: I0943e0078e1409eba1f89935239e37b92d0d2725\n'}, {'number': 5, 'created': '2023-03-27 08:35:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/e052d355af72fd51bb04f8723df26c02535df6ad', 'message': 'Fujitsu Driver: Modify extend volume\n\nThe logic of volume extension has been modified.\n\nWhen extending volumes, diffrent processes will be performed according to the type of pool.\n\nChange-Id: I0943e0078e1409eba1f89935239e37b92d0d2725\n'}, {'number': 6, 'created': '2023-03-28 01:40:37.000000000', 'files': ['cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_cli.py', 'cinder/tests/unit/volume/drivers/test_fujitsu_dx.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_fc.py', 'releasenotes/notes/fujitsu-modify-extend-9e09b6910acd7029.yaml', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_iscsi.py', 'cinder/volume/drivers/fujitsu/eternus_dx/constants.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/d4c7aeddde25a54c5b21a250c9357a257f658424', 'message': 'Fujitsu Driver: Modify extend volume\n\nThe logic of volume extension has been modified.\n\nWhen extending volumes, diffrent processes will be performed according to the type of pool.\n\nChange-Id: I0943e0078e1409eba1f89935239e37b92d0d2725\n'}]",11,852007,d4c7aeddde25a54c5b21a250c9357a257f658424,129,2,6,33609,,,0,"Fujitsu Driver: Modify extend volume

The logic of volume extension has been modified.

When extending volumes, diffrent processes will be performed according to the type of pool.

Change-Id: I0943e0078e1409eba1f89935239e37b92d0d2725
",git fetch https://review.opendev.org/openstack/cinder refs/changes/07/852007/6 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_cli.py', 'cinder/tests/unit/volume/drivers/test_fujitsu_dx.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_fc.py', 'cinder/volume/drivers/fujitsu/eternus_dx/constants.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_common.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_iscsi.py']",6,6c76889904853b441005da05e097692a0babc07c,fujitsu-driver-update," model_update = self.common.create_volume(volume) return model_update return {'provider_location': str(element_path), 'metadata': metadata} return {'provider_location': str(element_path), 'metadata': metadata} self.common.delete_volume(volume) return {'provider_location': str(element_path)} self.common.delete_snapshot(snapshot) self.common.terminate_connection(volume, connector) 'pool name: %s.', pool_name) LOG.debug('extend_volume, ' 'volume id: %s, Enter method.', volume['id']) LOG.debug('extend_volume, ' 'used pool name: %s, Exit method.', used_pool_name)","import six LOG.info('create_volume, volume id: %s, Enter method.', volume['id']) element_path, metadata = self.common.create_volume(volume) v_metadata = volume.get('volume_metadata') if v_metadata: for data in v_metadata: metadata[data['key']] = data['value'] else: v_metadata = volume.get('metadata', {}) metadata.update(v_metadata) LOG.info('create_volume, info: %s, Exit method.', metadata) return {'provider_location': six.text_type(element_path), 'metadata': metadata} LOG.info('create_volume_from_snapshot, ' 'volume id: %(vid)s, snap id: %(sid)s, Enter method.', {'vid': volume['id'], 'sid': snapshot['id']}) LOG.info('create_volume_from_snapshot, ' 'info: %s, Exit method.', metadata) return {'provider_location': six.text_type(element_path), 'metadata': metadata} LOG.info('create_cloned_volume, ' 'target volume id: %(tid)s, ' 'source volume id: %(sid)s, Enter method.', {'tid': volume['id'], 'sid': src_vref['id']}) LOG.info('create_cloned_volume, info: %s, Exit method.', metadata) return {'provider_location': six.text_type(element_path), 'metadata': metadata} LOG.info('delete_volume, volume id: %s, Enter method.', volume['id']) vol_exist = self.common.delete_volume(volume) LOG.info('delete_volume, delete: %s, Exit method.', vol_exist) return LOG.info('create_snapshot, snap id: %(sid)s, volume id: %(vid)s, ' 'Enter method.', {'sid': snapshot['id'], 'vid': snapshot['volume_id']}) LOG.info('create_snapshot, info: %s, Exit method.', metadata) return {'provider_location': six.text_type(element_path)} LOG.info('delete_snapshot, snap id: %(sid)s, volume id: %(vid)s, ' 'Enter method.', {'sid': snapshot['id'], 'vid': snapshot['volume_id']}) vol_exist = self.common.delete_snapshot(snapshot) LOG.info('delete_snapshot, delete: %s, Exit method.', vol_exist) return LOG.info('initialize_connection, volume id: %(vid)s, ' 'initiator: %(initiator)s, Enter method.', {'vid': volume['id'], 'initiator': connector['initiator']}) LOG.info('initialize_connection, info: %s, Exit method.', info) initiator = connector.get('initiator') if connector else None LOG.info('terminate_connection, volume id: %(vid)s, ' 'initiator: %(initiator)s, Enter method.', {'vid': volume['id'], 'initiator': initiator}) map_exist = self.common.terminate_connection(volume, connector) LOG.info('terminate_connection, unmap: %s, Exit method.', map_exist) return LOG.debug('get_volume_stats, refresh: %s, Enter method.', refresh) 'pool name: %s, Exit method.', pool_name) LOG.info('extend_volume, volume id: %s, Enter method.', volume['id']) LOG.info('extend_volume, used pool name: %s, Exit method.', used_pool_name)",754,518
openstack%2Fcharm-cinder-k8s~stable%2F2023.1.1~I562374a15eb607090ee35b47819190a9aedee482,openstack/charm-cinder-k8s,stable/2023.1.1,I562374a15eb607090ee35b47819190a9aedee482,2023.1.1 release updates,MERGED,2023-07-05 14:43:42.000000000,2023-07-06 02:34:33.000000000,2023-07-06 02:08:18.000000000,"[{'_account_id': 10366}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 14:43:42.000000000', 'files': ['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml'], 'web_link': 'https://opendev.org/openstack/charm-cinder-k8s/commit/7b12bb3c49ca63394e43d7d96543bcf3d52e61db', 'message': '2023.1.1 release updates\n\nUpdate publishing target to push to candidate channel.\n\nRefresh bundles to deploy from candidate channel for testing.\n\nUpdate .gitreview default branch.\n\nChange-Id: I562374a15eb607090ee35b47819190a9aedee482\n'}]",0,887705,7b12bb3c49ca63394e43d7d96543bcf3d52e61db,7,2,1,935,,,0,"2023.1.1 release updates

Update publishing target to push to candidate channel.

Refresh bundles to deploy from candidate channel for testing.

Update .gitreview default branch.

Change-Id: I562374a15eb607090ee35b47819190a9aedee482
",git fetch https://review.opendev.org/openstack/charm-cinder-k8s refs/changes/05/887705/1 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml']",3,7b12bb3c49ca63394e43d7d96543bcf3d52e61db,release/2023.1.1, channel: 3.9/candidate channel: 2023.1/candidate, channel: 3.9/edge channel: 2023.1/edge,4,4
openstack%2Fcharm-nova-k8s~stable%2F2023.1.1~I1c79a4d74ad88425c40ea72e3dd9d2b14f51365f,openstack/charm-nova-k8s,stable/2023.1.1,I1c79a4d74ad88425c40ea72e3dd9d2b14f51365f,2023.1.1 release updates,MERGED,2023-07-05 14:58:04.000000000,2023-07-06 02:32:25.000000000,2023-07-06 02:08:17.000000000,"[{'_account_id': 10366}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 14:58:04.000000000', 'files': ['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml'], 'web_link': 'https://opendev.org/openstack/charm-nova-k8s/commit/40e880c203eab4da5594c4f1fd8c800f1e7dd738', 'message': '2023.1.1 release updates\n\nUpdate publishing target to push to candidate channel.\n\nRefresh bundles to deploy from candidate channel for testing.\n\nUpdate .gitreview default branch.\n\nChange-Id: I1c79a4d74ad88425c40ea72e3dd9d2b14f51365f\n'}]",0,887717,40e880c203eab4da5594c4f1fd8c800f1e7dd738,7,2,1,935,,,0,"2023.1.1 release updates

Update publishing target to push to candidate channel.

Refresh bundles to deploy from candidate channel for testing.

Update .gitreview default branch.

Change-Id: I1c79a4d74ad88425c40ea72e3dd9d2b14f51365f
",git fetch https://review.opendev.org/openstack/charm-nova-k8s refs/changes/17/887717/1 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml']",3,40e880c203eab4da5594c4f1fd8c800f1e7dd738,release/2023.1.1, channel: 3.9/candidate channel: 2023.1/candidate channel: 2023.1/candidate, channel: 3.9/edge channel: 2023.1/edge channel: 2023.1/edge,5,5
openstack%2Fcharm-cinder-ceph-k8s~stable%2F2023.1.1~I6398606791dd11863f5f33e9f018d76e3ed0402e,openstack/charm-cinder-ceph-k8s,stable/2023.1.1,I6398606791dd11863f5f33e9f018d76e3ed0402e,2023.1.1 release updates,MERGED,2023-07-05 14:40:29.000000000,2023-07-06 02:29:29.000000000,2023-07-06 02:08:19.000000000,"[{'_account_id': 10366}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 14:40:29.000000000', 'files': ['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml'], 'web_link': 'https://opendev.org/openstack/charm-cinder-ceph-k8s/commit/a4eec1921bfc5f2a3166c165a55643ac0296ff21', 'message': '2023.1.1 release updates\n\nUpdate publishing target to push to candidate channel.\n\nRefresh bundles to deploy from candidate channel for testing.\n\nUpdate .gitreview default branch.\n\nChange-Id: I6398606791dd11863f5f33e9f018d76e3ed0402e\n'}]",0,887703,a4eec1921bfc5f2a3166c165a55643ac0296ff21,8,3,1,935,,,0,"2023.1.1 release updates

Update publishing target to push to candidate channel.

Refresh bundles to deploy from candidate channel for testing.

Update .gitreview default branch.

Change-Id: I6398606791dd11863f5f33e9f018d76e3ed0402e
",git fetch https://review.opendev.org/openstack/charm-cinder-ceph-k8s refs/changes/03/887703/1 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml']",3,a4eec1921bfc5f2a3166c165a55643ac0296ff21,release/2023.1.1, channel: 3.9/candidate channel: 2023.1/candidate channel: 2023.1/candidate, channel: 3.9/edge channel: 2023.1/edge channel: 2023.1/edge,5,5
openstack%2Fcharm-placement-k8s~stable%2F2023.1.1~I806b1f0401f978394231861dd0d390801d8aa7e7,openstack/charm-placement-k8s,stable/2023.1.1,I806b1f0401f978394231861dd0d390801d8aa7e7,2023.1.1 release updates,MERGED,2023-07-05 14:59:53.000000000,2023-07-06 02:27:15.000000000,2023-07-06 02:10:42.000000000,"[{'_account_id': 10366}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 14:59:53.000000000', 'files': ['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml'], 'web_link': 'https://opendev.org/openstack/charm-placement-k8s/commit/e3f2d453d5c31a58657bfa2907478c0b0479bea1', 'message': '2023.1.1 release updates\n\nUpdate publishing target to push to candidate channel.\n\nRefresh bundles to deploy from candidate channel for testing.\n\nUpdate .gitreview default branch.\n\nChange-Id: I806b1f0401f978394231861dd0d390801d8aa7e7\n'}]",0,887718,e3f2d453d5c31a58657bfa2907478c0b0479bea1,7,2,1,935,,,0,"2023.1.1 release updates

Update publishing target to push to candidate channel.

Refresh bundles to deploy from candidate channel for testing.

Update .gitreview default branch.

Change-Id: I806b1f0401f978394231861dd0d390801d8aa7e7
",git fetch https://review.opendev.org/openstack/charm-placement-k8s refs/changes/18/887718/1 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml']",3,e3f2d453d5c31a58657bfa2907478c0b0479bea1,release/2023.1.1, channel: 8.0/stable channel: 3.9/candidate channel: 2023.1/candidate channel: 2023.1/candidate, channel: edge channel: 3.9/edge channel: 2023.1/edge channel: 2023.1/edge,6,6
openstack%2Fcharm-keystone-k8s~stable%2F2023.1.1~Ia39724a6579841f7ef8282766990446ae466c1c8,openstack/charm-keystone-k8s,stable/2023.1.1,Ia39724a6579841f7ef8282766990446ae466c1c8,2023.1.1 release updates,MERGED,2023-07-05 14:48:44.000000000,2023-07-06 02:24:53.000000000,2023-07-06 01:57:57.000000000,"[{'_account_id': 10366}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 14:48:44.000000000', 'files': ['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml'], 'web_link': 'https://opendev.org/openstack/charm-keystone-k8s/commit/d5ff9362c041e8d9e910be4edd2aeb98bb89f816', 'message': '2023.1.1 release updates\n\nUpdate publishing target to push to candidate channel.\n\nRefresh bundles to deploy from candidate channel for testing.\n\nUpdate .gitreview default branch.\n\nChange-Id: Ia39724a6579841f7ef8282766990446ae466c1c8\n'}]",0,887708,d5ff9362c041e8d9e910be4edd2aeb98bb89f816,7,2,1,935,,,0,"2023.1.1 release updates

Update publishing target to push to candidate channel.

Refresh bundles to deploy from candidate channel for testing.

Update .gitreview default branch.

Change-Id: Ia39724a6579841f7ef8282766990446ae466c1c8
",git fetch https://review.opendev.org/openstack/charm-keystone-k8s refs/changes/08/887708/1 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml']",3,d5ff9362c041e8d9e910be4edd2aeb98bb89f816,release/2023.1.1, channel: 3.9/candidate channel: 2023.1/candidate, channel: 3.9/edge channel: 2023.1/edge,4,4
openstack%2Fcharm-neutron-k8s~stable%2F2023.1.1~Iaa6e08331ab95b0391518aa7d5971b5ee84af8d1,openstack/charm-neutron-k8s,stable/2023.1.1,Iaa6e08331ab95b0391518aa7d5971b5ee84af8d1,2023.1.1 release updates,MERGED,2023-07-05 14:50:24.000000000,2023-07-06 02:20:36.000000000,2023-07-06 02:06:52.000000000,"[{'_account_id': 10366}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 14:50:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-k8s/commit/e91cb5b04114441a3b2912c01d64d9c5ba491298', 'message': '2023.1.1 release updates\n\nUpdate publishing target to push to candidate channel.\n\nRefresh bundles to deploy from candidate channel for testing.\n\nUpdate .gitreview default branch.\n\nChange-Id: Iaa6e08331ab95b0391518aa7d5971b5ee84af8d1\n'}, {'number': 2, 'created': '2023-07-05 15:09:25.000000000', 'files': ['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml'], 'web_link': 'https://opendev.org/openstack/charm-neutron-k8s/commit/138f434d1d2753bb2e602146d10b4273f79a435c', 'message': '2023.1.1 release updates\n\nUpdate publishing target to push to candidate channel.\n\nRefresh bundles to deploy from candidate channel for testing.\n\nUpdate .gitreview default branch.\n\nChange-Id: Iaa6e08331ab95b0391518aa7d5971b5ee84af8d1\n'}]",0,887709,138f434d1d2753bb2e602146d10b4273f79a435c,9,2,2,935,,,0,"2023.1.1 release updates

Update publishing target to push to candidate channel.

Refresh bundles to deploy from candidate channel for testing.

Update .gitreview default branch.

Change-Id: Iaa6e08331ab95b0391518aa7d5971b5ee84af8d1
",git fetch https://review.opendev.org/openstack/charm-neutron-k8s refs/changes/09/887709/2 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml']",3,e91cb5b04114441a3b2912c01d64d9c5ba491298,release/2023.1.1, channel: 3.9/candidate channel: yoga/candidate channel: 23.03/candidate channel: candidate, channel: 3.9/edge channel: yoga/edge channel: 23.03/edge channel: edge,6,6
openstack%2Fcharm-horizon-k8s~stable%2F2023.1.1~I8f4bceb4481143e4ecbb6bac3ebef1e88f5793cd,openstack/charm-horizon-k8s,stable/2023.1.1,I8f4bceb4481143e4ecbb6bac3ebef1e88f5793cd,2023.1.1 release updates,MERGED,2023-07-05 14:46:50.000000000,2023-07-06 02:19:22.000000000,2023-07-06 02:03:28.000000000,"[{'_account_id': 10366}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 14:46:50.000000000', 'files': ['osci.yaml', '.gitreview'], 'web_link': 'https://opendev.org/openstack/charm-horizon-k8s/commit/6d979cda095e0786b06d77187c4b6ee7369580c2', 'message': '2023.1.1 release updates\n\nUpdate publishing target to push to candidate channel.\n\nUpdate .gitreview default branch.\n\nChange-Id: I8f4bceb4481143e4ecbb6bac3ebef1e88f5793cd\n'}]",0,887707,6d979cda095e0786b06d77187c4b6ee7369580c2,8,3,1,935,,,0,"2023.1.1 release updates

Update publishing target to push to candidate channel.

Update .gitreview default branch.

Change-Id: I8f4bceb4481143e4ecbb6bac3ebef1e88f5793cd
",git fetch https://review.opendev.org/openstack/charm-horizon-k8s refs/changes/07/887707/1 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', '.gitreview']",2,6d979cda095e0786b06d77187c4b6ee7369580c2,release/2023.1.1,defaultbranch=stable/2023.1.1,defaultbranch=main,2,2
openstack%2Fcharm-glance-k8s~stable%2F2023.1.1~I35198159c8e6ba40a6e83261c83e0d0b147967fa,openstack/charm-glance-k8s,stable/2023.1.1,I35198159c8e6ba40a6e83261c83e0d0b147967fa,2023.1.1 release updates,MERGED,2023-07-05 14:45:04.000000000,2023-07-06 02:17:55.000000000,2023-07-06 01:58:12.000000000,"[{'_account_id': 10366}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 14:45:04.000000000', 'files': ['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml'], 'web_link': 'https://opendev.org/openstack/charm-glance-k8s/commit/2fd7c868b6c432b9648e42fe521842fef19dacd5', 'message': '2023.1.1 release updates\n\nUpdate publishing target to push to candidate channel.\n\nRefresh bundles to deploy from candidate channel for testing.\n\nUpdate .gitreview default branch.\n\nChange-Id: I35198159c8e6ba40a6e83261c83e0d0b147967fa\n'}]",0,887706,2fd7c868b6c432b9648e42fe521842fef19dacd5,7,2,1,935,,,0,"2023.1.1 release updates

Update publishing target to push to candidate channel.

Refresh bundles to deploy from candidate channel for testing.

Update .gitreview default branch.

Change-Id: I35198159c8e6ba40a6e83261c83e0d0b147967fa
",git fetch https://review.opendev.org/openstack/charm-glance-k8s refs/changes/06/887706/1 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', '.gitreview', 'tests/bundles/smoke.yaml']",3,2fd7c868b6c432b9648e42fe521842fef19dacd5,release/2023.1.1, channel: 3.9/candidate channel: 2023.1/candidate, channel: 3.9/edge channel: 2023.1/edge,4,4
openstack%2Fnova-specs~master~Idcfa51a9854202a0c627c3cfd370d29c4357cbd9,openstack/nova-specs,master,Idcfa51a9854202a0c627c3cfd370d29c4357cbd9,"Re-propose ""Add maxphysaddr support for Libvirt"" for 2023.2 Bobcat",MERGED,2023-03-28 09:54:52.000000000,2023-07-06 01:43:17.000000000,2023-07-06 01:42:10.000000000,"[{'_account_id': 7166}, {'_account_id': 9708}, {'_account_id': 11604}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-03-28 09:54:52.000000000', 'files': ['specs/2023.2/approved/libvirt-maxphysaddr-support.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/518e3357c28759e20a92add1175de721e1571241', 'message': 'Re-propose ""Add maxphysaddr support for Libvirt"" for 2023.2 Bobcat\n\nblueprint: libvirt-maxphysaddr-support\nSigned-off-by: Nobuhiro MIKI <nmiki@yahoo-corp.jp>\nChange-Id: Idcfa51a9854202a0c627c3cfd370d29c4357cbd9\n'}]",4,878753,518e3357c28759e20a92add1175de721e1571241,13,4,1,31652,,,0,"Re-propose ""Add maxphysaddr support for Libvirt"" for 2023.2 Bobcat

blueprint: libvirt-maxphysaddr-support
Signed-off-by: Nobuhiro MIKI <nmiki@yahoo-corp.jp>
Change-Id: Idcfa51a9854202a0c627c3cfd370d29c4357cbd9
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/53/878753/1 && git format-patch -1 --stdout FETCH_HEAD,['specs/2023.2/approved/libvirt-maxphysaddr-support.rst'],1,518e3357c28759e20a92add1175de721e1571241,bp/libvirt-maxphysaddr-support,".. This work is licensed under a Creative Commons Attribution 3.0 Unported License. http://creativecommons.org/licenses/by/3.0/legalcode ========================================== Add maxphysaddr support for Libvirt ========================================== https://blueprints.launchpad.net/nova/+spec/libvirt-maxphysaddr-support This blueprint propose new flavor extra_specs to control the physical address bits of vCPUs in Libvirt guests. Problem description =================== When booting a guest with 1TB+ RAM, the default physical address bits are too small and the boot fails [1]_. So a knob is needed to specify the appropriate physical address bits. Use Cases --------- Booting a guest with large RAM. Proposed change =============== In Libvirt v8.7.0+ and QEMU v2.7.0+, physical address bits can be specified with following XML elements [2]_ [3]_. The former means to adopt any physical address bits, the latter means to adopt the physical address bits of the host CPU. - ``<maxphysaddr mode='emulate' bits='42'/>`` - ``<maxphysaddr mode='passthrough'/>`` Flavor extra_specs ----------------------------------------------- Here I suggest the following two flavor extra_specs. Of course, if these are omitted, the behavior is the same as before. - ``hw:maxphysaddr_mode`` can be either ``emulate`` or ``passthrough``. - ``hw:maxphysaddr_bits`` takes a positive integer value. Only meaningful and must be specified if ``hw:maxphysaddr_mode=emulate``. Nova scheduler changes ---------------------- Nova scheduler also needs to be modified to take these two properties into account. There can be a mix of supported and unsupported hosts depending on Libvirt and QEMU versions. So add new traits ``COMPUTE_ADDRESS_SPACE_PASSTHROUGH`` and ``COMPUTE_ADDRESS_SPACE_EMULATED`` to check the scheduled host supports this feature. ``trait:COMPUTE_ADDRESS_SPACE_PASSTHROUGH=required`` is automatically added if ``hw:maxphysaddr_mode=passthrough`` is specified in flavor extra_specs. And same for ``hw:maxphysaddr_mode=emulate``. Passthrough and emulate modes have different properties. So let's consider the two separately. The case of ``hw:maxphysaddr_mode=passthrough``. In this case, ``cpu_mode=host-passthrough`` is a requirement, which is already taken into account in nova scheduling, and no additional modifications are required in this proposal. It is not guaranteed whether the instance can be migrated by nova. So the admin needs to make sure that targets of cold and live migration have similar hardware and software. This restriction is similar for ``cpu_mode=host-passthrough``. The case of ``hw:maxphysaddr_mode=emulate``. In nova scheduling, it is necessary to check that the hypervisor supports at least ``hw:maxphysaddr_bits``. The maximum number of bits supported by hypervisor can be obtained by using libvirt capabilities [4]_. Therefore, ``ComputeCapabilitiesFilter`` can be used to compare the number of bits in scheduling. For example, this can be accomplished by adding ``capabilities:cpu_info:maxphysaddr:bits>=42`` automatically. Cold migration and live migration can also be realized with this filter and ``COMPUTE_ADDRESS_SPACE_EMULATED`` trait. So the overall flavor extra_specs look like the following:: openstack flavor set <flavor> \ --property hw:maxphysaddr_mode=emulate \ --property hw:maxphysaddr_bits=42 .. note:: Since ComputeCapabilitiesFilter only supports flavor extra_specs and not image properties [5]_, this proposal is out of scope for image properties. Alternatives ------------ Before the ``maxphysaddr`` option was introduced into Libvirt, it was specified as a workaround with the QEMU comanndline parameter. But this alternative is not allowed in nova. Also, some Linux distributions may have machine types with ``host-phys-bits=true`` [6]_. For example, ``pc-i440fx-bionic-hpb`` and ``pc-q35-bionic-hpb``. However, this alternative has following two issues and cannot be adopted for general-purpose use cases. - Ubuntu package maintainers are applying a patch to QEMU [7]_. It means this is not included in vanilla QEMU and is not available in other distributions. - This is only the case for ``hw:maxphysaddr_mode=passthrough`` and does not include ``hw:maxphysaddr_mode=emulate``. Since ``hw:maxphysaddr_mode=passthrough`` requires ``cpu_mode=host-passthrough`` to be used [8]_, this alternative cannot be used with ``cpu_mode=custom`` or ``cpu_mode=host-model``. So, this alternative is not sufficient for a cloud with many different CPU models. As for scheduling, placement does not currently support numeric traits, so the maximum number of bits supported by hypervisor cannot be checked by this mechanism. Data model impact ----------------- None REST API impact --------------- None Security impact --------------- None Notifications impact -------------------- None Other end user impact --------------------- None Performance Impact ------------------ None Other deployer impact --------------------- Operators should specify appropriate flavor extra_specs as needed. Developer impact ---------------- None Upgrade impact -------------- As described earlier, the new traits ``COMPUTE_ADDRESS_SPACE_PASSTHROUGH`` and ``COMPUTE_ADDRESS_SPACE_EMULATED`` signal if the upgraded compute nodes support this feature. Implementation ============== Assignee(s) ----------- Primary assignee: nmiki Other contributors: None Feature Liaison --------------- Feature liaison: Liaison Needed Work Items ---------- * Add new guest configs * Add new fileds in nova/api/validation/extra_specs/hw.py * Add new fields in LibvirtConfigCPU in nova/virt/livbirt/config.py * Add new traits to check Libvirt and QEMU versions * Add new field ``maxphysaddr`` to ``cpu_info`` in nova/virt/libvirt/driver.py * Add docs and release notes for new flavor extra_specs Dependencies ============ Libivrt v8.7.0+. QEMU v2.7.0+. Testing ======= Add the following unit tests: - check that proposed flavor extra_specs are properly validated - check that intended XML elements are output - check that traits are properly added and used - check that new field in ``ComputeCapabilitiesFilter`` is property added and used Documentation Impact ==================== For operators, the documentation describes what proposed flavor extra_specs mean and how they should be set. References ========== .. [1] https://bugs.launchpad.net/ubuntu/+source/libvirt/+bug/1769053 .. [2] https://libvirt.org/news.html#v8-7-0-2022-09-01 .. [3] https://github.com/libvirt/libvirt/commit/1c1a7cdd4096c59fb0c374529e1e5aea8d43ee9c .. [4] https://libvirt.org/formatcaps.html#examples .. [5] https://docs.openstack.org/nova/latest/admin/scheduling.html#computecapabilitiesfilter .. [6] https://cpaelzer.github.io/blogs/005-guests-bigger-than-1tb/ .. [7] https://git.launchpad.net/~paelzer/ubuntu/+source/qemu/commit/?id=6ba8b5c843d405e1b067dc8b98ecb8545af78a2b .. [8] https://github.com/libvirt/libvirt/blob/v8.7.0/src/qemu/qemu_validate.c#L346-L351 History ======= .. list-table:: Revisions :header-rows: 1 * - Release Name - Description * - 2023.1 Antelope - Introduced * - 2023.2 Bobcat - Reproposed ",,239,0
openstack%2Fnova-specs~master~I345570605eb3ed5f2e08f7e3660d9f5d91f4734b,openstack/nova-specs,master,I345570605eb3ed5f2e08f7e3660d9f5d91f4734b,Fix build failures caused by changes in Pillow v10.0.0,ABANDONED,2023-07-05 09:44:48.000000000,2023-07-06 01:25:59.000000000,,"[{'_account_id': 7166}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 09:44:48.000000000', 'files': ['doc/requirements.txt'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/649f44213e62ad5659b6dde9f04e6ecb0682914a', 'message': 'Fix build failures caused by changes in Pillow v10.0.0\n\nThis is due to the removal of the ImageDraw.textsize() method from\nversion 10.0.0 or later [1] in Pillow package on which nova-specs\nbuilds depend. This was expected and is not a problem in itself.\n\nWhen building nova-specs, we use Pillow package via blockdiag\npackage. A patch to fix this has already been proposed for blockdiag [2],\nbut there is no indication that it will be merged. Fixing blockdiag\npackage is the legitimate way to go, but I have not found an active\nmaintainer, so this patch fix it.\n\n[1]: https://pillow.readthedocs.io/en/stable/deprecations.html#font-size-and-offset-methods\n[2]: https://github.com/blockdiag/blockdiag/pull/171\n\nSigned-off-by: Nobuhiro MIKI <nmiki@yahoo-corp.jp>\nChange-Id: I345570605eb3ed5f2e08f7e3660d9f5d91f4734b\n'}]",4,887661,649f44213e62ad5659b6dde9f04e6ecb0682914a,8,2,1,31652,,,0,"Fix build failures caused by changes in Pillow v10.0.0

This is due to the removal of the ImageDraw.textsize() method from
version 10.0.0 or later [1] in Pillow package on which nova-specs
builds depend. This was expected and is not a problem in itself.

When building nova-specs, we use Pillow package via blockdiag
package. A patch to fix this has already been proposed for blockdiag [2],
but there is no indication that it will be merged. Fixing blockdiag
package is the legitimate way to go, but I have not found an active
maintainer, so this patch fix it.

[1]: https://pillow.readthedocs.io/en/stable/deprecations.html#font-size-and-offset-methods
[2]: https://github.com/blockdiag/blockdiag/pull/171

Signed-off-by: Nobuhiro MIKI <nmiki@yahoo-corp.jp>
Change-Id: I345570605eb3ed5f2e08f7e3660d9f5d91f4734b
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/61/887661/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/requirements.txt'],1,649f44213e62ad5659b6dde9f04e6ecb0682914a,add_ver_constraint_pillow,Pillow<=9.5.0,,1,0
openstack%2Ftrove~master~Ide59528d9526fdf1c741f0749ca2263721374c65,openstack/trove,master,Ide59528d9526fdf1c741f0749ca2263721374c65,update requirements,ABANDONED,2023-07-05 07:07:56.000000000,2023-07-06 01:18:52.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-05 07:07:56.000000000', 'files': ['requirements.txt'], 'web_link': 'https://opendev.org/openstack/trove/commit/44de13cc61eab48e474a336e7862c3e975f2d2f9', 'message': 'update requirements\n\nAdd flask and pyroute2 module in requirements\n\nChange-Id: Ide59528d9526fdf1c741f0749ca2263721374c65\n'}]",0,887653,44de13cc61eab48e474a336e7862c3e975f2d2f9,4,1,1,26285,,,0,"update requirements

Add flask and pyroute2 module in requirements

Change-Id: Ide59528d9526fdf1c741f0749ca2263721374c65
",git fetch https://review.opendev.org/openstack/trove refs/changes/53/887653/1 && git format-patch -1 --stdout FETCH_HEAD,['requirements.txt'],1,44de13cc61eab48e474a336e7862c3e975f2d2f9,, # for trove network driver Flask>=2.2.3 # BSD pyroute2>=0.7.7;sys_platform!='win32' # Apache-2.0 (+ dual licensed GPL2),,4,0
openstack%2Fpython-novaclient~master~I47d3b572247cc63e696c3feb0062fff8a633a55c,openstack/python-novaclient,master,I47d3b572247cc63e696c3feb0062fff8a633a55c,Do exact-matching when finding one instance by name,ABANDONED,2023-06-29 11:50:03.000000000,2023-07-06 01:18:03.000000000,,"[{'_account_id': 679}, {'_account_id': 22348}, {'_account_id': 34564}]","[{'number': 1, 'created': '2023-06-29 11:50:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-novaclient/commit/4d3fc0066169d0744c922c1fa68ace0571667357', 'message': 'Do exact-matching when finding one instance by name\n\nWhen we search an instance by its display_name, Nova API considered\nthe display_name as a regular expression.\n\nhttps://docs.openstack.org/api-ref/compute/?expanded=list-servers-detail\n> name (Optional)\n> query\n> string\n> Filters the response by a server name, as a string. You can use regular expressions in the query.\n\nHowever, in the current implementation, nova cli filter the\nresponse of the Nova API by exact-matching, not regular expression\nmatching. Because of this inconsistency, we cannot find instances whose\ndisplay_name contains special characters which has special meanings in\nregular expression, such as (){}[]?*, etc.\n\nThis change escapes these special characters before calling the Nova\nAPI. By this change, we can find instances by its display_name even if\nit contains these special characters.\n\nCloses-Bug: #2025358\nChange-Id: I47d3b572247cc63e696c3feb0062fff8a633a55c\n'}, {'number': 2, 'created': '2023-06-30 08:13:50.000000000', 'files': ['novaclient/base.py', 'novaclient/v2/servers.py', 'novaclient/tests/unit/v2/test_shell.py'], 'web_link': 'https://opendev.org/openstack/python-novaclient/commit/2b601b4fa834d79948e14a2e36beddca06c56b0c', 'message': 'Do exact-matching when finding one instance by name\n\nWhen we search an instance by its display_name, Nova API considered\nthe display_name as a regular expression.\n\nhttps://docs.openstack.org/api-ref/compute/?expanded=list-servers-detail\n> name (Optional)\n> query\n> string\n> Filters the response by a server name, as a string. You can use regular expressions in the query.\n\nHowever, in the current implementation, nova cli filter the\nresponse of the Nova API by exact-matching, not regular expression\nmatching. Because of this inconsistency, we cannot find instances whose\ndisplay_name contains special characters which has special meanings in\nregular expression, such as (){}[]?*, etc.\n\nThis change escapes these special characters before calling the Nova\nAPI. By this change, we can find instances by its display_name even if\nit contains these special characters.\n\nCloses-Bug: #2025358\nChange-Id: I47d3b572247cc63e696c3feb0062fff8a633a55c\n'}]",0,887268,2b601b4fa834d79948e14a2e36beddca06c56b0c,6,3,2,34564,,,0,"Do exact-matching when finding one instance by name

When we search an instance by its display_name, Nova API considered
the display_name as a regular expression.

https://docs.openstack.org/api-ref/compute/?expanded=list-servers-detail
> name (Optional)
> query
> string
> Filters the response by a server name, as a string. You can use regular expressions in the query.

However, in the current implementation, nova cli filter the
response of the Nova API by exact-matching, not regular expression
matching. Because of this inconsistency, we cannot find instances whose
display_name contains special characters which has special meanings in
regular expression, such as (){}[]?*, etc.

This change escapes these special characters before calling the Nova
API. By this change, we can find instances by its display_name even if
it contains these special characters.

Closes-Bug: #2025358
Change-Id: I47d3b572247cc63e696c3feb0062fff8a633a55c
",git fetch https://review.opendev.org/openstack/python-novaclient refs/changes/68/887268/2 && git format-patch -1 --stdout FETCH_HEAD,"['novaclient/base.py', 'novaclient/v2/servers.py', 'novaclient/tests/unit/v2/test_shell.py']",3,4d3fc0066169d0744c922c1fa68ace0571667357,bug/2025358," self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) self.assert_called('GET', '/servers?name=sample%5C-server', pos=0) '/servers?all_tenants=1&name=sample%5C-server', pos=0) '/servers?all_tenants=1&name=sample%5C-server', pos=0) '/servers?deleted=True&name=sample%5C-server', pos=0) '/servers?name=sample%5C-server', pos=-6) '/servers?name=sample%5C-server2', '/servers?all_tenants=1&name=sample%5C-server', pos=0) '/servers?all_tenants=1&name=sample%5C-server2', '/servers?all_tenants=1&name=sample%5C-server', pos=0)"," self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) self.assert_called('GET', '/servers?name=sample-server', pos=0) '/servers?all_tenants=1&name=sample-server', pos=0) '/servers?all_tenants=1&name=sample-server', pos=0) '/servers?deleted=True&name=sample-server', pos=0) '/servers?name=sample-server', pos=-6) '/servers?name=sample-server2', '/servers?all_tenants=1&name=sample-server', pos=0) '/servers?all_tenants=1&name=sample-server2', '/servers?all_tenants=1&name=sample-server', pos=0)",45,25
openstack%2Fopenstacksdk~master~I9c83f044e6ff36138b0788207912fe5d5a518980,openstack/openstacksdk,master,I9c83f044e6ff36138b0788207912fe5d5a518980,Do exact-matching when finding one instance by name,ABANDONED,2023-06-28 02:43:31.000000000,2023-07-06 01:18:01.000000000,,"[{'_account_id': 22348}, {'_account_id': 34564}]","[{'number': 1, 'created': '2023-06-28 02:43:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/8e60a65cf0d8963b93027b9d49de602f3851334f', 'message': 'Do exact-matching when finding one instance by name\n\nWhen we search an instance by its display_name, Nova API considered\nthe display_name as a regular expression.\n\nhttps://docs.openstack.org/api-ref/compute/?expanded=list-servers-detail\n> name (Optional)\n> query\n> string\n> Filters the response by a server name, as a string. You can use regular expressions in the query.\n\nHowever, in the current implementation, openstack cli filter the\nresponse of the Nova API by exact-matching, not regular expression\nmatching. Because of this inconsistency, we cannot find instances whose\ndisplay_name contains special characters which has special meanings in\nregular expression, such as (){}[]?*, etc.\n\nThis change escapes these special characters before calling the Nova\nAPI. By this change, we can find instances by its display_name even if\nit contains these special characters.\n\nCloses-Bug: #2025143\nChange-Id: I9c83f044e6ff36138b0788207912fe5d5a518980\n'}, {'number': 2, 'created': '2023-06-28 09:07:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/e07230ffda41391b4687ab0c8e55ad37a67b2314', 'message': 'Do exact-matching when finding one instance by name\n\nWhen we search an instance by its display_name, Nova API considered\nthe display_name as a regular expression.\n\nhttps://docs.openstack.org/api-ref/compute/?expanded=list-servers-detail\n> name (Optional)\n> query\n> string\n> Filters the response by a server name, as a string. You can use regular expressions in the query.\n\nHowever, in the current implementation, openstack cli filter the\nresponse of the Nova API by exact-matching, not regular expression\nmatching. Because of this inconsistency, we cannot find instances whose\ndisplay_name contains special characters which has special meanings in\nregular expression, such as (){}[]?*, etc.\n\nThis change escapes these special characters before calling the Nova\nAPI. By this change, we can find instances by its display_name even if\nit contains these special characters.\n\nCloses-Bug: #2025143\nChange-Id: I9c83f044e6ff36138b0788207912fe5d5a518980\n'}, {'number': 3, 'created': '2023-06-29 10:36:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/f03c6005b90d0a1fa11eaf0d7570bec73f46ad8d', 'message': 'Do exact-matching when finding one instance by name\n\nWhen we search an instance by its display_name, Nova API considered\nthe display_name as a regular expression.\n\nhttps://docs.openstack.org/api-ref/compute/?expanded=list-servers-detail\n> name (Optional)\n> query\n> string\n> Filters the response by a server name, as a string. You can use regular expressions in the query.\n\nHowever, in the current implementation, openstack cli filter the\nresponse of the Nova API by exact-matching, not regular expression\nmatching. Because of this inconsistency, we cannot find instances whose\ndisplay_name contains special characters which has special meanings in\nregular expression, such as (){}[]?*, etc.\n\nThis change escapes these special characters before calling the Nova\nAPI. By this change, we can find instances by its display_name even if\nit contains these special characters.\n\nStory: 2010809\nTask: 48311\nChange-Id: I9c83f044e6ff36138b0788207912fe5d5a518980\n'}, {'number': 4, 'created': '2023-06-29 12:26:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/458b9322abe77924fb69834cb696241757df4d09', 'message': 'Do exact-matching when finding one instance by name\n\nWhen we search an instance by its display_name, Nova API considered\nthe display_name as a regular expression.\n\nhttps://docs.openstack.org/api-ref/compute/?expanded=list-servers-detail\n> name (Optional)\n> query\n> string\n> Filters the response by a server name, as a string. You can use regular expressions in the query.\n\nHowever, in the current implementation, compute.find_server filters the\nresponse of the Nova API by exact-matching, not regular expression\nmatching. Because of this inconsistency, we cannot find instances whose\ndisplay_name contains special characters which has special meanings in\nregular expression, such as (){}[]?*, etc.\n\nThis change escapes these special characters before calling the Nova\nAPI. By this change, we can find instances by its display_name even if\nit contains these special characters.\n\nStory: 2010809\nTask: 48311\nChange-Id: I9c83f044e6ff36138b0788207912fe5d5a518980\n'}, {'number': 5, 'created': '2023-06-29 12:30:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/61f1090dbab92581648c9ea704c0d789beb1ad58', 'message': 'Do exact-matching when finding one instance by name\n\nWhen we search an instance by its display_name, Nova API considered\nthe display_name as a regular expression.\n\nhttps://docs.openstack.org/api-ref/compute/?expanded=list-servers-detail\n> name (Optional)\n> query\n> string\n> Filters the response by a server name, as a string. You can use regular expressions in the query.\n\nHowever, in the current implementation, compute.find_server filters the\nresponse of the Nova API by exact-matching, not regular expression\nmatching. Because of this inconsistency, we cannot find instances whose\ndisplay_name contains special characters which has special meanings in\nregular expression, such as (){}[]?*, etc.\n\nThis change escapes these special characters before calling the Nova\nAPI. By this change, we can find instances by its display_name even if\nit contains these special characters.\n\nStory: 2010809\nTask: 48311\nChange-Id: I9c83f044e6ff36138b0788207912fe5d5a518980\n'}, {'number': 6, 'created': '2023-06-30 08:13:49.000000000', 'files': ['openstack/tests/unit/cloud/test_update_server.py', 'openstack/compute/v2/server.py', 'openstack/resource.py'], 'web_link': 'https://opendev.org/openstack/openstacksdk/commit/dde0eca51b012d1bbf58c325341a4c156896fd0f', 'message': 'Do exact-matching when finding one instance by name\n\nWhen we search an instance by its display_name, Nova API considered\nthe display_name as a regular expression.\n\nhttps://docs.openstack.org/api-ref/compute/?expanded=list-servers-detail\n> name (Optional)\n> query\n> string\n> Filters the response by a server name, as a string. You can use regular expressions in the query.\n\nHowever, in the current implementation, compute.find_server filters the\nresponse of the Nova API by exact-matching, not regular expression\nmatching. Because of this inconsistency, we cannot find instances whose\ndisplay_name contains special characters which has special meanings in\nregular expression, such as (){}[]?*, etc.\n\nThis change escapes these special characters before calling the Nova\nAPI. By this change, we can find instances by its display_name even if\nit contains these special characters.\n\nStory: 2010809\nTask: 48311\nChange-Id: I9c83f044e6ff36138b0788207912fe5d5a518980\n'}]",1,887127,dde0eca51b012d1bbf58c325341a4c156896fd0f,12,2,6,34564,,,0,"Do exact-matching when finding one instance by name

When we search an instance by its display_name, Nova API considered
the display_name as a regular expression.

https://docs.openstack.org/api-ref/compute/?expanded=list-servers-detail
> name (Optional)
> query
> string
> Filters the response by a server name, as a string. You can use regular expressions in the query.

However, in the current implementation, compute.find_server filters the
response of the Nova API by exact-matching, not regular expression
matching. Because of this inconsistency, we cannot find instances whose
display_name contains special characters which has special meanings in
regular expression, such as (){}[]?*, etc.

This change escapes these special characters before calling the Nova
API. By this change, we can find instances by its display_name even if
it contains these special characters.

Story: 2010809
Task: 48311
Change-Id: I9c83f044e6ff36138b0788207912fe5d5a518980
",git fetch https://review.opendev.org/openstack/openstacksdk refs/changes/27/887127/1 && git format-patch -1 --stdout FETCH_HEAD,['openstack/compute/v2/server.py'],1,8e60a65cf0d8963b93027b9d49de602f3851334f,bug/2025143,"import re @classmethod def find( cls, session, name_or_id, ignore_missing=True, list_base_path=None, *, microversion=None, all_projects=None, **params, ): session = cls._get_session(session) # Try to short-circuit by looking directly for a matching ID. try: match = cls.existing( id=name_or_id, connection=session._get_connection(), **params, ) return match.fetch(session, microversion=microversion, **params) except ( exceptions.NotFoundException, exceptions.BadRequestException, exceptions.ForbiddenException, ): # NOTE(gtema): There are few places around openstack that return # 400 if we try to GET resource and it doesn't exist. pass if list_base_path: params['base_path'] = list_base_path # all_projects is a special case that is used by multiple services. We # handle it here since it doesn't make sense to pass it to the .fetch # call above if all_projects is not None: params['all_projects'] = all_projects if ( 'name' in cls._query_mapping._mapping.keys() and 'name' not in params ): params['name'] = re.escape(name_or_id) data = cls.list(session, **params) result = cls._get_one_match(name_or_id, data) if result is not None: return result if ignore_missing: return None raise exceptions.ResourceNotFound( ""No %s found for %s"" % (cls.__name__, name_or_id) ) ",,61,0
openstack%2Fcinder~master~Id6d4219ea2be797e4da6c43b3a950983f3fc951f,openstack/cinder,master,Id6d4219ea2be797e4da6c43b3a950983f3fc951f,WIP: db: Use same session in tests,NEW,2023-06-27 12:00:57.000000000,2023-07-06 00:58:05.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-06-27 12:00:57.000000000', 'files': ['cinder/tests/unit/test.py', 'cinder/tests/unit/db/test_purge.py', 'cinder/db/sqlalchemy/api.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/2391df3971283f9c79f7f4cd30713a647db8c239', 'message': 'WIP: db: Use same session in tests\n\nI have yet to figure out what is going on here, but when using\nSQLAlchemy 2.0 we seem to end up with separate sessions. This is problem\nfor SQLite in memory. Potential solutions include closing out what we\nhave done here or moving the tests to MySQL/PostgreSQL.\n\nChange-Id: Id6d4219ea2be797e4da6c43b3a950983f3fc951f\nSigned-off-by: Stephen Finucane <stephenfin@redhat.com>\n'}]",0,887033,2391df3971283f9c79f7f4cd30713a647db8c239,14,1,1,15334,,,0,"WIP: db: Use same session in tests

I have yet to figure out what is going on here, but when using
SQLAlchemy 2.0 we seem to end up with separate sessions. This is problem
for SQLite in memory. Potential solutions include closing out what we
have done here or moving the tests to MySQL/PostgreSQL.

Change-Id: Id6d4219ea2be797e4da6c43b3a950983f3fc951f
Signed-off-by: Stephen Finucane <stephenfin@redhat.com>
",git fetch https://review.opendev.org/openstack/cinder refs/changes/33/887033/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/unit/test.py', 'cinder/tests/unit/db/test_purge.py', 'cinder/db/sqlalchemy/api.py']",3,2391df3971283f9c79f7f4cd30713a647db8c239,sqlalchemy-20," main_context_manager.configure( sqlite_fk=True, **dict(conf.database), ) # metadata = MetaData() # metadata.reflect(context.session.get_bind())", main_context_manager.configure(**dict(conf.database)),23,10
openstack%2Fkeystone~master~Iea594efe528318cfc168a06ed8c00eaf6d9483d1,openstack/keystone,master,Iea594efe528318cfc168a06ed8c00eaf6d9483d1,sql: Delay importing SQL modules,MERGED,2023-04-06 10:48:29.000000000,2023-07-06 00:24:16.000000000,2023-07-06 00:23:01.000000000,"[{'_account_id': 7414}, {'_account_id': 14250}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-04-06 10:48:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/4eb0ff0e0b6fcce0c710fcd8dba530d57300a136', 'message': 'sql: Delay importing SQL modules\n\nThese need access to CONF variables. We need to delay importing until\nafter the singleton is configured.\n\nChange-Id: Iea594efe528318cfc168a06ed8c00eaf6d9483d1\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}, {'number': 2, 'created': '2023-06-27 09:58:52.000000000', 'files': ['keystone/common/sql/migrations/manage.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/a5544ea3377fc0abc285c24b4007322a68c46771', 'message': 'sql: Delay importing SQL modules\n\nThese need access to CONF variables. We need to delay importing until\nafter the singleton is configured.\n\nChange-Id: Iea594efe528318cfc168a06ed8c00eaf6d9483d1\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}]",2,879733,a5544ea3377fc0abc285c24b4007322a68c46771,19,3,2,15334,,,0,"sql: Delay importing SQL modules

These need access to CONF variables. We need to delay importing until
after the singleton is configured.

Change-Id: Iea594efe528318cfc168a06ed8c00eaf6d9483d1
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/keystone refs/changes/33/879733/2 && git format-patch -1 --stdout FETCH_HEAD,['keystone/common/sql/migrations/manage.py'],1,4eb0ff0e0b6fcce0c710fcd8dba530d57300a136,sqlalchemy-20,"import sysdef import_sql_modules(): # We need to import all of these so the tables are registered. It would be # easier if these were all in a central location :( import keystone.application_credential.backends.sql # noqa: F401 import keystone.assignment.backends.sql # noqa: F401 import keystone.assignment.role_backends.sql_model # noqa: F401 import keystone.catalog.backends.sql # noqa: F401 import keystone.credential.backends.sql # noqa: F401 import keystone.endpoint_policy.backends.sql # noqa: F401 import keystone.federation.backends.sql # noqa: F401 import keystone.identity.backends.sql_model # noqa: F401 import keystone.identity.mapping_backends.sql # noqa: F401 import keystone.limit.backends.sql # noqa: F401 import keystone.oauth1.backends.sql # noqa: F401 import keystone.policy.backends.sql # noqa: F401 import keystone.resource.backends.sql_model # noqa: F401 import keystone.resource.config_backends.sql # noqa: F401 import keystone.revoke.backends.sql # noqa: F401 import keystone.trust.backends.sql # noqa: F401 def main(argv): keystone.conf.set_default_for_default_log_levels() user_supplied_config_file = False if argv: for argument in argv: if argument == '--config-file': user_supplied_config_file = True if not CONF.default_config_files and not user_supplied_config_file: LOG.warning('Config file not found, using default configs.') import_sql_modules() main(sys.argv)",# We need to import all of these so the tables are registered. It would be # easier if these were all in a central location :( import keystone.application_credential.backends.sql # noqa: F401 import keystone.assignment.backends.sql # noqa: F401 import keystone.assignment.role_backends.sql_model # noqa: F401 import keystone.catalog.backends.sql # noqa: F401 import keystone.credential.backends.sql # noqa: F401 import keystone.endpoint_policy.backends.sql # noqa: F401 import keystone.federation.backends.sql # noqa: F401 import keystone.identity.backends.sql_model # noqa: F401 import keystone.identity.mapping_backends.sql # noqa: F401 import keystone.limit.backends.sql # noqa: F401 import keystone.oauth1.backends.sql # noqa: F401 import keystone.policy.backends.sql # noqa: F401 import keystone.resource.backends.sql_model # noqa: F401 import keystone.resource.config_backends.sql # noqa: F401 import keystone.revoke.backends.sql # noqa: F401 import keystone.trust.backends.sql # noqa: F401 def main(): main(),37,21
openstack%2Fmagnum~stable%2F2023.1~Ibc5090ac5d30534f278da3b1a694b796dbdfd937,openstack/magnum,stable/2023.1,Ibc5090ac5d30534f278da3b1a694b796dbdfd937,Upgrade system monitoring to use kube-prometheus-stack helm chart,NEW,2023-07-05 23:45:24.000000000,2023-07-06 00:23:00.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-05 23:45:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/e524accce7c50c4cc210a457d5b95306c80e5b3e', 'message': '[catalystcloud] Upgrade system monitoring to use kube-prometheus-stack helm chart\n\n* Add variables to specify kube-prometheus-stack chart tag version\n* Additional variables to specify dependency versions\n* Add fragment to include kube-prometheus-stack chart in\n  heat-config-deploy script\n* Change conditional to make make kube-prometheus-stack and\n  prometheus-operator charts mutually exclusive of one another.\n  Prometheus-operator helm chart should run only if kube-prometheus-stack\n  chart tag is not defined.\n\nChange-Id: Ibc5090ac5d30534f278da3b1a694b796dbdfd937\n'}, {'number': 2, 'created': '2023-07-05 23:47:38.000000000', 'files': ['doc/source/user/index.rst', 'magnum/tests/unit/drivers/test_template_definition.py', 'doc/source/user/monitoring.rst', 'magnum/drivers/k8s_fedora_coreos_v1/templates/kubecluster.yaml', 'magnum/drivers/k8s_fedora_coreos_v1/templates/kubemaster.yaml', 'magnum/drivers/common/templates/kubernetes/fragments/write-heat-params-master.sh', 'magnum/tests/unit/conductor/handlers/test_k8s_cluster_conductor.py', 'magnum/drivers/heat/k8s_template_def.py', 'magnum/drivers/common/templates/kubernetes/helm/prometheus-operator.sh', 'magnum/drivers/heat/k8s_fedora_template_def.py', 'magnum/drivers/common/templates/kubernetes/helm/kube-prometheus-stack.sh', 'magnum/drivers/common/templates/kubernetes/helm/prometheus-adapter.sh'], 'web_link': 'https://opendev.org/openstack/magnum/commit/7bb3a171a938b538f04bd8229fc837367e5300be', 'message': 'Upgrade system monitoring to use kube-prometheus-stack helm chart\n\n* Add variables to specify kube-prometheus-stack chart tag version\n* Additional variables to specify dependency versions\n* Add fragment to include kube-prometheus-stack chart in\n  heat-config-deploy script\n* Change conditional to make make kube-prometheus-stack and\n  prometheus-operator charts mutually exclusive of one another.\n  Prometheus-operator helm chart should run only if kube-prometheus-stack\n  chart tag is not defined.\n\nChange-Id: Ibc5090ac5d30534f278da3b1a694b796dbdfd937\n'}]",0,887745,7bb3a171a938b538f04bd8229fc837367e5300be,3,1,2,35921,,,0,"Upgrade system monitoring to use kube-prometheus-stack helm chart

* Add variables to specify kube-prometheus-stack chart tag version
* Additional variables to specify dependency versions
* Add fragment to include kube-prometheus-stack chart in
  heat-config-deploy script
* Change conditional to make make kube-prometheus-stack and
  prometheus-operator charts mutually exclusive of one another.
  Prometheus-operator helm chart should run only if kube-prometheus-stack
  chart tag is not defined.

Change-Id: Ibc5090ac5d30534f278da3b1a694b796dbdfd937
",git fetch https://review.opendev.org/openstack/magnum refs/changes/45/887745/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/user/index.rst', 'magnum/tests/unit/drivers/test_template_definition.py', 'doc/source/user/monitoring.rst', 'magnum/drivers/k8s_fedora_coreos_v1/templates/kubecluster.yaml', 'magnum/drivers/k8s_fedora_coreos_v1/templates/kubemaster.yaml', 'magnum/drivers/common/templates/kubernetes/fragments/write-heat-params-master.sh', 'magnum/tests/unit/conductor/handlers/test_k8s_cluster_conductor.py', 'magnum/drivers/heat/k8s_template_def.py', 'magnum/drivers/common/templates/kubernetes/helm/prometheus-operator.sh', 'magnum/drivers/heat/k8s_fedora_template_def.py', 'magnum/drivers/common/templates/kubernetes/helm/kube-prometheus-stack.sh', 'magnum/drivers/common/templates/kubernetes/helm/prometheus-adapter.sh']",12,e524accce7c50c4cc210a457d5b95306c80e5b3e,upgrade-kube-prometheus-stack, repository: ${CONTAINER_INFRA_PREFIX:-k8s.gcr.io/prometheus-adapter/}prometheus-adapter-${ARCH} tag: ${PROMETHEUS_ADAPTER_TAG:-v0.10.0}, repository: ${CONTAINER_INFRA_PREFIX:-docker.io/directxman12/}k8s-prometheus-adapter-${ARCH},1279,5
openstack%2Fbifrost~stable%2Fzed~I208182e65884d63548d78c68f676b899c562a2dc,openstack/bifrost,stable/zed,I208182e65884d63548d78c68f676b899c562a2dc,CI: Update cached cirros image to 0.5.3,MERGED,2023-07-05 14:05:36.000000000,2023-07-05 23:16:18.000000000,2023-07-05 23:15:24.000000000,"[{'_account_id': 15519}, {'_account_id': 22348}, {'_account_id': 23851}]","[{'number': 1, 'created': '2023-07-05 14:05:36.000000000', 'files': ['playbooks/test-bifrost.yaml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/f38f1d8e01314e408c7d882d9c5cab381969c404', 'message': ""CI: Update cached cirros image to 0.5.3\n\nBifrost CI is currently failing to fetch the cirros image from cache:\n\n    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'\n\nThis may be caused by the removal of cirros-0.5.1 images from cache in\nchange Ibada405e0c1183559f428c749d0e54d0a45a2223.\n\nSwitch to cirros version 0.5.3 image instead.\n\nChange-Id: I208182e65884d63548d78c68f676b899c562a2dc\n""}]",1,887699,f38f1d8e01314e408c7d882d9c5cab381969c404,10,3,1,10239,,,0,"CI: Update cached cirros image to 0.5.3

Bifrost CI is currently failing to fetch the cirros image from cache:

    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'

This may be caused by the removal of cirros-0.5.1 images from cache in
change Ibada405e0c1183559f428c749d0e54d0a45a2223.

Switch to cirros version 0.5.3 image instead.

Change-Id: I208182e65884d63548d78c68f676b899c562a2dc
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/99/887699/1 && git format-patch -1 --stdout FETCH_HEAD,['playbooks/test-bifrost.yaml'],1,f38f1d8e01314e408c7d882d9c5cab381969c404,, cirros_deploy_image_upstream_url: file:///opt/cache/files/cirros-0.5.3-x86_64-disk.img, cirros_deploy_image_upstream_url: file:///opt/cache/files/cirros-0.5.1-x86_64-disk.img,1,1
openstack%2Fswift~master~I6fc9eaf0903bdbe4ccb82c4e7647478a1eef9785,openstack/swift,master,I6fc9eaf0903bdbe4ccb82c4e7647478a1eef9785,"docs: Format metrics in fixed-width font, not italics",MERGED,2023-05-25 20:59:25.000000000,2023-07-05 22:50:12.000000000,2023-07-05 22:48:46.000000000,"[{'_account_id': 1179}, {'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-05-25 20:59:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/ffbe64f9339e85eaaa74a5ed54867636eabab7f0', 'message': 'docs: Format metrics in fixed-width font, not italics\n\nChange-Id: I6fc9eaf0903bdbe4ccb82c4e7647478a1eef9785\n'}, {'number': 2, 'created': '2023-05-30 18:40:07.000000000', 'files': ['doc/source/metrics/object_server.rst', 'doc/source/metrics/container_sync.rst', 'doc/source/metrics/container_server.rst', 'doc/source/metrics/container_auditor.rst', 'doc/source/metrics/container_updater.rst', 'doc/source/metrics/object_replicator.rst', 'doc/source/metrics/object_updater.rst', 'doc/source/metrics/proxy_server.rst', 'doc/source/metrics/account_auditor.rst', 'doc/source/metrics/account_reaper.rst', 'doc/source/metrics/account_replicator.rst', 'doc/source/metrics/object_reconstructor.rst', 'doc/source/metrics/object_expirer.rst', 'doc/source/metrics/container_replicator.rst', 'doc/source/metrics/object_auditor.rst', 'doc/source/metrics/account_server.rst'], 'web_link': 'https://opendev.org/openstack/swift/commit/1f298714aff931b8041bbfeb2d87e9547dcaefee', 'message': 'docs: Format metrics in fixed-width font, not italics\n\nChange-Id: I6fc9eaf0903bdbe4ccb82c4e7647478a1eef9785\n'}]",2,884444,1f298714aff931b8041bbfeb2d87e9547dcaefee,10,3,2,15343,,,0,"docs: Format metrics in fixed-width font, not italics

Change-Id: I6fc9eaf0903bdbe4ccb82c4e7647478a1eef9785
",git fetch https://review.opendev.org/openstack/swift refs/changes/44/884444/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/metrics/object_server.rst', 'doc/source/metrics/container_sync.rst', 'doc/source/metrics/container_server.rst', 'doc/source/metrics/container_auditor.rst', 'doc/source/metrics/container_updater.rst', 'doc/source/metrics/object_replicator.rst', 'doc/source/metrics/object_updater.rst', 'doc/source/metrics/proxy_server.rst', 'doc/source/metrics/account_auditor.rst', 'doc/source/metrics/account_reaper.rst', 'doc/source/metrics/account_replicator.rst', 'doc/source/metrics/object_reconstructor.rst', 'doc/source/metrics/object_expirer.rst', 'doc/source/metrics/container_replicator.rst', 'doc/source/metrics/object_auditor.rst', 'doc/source/metrics/account_server.rst']",16,ffbe64f9339e85eaaa74a5ed54867636eabab7f0,," which increment ``errors`` are not included in the timing data. ========================================== ======================================================= Metric Name Description ------------------------------------------ ------------------------------------------------------- ``account-server.DELETE.errors.timing`` Timing data for each DELETE request resulting in an error: bad request, not mounted, missing timestamp. ``account-server.DELETE.timing`` Timing data for each DELETE request not resulting in an error. ``account-server.PUT.errors.timing`` Timing data for each PUT request resulting in an error: bad request, not mounted, conflict, recently-deleted. ``account-server.PUT.timing`` Timing data for each PUT request not resulting in an error. ``account-server.HEAD.errors.timing`` Timing data for each HEAD request resulting in an error: bad request, not mounted. ``account-server.HEAD.timing`` Timing data for each HEAD request not resulting in an error. ``account-server.GET.errors.timing`` Timing data for each GET request resulting in an error: bad request, not mounted, bad delimiter, account listing limit too high, bad accept header. ``account-server.GET.timing`` Timing data for each GET request not resulting in an error. ``account-server.REPLICATE.errors.timing`` Timing data for each REPLICATE request resulting in an error: bad request, not mounted. ``account-server.REPLICATE.timing`` Timing data for each REPLICATE request not resulting in an error. ``account-server.POST.errors.timing`` Timing data for each POST request resulting in an error: bad request, bad or missing timestamp, not mounted. ``account-server.POST.timing`` Timing data for each POST request not resulting in an error. ========================================== ======================================================="," which increment `errors` are not included in the timing data. ======================================== ======================================================= Metric Name Description ---------------------------------------- ------------------------------------------------------- `account-server.DELETE.errors.timing` Timing data for each DELETE request resulting in an error: bad request, not mounted, missing timestamp. `account-server.DELETE.timing` Timing data for each DELETE request not resulting in an error. `account-server.PUT.errors.timing` Timing data for each PUT request resulting in an error: bad request, not mounted, conflict, recently-deleted. `account-server.PUT.timing` Timing data for each PUT request not resulting in an error. `account-server.HEAD.errors.timing` Timing data for each HEAD request resulting in an error: bad request, not mounted. `account-server.HEAD.timing` Timing data for each HEAD request not resulting in an error. `account-server.GET.errors.timing` Timing data for each GET request resulting in an error: bad request, not mounted, bad delimiter, account listing limit too high, bad accept header. `account-server.GET.timing` Timing data for each GET request not resulting in an error. `account-server.REPLICATE.errors.timing` Timing data for each REPLICATE request resulting in an error: bad request, not mounted. `account-server.REPLICATE.timing` Timing data for each REPLICATE request not resulting in an error. `account-server.POST.errors.timing` Timing data for each POST request resulting in an error: bad request, bad or missing timestamp, not mounted. `account-server.POST.timing` Timing data for each POST request not resulting in an error. ======================================== =======================================================",368,368
openstack%2Fswift~master~Ia386736b9b283858931794690538871b6e1ad9c8,openstack/swift,master,Ia386736b9b283858931794690538871b6e1ad9c8,Fix handling of non-ASCII accounts,MERGED,2023-03-11 00:49:12.000000000,2023-07-05 22:49:22.000000000,2023-07-05 22:48:12.000000000,"[{'_account_id': 1179}, {'_account_id': 15343}, {'_account_id': 22348}, {'_account_id': 32761}]","[{'number': 1, 'created': '2023-03-11 00:49:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/swift/commit/870615cb81875598b7b9f8af05533606ccfa1f50', 'message': 'Fix handling of non-ASCII accounts\n\nRelated-Change: I4ecfae2bca6ffa08ad15e584579ebce707f4628d\nRelated-Change: I1e244c231753b8f4b6f1cf95cb0ae4c3c959ae0f\nChange-Id: Ia386736b9b283858931794690538871b6e1ad9c8\n'}, {'number': 2, 'created': '2023-06-13 22:28:50.000000000', 'files': ['test/functional/test_dlo.py', 'test/unit/common/middleware/test_tempauth.py', 'test/unit/proxy/test_server.py', 'swift/common/middleware/tempauth.py', 'test/functional/test_slo.py', 'test/functional/test_object_versioning.py', 'test/functional/test_domain_remap.py', 'swift/proxy/controllers/container.py', 'swift/proxy/controllers/base.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/b46b735a3e7b1f5b515edde6046e0c53dc3923ae', 'message': 'Fix handling of non-ASCII accounts\n\nRelated-Change: I4ecfae2bca6ffa08ad15e584579ebce707f4628d\nRelated-Change: I1e244c231753b8f4b6f1cf95cb0ae4c3c959ae0f\nChange-Id: Ia386736b9b283858931794690538871b6e1ad9c8\n'}]",23,877146,b46b735a3e7b1f5b515edde6046e0c53dc3923ae,27,4,2,15343,,,0,"Fix handling of non-ASCII accounts

Related-Change: I4ecfae2bca6ffa08ad15e584579ebce707f4628d
Related-Change: I1e244c231753b8f4b6f1cf95cb0ae4c3c959ae0f
Change-Id: Ia386736b9b283858931794690538871b6e1ad9c8
",git fetch https://review.opendev.org/openstack/swift refs/changes/46/877146/2 && git format-patch -1 --stdout FETCH_HEAD,"['test/functional/test_dlo.py', 'test/unit/common/middleware/test_tempauth.py', 'swift/common/middleware/tempauth.py', 'test/functional/test_slo.py', 'test/functional/test_object_versioning.py', 'test/functional/test_domain_remap.py', 'swift/proxy/controllers/container.py', 'swift/proxy/controllers/base.py']",8,870615cb81875598b7b9f8af05533606ccfa1f50,," resp = self.make_requests(Request.blank(str_to_wsgi('/v1' + path)),"," resp = self.make_requests(Request.blank('/v1' + path),",41,23
openstack%2Fswift~master~I4976b3ee24e4ec498c66359f391813261d42c495,openstack/swift,master,I4976b3ee24e4ec498c66359f391813261d42c495,s3api: emit metrics for error responses,MERGED,2023-06-29 11:28:18.000000000,2023-07-05 22:48:43.000000000,2023-07-05 22:48:43.000000000,"[{'_account_id': 7233}, {'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 11:28:18.000000000', 'files': ['test/unit/common/middleware/s3api/__init__.py', 'swift/common/middleware/s3api/s3response.py', 'swift/common/middleware/s3api/s3request.py', 'swift/common/middleware/s3api/s3api.py', 'test/unit/common/middleware/s3api/test_s3api.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/f9af0b70b7ce22014e5756d3bdf863f929f6c71b', 'message': 's3api: emit metrics for error responses\n\nUsers sometimes ask why their request received a 403\nresponse. Sometimes s3api will include a reason in the response body,\nbut client code may not make this visible to the user. To provide some\nother insights, this patch adds statsd metrics when error responses,\nsuch as 403, are returned from the s3api middleware.\n\nThe new metrics have the form:\n\n  s3api.<status_int>.<error_class>[.reason]\n\nFor example:\n\ns3api.403.SignatureDoesNotMatch\ns3api.403.RequestTimeTooSkewed\ns3api.403.AccessDenied.invalid_date\ns3api.400.InvalidBucketName\n\nChange-Id: I4976b3ee24e4ec498c66359f391813261d42c495\n'}]",7,887267,f9af0b70b7ce22014e5756d3bdf863f929f6c71b,12,3,1,7847,,,0,"s3api: emit metrics for error responses

Users sometimes ask why their request received a 403
response. Sometimes s3api will include a reason in the response body,
but client code may not make this visible to the user. To provide some
other insights, this patch adds statsd metrics when error responses,
such as 403, are returned from the s3api middleware.

The new metrics have the form:

  s3api.<status_int>.<error_class>[.reason]

For example:

s3api.403.SignatureDoesNotMatch
s3api.403.RequestTimeTooSkewed
s3api.403.AccessDenied.invalid_date
s3api.400.InvalidBucketName

Change-Id: I4976b3ee24e4ec498c66359f391813261d42c495
",git fetch https://review.opendev.org/openstack/swift refs/changes/67/887267/1 && git format-patch -1 --stdout FETCH_HEAD,"['test/unit/common/middleware/s3api/__init__.py', 'swift/common/middleware/s3api/s3request.py', 'swift/common/middleware/s3api/s3response.py', 'swift/common/middleware/s3api/s3api.py', 'test/unit/common/middleware/s3api/test_s3api.py']",5,f9af0b70b7ce22014e5756d3bdf863f929f6c71b,p-s3api-metrics,"from swift.common.middleware.s3api.s3response import ErrorResponse, \ AccessDenied self.assertEqual({'403.AccessDenied.invalid_header_auth': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'405.MethodNotAllowed': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'405.MethodNotAllowed': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'403.AccessDenied.expired': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'403.AccessDenied.invalid_expires': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'403.AccessDenied.invalid_query_auth': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'403.AccessDenied.invalid_query_auth': 1}, self.s3api.logger.logger.get_increment_counts()) self.s3api.logger.logger.clear() self.assertEqual({'400.AuthorizationQueryParametersError': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'403.AccessDenied.invalid_date': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'400.InvalidArgument': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'400.AuthorizationHeaderMalformed': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'403.AccessDenied.invalid_credential': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'403.AccessDenied.invalid_query_auth': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'400.InvalidURI': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'400.InvalidDigest': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'400.InvalidDigest': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'400.InvalidDigest': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'400.InvalidDigest': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'400.InvalidArgument': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'400.InvalidStorageClass': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'400.InvalidArgument': 1}, self.s3api.logger.logger.get_increment_counts()) self.s3api.logger.logger.clear() self.assertEqual({'501.NotImplemented': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'501.NotImplemented': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({}, self.s3api.logger.logger.get_increment_counts()) self.s3api.logger.logger.clear() self.assertEqual({'501.NotImplemented': 1}, self.s3api.logger.logger.get_increment_counts()) self.s3api.logger.logger.clear() self.assertEqual({'501.NotImplemented': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'405.MethodNotAllowed': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'403.AccessDenied.invalid_date': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual({'400.InvalidRequest': 1}, self.s3api.logger.logger.get_increment_counts()) def test(auth_str, error, msg, metric, extra=b''): self.s3api.logger.logger.clear() self.assertEqual({metric: 1}, self.s3api.logger.logger.get_increment_counts()) test(auth_str, 'AccessDenied', 'Access Denied.', '403.AccessDenied.invalid_credential') 'and Signature.', '400.AuthorizationHeaderMalformed') '400.AuthorizationHeaderMalformed', b'<Region>us-east-1</Region>') 'incorrect service ""not-s3"". This endpoint belongs to ""s3"".', '400.AuthorizationHeaderMalformed') 'This endpoint uses ""aws4_request"".', '400.AuthorizationHeaderMalformed') test(auth_str, 'AccessDenied', 'Access Denied.', '403.AccessDenied.invalid_header_auth') self.assertEqual({'403.AccessDenied.invalid_expires': 1}, self.s3api.logger.logger.get_increment_counts()) self.s3api.logger.logger.clear() self.assertEqual({'403.AccessDenied.invalid_expires': 1}, self.s3api.logger.logger.get_increment_counts()) self.assertEqual( {'403.SignatureDoesNotMatch': 1}, self.s3api.logger.logger.get_increment_counts()) def test_s3api_with_time_skew(self): def do_test(skew): req = Request.blank( '/object', environ={'HTTP_HOST': 'bucket.localhost:80', 'REQUEST_METHOD': 'GET', 'HTTP_AUTHORIZATION': 'AWS test:tester:hmac'}, headers={'Date': self.get_date_header(skew=skew)}) self.s3api.logger.logger.clear() return self.call_s3api(req) status, _, body = do_test(800) self.assertEqual('200 OK', status) self.assertFalse(self.s3api.logger.logger.get_increment_counts()) status, _, body = do_test(-800) self.assertEqual('200 OK', status) self.assertFalse(self.s3api.logger.logger.get_increment_counts()) status, _, body = do_test(1000) self.assertEqual('403 Forbidden', status) self.assertEqual(self._get_error_code(body), 'RequestTimeTooSkewed') self.assertEqual({'403.RequestTimeTooSkewed': 1}, self.s3api.logger.logger.get_increment_counts()) status, _, body = do_test(-1000) self.assertEqual('403 Forbidden', status) self.assertEqual(self._get_error_code(body), 'RequestTimeTooSkewed') self.assertEqual({'403.RequestTimeTooSkewed': 1}, self.s3api.logger.logger.get_increment_counts()) self.s3api.conf.allowable_clock_skew = 100 status, _, body = do_test(800) self.assertEqual('403 Forbidden', status) self.assertEqual(self._get_error_code(body), 'RequestTimeTooSkewed') self.assertEqual({'403.RequestTimeTooSkewed': 1}, self.s3api.logger.logger.get_increment_counts()) def test_s3api_error_metric(self): class KaboomResponse(ErrorResponse): _code = 'ka boom' def do_test(err_response): req = Request.blank( '/object', environ={'HTTP_HOST': 'bucket.localhost:80', 'REQUEST_METHOD': 'GET', 'HTTP_AUTHORIZATION': 'AWS test:tester:hmac'}, headers={'Date': self.get_date_header()}) self.s3api.logger.logger.clear() with mock.patch.object( self.s3api, 'handle_request', side_effect=err_response): self.call_s3api(req) do_test(ErrorResponse(status=403, msg='not good', reason='bad')) self.assertEqual({'403.ErrorResponse.bad': 1}, self.s3api.logger.logger.get_increment_counts()) do_test(AccessDenied(msg='no entry', reason='invalid_date')) self.assertEqual({'403.AccessDenied.invalid_date': 1}, self.s3api.logger.logger.get_increment_counts()) # check whitespace replaced with underscore do_test(KaboomResponse(status=400, msg='boom', reason='boom boom')) self.assertEqual({'400.ka_boom.boom_boom': 1}, self.s3api.logger.logger.get_increment_counts()) "," def test(auth_str, error, msg, extra=b''): test(auth_str, 'AccessDenied', 'Access Denied.') 'and Signature.') b'<Region>us-east-1</Region>') 'incorrect service ""not-s3"". This endpoint belongs to ""s3"".') 'This endpoint uses ""aws4_request"".') test(auth_str, 'AccessDenied', 'Access Denied.')",197,31
openstack%2Fswift~master~Iadc54111e79910dd1809e21facba81153ca61822,openstack/swift,master,Iadc54111e79910dd1809e21facba81153ca61822,tests: Stop requiring <1ms test runtime,MERGED,2023-06-28 19:57:22.000000000,2023-07-05 22:48:15.000000000,2023-07-05 22:48:15.000000000,"[{'_account_id': 597}, {'_account_id': 15343}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-28 19:57:22.000000000', 'files': ['test/unit/obj/test_replicator.py'], 'web_link': 'https://opendev.org/openstack/swift/commit/82b5335fd5420a09c1495217b68afb6eb044af15', 'message': 'tests: Stop requiring <1ms test runtime\n\nChange-Id: Iadc54111e79910dd1809e21facba81153ca61822\n'}]",0,887226,82b5335fd5420a09c1495217b68afb6eb044af15,8,3,1,15343,,,0,"tests: Stop requiring <1ms test runtime

Change-Id: Iadc54111e79910dd1809e21facba81153ca61822
",git fetch https://review.opendev.org/openstack/swift refs/changes/26/887226/1 && git format-patch -1 --stdout FETCH_HEAD,['test/unit/obj/test_replicator.py'],1,82b5335fd5420a09c1495217b68afb6eb044af15,," with mock.patch( 'swift.obj.replicator.subprocess.Popen') as mock_popen, \ mock.patch('time.time', side_effect=[123.4, 123.5]): '192.168.50.30::object/d8/objects/241 (0.100)']) with mock.patch( 'swift.obj.replicator.subprocess.Popen') as mock_popen, \ mock.patch('time.time', side_effect=[123.4, 123.5]): '192.168.50.30::object/d8/objects/241 (0.100)'])", with mock.patch('swift.obj.replicator.subprocess.Popen') as mock_popen: '192.168.50.30::object/d8/objects/241 (0.000)']) with mock.patch('swift.obj.replicator.subprocess.Popen') as mock_popen: '192.168.50.30::object/d8/objects/241 (0.000)']),8,4
openstack%2Fcharm-ironic-api~stable%2Fussuri~Ic84e4706b93c38916e89b91dfc30bf32396e5213,openstack/charm-ironic-api,stable/ussuri,Ic84e4706b93c38916e89b91dfc30bf32396e5213,Add support for using service tokens,MERGED,2023-06-30 17:22:51.000000000,2023-07-05 22:39:41.000000000,2023-07-05 22:39:41.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-06-30 17:22:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ironic-api/commit/780a7c0e2661a9a5b0f195d0d51fc5c1288906ce', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-api to send a service token along with the\nreceived user token on requests to other services. This allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Ironic. Also with this patch Ironic will\naccept request from other services with invalid user tokens but valid\nservice tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/yoga\n\nCloses-Bug: #1992840\nChange-Id: Ic84e4706b93c38916e89b91dfc30bf32396e5213\n(cherry picked from commit 02b7180a6b9cd9c22c4b672ae8510ea14af5e0df)\n(cherry picked from commit fca337274407d8c145c33fe5ce4805497878eb04)\n'}, {'number': 2, 'created': '2023-06-30 17:23:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ironic-api/commit/14ea4ee0753d86124a6e06abd0c985a6f6d466db', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-api to send a service token along with the\nreceived user token on requests to other services. This allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Ironic. Also with this patch Ironic will\naccept request from other services with invalid user tokens but valid\nservice tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/ussuri\n\nCloses-Bug: #1992840\nChange-Id: Ic84e4706b93c38916e89b91dfc30bf32396e5213\n(cherry picked from commit 02b7180a6b9cd9c22c4b672ae8510ea14af5e0df)\n(cherry picked from commit fca337274407d8c145c33fe5ce4805497878eb04)\n'}, {'number': 3, 'created': '2023-07-03 13:02:04.000000000', 'files': ['src/templates/train/ironic.conf', 'src/templates/parts/keystone-authtoken', 'src/build.lock'], 'web_link': 'https://opendev.org/openstack/charm-ironic-api/commit/b9bee8cee23aa2cf22401b393de7639b81893431', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-api to send a service token along with the\nreceived user token on requests to other services. This allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Ironic. Also with this patch Ironic will\naccept request from other services with invalid user tokens but valid\nservice tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/ussuri\n\nCloses-Bug: #1992840\nDepends-On: https://review.opendev.org/c/openstack/charm-ironic-api/+/887514\nChange-Id: Ic84e4706b93c38916e89b91dfc30bf32396e5213\n(cherry picked from commit 02b7180a6b9cd9c22c4b672ae8510ea14af5e0df)\n(cherry picked from commit fca337274407d8c145c33fe5ce4805497878eb04)\n'}]",0,887421,b9bee8cee23aa2cf22401b393de7639b81893431,13,5,3,11805,,,0,"Add support for using service tokens

This patch configures ironic-api to send a service token along with the
received user token on requests to other services. This allow those
other services to accept the request even if the user token has been
invalidated since received by Ironic. Also with this patch Ironic will
accept request from other services with invalid user tokens but valid
service tokens.

Update src/build.lock to get backported patches into
charm-helpers@stable/ussuri

Closes-Bug: #1992840
Depends-On: https://review.opendev.org/c/openstack/charm-ironic-api/+/887514
Change-Id: Ic84e4706b93c38916e89b91dfc30bf32396e5213
(cherry picked from commit 02b7180a6b9cd9c22c4b672ae8510ea14af5e0df)
(cherry picked from commit fca337274407d8c145c33fe5ce4805497878eb04)
",git fetch https://review.opendev.org/openstack/charm-ironic-api refs/changes/21/887421/1 && git format-patch -1 --stdout FETCH_HEAD,"['src/templates/train/ironic.conf', 'src/templates/parts/keystone-authtoken', 'src/build.lock']",3,780a7c0e2661a9a5b0f195d0d51fc5c1288906ce,bug/1992840," ""version"": ""87f9a8df625ce6cd0c2a04da2d718ab7f9c1aad3"",} "," ""version"": ""a99a667d343ab3c11074d8bc8c6d8b5d638f73b7"",}",6,2
openstack%2Fcharm-ironic-api~stable%2Fussuri~I8f1cd484ceeb79f57276cd69e14a5d47a2f7073d,openstack/charm-ironic-api,stable/ussuri,I8f1cd484ceeb79f57276cd69e14a5d47a2f7073d,Install libpq-dev bindep for py36,MERGED,2023-07-03 13:00:32.000000000,2023-07-05 22:39:40.000000000,2023-07-05 22:39:40.000000000,"[{'_account_id': 8992}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 13:00:32.000000000', 'files': ['bindep.txt'], 'web_link': 'https://opendev.org/openstack/charm-ironic-api/commit/417b0b619193380e911c3122b2c0c69f0b66e8db', 'message': 'Install libpq-dev bindep for py36\n\nWithout this py36 tests are failing with:\n""Error: pg_config executable not found""\n\nChange-Id: I8f1cd484ceeb79f57276cd69e14a5d47a2f7073d\n'}]",1,887514,417b0b619193380e911c3122b2c0c69f0b66e8db,10,4,1,11805,,,0,"Install libpq-dev bindep for py36

Without this py36 tests are failing with:
""Error: pg_config executable not found""

Change-Id: I8f1cd484ceeb79f57276cd69e14a5d47a2f7073d
",git fetch https://review.opendev.org/openstack/charm-ironic-api refs/changes/14/887514/1 && git format-patch -1 --stdout FETCH_HEAD,['bindep.txt'],1,417b0b619193380e911c3122b2c0c69f0b66e8db,,libpq-dev [platform:dpkg] ,,1,0
openstack%2Fcharm-ironic-conductor~stable%2Fussuri~Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9,openstack/charm-ironic-conductor,stable/ussuri,Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9,Add support for using service tokens,MERGED,2023-06-30 17:17:42.000000000,2023-07-05 22:39:18.000000000,2023-07-05 22:39:18.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-30 17:17:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ironic-conductor/commit/a2d21c8d8a90ad0154e946845090cb7370e7db87', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-conductor to send a service token along\nwith the received user token on requests to other services. This allow\nthose other services to accept the request even if the user token has\nbeen invalidated since received by Ironic. Also with this patch Ironic\nwill accept request from other services with invalid user tokens but\nvalid service tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/zed\n\nCloses-Bug: #1992840\nChange-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9\n(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)\n(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)\n'}, {'number': 2, 'created': '2023-06-30 17:24:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ironic-conductor/commit/7bd595fb9e6c101ef0a0e2e8185e8283ff3e71c8', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-conductor to send a service token along\nwith the received user token on requests to other services. This allow\nthose other services to accept the request even if the user token has\nbeen invalidated since received by Ironic. Also with this patch Ironic\nwill accept request from other services with invalid user tokens but\nvalid service tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/ussuri\n\nCloses-Bug: #1992840\nChange-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9\n(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)\n(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)\n'}, {'number': 3, 'created': '2023-07-03 13:01:17.000000000', 'files': ['src/templates/train/ironic.conf', 'src/templates/parts/keystone-authtoken', 'src/build.lock'], 'web_link': 'https://opendev.org/openstack/charm-ironic-conductor/commit/0f0d5692aeadad01785f01c487902ccb272c2b24', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-conductor to send a service token along\nwith the received user token on requests to other services. This allow\nthose other services to accept the request even if the user token has\nbeen invalidated since received by Ironic. Also with this patch Ironic\nwill accept request from other services with invalid user tokens but\nvalid service tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/ussuri\n\nCloses-Bug: #1992840\nDepends-On: https://review.opendev.org/c/openstack/charm-ironic-conductor/+/887512\nChange-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9\n(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)\n(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)\n'}]",0,887417,0f0d5692aeadad01785f01c487902ccb272c2b24,12,4,3,11805,,,0,"Add support for using service tokens

This patch configures ironic-conductor to send a service token along
with the received user token on requests to other services. This allow
those other services to accept the request even if the user token has
been invalidated since received by Ironic. Also with this patch Ironic
will accept request from other services with invalid user tokens but
valid service tokens.

Update src/build.lock to get backported patches into
charm-helpers@stable/ussuri

Closes-Bug: #1992840
Depends-On: https://review.opendev.org/c/openstack/charm-ironic-conductor/+/887512
Change-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9
(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)
(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)
",git fetch https://review.opendev.org/openstack/charm-ironic-conductor refs/changes/17/887417/2 && git format-patch -1 --stdout FETCH_HEAD,"['src/templates/train/ironic.conf', 'src/templates/parts/keystone-authtoken', 'src/build.lock']",3,a2d21c8d8a90ad0154e946845090cb7370e7db87,bug/1992840," ""version"": ""87f9a8df625ce6cd0c2a04da2d718ab7f9c1aad3"",} "," ""version"": ""a99a667d343ab3c11074d8bc8c6d8b5d638f73b7"",}",6,2
openstack%2Fcharm-ironic-conductor~stable%2Fussuri~Ia02517f071dd0b512527dd92bf8a1f918b8ae6a0,openstack/charm-ironic-conductor,stable/ussuri,Ia02517f071dd0b512527dd92bf8a1f918b8ae6a0,Install libpq-dev bindep for py36,MERGED,2023-07-03 12:58:33.000000000,2023-07-05 22:36:28.000000000,2023-07-05 22:36:28.000000000,"[{'_account_id': 8992}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 12:58:33.000000000', 'files': ['bindep.txt'], 'web_link': 'https://opendev.org/openstack/charm-ironic-conductor/commit/15ed6c342765d3c91ed541e1f7f57e37e581e7f4', 'message': 'Install libpq-dev bindep for py36\n\nWithout this py36 tests are failing with:\n""Error: pg_config executable not found""\n\nChange-Id: Ia02517f071dd0b512527dd92bf8a1f918b8ae6a0\n'}]",0,887512,15ed6c342765d3c91ed541e1f7f57e37e581e7f4,8,4,1,11805,,,0,"Install libpq-dev bindep for py36

Without this py36 tests are failing with:
""Error: pg_config executable not found""

Change-Id: Ia02517f071dd0b512527dd92bf8a1f918b8ae6a0
",git fetch https://review.opendev.org/openstack/charm-ironic-conductor refs/changes/12/887512/1 && git format-patch -1 --stdout FETCH_HEAD,['bindep.txt'],1,15ed6c342765d3c91ed541e1f7f57e37e581e7f4,,libpq-dev [platform:dpkg] ,,1,0
openstack%2Fcinder~master~Id0f57cc855cd7470a7b1ca299c87c079a2e48d1e,openstack/cinder,master,Id0f57cc855cd7470a7b1ca299c87c079a2e48d1e,Add benji-backup.me driver to cinder,NEW,2022-06-24 21:42:47.000000000,2023-07-05 21:58:16.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2022-06-24 21:42:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/f302d7bcfbbf7869e0aa2891aa38e96936233a57', 'message': 'Add benji-backup.me driver to ceph and other backends\n\nSigned-off-by: Jesper Schmtiz Mouridsen <jesper@schmitz.computer>\nChange-Id: Id0f57cc855cd7470a7b1ca299c87c079a2e48d1e\n'}, {'number': 2, 'created': '2022-06-25 15:15:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1640140c31d9f1a82b9d0c75c94a805073560421', 'message': 'Add benji-backup.me driver to ceph and other backends\n\nSigned-off-by: Jesper Schmtiz Mouridsen <jesper@schmitz.computer>\nChange-Id: Id0f57cc855cd7470a7b1ca299c87c079a2e48d1e\n'}, {'number': 3, 'created': '2023-06-24 17:54:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/fb601c9c8f678dde77f4afa3705198688aac25c8', 'message': 'Add benji-backup.me driver to ceph and other backends\n\nSigned-off-by: Jesper Schmtiz Mouridsen <jesper@schmitz.computer>\nChange-Id: Id0f57cc855cd7470a7b1ca299c87c079a2e48d1e\n'}, {'number': 4, 'created': '2023-06-25 17:34:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/51e94d30d214b2f3fdf789882d3f526c32e97eab', 'message': 'Add benji-backup.me driver to ceph and other backends\n\nSigned-off-by: Jesper Schmtiz Mouridsen <jesper@schmitz.computer>\nChange-Id: Id0f57cc855cd7470a7b1ca299c87c079a2e48d1e\n'}, {'number': 5, 'created': '2023-06-25 19:57:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/0d7e5d430cf67f1133abfc733c1e0ecbde12f750', 'message': 'Add benji-backup.me driver to cinder\n\n The backup driver supports\n ceph and other cinder volume backends\n\nSigned-off-by: Jesper Schmtiz Mouridsen <jesper@schmitz.computer>\nChange-Id: Id0f57cc855cd7470a7b1ca299c87c079a2e48d1e\n'}, {'number': 6, 'created': '2023-07-02 16:41:33.000000000', 'files': ['cinder/opts.py', 'tools/install-requirements-wrapper.sh', 'cinder/backup/drivers/benjidriver.py', 'benji-requirements.txt', 'mypy-files.txt', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/cinder/commit/0614457b804b42a63546090f423dd3c78268e433', 'message': 'Add benji-backup.me driver to cinder\n\n The backup driver supports\n ceph and other cinder volume backends\n\nSigned-off-by: Jesper Schmtiz Mouridsen <jesper@schmitz.computer>\nChange-Id: Id0f57cc855cd7470a7b1ca299c87c079a2e48d1e\n'}]",4,847620,0614457b804b42a63546090f423dd3c78268e433,96,1,6,29260,,,0,"Add benji-backup.me driver to cinder

 The backup driver supports
 ceph and other cinder volume backends

Signed-off-by: Jesper Schmtiz Mouridsen <jesper@schmitz.computer>
Change-Id: Id0f57cc855cd7470a7b1ca299c87c079a2e48d1e
",git fetch https://review.opendev.org/openstack/cinder refs/changes/20/847620/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/backup/drivers/benjidriver.py'],1,f302d7bcfbbf7869e0aa2891aa38e96936233a57,,"# Copyright 2022 Jesper Schmitz Mouridsen. # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from datetime import datetime from typing import Optional, Tuple # noqa: H301 from benji.benji import Benji from benji.benji import VersionUid import benji.config as benji_config from benji.io.factory import IOFactory from benji.storage.factory import StorageFactory import eventlet from os_brick.remotefs import remotefs as remotefs_brick from oslo_config import cfg from oslo_log import log as logging import rados import rbd from cinder.backup import driver from cinder import exception from cinder import utils import cinder.volume.drivers.rbd as rbd_driver def setup_logging(): extra_log_level_defaults = [ 'dogpile=INFO', 'routes=INFO', 'benji=INFO' ] logging.set_defaults( default_log_levels=logging.get_default_log_levels() + extra_log_level_defaults) logging.setup(CONF, ""cinder-backup"") service_opts = [ cfg.StrOpt('benji_storage_name', default=None, help='Name of benji storage from benji.yaml to use'), cfg.StrOpt('benji_nfs_share_path', default=None, help='NFS share in hostname:path, ipv4addr:path, ' 'or ""[ipv6addr]:path"" format.'), cfg.StrOpt('benji_backup_mount_point_base', default='$state_path/benji_backup_mount', help='Base dir containing mount point for NFS share.'), cfg.StrOpt('benji_io_scheme_ceph', default='cinder-volumes', help='Benji io scheme for ceph sources to backup'), cfg.StrOpt('benji_io_scheme_file', default='file', help='Benji io scheme for file sources to backup'), cfg.StrOpt('benji_snapshot_prefix', default='benjibackup-', help='prefix for rbd snaphots'), cfg.StrOpt('benji_premade_snapshot_prefix', default='benjibackup_premade-', help='prefix for premade rbd snaphots'), ] CONF = cfg.CONF CONF.register_opts(service_opts) setup_logging() LOG = logging.getLogger(__name__) class BenjiBackupDriver(driver.BackupDriver): """"""Provides backup, restore and delete using benji-backup system."""""" def __init__(self, context, db=None): super().__init__(context) self.rados = rados self.rbd = rbd self.diff_list = [] if CONF.benji_nfs_share_path is not None: self.nfs_mount_path = self._mount_nfs() bconfig = benji_config.Config() if CONF.benji_storage_name is None: self.storage_name = bconfig.get(""defaultStorage"") else: self.storage_name = CONF.benji_storage_name self.storages = bconfig.get(""storages"") IOFactory.initialize(bconfig) StorageFactory.initialize(bconfig) self.b_backup = Benji(bconfig) self.ios = bconfig.get(""ios"") def check_for_setup_error(self): rbdmodules = [iomodule for iomodule in self.ios if iomodule[""module""] == ""rbd""] config_modules = [iomodule for iomodule in rbdmodules if iomodule[""name""] == CONF.benji_io_scheme_ceph] if rbdmodules and not config_modules: raise exception.BackupDriverException( reason=""No ios module rbd named %(name)s in benji.yaml"" % {'name': CONF.benji_io_scheme_ceph}) storage = [storage for storage in self.storages if storage[""name""] == self.storage_name] LOG.debug(storage) if CONF.benji_nfs_share_path is None: return if not storage: raise exception.BackupDriverException( reason=""storage name '%(storage_name)s"" ""' not found in /etc/benji.yaml"" % {'storage_name': self.storage_name}) if storage[0][""configuration""][""path""] != self.nfs_mount_path: raise exception.BackupDriverException( reason=""mount point '%(mount_point)s' not found in "" ""/etc/benji.yaml in "" ""storage '%(storage_name)s' configuration path"" % {'mount_point': self.nfs_mount_path, 'storage_name': self.storage_name}) def _mount_nfs(self): remotefsclient = remotefs_brick.RemoteFsClient( 'nfs', utils.get_root_helper(), nfs_mount_point_base=CONF.benji_backup_mount_point_base, nfs_mount_options="""") remotefsclient.mount(CONF.benji_nfs_share_path) nfs_share_path = CONF.benji_nfs_share_path mount_point = remotefsclient.get_mount_point(nfs_share_path) LOG.info(""Usinng '%(mount_point)s'"", {'mount_point': mount_point}) return mount_point def iterate_cb(self, offset, length, exists): self.diff_list.append((offset, length, exists)) def _connect_to_rados(self, pool: Optional[str] = None) -> Tuple['rados.Rados', 'rados.Ioctx']: """"""Establish connection to the Ceph cluster."""""" client = eventlet.tpool.Proxy(rados.Rados( rados_id=self.ceph_user, conffile=self.ceph_conf )) try: client.connect() pool_to_open = pool or self.pool ioctx = client.open_ioctx(pool_to_open) return client, ioctx except rados.Error: # shutdown cannot raise an exception client.shutdown() raise @staticmethod def _disconnect_from_rados(client: 'rados.Rados', ioctx: 'rados.Ioctx') -> None: """"""Terminate connection with the Ceph cluster."""""" # closing an ioctx cannot raise an exception LOG.debug(""disconnect"") ioctx.close() client.shutdown() def _rename_snap(self, source_rbd, premade_snap, snapshot): LOG.info(""renaming '%(premade_snap)s' to '%(snap)s'"", { 'premade_snap': premade_snap, 'snap': snapshot}) source_rbd.rename_snap(premade_snap, snapshot) def _get_snap_list(self, source_rbd: rbd.Image, snap_prefix): snapshotlist = source_rbd.list_snaps() benji_snaps = [ snapshot[""name""] for snapshot in snapshotlist if snapshot[""name""].startswith(snap_prefix) ] return benji_snaps def _get_snap_path(self, volume_id, snapshot, include_io_scheme=False): image_name = CONF.volume_name_template % volume_id if include_io_scheme: return ""{0}:{1}/{2}@{3}"".format(self.io_scheme, self.pool, image_name, snapshot) else: return '{0}/{1}@{2}'.format(self.pool, image_name, snapshot) def get_metadata(self, volume_id): return self.backup_meta_api.get(volume_id) def put_metadata(self, volume_id, json_metadata): self.backup_meta_api.put(volume_id, json_metadata) def _parse_backend(self, volume_id): # Using DEFAULT section to configure drivers # is not supported since Ocata. if len(CONF.enabled_backends) > 1: src_volume = self.db.volume_get(self.context, volume_id) backend = [b.value for b in src_volume.volume_type.extra_specs if b.key == ""volume_backend_name""] LOG.debug(src_volume.volume_type.extra_specs) LOG.debug(""'%(backend)s'"", {'backend': backend}) else: backend = CONF.enabled_backends return backend def _backup_rbd_differential(self, backup): with eventlet.tpool.Proxy(rbd_driver.RADOSClient( self, self.pool)) as client: with eventlet.tpool.Proxy(self.rbd.Image( client.ioctx, CONF.volume_name_template % backup.volume_id, read_only=False)) as source_rbd: benji_snaps_unsorted = self._get_snap_list( source_rbd, CONF.benji_snapshot_prefix) # sort the list by name without the prefix, i.e by date. benji_snaps = sorted(benji_snaps_unsorted, key=lambda x: x.split( CONF.benji_snapshot_prefix)[1]) LOG.debug(benji_snaps) for delete_snapname in benji_snaps[:-1]: source_rbd.remove_snap(delete_snapname) last_snapshot = benji_snaps[-1] versions = self.b_backup.find_versions_with_filter( f'volume == ""{backup.volume_id}""\ and snapshot == ""{last_snapshot}""\ and status == ""valid""') if len(versions) == 0: raise exception.BackupDriverException( reason = 'latest snapshot is not in ' + 'benji db please fallback to full backup') LOG.debug(versions[0].uid) now = datetime.utcnow() snapshot = now.strftime(CONF.benji_snapshot_prefix + '%Y-%m-%dT%H:%M:%SZ') premade_snaps = self._get_snap_list( source_rbd, CONF.benji_premade_snapshot_prefix) LOG.debug(premade_snaps) if len(premade_snaps) > 0: self._rename_snap(source_rbd, premade_snaps[-1], snapshot) else: source_rbd.create_snap(snapshot) with eventlet.tpool.Proxy( self.rbd.Image( client.ioctx, CONF.volume_name_template % backup.volume_id, snapshot=snapshot, read_only=False)) as source_rbd_snap: self.diff_list = [] source_rbd_snap.diff_iterate( 0, source_rbd_snap.size(), last_snapshot, self.iterate_cb, whole_object=True) source_rbd.remove_snap(last_snapshot) self.b_backup.backup(version_uid=backup.id, base_version_uid=versions[0].uid, volume=backup.volume_id, snapshot=snapshot, storage_name=self.storage_name, source=self._get_snap_path( backup.volume_id, snapshot, include_io_scheme=True), hints=self.diff_list) self._add_labels(backup) source_rbd_snap.close() source_rbd.close() return def _backup_rbd_initial(self, backup): with eventlet.tpool.Proxy(rbd_driver.RADOSClient( self, self.pool)) as client: with eventlet.tpool.Proxy(self.rbd.Image( client.ioctx, CONF.volume_name_template % backup.volume_id, read_only=False)) as source_rbd: now = datetime.utcnow() snapshot = now.strftime(CONF.benji_snapshot_prefix + '%Y-%m-%dT%H:%M:%SZ') premade_snaps = self._get_snap_list( source_rbd, CONF.benji_premade_snapshot_prefix) LOG.debug(premade_snaps) if len(premade_snaps) > 0: self._rename_snap(source_rbd, premade_snaps[-1], snapshot) else: LOG.debug(source_rbd.get_name()) LOG.debug(source_rbd.create_snap(snapshot)) with eventlet.tpool.Proxy(self.rbd.Image( client.ioctx, CONF.volume_name_template % backup.volume_id, snapshot=snapshot, read_only=False)) as source_rbd_snap: self.diff_list = [] source_rbd_snap.diff_iterate(0, source_rbd.size(), None, self.iterate_cb, whole_object=True) self.b_backup.backup(version_uid=backup.id, volume=backup.volume_id, snapshot=snapshot, storage_name=self.storage_name, source=self._get_snap_path( backup.volume_id, snapshot, include_io_scheme=True), hints=self.diff_list) self._add_labels(backup) LOG.debug(""done"") source_rbd_snap.close() source_rbd.close() return def _add_labels(self, backup): Benji.add_label(version_uid=backup.id, key='openstack-project-id', value=backup.project_id) Benji.add_label(version_uid=backup.id, key='openstack-user-id', value=backup.user_id) def _backup_file(self, backup, volume_file): source = f'{self.io_scheme}:{volume_file._obj.name}' self.b_backup.backup(version_uid=backup.id, source=source, snapshot="""", storage_name=self.storage_name, volume=backup.volume_id) self._add_labels(backup) def backup(self, backup, volume_file, backup_metadata=False): """"""Start a backup of a specified volume. Some I/O operations may block greenthreads, so in order to prevent starvation parameter volume_file will be a proxy that will execute all methods in native threads, so the method implementation doesn't need to worry about that.. """""" backend = self._parse_backend(backup.volume_id) if hasattr(volume_file, 'rbd_image'): self.io_scheme = CONF.benji_io_scheme_ceph CONF.register_opts(rbd_driver.RBD_OPTS, group=backend[0]) LOG.debug(CONF[backend[0]].rbd_pool) self.pool = CONF[backend[0]].rbd_pool LOG.debug(self.pool) self.ceph_user = CONF[backend[0]].rbd_user self.ceph_conf = CONF[backend[0]].rbd_ceph_conf else: self.io_scheme = CONF.benji_io_scheme_file if backup[""snapshot_id""] is not None: if hasattr(volume_file, 'rbd_image'): ceph_name = volume_file.rbd_image.image.get_name() source = f'{self.io_scheme}:{self.pool}/{ceph_name}' else: dev_file = volume_file._obj.name source = f'{CONF.benji_io_scheme_file}:{dev_file}' self.b_backup.backup(version_uid=backup.id, source=source, storage_name=self.storage_name, snapshot="""", volume=backup.volume_id) self._add_labels(backup) elif backup['parent_id'] is None: if self.io_scheme == CONF.benji_io_scheme_ceph: self._backup_rbd_initial(backup) elif self.io_scheme == CONF.benji_io_scheme_file: self._backup_file(backup, volume_file) elif backup['parent_id'] is not None: if self.io_scheme == CONF.benji_io_scheme_ceph: self._backup_rbd_differential(backup) elif self.io_scheme == CONF.benji_io_scheme_file: self._backup_file(backup, volume_file) def restore(self, backup, volume_id, volume_file): """"""Restore a saved backup. Some I/O operations may block greenthreads, so in order to prevent G starvation parameter volume_file will be a proxy that will execute all methods in native threads, so the method implementation doesn't need to worry about that.. May raise BackupRestoreCancel to indicate that the restoration of a volume has been aborted by changing the backup status. """""" backend = self._parse_backend(volume_id) force = True sparse = False if hasattr(volume_file, 'rbd_image'): image_name = volume_file.rbd_image.image.get_name() CONF.register_opts(rbd_driver.RBD_OPTS, group=backend[0]) pool = CONF[backend[0]].rbd_pool target = f'{CONF.benji_io_scheme_ceph}:{pool}/{image_name}' sparse = True else: target = f'{CONF.benji_io_scheme_file}:{volume_file._obj.name}' LOG.debug(self.b_backup.restore(version_uid=backup.id, target=target, sparse=sparse, force=force)) return def delete_backup(self, backup): """"""Delete a saved backup."""""" try: self.b_backup.rm(VersionUid(backup.id), force=True) except KeyError: LOG.warning(""'%(version_uid)s' not found in benji database"", {'version_uid': backup.id}) return def export_record(self, backup): """"""Export driver specific backup record information. If backup backend needs additional driver specific information to import backup record back into the system it must overwrite this method and return it here as a dictionary so it can be serialized into a string. Default backup driver implementation has no extra information. :param backup: backup object to export :returns: driver_info - dictionary with extra information """""" return {} def import_record(self, backup, driver_info): """"""Import driver specific backup record information. If backup backend needs additional driver specific information to import backup record back into the system it must overwrite this method since it will be called with the extra information that was provided by export_record when exporting the backup. Default backup driver implementation does nothing since it didn't export any specific data in export_record. :param backup: backup object to export :param driver_info: dictionary with driver specific backup record information :returns: nothing """""" return ",,455,0
openstack%2Fcinder~master~I04373baafd9778c6f2983340de737687a6d8f52d,openstack/cinder,master,I04373baafd9778c6f2983340de737687a6d8f52d,db: Remove unnecessary 'configure' call,NEW,2023-02-17 12:57:16.000000000,2023-07-05 21:00:08.000000000,,"[{'_account_id': 22348}, {'_account_id': 30615}, {'_account_id': 32238}]","[{'number': 1, 'created': '2023-02-17 12:57:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/7abf9164bef4aeb71651476d022cd1a0e5449794', 'message': ""db: Remove unnecessary 'configure' call\n\nWe have the following note inside the 'configure' function:\n\n  # NOTE(lhx_): oslo_db.enginefacade is configured in tests the same\n  # way as it's done for any other services that uses the db\n\nHowever, unlike nova [1], it seems we're not actually calling this\nanywhere except tests. We could add these calls but given things have\nbeen working fine without them, it seems more sensible to just...drop\nthe thing.\n\n[1] https://github.com/openstack/nova/blob/25.0.1/nova/config.py#L108-L110\n\nChange-Id: I04373baafd9778c6f2983340de737687a6d8f52d\nSigned-off-by: Stephen Finucane <stephenfin@redhat.com>\n""}, {'number': 2, 'created': '2023-03-16 09:18:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/43a9d49cac9a88370d1201da39f3284055bb7b2d', 'message': ""db: Remove unnecessary 'configure' call\n\nWe have the following note inside the 'configure' function:\n\n  # NOTE(lhx_): oslo_db.enginefacade is configured in tests the same\n  # way as it's done for any other services that uses the db\n\nHowever, unlike nova [1], it seems we're not actually calling this\nanywhere except tests. We could add these calls but given things have\nbeen working fine without them, it seems more sensible to just...drop\nthe thing.\n\n[1] https://github.com/openstack/nova/blob/25.0.1/nova/config.py#L108-L110\n\nChange-Id: I04373baafd9778c6f2983340de737687a6d8f52d\nSigned-off-by: Stephen Finucane <stephenfin@redhat.com>\n""}, {'number': 3, 'created': '2023-03-23 12:31:50.000000000', 'files': ['cinder/tests/unit/test.py', 'cinder/db/sqlalchemy/api.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/e710a23613545430a413eddc6850356e5ac2a14b', 'message': ""db: Remove unnecessary 'configure' call\n\nWe have the following note inside the 'configure' function:\n\n  # NOTE(lhx_): oslo_db.enginefacade is configured in tests the same\n  # way as it's done for any other services that uses the db\n\nHowever, unlike nova [1], it seems we're not actually calling this\nanywhere except tests. We could add these calls but given things have\nbeen working fine without them, it seems more sensible to just...drop\nthe thing.\n\n[1] https://github.com/openstack/nova/blob/25.0.1/nova/config.py#L108-L110\n\nChange-Id: I04373baafd9778c6f2983340de737687a6d8f52d\nSigned-off-by: Stephen Finucane <stephenfin@redhat.com>\n""}]",2,874240,e710a23613545430a413eddc6850356e5ac2a14b,74,3,3,15334,,,0,"db: Remove unnecessary 'configure' call

We have the following note inside the 'configure' function:

  # NOTE(lhx_): oslo_db.enginefacade is configured in tests the same
  # way as it's done for any other services that uses the db

However, unlike nova [1], it seems we're not actually calling this
anywhere except tests. We could add these calls but given things have
been working fine without them, it seems more sensible to just...drop
the thing.

[1] https://github.com/openstack/nova/blob/25.0.1/nova/config.py#L108-L110

Change-Id: I04373baafd9778c6f2983340de737687a6d8f52d
Signed-off-by: Stephen Finucane <stephenfin@redhat.com>
",git fetch https://review.opendev.org/openstack/cinder refs/changes/40/874240/3 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/unit/test.py', 'cinder/db/sqlalchemy/api.py']",2,7abf9164bef4aeb71651476d022cd1a0e5449794,sqlalchemy-20,,"def configure(conf): main_context_manager.configure(**dict(conf.database)) # NOTE(geguileo): To avoid a cyclical dependency we import the # group here. Dependency cycle is objects.base requires db.api, # which requires db.sqlalchemy.api, which requires service which # requires objects.base CONF.import_group(""profiler"", ""cinder.service"") if CONF.profiler.enabled: if CONF.profiler.trace_sqlalchemy: lambda eng: osprofiler_sqlalchemy.add_tracing(sa, eng, ""db"") ",1,21
openstack%2Fdevstack-plugin-ceph~master~I5b852797dc05f1de55a2b294289938b9962c38e1,openstack/devstack-plugin-ceph,master,I5b852797dc05f1de55a2b294289938b9962c38e1,Unskip rebuild_volume_backed_server test,MERGED,2023-06-27 19:19:23.000000000,2023-07-05 20:47:24.000000000,2023-07-05 20:46:33.000000000,"[{'_account_id': 8556}, {'_account_id': 16643}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-27 19:19:23.000000000', 'files': ['tempest_skiplist.txt'], 'web_link': 'https://opendev.org/openstack/devstack-plugin-ceph/commit/c4d753d37a7b431ef91ab44c8abeb0bcc4a39334', 'message': 'Unskip rebuild_volume_backed_server test\n\nThis was actually due to a too-strict-for-AIO-machines default timeout,\nwhich is bumped in the dependent patch.\n\nCloses-Bug: #2025096\nDepends-On: https://review.opendev.org/c/openstack/devstack/+/887110\nChange-Id: I5b852797dc05f1de55a2b294289938b9962c38e1\n'}]",7,887111,c4d753d37a7b431ef91ab44c8abeb0bcc4a39334,24,3,1,4393,,,0,"Unskip rebuild_volume_backed_server test

This was actually due to a too-strict-for-AIO-machines default timeout,
which is bumped in the dependent patch.

Closes-Bug: #2025096
Depends-On: https://review.opendev.org/c/openstack/devstack/+/887110
Change-Id: I5b852797dc05f1de55a2b294289938b9962c38e1
",git fetch https://review.opendev.org/openstack/devstack-plugin-ceph refs/changes/11/887111/1 && git format-patch -1 --stdout FETCH_HEAD,['tempest_skiplist.txt'],1,c4d753d37a7b431ef91ab44c8abeb0bcc4a39334,reimage-timeout,," # TODO: Due to password injection issue during rebuild, this test fail on ceph job 100%. # Remve this test from this file once bug: https://bugs.launchpad.net/tempest/+bug/2025096 # is fixed. tempest.api.compute.servers.test_server_actions.ServerActionsV293TestJSON.test_rebuild_volume_backed_server",0,5
openstack%2Fkeystone~master~Id92d74d368356d67d5a9be6d3eada44cd190a35d,openstack/keystone,master,Id92d74d368356d67d5a9be6d3eada44cd190a35d,Add job to test with SQLAlchemy master (2.x),MERGED,2023-06-20 15:33:40.000000000,2023-07-05 20:43:03.000000000,2023-07-05 20:40:42.000000000,"[{'_account_id': 7414}, {'_account_id': 14250}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-20 15:33:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/2fd52713bb64b6c68c670a98cc4e90cc106ce9be', 'message': 'Add job to test with SQLAlchemy master (2.x)\n\nChange-Id: Id92d74d368356d67d5a9be6d3eada44cd190a35d\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}, {'number': 2, 'created': '2023-06-27 09:58:52.000000000', 'files': ['.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/keystone/commit/b2d638e902b0164b1a9531aa1d12a56f103206ec', 'message': 'Add job to test with SQLAlchemy master (2.x)\n\nChange-Id: Id92d74d368356d67d5a9be6d3eada44cd190a35d\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}]",0,886509,b2d638e902b0164b1a9531aa1d12a56f103206ec,12,3,2,15334,,,0,"Add job to test with SQLAlchemy master (2.x)

Change-Id: Id92d74d368356d67d5a9be6d3eada44cd190a35d
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/keystone refs/changes/09/886509/1 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.yaml'],1,2fd52713bb64b6c68c670a98cc4e90cc106ce9be,sqlalchemy-20,"# Temporary job until SQLAlchemy 2.0 is no longer blocked by upper-requirements - job: name: keystone-tox-py310-with-sqlalchemy-2x parent: openstack-tox-py310 description: | Run unit tests with main branch of SQLAlchemy, alembic and oslo.db. Takes advantage of the base tox job's install-siblings feature. # The job only tests the latest and shouldn't be run on the stable branches branches: ^(?!stable) required-projects: - name: github.com/sqlalchemy/sqlalchemy override-checkout: main - name: github.com/sqlalchemy/alembic override-checkout: main - name: openstack/oslo.db - keystone-tox-py310-with-sqlalchemy-2x - keystone-tox-py310-with-sqlalchemy-2x",,18,0
openstack%2Foctavia~stable%2Fyoga~I60ce8c702f5bd5704f5949acb1dd7823a11e91b9,openstack/octavia,stable/yoga,I60ce8c702f5bd5704f5949acb1dd7823a11e91b9,DNM/WIP Testing tips job,NEW,2023-07-05 20:27:43.000000000,2023-07-05 20:33:35.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-05 20:27:43.000000000', 'files': ['zuul.d/projects.yaml'], 'web_link': 'https://opendev.org/openstack/octavia/commit/9a627e947299586fc0b0ba02b522c0b4c6ea6ddc', 'message': 'DNM/WIP Testing tips job\n\nChange-Id: I60ce8c702f5bd5704f5949acb1dd7823a11e91b9\n'}]",0,887741,9a627e947299586fc0b0ba02b522c0b4c6ea6ddc,2,1,1,29244,,,0,"DNM/WIP Testing tips job

Change-Id: I60ce8c702f5bd5704f5949acb1dd7823a11e91b9
",git fetch https://review.opendev.org/openstack/octavia refs/changes/41/887741/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/projects.yaml'],1,9a627e947299586fc0b0ba02b522c0b4c6ea6ddc,, jobs: [], - check-requirements - periodic-stable-jobs-neutron - openstack-cover-jobs - openstack-lower-constraints-jobs - openstack-python3-yoga-jobs - publish-openstack-docs-pti - release-notes-jobs-python3 jobs: - openstack-tox-pip-check-reqs: irrelevant-files: - ^.*\.rst$ - ^api-ref/.*$ - ^doc/.*$ - ^etc/.*$ - ^octavia/tests/.*$ - ^releasenotes/.*$ - openstack-tox-functional-py36: irrelevant-files: - ^.*\.rst$ - ^api-ref/.*$ - ^doc/.*$ - ^etc/.*$ - ^octavia/tests/unit/.*$ - ^releasenotes/.*$ - octavia-v2-dsvm-noop-api: irrelevant-files: &irrelevant-files - ^.*\.rst$ - ^api-ref/.*$ - ^doc/.*$ - ^octavia/tests/.*$ - ^releasenotes/.*$ - octavia-v2-dsvm-scenario: irrelevant-files: *irrelevant-files - octavia-v2-dsvm-tls-barbican: irrelevant-files: *irrelevant-files - octavia-grenade: irrelevant-files: - ^.*\.rst$ - ^api-ref/.*$ - ^doc/.*$ - ^octavia/tests/.*$ - ^releasenotes/.*$ - ^setup.cfg$ - ^tools/.*$ - ^(test-|)requirements.txt$ - ^tox.ini$ - octavia-v2-dsvm-tls-barbican-fips: irrelevant-files: *irrelevant-files voting: false - octavia-v2-act-stdby-dsvm-scenario: irrelevant-files: *irrelevant-files voting: false - octavia-v2-dsvm-cinder-amphora: irrelevant-files: *irrelevant-files voting: false - octavia-v2-dsvm-scenario-two-node: irrelevant-files: *irrelevant-files voting: false - octavia-v2-dsvm-scenario-ipv6-only: irrelevant-files: *irrelevant-files voting: false - octavia-v2-dsvm-scenario-amphora-v2: irrelevant-files: *irrelevant-files voting: false - octavia-v2-dsvm-scenario-amphora-v2-no-jobboard: irrelevant-files: *irrelevant-files voting: false - octavia-v2-dsvm-scenario-centos-8-stream: irrelevant-files: *irrelevant-files voting: false,1,70
openstack%2Fmanila~master~I0db507db5dc848d124a4fb59c7feccae6fac07f8,openstack/manila,master,I0db507db5dc848d124a4fb59c7feccae6fac07f8,Follow up SVM Migrate change,NEW,2021-09-09 19:57:03.000000000,2023-07-05 20:28:47.000000000,,"[{'_account_id': 16643}, {'_account_id': 22348}, {'_account_id': 30002}, {'_account_id': 30998}, {'_account_id': 31721}]","[{'number': 1, 'created': '2021-09-09 19:57:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/59e280b7f5f48f4897d389863351e5b3b7ad3a3b', 'message': 'Follow up SVM Migrate change\n\nAddresses few comments that were open points in the SVM migrate\nchange merged during the Xena release.\n\nChange-Id: I0db507db5dc848d124a4fb59c7feccae6fac07f8\n'}, {'number': 2, 'created': '2021-10-26 12:13:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/89912dac56892d3fe9c9c9529e96c6db6b5bbcb2', 'message': 'Follow up SVM Migrate change\n\nAddresses few comments that were open points in the SVM migrate\nchange merged during the Xena release.\n\nChange-Id: I0db507db5dc848d124a4fb59c7feccae6fac07f8\n'}, {'number': 3, 'created': '2022-07-08 21:38:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila/commit/6d40b24ecd57321115c42366466e6485f597e430', 'message': 'Follow up SVM Migrate change\n\nAddresses few comments that were open points in the SVM migrate\nchange merged during the Xena release.\n\nChange-Id: I0db507db5dc848d124a4fb59c7feccae6fac07f8\n'}, {'number': 4, 'created': '2023-07-05 13:28:42.000000000', 'files': ['manila/share/drivers/netapp/dataontap/cluster_mode/lib_multi_svm.py', 'manila/share/drivers/netapp/dataontap/client/api.py', 'manila/tests/share/drivers/netapp/dataontap/client/test_api.py'], 'web_link': 'https://opendev.org/openstack/manila/commit/63ccba9fba9f9ea052aff22604dace59fa37fdd9', 'message': 'Follow up SVM Migrate change\n\nAddresses few comments that were open points in the SVM migrate\nchange merged during the Xena release.\n\nChange-Id: I0db507db5dc848d124a4fb59c7feccae6fac07f8\n'}]",14,808131,63ccba9fba9f9ea052aff22604dace59fa37fdd9,54,5,4,29632,,,0,"Follow up SVM Migrate change

Addresses few comments that were open points in the SVM migrate
change merged during the Xena release.

Change-Id: I0db507db5dc848d124a4fb59c7feccae6fac07f8
",git fetch https://review.opendev.org/openstack/manila refs/changes/31/808131/1 && git format-patch -1 --stdout FETCH_HEAD,"['manila/share/drivers/netapp/dataontap/cluster_mode/lib_multi_svm.py', 'manila/share/drivers/netapp/dataontap/client/api.py', 'manila/tests/share/drivers/netapp/dataontap/client/test_api.py']",3,59e280b7f5f48f4897d389863351e5b3b7ad3a3b,,"import six from lxml import etreeclass NetAppApiBaseClientTests(test.TestCase): """"""Test case for NetApp API server methods"""""" def setUp(self): self.root = api.BaseClient('127.0.0.1', style=api.STYLE_LOGIN_PASSWORD) super(NetAppApiBaseClientTests, self).setUp() @ddt.data(api.STYLE_LOGIN_PASSWORD, api.STYLE_CERTIFICATE) def test_get_style(self, expected_style): self.root._auth_style = expected_style style = self.root.get_style() self.assertEqual(expected_style, style) @ddt.data(api.STYLE_LOGIN_PASSWORD, api.STYLE_CERTIFICATE) def test_set_style(self, expected_style): self.root.set_style(expected_style) self.assertIs(True, hasattr(self.root, '_auth_style')) self.assertEqual(expected_style, self.root._auth_style) def test_set_style_error(self): self.assertRaises( ValueError, self.root.set_transport_type, 'invalid_transport_type' ) @ddt.data(api.TRANSPORT_TYPE_HTTP, api.TRANSPORT_TYPE_HTTPS) def test_get_transport_type(self, expected_type): self.root._protocol = expected_type type = self.root.get_transport_type() self.assertEqual(expected_type, type) @ddt.data(api.TRANSPORT_TYPE_HTTP, api.TRANSPORT_TYPE_HTTPS) def test_set_transport_type(self, expected_type): self.root.set_transport_type(expected_type) expected_type = expected_type.lower() self.assertIs(True, hasattr(self.root, '_protocol')) self.assertEqual(expected_type, self.root._protocol) def test_set_transport_type_error(self): self.assertRaises( ValueError, self.root.set_transport_type, 'invalid_style' ) @ddt.data(api.ZapiClient.SERVER_TYPE_FILER, api.ZapiClient.SERVER_TYPE_DFM) def test_get_server_type(self, expected_type): self.root._server_type = expected_type type = self.root.get_server_type() self.assertEqual(expected_type, type) def test_set_server_type(self): self.assertRaises( NotImplementedError, self.root.set_server_type, 'invalid_type' ) def test_set_api_version_error(self): self.mock_object(six, 'text_type', mock.Mock(side_effect=ValueError())) self.assertRaises( ValueError, self.root.set_api_version, '1', '15' ) @ddt.data(True, False) def test_get_api_version(self, has_api_version): expected_api_version = None if has_api_version: self.root._api_version = '1.15' self.root._api_major_version = 1 self.root._api_minor_version = 15 expected_api_version = (1, 15) api_version = self.root.get_api_version() self.assertEqual(expected_api_version, api_version) def test_set_port_value_error(self): self.assertRaises( ValueError, self.root.set_port, 'not_an_integer' ) def test_get_port(self): expected_port = 80 self.root._port = expected_port port = self.root.get_port() self.assertEqual(expected_port, port) def test_get_timeout(self): expected_timeout = 30 self.root._timeout = expected_timeout timeout = self.root.get_timeout() self.assertEqual(timeout, expected_timeout) def test_set_timeout(self): expected_timeout = 30 self.root.set_timeout(expected_timeout) self.assertIs(True, hasattr(self.root, '_timeout')) self.assertEqual(self.root._timeout, expected_timeout) def test_set_username(self): username = 'admin' self.root.set_username(username) self.assertIs(True, hasattr(self.root, '_username')) self.assertEqual(username, self.root._username) def test_set_password(self): password = '12345' self.root.set_password(password) self.assertIs(True, hasattr(self.root, '_password')) self.assertEqual(password, self.root._password) @ddt.data(api.STYLE_LOGIN_PASSWORD, api.STYLE_CERTIFICATE) def test__build_session(self, auth_style): self.root._auth_style = auth_style mock_basic_auth_handler = mock.Mock() mock_cert_auth_handler = mock.Mock() mock_build_headers = mock.Mock() mock_session = fake.FAKE_HTTP_SESSION expected_auth_handler = ( mock_basic_auth_handler if auth_style == api.STYLE_LOGIN_PASSWORD else mock_cert_auth_handler) self.mock_object(self.root, '_create_basic_auth_handler', mock.Mock(return_value=mock_basic_auth_handler)) self.mock_object(self.root, '_create_certificate_auth_handler', mock.Mock(return_value=mock_cert_auth_handler)) self.mock_object(requests, 'Session', mock.Mock(return_value=mock_session)) self.mock_object(self.root, '_build_headers', mock.Mock(return_value=mock_build_headers)) self.root._build_session() self.assertEqual(self.root._session, mock_session) self.assertEqual(self.root._session.auth, expected_auth_handler) self.assertEqual(self.root._session.verify, True) self.assertEqual(self.root._session.headers, mock_build_headers) def test__build_headers(self): self.assertRaises( NotImplementedError, self.root._build_headers ) def test__create_basic_auth_handler(self): mock_basic_auth = mock.Mock() self.root._password = '12345' self.root._username = 'admin' self.mock_object(requests.auth, 'HTTPBasicAuth', mock.Mock(return_value=mock_basic_auth)) basic_auth = self.root._create_basic_auth_handler() self.assertEqual(basic_auth, mock_basic_auth) @ddt.ddt @ddt.data( (api.TRANSPORT_TYPE_HTTP, api.ZapiClient.SERVER_TYPE_FILER, 80), (api.TRANSPORT_TYPE_HTTP, api.ZapiClient.SERVER_TYPE_DFM, 8088), (api.TRANSPORT_TYPE_HTTPS, api.ZapiClient.SERVER_TYPE_FILER, 443), (api.TRANSPORT_TYPE_HTTPS, api.ZapiClient.SERVER_TYPE_DFM, 8488)) @ddt.unpack def test__set_port(self, protocol, server_type, expected_port): self.root._protocol = protocol self.root._server_type = server_type self.root._set_port() self.assertEqual(self.root._port, str(expected_port)) @ddt.data( (api.ZapiClient.SERVER_TYPE_FILER, api.ZapiClient.URL_FILER), (api.ZapiClient.SERVER_TYPE_DFM, api.ZapiClient.URL_DFM)) @ddt.unpack def test_set_server_type(self, server_type, expected_url): self.root.set_server_type(server_type) self.assertIs(True, hasattr(self.root, '_server_type')) self.assertEqual(self.root._server_type, server_type) self.assertIs(True, hasattr(self.root, '_url')) self.assertEqual(self.root._url, expected_url) @ddt.data(fake.FAKE_XML_STR) 'trace_pattern': '(.*)', 'log': False, 'timeout': 10}, 'trace_pattern': '(?!(volume)).*', 'log': False, 'timeout': None}, 'trace_pattern': '(.*)', 'log': True, 'timeout': None}, 'trace_pattern': '^volume-(info|get-iter)$', 'log': True, 'timeout': None}) def test_invoke_elem_valid(self, trace_enabled, trace_pattern, log, timeout): if timeout: self.root._timeout = timeout @ddt.data(fake.FAKE_RESULT_API_ERRNO_VALID, fake.FAKE_RESULT_API_ERRNO_INVALID) def test_invoke_successfully_error(self, errored_na_element): na_elem = api.NaElement('netapp') self.mock_object( self.root, 'invoke_elem', mock.Mock(return_value=errored_na_element)) self.assertRaises( api.NaApiError, self.root.invoke_successfully, na_elem ) self.root.invoke_elem.assert_called_once_with( na_elem, enable_tunneling=False) def test_invoke_successfully(self): na_elem = api.NaElement('netapp') api.NaElement.translate_struct = mock.Mock() api_args = { 'source-volume': fake.SHARE_NAME, 'vserver': fake.VSERVER_NAME, } expected_result = fake.FAKE_RESULT_SUCCESS self.mock_object(self.root, 'invoke_elem', mock.Mock(return_value=expected_result)) result = self.root.invoke_successfully( na_elem, api_args=api_args) self.assertEqual(expected_result, result) api.NaElement.translate_struct.assert_called_once_with( api_args) self.root.invoke_elem.assert_called_once_with( na_elem, enable_tunneling=False) def test__create_request(self): na_elem = api.NaElement('netapp') na_element = api.NaElement('vserver-get-iter') self.root._api_version = '1.20' self.root._ns = self.root.NETAPP_NS expected_result = na_elem expected_result.add_attr('xmlns', self.root.NETAPP_NS) expected_result.add_attr('version', '1.20') expected_result.add_child_elem(na_element) api.NaElement.add_child_elem = mock.Mock() self.mock_object(api, 'NaElement', mock.Mock(return_value=na_elem)) self.mock_object(self.root, '_enable_tunnel_request') result = self.root._create_request(na_element, enable_tunneling=True) self.assertEqual(expected_result, result) self.root._enable_tunnel_request.assert_called_once_with( na_elem) @ddt.data( {'vfiler': fake.VSERVER_NAME, 'vserver': None}, {'vfiler': None, 'vserver': fake.VSERVER_NAME}, ) @ddt.unpack def test__enable_tunnel_request_no_api_version(self, vfiler, vserver): request_elem = api.NaElement('root') if vfiler: self.root._vfiler = vfiler if vserver: self.root._vserver = vserver self.assertRaises( ValueError, self.root._enable_tunnel_request, request_elem ) @ddt.data( {'vfiler': fake.VSERVER_NAME, 'vserver': None}, {'vfiler': None, 'vserver': fake.VSERVER_NAME}, ) @ddt.unpack def test__enable_tunnel_request(self, vfiler, vserver): request_elem = api.NaElement('root') if vfiler: self.root._vfiler = vfiler self.root._api_major_version = 1 self.root._api_minor_version = 7 if vserver: self.root._vserver = vserver self.root._api_major_version = 1 self.root._api_minor_version = 15 self.root._enable_tunnel_request(request_elem) if vfiler: self.assertEqual(request_elem.get_attr('vfiler'), vfiler) if vserver: self.assertEqual(request_elem.get_attr('vfiler'), vserver) def test__parse_response(self): response_string = ( api.NaElement(fake.VSERVER_GET_ITER_RESPONSE).to_string()) response_xml = etree.XML(response_string) self.mock_object(etree, 'XML', mock.Mock(return_value=response_xml)) result = self.root._parse_response(response_string) self.assertEqual(response_string, result.to_string()) def test__get_result(self): expected_result = fake.FAKE_RESULT_SUCCESS.get_child_by_name('results') self.mock_object(self.root, '_parse_response', mock.Mock(return_value=fake.FAKE_RESULT_SUCCESS)) result = self.root._get_result(fake.VSERVER_GET_ITER_RESPONSE) self.assertEqual(expected_result, result) @ddt.data('2001:db8:3333:4444:5555:6666:7777:8888', '10.20.30.40') def test__get_url(self, host): self.root._host = host self.root._protocol = protocol = 'http' self.root._port = port = '80' self.root._url = url = 'fakeurl/' if ':' in host: host = '[%s]' % host expected_url = ('%s://%s:%s/%s' % ( protocol, host, port, url)) result = self.root._get_url() self.assertEqual(expected_url, result) def test__build_headers(self): expected_headers = {'Content-Type': 'text/xml'} result = self.root._build_headers() self.assertEqual(expected_headers, result) 'body': fake.FAKE_HTTP_BODY, 'timeout': 10, 'body': fake.FAKE_HTTP_BODY, 'timeout': None, 'body': fake.FAKE_HTTP_BODY, 'timeout': None, 'body': fake.FAKE_HTTP_BODY, 'timeout': None, body, timeout): expected_post_params = {} if timeout: self.root._timeout = timeout expected_post_params['timeout'] = timeout expected_url, data=body, **expected_post_params) def test_invoke_sucessfully(self): body = fake.FAKE_HTTP_BODY query = fake.FAKE_HTTP_QUERY api_args = { ""body"": body, ""query"": query } self.mock_object(self.root, 'invoke_elem', mock.Mock(return_value=fake.FAKE_JOB_SUCCESS_STATE)) result = self.root.invoke_successfully(fake.FAKE_NA_ELEMENT, api_args) self.assertEqual(result, fake.FAKE_JOB_SUCCESS_STATE) self.root.invoke_elem.assert_called_once_with( fake.FAKE_NA_ELEMENT, api_args=api_args) @ddt.data(api.ESIS_CLONE_NOT_LICENSED, None) def test_invoke_sucessfuly_error(self, error_code): body = fake.FAKE_HTTP_BODY query = fake.FAKE_HTTP_QUERY api_args = { ""body"": body, ""query"": query } fake_job_error_state = fake.FAKE_JOB_SUCCESS_STATE fake_job_error_state['state'] = 'error' fake_job_error_state['error'] = { 'error': { 'code': error_code, 'message': 'error reason' }, } self.mock_object(self.root, 'invoke_elem', mock.Mock(return_value=fake_job_error_state)) self.assertRaises( api.NaApiError, self.root.invoke_successfully, fake.FAKE_NA_ELEMENT, api_args) self.root.invoke_elem.assert_called_once_with( fake.FAKE_NA_ELEMENT, api_args=api_args) def test__build_headers(self): expected_headers = { ""Accept"": ""application/json"", ""Content-Type"": ""application/json"" } headers = self.root._build_headers() self.assertEqual(expected_headers, headers) @ddt.ddt class NaServerTestCase(test.TestCase): """"""Test case for NetApp API Rest server methods"""""" def setUp(self): self.root = api.NaServer('127.0.0.1') super(NaServerTestCase, self).setUp() def _mock_setter(self, attribute_to_mock): self.mock_object(self.root.zapi_client, attribute_to_mock) self.mock_object(self.root.rest_client, attribute_to_mock) def test_get_transport_type(self): use_zapi = False self.mock_object(self.root, 'get_client', mock.Mock(return_value=self.root.zapi_client)) self.mock_object(self.root.zapi_client, 'get_transport_type') self.root.get_transport_type(use_zapi_client=use_zapi) self.root.get_client.assert_called_once_with(use_zapi=use_zapi) self.root.zapi_client.get_transport_type.assert_called_once() def test_set_transport_type(self): self._mock_setter('set_transport_type') transport_type = api.TRANSPORT_TYPE_HTTP self.root.set_transport_type(transport_type) for client in [self.root.zapi_client, self.root.rest_client]: client.set_transport_type.assert_called_once_with(transport_type) def test_get_style(self): use_zapi = False self.mock_object(self.root, 'get_client', mock.Mock(return_value=self.root.zapi_client)) self.mock_object(self.root.zapi_client, 'get_style') self.root.get_style(use_zapi_client=use_zapi) self.root.get_client.assert_called_once_with(use_zapi=use_zapi) self.root.zapi_client.get_style.assert_called_once() def test_set_style(self): self._mock_setter('set_style') style = api.STYLE_LOGIN_PASSWORD self.root.set_style(style) for client in [self.root.zapi_client, self.root.rest_client]: client.set_style.assert_called_once_with(style) def test_get_server_type(self): use_zapi = False self.mock_object(self.root, 'get_client', mock.Mock(return_value=self.root.zapi_client)) self.mock_object(self.root.zapi_client, 'get_server_type') self.root.get_server_type(use_zapi_client=use_zapi) self.root.get_client.assert_called_once_with(use_zapi=use_zapi) self.root.zapi_client.get_server_type.assert_called_once() def test_set_server_type(self): self._mock_setter('set_server_type') server_type = api.ZapiClient.SERVER_TYPE_FILER self.root.set_server_type(server_type) for client in [self.root.zapi_client]: client.set_server_type.assert_called_once_with(server_type) def test_get_api_version(self): use_zapi = False self.mock_object(self.root, 'get_client', mock.Mock(return_value=self.root.zapi_client)) self.mock_object(self.root.zapi_client, 'get_api_version') self.root.get_api_version(use_zapi_client=use_zapi) self.root.get_client.assert_called_once_with(use_zapi=use_zapi) self.root.zapi_client.get_api_version.assert_called_once() def test_set_port(self): self._mock_setter('set_port') port = 80 self.root.set_port(port) for client in [self.root.zapi_client, self.root.rest_client]: client.set_port.assert_called_once_with(port) def test_get_port(self): use_zapi = False self.mock_object(self.root, 'get_client', mock.Mock(return_value=self.root.zapi_client)) self.mock_object(self.root.zapi_client, 'get_port') self.root.get_port(use_zapi_client=use_zapi) self.root.get_client.assert_called_once_with(use_zapi=use_zapi) self.root.zapi_client.get_port.assert_called_once() def test_set_timeout(self): self._mock_setter('set_timeout') timeout = 10 self.root.set_timeout(timeout) for client in [self.root.zapi_client, self.root.rest_client]: client.set_timeout.assert_called_once_with(timeout) def test_get_timeout(self): use_zapi = False self.mock_object(self.root, 'get_client', mock.Mock(return_value=self.root.zapi_client)) self.mock_object(self.root.zapi_client, 'get_timeout') self.root.get_timeout(use_zapi_client=use_zapi) self.root.get_client.assert_called_once_with(use_zapi=use_zapi) self.root.zapi_client.get_timeout.assert_called_once() def test_get_vfiler(self): self.mock_object(self.root, 'get_client', mock.Mock(return_value=self.root.zapi_client)) self.mock_object(self.root.zapi_client, 'get_vfiler') self.root.get_vfiler() self.root.zapi_client.get_vfiler.assert_called_once() def test_get_vserver(self): self.mock_object(self.root, 'get_client', mock.Mock(return_value=self.root.zapi_client)) self.mock_object(self.root.zapi_client, 'get_vserver') self.root.get_vserver() self.root.get_client.assert_called_once() self.root.zapi_client.get_vserver.assert_called_once() def test_set_username(self): self._mock_setter('set_username') username = 'admin' self.root.set_username(username) for client in [self.root.zapi_client, self.root.rest_client]: client.set_username.assert_called_once_with(username) def test_set_password(self): self._mock_setter('set_password') username = '12345' self.root.set_password(username) for client in [self.root.zapi_client, self.root.rest_client]: client.set_password.assert_called_once_with(username) @ddt.data(True, False) def test_get_client(self, use_zapi): expected_client = ( self.root.zapi_client if use_zapi else self.root.rest_client) result_client = self.root.get_client(use_zapi=use_zapi) self.assertEqual(result_client, expected_client)"," @ddt.data(None, fake.FAKE_XML_STR) 'trace_pattern': '(.*)', 'log': False}, 'trace_pattern': '(?!(volume)).*', 'log': False}, 'trace_pattern': '(.*)', 'log': True}, 'trace_pattern': '^volume-(info|get-iter)$', 'log': True}) def test_invoke_elem_valid(self, trace_enabled, trace_pattern, log): 'body': fake.FAKE_HTTP_BODY 'body': fake.FAKE_HTTP_BODY 'body': fake.FAKE_HTTP_BODY 'body': fake.FAKE_HTTP_BODY body): expected_url, data=body)",608,17
openstack%2Fopenstack-ansible-os_heat~master~I956138b85392394da3eed73b2adadefa68460e8e,openstack/openstack-ansible-os_heat,master,I956138b85392394da3eed73b2adadefa68460e8e,DNM - test heat deployment,NEW,2023-07-05 17:48:06.000000000,2023-07-05 19:51:15.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-05 17:48:06.000000000', 'files': ['test'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_heat/commit/e97975ac454f4ff112ee5dc426f4b08493bf11d8', 'message': 'DNM - test heat deployment\n\nChange-Id: I956138b85392394da3eed73b2adadefa68460e8e\n'}]",0,887731,e97975ac454f4ff112ee5dc426f4b08493bf11d8,2,1,1,25023,,,0,"DNM - test heat deployment

Change-Id: I956138b85392394da3eed73b2adadefa68460e8e
",git fetch https://review.opendev.org/openstack/openstack-ansible-os_heat refs/changes/31/887731/1 && git format-patch -1 --stdout FETCH_HEAD,['test'],1,e97975ac454f4ff112ee5dc426f4b08493bf11d8,,,,0,0
openstack%2Fneutron~master~Ie4e807b1490d59390316ec20b499b7676acfe410,openstack/neutron,master,Ie4e807b1490d59390316ec20b499b7676acfe410,Switch fullstack/functional fips jobs to 9-stream,MERGED,2022-05-25 07:53:09.000000000,2023-07-05 19:42:15.000000000,2023-07-05 19:40:10.000000000,"[{'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-05-25 07:53:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/994668e707f92c1b73aec64ed956a94ff01e0d5d', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 2, 'created': '2022-05-26 05:23:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8a499499b14342e9bdb83f9b5ee518acf0f435d0', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 3, 'created': '2022-05-27 07:34:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/4421279cbbcdce4004a190976b4594f59b537e49', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 4, 'created': '2022-05-27 07:53:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c2ebb8b8818de9c578637820dc79dd22dbaf02e4', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 5, 'created': '2022-05-27 13:07:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c23a3603579348ee5e997a817f30055f8ccabb74', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 6, 'created': '2022-05-27 14:12:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/a129258d4ed17215dd98f5ac6ffd8b1e90d70b71', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 7, 'created': '2022-06-03 13:30:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/bdb23b2951c4ebdf3659eef8d89f4dd490a87fa6', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 8, 'created': '2022-06-03 14:17:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e2c03f163777647ddff131ae89fcee3a871617fe', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 9, 'created': '2022-07-27 18:50:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/d0c0655e552b2cf692cff5d00064958b21df7e03', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 10, 'created': '2022-07-28 12:09:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/2857226a51568e6780f85b67fe81d9bd35eae0eb', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 11, 'created': '2022-08-23 04:12:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/2b793873503190460063f689b0fadf2f5935f8d4', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nCloses-Bug: #1976323\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 12, 'created': '2022-10-19 09:58:45.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/b263399b2b2dadb00b86c8f229242065bb54b9ff', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nCloses-Bug: #1976323\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 13, 'created': '2023-03-31 14:10:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c119aa3ebd7d8dd50422dc9ba66e8c657f61c342', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nCloses-Bug: #1976323\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 14, 'created': '2023-03-31 14:13:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/26f793261652da8b1eab3a7049e7072b4beb009d', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nCloses-Bug: #1976323\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 15, 'created': '2023-04-10 05:52:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/9c6ded360b05b811dc91407145493efd8367345f', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nCloses-Bug: #1976323\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 16, 'created': '2023-04-11 12:59:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/e773914e412173188f5db48cd7c746b5fe358582', 'message': ""[WIP] Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nCloses-Bug: #1976323\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 17, 'created': '2023-06-30 12:32:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/4056842951fadc7aa6a4db72e273c569a2e1e31e', 'message': ""Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nCloses-Bug: #1976323\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 18, 'created': '2023-06-30 13:00:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/ebcf5add5233d6c095a3ba31d6bebe414d0c65ab', 'message': ""Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nCloses-Bug: #1976323\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}, {'number': 19, 'created': '2023-07-03 07:46:20.000000000', 'files': ['zuul.d/base.yaml', 'neutron/tests/fullstack/test_local_ip.py', 'roles/configure_functional_tests/tasks/main.yaml', 'neutron/tests/fullstack/test_l3_agent.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/42ae9448701b7925b736f5706140d414b53d9012', 'message': ""Switch fullstack/functional fips jobs to 9-stream\n\nMaster no longer support py3.6, so let's switch\nthese jobs to CentOS 9-stream which includes py3.9.\n\nAlso dbcounter[1] is not installable on CentOS 8-stream\nand hence these jobs are currently broken.\n\nOther fips jobs already switched with[2].\n\n[1] https://review.opendev.org/c/openstack/devstack/+/839820\n[2] https://review.opendev.org/c/openstack/neutron/+/833173\n\nCloses-Bug: #1976323\nChange-Id: Ie4e807b1490d59390316ec20b499b7676acfe410\n""}]",15,843245,42ae9448701b7925b736f5706140d414b53d9012,88,5,19,13861,,,0,"Switch fullstack/functional fips jobs to 9-stream

Master no longer support py3.6, so let's switch
these jobs to CentOS 9-stream which includes py3.9.

Also dbcounter[1] is not installable on CentOS 8-stream
and hence these jobs are currently broken.

Other fips jobs already switched with[2].

[1] https://review.opendev.org/c/openstack/devstack/+/839820
[2] https://review.opendev.org/c/openstack/neutron/+/833173

Closes-Bug: #1976323
Change-Id: Ie4e807b1490d59390316ec20b499b7676acfe410
",git fetch https://review.opendev.org/openstack/neutron refs/changes/45/843245/19 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/base.yaml', 'zuul.d/project.yaml', 'roles/configure_functional_tests/tasks/main.yaml']",3,994668e707f92c1b73aec64ed956a94ff01e0d5d,bug/1976323," OPENSTACK_RELEASE=victoria # For CentOS 9-Stream need to setup yoga repos if [[ $os_VENDOR == ""CentOSStream"" && $os_RELEASE -gt 8 ]]; then OPENSTACK_RELEASE=yoga fi install_package centos-release-openstack-${OPENSTACK_RELEASE}", install_package centos-release-openstack-victoria,12,27
openstack%2Fopenstack-ansible-os_rally~master~I016e457c0e4b7819d6d65af3bc35e06061f92d1c,openstack/openstack-ansible-os_rally,master,I016e457c0e4b7819d6d65af3bc35e06061f92d1c,Include proper commit in rally_upper_constraints_url,MERGED,2023-07-03 17:01:36.000000000,2023-07-05 19:16:18.000000000,2023-07-05 19:15:19.000000000,"[{'_account_id': 16011}, {'_account_id': 22348}, {'_account_id': 25023}, {'_account_id': 32666}]","[{'number': 1, 'created': '2023-07-03 17:01:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_rally/commit/bb9c8cae2a18489813edb4962135c039d7c1c7d3', 'message': 'Include proper commit in rally_upper_constraints_url\n\nCurrently, rally_upper_constraints_url always points to master branch.\nIt is not a valid behavior because u-c from master may not work for\nstable branches.\nThis change fixes rally_upper_constraints_url.\n\nChange-Id: I016e457c0e4b7819d6d65af3bc35e06061f92d1c\n'}, {'number': 2, 'created': '2023-07-04 10:59:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_rally/commit/7a37c8b8ae2956325e28815d2c8fc039b13d0d7d', 'message': 'Include proper commit in rally_upper_constraints_url\n\nCurrently, rally_upper_constraints_url always points to master branch.\nIt is not a valid behavior because u-c from master may not work for\nstable branches.\nThis change fixes rally_upper_constraints_url.\n\nDepends-On: https://review.opendev.org/c/openstack/openstack-ansible-rabbitmq_server/+/887592\n\nChange-Id: I016e457c0e4b7819d6d65af3bc35e06061f92d1c\n'}, {'number': 3, 'created': '2023-07-04 17:01:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_rally/commit/412e290c349ef7d6eb3314de532e57d56d489072', 'message': 'Include proper commit in rally_upper_constraints_url\n\nCurrently, rally_upper_constraints_url always points to master branch.\nIt is not a valid behavior because u-c from master may not work for\nstable branches.\nThis change fixes rally_upper_constraints_url.\n\nChange-Id: I016e457c0e4b7819d6d65af3bc35e06061f92d1c\n'}, {'number': 4, 'created': '2023-07-04 17:04:36.000000000', 'files': ['defaults/main.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-os_rally/commit/c65b91b4900292fb5c4a07dd4473fcf4554b111e', 'message': 'Include proper commit in rally_upper_constraints_url\n\nCurrently, rally_upper_constraints_url always points to master branch.\nIt is not a valid behavior because u-c from master may not work for\nstable branches.\nThis change fixes rally_upper_constraints_url.\n\nDepends-On: https://review.opendev.org/c/openstack/openstack-ansible-rabbitmq_server/+/887592\n\nChange-Id: I016e457c0e4b7819d6d65af3bc35e06061f92d1c\n'}]",2,887528,c65b91b4900292fb5c4a07dd4473fcf4554b111e,16,4,4,32666,,,0,"Include proper commit in rally_upper_constraints_url

Currently, rally_upper_constraints_url always points to master branch.
It is not a valid behavior because u-c from master may not work for
stable branches.
This change fixes rally_upper_constraints_url.

Depends-On: https://review.opendev.org/c/openstack/openstack-ansible-rabbitmq_server/+/887592

Change-Id: I016e457c0e4b7819d6d65af3bc35e06061f92d1c
",git fetch https://review.opendev.org/openstack/openstack-ansible-os_rally refs/changes/28/887528/3 && git format-patch -1 --stdout FETCH_HEAD,['defaults/main.yml'],1,bb9c8cae2a18489813edb4962135c039d7c1c7d3,,"rally_upper_constraints_url: ""{{ rally_openstack_git_repo }}/raw/{{ (rally_openstack_git_install_branch == 'master') | ternary('branch','commit') }}/{{ rally_openstack_git_install_branch }}/upper-constraints.txt""","rally_upper_constraints_url: ""{{ rally_openstack_git_repo }}/raw/branch/master/upper-constraints.txt""",1,1
openstack%2Fcinder~master~I04d3df9f21052e3088ad44a62f98f601c88be8d7,openstack/cinder,master,I04d3df9f21052e3088ad44a62f98f601c88be8d7,mypy: Annotate cinder/compute/nova.py,NEW,2023-01-04 20:23:37.000000000,2023-07-05 19:00:07.000000000,,"[{'_account_id': 22348}, {'_account_id': 30615}]","[{'number': 1, 'created': '2023-01-04 20:23:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/10d4fdd4d6a6ef48a868e6c2d018e87113354148', 'message': 'mypy: Annotate cinder/compute/nova.py\n\nChange-Id: I04d3df9f21052e3088ad44a62f98f601c88be8d7\n'}, {'number': 2, 'created': '2023-04-10 15:43:08.000000000', 'files': ['cinder/compute/nova.py', 'mypy-files.txt'], 'web_link': 'https://opendev.org/openstack/cinder/commit/e16027b8b3c372eeeeb0e5de1832bd374e6d86b0', 'message': 'mypy: Annotate cinder/compute/nova.py\n\nChange-Id: I04d3df9f21052e3088ad44a62f98f601c88be8d7\n'}]",1,869259,e16027b8b3c372eeeeb0e5de1832bd374e6d86b0,35,2,2,4523,,,0,"mypy: Annotate cinder/compute/nova.py

Change-Id: I04d3df9f21052e3088ad44a62f98f601c88be8d7
",git fetch https://review.opendev.org/openstack/cinder refs/changes/59/869259/2 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/compute/nova.py', 'mypy-files.txt']",2,10d4fdd4d6a6ef48a868e6c2d018e87113354148,,cinder/compute/__init__.py cinder/compute/nova.py,,51,13
openstack%2Fkeystone~master~I95fc21e1d0993de94a4eb61b2b51ada7ed81044b,openstack/keystone,master,I95fc21e1d0993de94a4eb61b2b51ada7ed81044b,db: Don't rely on branched connections,MERGED,2023-06-27 09:58:52.000000000,2023-07-05 18:14:41.000000000,2023-07-05 18:13:30.000000000,"[{'_account_id': 7414}, {'_account_id': 14250}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-27 09:58:52.000000000', 'files': ['keystone/common/sql/migrations/env.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/f477307bd8e71f0d9c37e67e79738fa36359dfbc', 'message': 'db: Don\'t rely on branched connections\n\nWe were previously calling \'connect()\' on the \'connectable\' object in\n\'run_migrations_online\', regardless of whether it was an \'Engine\' or\n\'Connection\' object. This worked because, as noted in an inline comment,\n""when connectable is already a Connection object, calling \'connect()\'\ngives us a *branched connection*."" This is no longer the case. From the\nSQLAlchemy docs [1]:\n\n  The Connection object does not support ""branching"", which was a\n  pattern by which a sub ""connection"" would be used that refers to this\n  connection as a parent.\n\nUpdate our code to reflect this change, using the newly updated example\nfrom the SQLAlchemy cookbook doc [2] as inspiration.\n\n[1] https://docs.sqlalchemy.org/en/14/core/future.html#sqlalchemy.future.Connection\n[2] https://alembic.sqlalchemy.org/en/latest/cookbook.html#connection-sharing\n\nChange-Id: I95fc21e1d0993de94a4eb61b2b51ada7ed81044b\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}]",4,887028,f477307bd8e71f0d9c37e67e79738fa36359dfbc,19,3,1,15334,,,0,"db: Don't rely on branched connections

We were previously calling 'connect()' on the 'connectable' object in
'run_migrations_online', regardless of whether it was an 'Engine' or
'Connection' object. This worked because, as noted in an inline comment,
""when connectable is already a Connection object, calling 'connect()'
gives us a *branched connection*."" This is no longer the case. From the
SQLAlchemy docs [1]:

  The Connection object does not support ""branching"", which was a
  pattern by which a sub ""connection"" would be used that refers to this
  connection as a parent.

Update our code to reflect this change, using the newly updated example
from the SQLAlchemy cookbook doc [2] as inspiration.

[1] https://docs.sqlalchemy.org/en/14/core/future.html#sqlalchemy.future.Connection
[2] https://alembic.sqlalchemy.org/en/latest/cookbook.html#connection-sharing

Change-Id: I95fc21e1d0993de94a4eb61b2b51ada7ed81044b
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/keystone refs/changes/28/887028/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/common/sql/migrations/env.py'],1,f477307bd8e71f0d9c37e67e79738fa36359dfbc,sqlalchemy-20," with connectable.connect() as connection: context.configure( connection=connection, target_metadata=target_metadata, render_as_batch=True, include_name=include_name, include_object=include_object, process_revision_directives=autogen.process_revision_directives, # noqa: E501 ) with context.begin_transaction(): context.run_migrations() else: connection=connectable,"," # when connectable is already a Connection object, calling connect() gives # us a *branched connection*. with connectable.connect() as connection: connection=connection,",13,5
openstack%2Fneutron~master~Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326,openstack/neutron,master,Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326,Set result when lswitch port exist,MERGED,2023-04-28 02:01:46.000000000,2023-07-05 17:58:08.000000000,2023-07-05 17:57:07.000000000,"[{'_account_id': 1131}, {'_account_id': 9845}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-04-28 02:01:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/80fd33ec917c4f0480cf1852ebe1147e80549988', 'message': 'Set result when lswitch port exist\n\nChange-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326\n'}, {'number': 2, 'created': '2023-05-24 06:40:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c67723b8f84e8dd9c433e199dec0c091c3dfa033', 'message': 'Set result when lswitch port exist\n\nChange-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326\n'}, {'number': 3, 'created': '2023-05-24 07:03:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/5e142641ad83b7fc56b23ca7a4fae1d8f7a2c0c0', 'message': 'Set result when lswitch port exist\n\nChange-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326\n'}, {'number': 4, 'created': '2023-06-28 03:05:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/3d9c57b59e595259c3edc2e6a0779a38713134ff', 'message': 'Set result when lswitch port exist\n\nCloses-Bug: #2025202\n\nChange-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326\n'}, {'number': 5, 'created': '2023-06-28 06:16:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/56d644f5c6728dc2d68ee59a3fa23850e5b38900', 'message': 'Set result when lswitch port exist\n\nA TypeError was thrown during a synchronization\ncommand(neutron-ovn-db-sync-util) execution. From the code[1][2], it\ncan be seen. The result of the AddLSwitchPortCommand command will be\npassed as a parameter to the UpdateLSwitchQosOptionsCommand. But if\nthe logical switch port exists, the result will not be set. Therefore,\nwhen the UpdateLSwitchQosOptionsCommand is executed, the port_id will\nnot be obtained, thereby throwing an exception TypeError.\nThis patch sets the result when the logical switch port exists.\n\n[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488\n[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505\n\nCloses-Bug: #2025202\n\nChange-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326\n'}, {'number': 6, 'created': '2023-07-04 06:15:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c4018af4bba11f7f33d3d48a09dc2e8f48fe7b5a', 'message': 'Set result when lswitch port exist\n\nA TypeError was thrown during a synchronization\ncommand(neutron-ovn-db-sync-util) execution. From the code[1][2], it\ncan be seen. The result of the AddLSwitchPortCommand command will be\npassed as a parameter to the UpdateLSwitchPortQosOptionsCommand. But\nif the logical switch port exists, the result will not be set. Therefore,\nwhen the UpdateLSwitchPortQosOptionsCommand is executed, the port_id\nwill not be obtained, thereby throwing an exception TypeError.\nThis patch sets the result when the logical switch port exists.\n\n[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488\n[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505\n\nCloses-Bug: #2025202\n\nChange-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326\n'}, {'number': 7, 'created': '2023-07-04 07:25:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/c4f583048c61d16ebbefc3a2f8734c1247136381', 'message': 'Set result when lswitch port exist\n\nA TypeError was thrown during a synchronization\ncommand(neutron-ovn-db-sync-util) execution. From the code[1][2], it\ncan be seen. The result of the AddLSwitchPortCommand command will be\npassed as a parameter to the UpdateLSwitchPortQosOptionsCommand. But\nif the logical switch port exists, the result will not be set. Therefore,\nwhen the UpdateLSwitchPortQosOptionsCommand is executed, the port_id\nwill not be obtained, thereby throwing an exception TypeError.\nThis patch sets the result when the logical switch port exists.\n\n[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488\n[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505\n\nCloses-Bug: #2025202\n\nChange-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326\n'}, {'number': 8, 'created': '2023-07-04 11:24:58.000000000', 'files': ['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_commands.py', 'neutron/tests/functional/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_ovn_client.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/commands.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/65bbbcee76fc2a7501d4c28e3cc716f1f9a6f763', 'message': 'Set result when lswitch port exist\n\nA TypeError was thrown during a synchronization\ncommand(neutron-ovn-db-sync-util) execution. From the code[1][2], it\ncan be seen. The result of the AddLSwitchPortCommand command will be\npassed as a parameter to the UpdateLSwitchPortQosOptionsCommand. But\nif the logical switch port exists, the result will not be set. Therefore,\nwhen the UpdateLSwitchPortQosOptionsCommand is executed, the port_id\nwill not be obtained, thereby throwing an exception TypeError.\nThis patch sets the result when the logical switch port exists.\n\n[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488\n[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505\n\nCloses-Bug: #2025202\n\nChange-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326\n'}]",14,881771,65bbbcee76fc2a7501d4c28e3cc716f1f9a6f763,49,4,8,30380,,,0,"Set result when lswitch port exist

A TypeError was thrown during a synchronization
command(neutron-ovn-db-sync-util) execution. From the code[1][2], it
can be seen. The result of the AddLSwitchPortCommand command will be
passed as a parameter to the UpdateLSwitchPortQosOptionsCommand. But
if the logical switch port exists, the result will not be set. Therefore,
when the UpdateLSwitchPortQosOptionsCommand is executed, the port_id
will not be obtained, thereby throwing an exception TypeError.
This patch sets the result when the logical switch port exists.

[1] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L488
[2] https://opendev.org/openstack/neutron/src/commit/b71f7ceb3e97e021cb9aeda757a7ffdeeff80e8e/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py#L505

Closes-Bug: #2025202

Change-Id: Ie09d9d3e5efa8a57ad11655c2eb31d2604bab326
",git fetch https://review.opendev.org/openstack/neutron refs/changes/71/881771/8 && git format-patch -1 --stdout FETCH_HEAD,['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/commands.py'],1,80fd33ec917c4f0480cf1852ebe1147e80549988,bug/2025202, self.result = port.uuid,,1,0
openstack%2Fneutron~master~I8236ec1f685a3ae7c503d3ff8148138a875d702a,openstack/neutron,master,I8236ec1f685a3ae7c503d3ff8148138a875d702a,Drop redundant index on ports table,MERGED,2023-06-07 11:02:23.000000000,2023-07-05 17:47:37.000000000,2023-07-05 17:46:26.000000000,"[{'_account_id': 1131}, {'_account_id': 8313}, {'_account_id': 9845}, {'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-07 11:02:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/670f0daac171d274ee2349ab76778317122b7b6a', 'message': ' db: Drop redundant index on ports table\n\n * There already exists a unique constraint on the same columns, making\n   an additional index redundant.\n\nCloses-Bug: #1988421\nChange-Id: I8236ec1f685a3ae7c503d3ff8148138a875d702a\n'}, {'number': 2, 'created': '2023-06-07 12:32:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/39f7ed45e42d3e8087200a28f2fc012cbd07c814', 'message': ' db: Drop redundant index on ports table\n\n * There already exists a unique constraint on the same columns, making\n   an additional index redundant.\n\nCloses-Bug: #1988421\nChange-Id: I8236ec1f685a3ae7c503d3ff8148138a875d702a\n'}, {'number': 3, 'created': '2023-06-12 08:16:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/5194528a15c8925c86632bef3381926756a99ac7', 'message': ' db: Drop redundant index on ports table\n\n * There already exists a unique constraint on the same columns, making\n   an additional index redundant.\n\nCloses-Bug: #1988421\nChange-Id: I8236ec1f685a3ae7c503d3ff8148138a875d702a\n'}, {'number': 4, 'created': '2023-06-16 09:02:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/406465ed8ebb504a75173b7294acfb7d0ab02b07', 'message': 'Drop redundant index on ports table\n\nThere already exists a unique constraint on the same columns, making\nan additional index redundant.\n\nCloses-Bug: #1988421\nChange-Id: I8236ec1f685a3ae7c503d3ff8148138a875d702a\n'}, {'number': 5, 'created': '2023-06-16 13:15:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/749f9533217059c576585c84be79a5e51e7a6065', 'message': 'Drop redundant index on ports table\n\nThere already exists a unique constraint on the same columns, making\nan additional index redundant.\n\nCloses-Bug: #1988421\nChange-Id: I8236ec1f685a3ae7c503d3ff8148138a875d702a\n'}, {'number': 6, 'created': '2023-06-30 09:10:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/8e2d305d9e3f8311ccf745916ebec4917d2f017e', 'message': 'Drop redundant index on ports table\n\nThere already exists a unique constraint on the same columns, making\nan additional index redundant.\n\nCloses-Bug: #1988421\nChange-Id: I8236ec1f685a3ae7c503d3ff8148138a875d702a\n'}, {'number': 7, 'created': '2023-06-30 15:04:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/a6e0743cfdde87da3029593cc3d75961d364142e', 'message': 'Drop redundant index on ports table\n\nThere already exists a unique constraint on the same columns, making\nan additional index redundant.\n\nCloses-Bug: #1988421\nChange-Id: I8236ec1f685a3ae7c503d3ff8148138a875d702a\n'}, {'number': 8, 'created': '2023-06-30 15:10:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/612f484af735bf3763626bb4e7fc6bfddd4c7274', 'message': 'Drop redundant index on ports table\n\nThere already exists a unique constraint on the same columns, making\nan additional index redundant.\n\nCloses-Bug: #1988421\nChange-Id: I8236ec1f685a3ae7c503d3ff8148138a875d702a\n'}, {'number': 9, 'created': '2023-07-03 07:51:46.000000000', 'files': ['neutron/db/migration/alembic_migrations/versions/2023.2/expand/b1199a3adbef_de_duplicate_indices_for_ports.py', 'neutron/db/migration/alembic_migrations/versions/EXPAND_HEAD', 'neutron/db/models_v2.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/b4eb5d71ab8594a24369d15e4dc6580a4c1cf705', 'message': 'Drop redundant index on ports table\n\nThere already exists a unique constraint on the same columns, making\nan additional index redundant.\n\nCloses-Bug: #1988421\nChange-Id: I8236ec1f685a3ae7c503d3ff8148138a875d702a\n'}]",22,885456,b4eb5d71ab8594a24369d15e4dc6580a4c1cf705,48,6,9,32755,,,0,"Drop redundant index on ports table

There already exists a unique constraint on the same columns, making
an additional index redundant.

Closes-Bug: #1988421
Change-Id: I8236ec1f685a3ae7c503d3ff8148138a875d702a
",git fetch https://review.opendev.org/openstack/neutron refs/changes/56/885456/2 && git format-patch -1 --stdout FETCH_HEAD,['neutron/db/models_v2.py'],1,670f0daac171d274ee2349ab76778317122b7b6a,bug/1988421,," 'ix_ports_network_id_mac_address', 'network_id', 'mac_address'), sa.Index(",0,2
openstack%2Fneutron~master~I54b3dbf64ad313f6e3c34a2c774975f6327843c4,openstack/neutron,master,I54b3dbf64ad313f6e3c34a2c774975f6327843c4,doc: fix typo in metering-agent.rst,MERGED,2023-07-03 07:22:10.000000000,2023-07-05 17:27:24.000000000,2023-07-05 17:25:59.000000000,"[{'_account_id': 1131}, {'_account_id': 8313}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 07:22:10.000000000', 'files': ['doc/source/configuration/metering-agent.rst'], 'web_link': 'https://opendev.org/openstack/neutron/commit/f2dd2d3cac4a3cf2602425cc3266b54993454f35', 'message': 'doc: fix typo in metering-agent.rst\n\nSigned-off-by: Sahid Orentino Ferdjaoui <sahid.ferdjaoui@industrialdiscipline.com>\nChange-Id: I54b3dbf64ad313f6e3c34a2c774975f6327843c4\n'}]",2,887477,f2dd2d3cac4a3cf2602425cc3266b54993454f35,9,3,1,7730,,,0,"doc: fix typo in metering-agent.rst

Signed-off-by: Sahid Orentino Ferdjaoui <sahid.ferdjaoui@industrialdiscipline.com>
Change-Id: I54b3dbf64ad313f6e3c34a2c774975f6327843c4
",git fetch https://review.opendev.org/openstack/neutron refs/changes/77/887477/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/configuration/metering-agent.rst'],1,f2dd2d3cac4a3cf2602425cc3266b54993454f35,fixtypo,* ``report_interval``: the interval in seconds used to generated the report,* ``report_interval``: the interval in secodns used to generated the report,1,1
openstack%2Fneutron~stable%2Fyoga~I41ffffeca433faab2244ff3d1876ca078ce5ebfb,openstack/neutron,stable/yoga,I41ffffeca433faab2244ff3d1876ca078ce5ebfb,Load FIP information during initialize not init,MERGED,2023-07-03 09:18:59.000000000,2023-07-05 17:27:15.000000000,2023-07-05 17:25:55.000000000,"[{'_account_id': 1131}, {'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 09:18:59.000000000', 'files': ['neutron/agent/l3/dvr_local_router.py', 'neutron/tests/unit/agent/l3/test_dvr_local_router.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/226e29af1ad39f0b9f9a773fcc1116e7f02f8184', 'message': ""Load FIP information during initialize not init\n\nDvrLocalRouter._load_used_fip_information() is called during the class\ninit however in some cases it tries to access a network namespace which\nhasn't yet been created. This results in NetworkNamespaceNotFound.\n\nThis change ensures that we instead create any FIP priority rules after\nthe network namespace has been created by calling\n_load_used_fip_information() from the initialize function rather than\nin the class instantiation.\n\nCloses-Bug: #2025129\nChange-Id: I41ffffeca433faab2244ff3d1876ca078ce5ebfb\n(cherry picked from commit c8c74f12e048e7858eee332883dbe7c1dc1d0f0c)\n""}]",1,887457,226e29af1ad39f0b9f9a773fcc1116e7f02f8184,12,6,1,35825,,,0,"Load FIP information during initialize not init

DvrLocalRouter._load_used_fip_information() is called during the class
init however in some cases it tries to access a network namespace which
hasn't yet been created. This results in NetworkNamespaceNotFound.

This change ensures that we instead create any FIP priority rules after
the network namespace has been created by calling
_load_used_fip_information() from the initialize function rather than
in the class instantiation.

Closes-Bug: #2025129
Change-Id: I41ffffeca433faab2244ff3d1876ca078ce5ebfb
(cherry picked from commit c8c74f12e048e7858eee332883dbe7c1dc1d0f0c)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/57/887457/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/agent/l3/dvr_local_router.py', 'neutron/tests/unit/agent/l3/test_dvr_local_router.py']",2,226e29af1ad39f0b9f9a773fcc1116e7f02f8184,fix_dvr_net_ns_bug-stable/yoga," @mock.patch.object(router_info.RouterInfo, 'initialize') def test_initialize_dvr_local_router(self, super_initialize): ri = self._create_router() self.mock_load_fip.assert_not_called() ri.initialize(self.process_monitor) super_initialize.assert_called_once_with(self.process_monitor) self.mock_load_fip.assert_called_once() ",,12,0
openstack%2Fneutron~stable%2Fzed~I41ffffeca433faab2244ff3d1876ca078ce5ebfb,openstack/neutron,stable/zed,I41ffffeca433faab2244ff3d1876ca078ce5ebfb,Load FIP information during initialize not init,MERGED,2023-07-03 09:18:35.000000000,2023-07-05 17:27:07.000000000,2023-07-05 17:25:51.000000000,"[{'_account_id': 1131}, {'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 09:18:35.000000000', 'files': ['neutron/agent/l3/dvr_local_router.py', 'neutron/tests/unit/agent/l3/test_dvr_local_router.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/7972c1e2245fcafc207a32e116516d63e6cd0d0a', 'message': ""Load FIP information during initialize not init\n\nDvrLocalRouter._load_used_fip_information() is called during the class\ninit however in some cases it tries to access a network namespace which\nhasn't yet been created. This results in NetworkNamespaceNotFound.\n\nThis change ensures that we instead create any FIP priority rules after\nthe network namespace has been created by calling\n_load_used_fip_information() from the initialize function rather than\nin the class instantiation.\n\nCloses-Bug: #2025129\nChange-Id: I41ffffeca433faab2244ff3d1876ca078ce5ebfb\n(cherry picked from commit c8c74f12e048e7858eee332883dbe7c1dc1d0f0c)\n""}]",0,887456,7972c1e2245fcafc207a32e116516d63e6cd0d0a,9,6,1,35825,,,0,"Load FIP information during initialize not init

DvrLocalRouter._load_used_fip_information() is called during the class
init however in some cases it tries to access a network namespace which
hasn't yet been created. This results in NetworkNamespaceNotFound.

This change ensures that we instead create any FIP priority rules after
the network namespace has been created by calling
_load_used_fip_information() from the initialize function rather than
in the class instantiation.

Closes-Bug: #2025129
Change-Id: I41ffffeca433faab2244ff3d1876ca078ce5ebfb
(cherry picked from commit c8c74f12e048e7858eee332883dbe7c1dc1d0f0c)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/56/887456/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/agent/l3/dvr_local_router.py', 'neutron/tests/unit/agent/l3/test_dvr_local_router.py']",2,7972c1e2245fcafc207a32e116516d63e6cd0d0a,fix_dvr_net_ns_bug-stable/zed," @mock.patch.object(router_info.RouterInfo, 'initialize') def test_initialize_dvr_local_router(self, super_initialize): ri = self._create_router() self.mock_load_fip.assert_not_called() ri.initialize(self.process_monitor) super_initialize.assert_called_once_with(self.process_monitor) self.mock_load_fip.assert_called_once() ",,12,0
openstack%2Fneutron~stable%2F2023.1~I41ffffeca433faab2244ff3d1876ca078ce5ebfb,openstack/neutron,stable/2023.1,I41ffffeca433faab2244ff3d1876ca078ce5ebfb,Load FIP information during initialize not init,MERGED,2023-07-03 09:18:14.000000000,2023-07-05 17:27:04.000000000,2023-07-05 17:25:46.000000000,"[{'_account_id': 1131}, {'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 21798}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 09:18:14.000000000', 'files': ['neutron/agent/l3/dvr_local_router.py', 'neutron/tests/unit/agent/l3/test_dvr_local_router.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/06694e336e8fe8540849ad771bb782c8ab4caa35', 'message': ""Load FIP information during initialize not init\n\nDvrLocalRouter._load_used_fip_information() is called during the class\ninit however in some cases it tries to access a network namespace which\nhasn't yet been created. This results in NetworkNamespaceNotFound.\n\nThis change ensures that we instead create any FIP priority rules after\nthe network namespace has been created by calling\n_load_used_fip_information() from the initialize function rather than\nin the class instantiation.\n\nCloses-Bug: #2025129\nChange-Id: I41ffffeca433faab2244ff3d1876ca078ce5ebfb\n(cherry picked from commit c8c74f12e048e7858eee332883dbe7c1dc1d0f0c)\n""}]",0,887455,06694e336e8fe8540849ad771bb782c8ab4caa35,9,6,1,35825,,,0,"Load FIP information during initialize not init

DvrLocalRouter._load_used_fip_information() is called during the class
init however in some cases it tries to access a network namespace which
hasn't yet been created. This results in NetworkNamespaceNotFound.

This change ensures that we instead create any FIP priority rules after
the network namespace has been created by calling
_load_used_fip_information() from the initialize function rather than
in the class instantiation.

Closes-Bug: #2025129
Change-Id: I41ffffeca433faab2244ff3d1876ca078ce5ebfb
(cherry picked from commit c8c74f12e048e7858eee332883dbe7c1dc1d0f0c)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/55/887455/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/agent/l3/dvr_local_router.py', 'neutron/tests/unit/agent/l3/test_dvr_local_router.py']",2,06694e336e8fe8540849ad771bb782c8ab4caa35,fix_dvr_net_ns_bug-stable/2023.1," @mock.patch.object(router_info.RouterInfo, 'initialize') def test_initialize_dvr_local_router(self, super_initialize): ri = self._create_router() self.mock_load_fip.assert_not_called() ri.initialize(self.process_monitor) super_initialize.assert_called_once_with(self.process_monitor) self.mock_load_fip.assert_called_once() ",,12,0
openstack%2Fdesignate~stable%2F2023.1~I8af9b5cf8c1473bbf7db71a1fb848fb64509db84,openstack/designate,stable/2023.1,I8af9b5cf8c1473bbf7db71a1fb848fb64509db84,Fix list zones if shared with multiple projects,ABANDONED,2023-07-05 16:33:40.000000000,2023-07-05 16:35:14.000000000,,[],"[{'number': 1, 'created': '2023-07-05 16:33:40.000000000', 'files': ['designate/storage/sqlalchemy/__init__.py', 'releasenotes/notes/Fix-zone-list-when-zone-shared-more-than-once-288b57cafeba82df.yaml', 'designate/tests/test_storage/test_storage.py'], 'web_link': 'https://opendev.org/openstack/designate/commit/0b56ffe1b95748e1bb51f74b8ae47d550178ff55', 'message': 'Fix list zones if shared with multiple projects\n\nThis patch fixes a bug when listing zones or updating recordsets in\nzones that are shared with more than one project.\n\nCloses-Bug: #2025295\nChange-Id: I8af9b5cf8c1473bbf7db71a1fb848fb64509db84\n(cherry picked from commit 011ebe2e7cd0df0c7f0869f0c7abbce79434821a)\n'}]",0,887675,0b56ffe1b95748e1bb51f74b8ae47d550178ff55,2,0,1,11628,,,0,"Fix list zones if shared with multiple projects

This patch fixes a bug when listing zones or updating recordsets in
zones that are shared with more than one project.

Closes-Bug: #2025295
Change-Id: I8af9b5cf8c1473bbf7db71a1fb848fb64509db84
(cherry picked from commit 011ebe2e7cd0df0c7f0869f0c7abbce79434821a)
",git fetch https://review.opendev.org/openstack/designate refs/changes/75/887675/1 && git format-patch -1 --stdout FETCH_HEAD,"['designate/storage/sqlalchemy/__init__.py', 'releasenotes/notes/Fix-zone-list-when-zone-shared-more-than-once-288b57cafeba82df.yaml', 'designate/tests/test_storage/test_storage.py']",3,0b56ffe1b95748e1bb51f74b8ae47d550178ff55,,"<<<<<<< HEAD (e74285 Re-enable test jobs) ======= # Copyright 2012 Managed I.T. # # Author: Kiall Mac Innes <kiall@managedit.ie> # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. import math from sqlalchemy import text from unittest import mock from oslo_config import cfg from oslo_log import log as logging from oslo_messaging.rpc import dispatcher as rpc_dispatcher from designate.conf.mdns import DEFAULT_MDNS_PORT from designate import exceptions from designate import objects from designate import storage from designate.storage import sql from designate.tests import TestCase from designate.utils import generate_uuid LOG = logging.getLogger(__name__) class SqlalchemyStorageTest(TestCase): def setUp(self): super(SqlalchemyStorageTest, self).setUp() self.storage = storage.get_storage() # TODO(kiall): Someone, Somewhere, could probably make use of a # assertNestedDictContainsSubset(), cleanup and put somewhere # better. def assertNestedDictContainsSubset(self, expected, actual): for key, value in expected.items(): if isinstance(value, dict): self.assertNestedDictContainsSubset(value, actual.get(key, {})) elif isinstance(value, list): self.assertEqual(len(value), len(actual[key])) for index, item in enumerate(value): self.assertNestedDictContainsSubset( item, actual[key][index]) else: self.assertEqual(value, actual[key]) def create_quota(self, **kwargs): """""" This create method has been kept in the StorageTestCase class as quotas are treated differently to other resources in Central. """""" context = kwargs.pop('context', self.admin_context) fixture = kwargs.pop('fixture', 0) values = self.get_quota_fixture(fixture=fixture, values=kwargs) if 'tenant_id' not in values: values['tenant_id'] = context.project_id return self.storage.create_quota(context, values) def create_pool_nameserver(self, pool, **kwargs): # NOTE(kiall): We add this method here, rather than in the base test # case, as the base methods expect to make a central API # call. If a central API method is exposed for this, we # should remove this and add to the base. context = kwargs.pop('context', self.admin_context) fixture = kwargs.pop('fixture', 0) values = self.get_pool_nameserver_fixture( fixture=fixture, values=kwargs) if 'pool_id' not in values: values['pool_id'] = pool.id return self.storage.create_pool_nameserver( context, pool.id, objects.PoolNameserver.from_dict(values)) def create_pool_target(self, pool, **kwargs): # NOTE(kiall): We add this method here, rather than in the base test # case, as the base methods expect to make a central API # call. If a central API method is exposed for this, we # should remove this and add to the base. context = kwargs.pop('context', self.admin_context) fixture = kwargs.pop('fixture', 0) values = self.get_pool_target_fixture( fixture=fixture, values=kwargs) if 'pool_id' not in values: values['pool_id'] = pool.id return self.storage.create_pool_target( context, pool.id, objects.PoolTarget.from_dict(values)) def create_pool_also_notify(self, pool, **kwargs): # NOTE(kiall): We add this method here, rather than in the base test # case, as the base methods expect to make a central API # call. If a central API method is exposed for this, we # should remove this and add to the base. context = kwargs.pop('context', self.admin_context) fixture = kwargs.pop('fixture', 0) values = self.get_pool_also_notify_fixture( fixture=fixture, values=kwargs) if 'pool_id' not in values: values['pool_id'] = pool.id return self.storage.create_pool_also_notify( context, pool.id, objects.PoolAlsoNotify.from_dict(values)) # Paging Tests def _ensure_paging(self, data, method, criterion=None): """""" Given an array of created items we iterate through them making sure they match up to things returned by paged results. """""" results = None item_number = 0 criterion = criterion or {} for current_page in range(0, int(math.ceil(float(len(data)) / 2))): LOG.critical('Validating results on page %d', current_page) if results is not None: results = method( self.admin_context, limit=2, marker=results[-1]['id'], criterion=criterion ) else: results = method(self.admin_context, limit=2, criterion=criterion) LOG.critical('Results: %d', len(results)) for result_number, result in enumerate(results): LOG.critical('Validating result %d on page %d', result_number, current_page) self.assertEqual( data[item_number]['id'], results[result_number]['id']) item_number += 1 def test_paging_marker_not_found(self): self.assertRaisesRegex( exceptions.MarkerNotFound, 'Marker None could not be found', self.storage.find_pool_attributes, self.admin_context, marker=generate_uuid(), limit=5 ) def test_paging_marker_invalid(self): self.assertRaises( exceptions.InvalidMarker, self.storage.find_pool_attributes, self.admin_context, marker='4' ) def test_paging_limit_invalid(self): self.assertRaisesRegex( exceptions.ValueError, r'invalid literal for int\(\) with base 10: \'z\'', self.storage.find_pool_attributes, self.admin_context, limit='z' ) def test_paging_sort_dir_invalid(self): self.assertRaisesRegex( exceptions.ValueError, r'Unknown sort direction, must be \'desc\' or \'asc\'', self.storage.find_pool_attributes, self.admin_context, sort_dir='invalid_sort_dir' ) def test_paging_sort_key_invalid(self): self.assertRaisesRegex( exceptions.InvalidSortKey, 'Sort key supplied is invalid: None', self.storage.find_pool_attributes, self.admin_context, sort_key='invalid_sort_key' ) # Quota Tests def test_create_quota(self): values = self.get_quota_fixture() values['tenant_id'] = self.admin_context.project_id result = self.storage.create_quota(self.admin_context, values) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNone(result['updated_at']) self.assertEqual(self.admin_context.project_id, result['tenant_id']) self.assertEqual(values['resource'], result['resource']) self.assertEqual(values['hard_limit'], result['hard_limit']) def test_create_quota_duplicate(self): # Create the initial quota self.create_quota() self.assertRaisesRegex( exceptions.DuplicateQuota, 'Duplicate Quota', self.create_quota ) def test_find_quotas(self): actual = self.storage.find_quotas(self.admin_context) self.assertEqual(0, len(actual)) # Create a single quota quota_one = self.create_quota() actual = self.storage.find_quotas(self.admin_context) self.assertEqual(1, len(actual)) self.assertEqual(quota_one['tenant_id'], actual[0]['tenant_id']) self.assertEqual(quota_one['resource'], actual[0]['resource']) self.assertEqual(quota_one['hard_limit'], actual[0]['hard_limit']) # Create a second quota quota_two = self.create_quota(fixture=1) actual = self.storage.find_quotas(self.admin_context) self.assertEqual(2, len(actual)) self.assertEqual(quota_two['tenant_id'], actual[1]['tenant_id']) self.assertEqual(quota_two['resource'], actual[1]['resource']) self.assertEqual(quota_two['hard_limit'], actual[1]['hard_limit']) def test_find_quotas_criterion(self): quota_one = self.create_quota() quota_two = self.create_quota(fixture=1) criterion = dict( tenant_id=quota_one['tenant_id'], resource=quota_one['resource'] ) results = self.storage.find_quotas(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(quota_one['tenant_id'], results[0]['tenant_id']) self.assertEqual(quota_one['resource'], results[0]['resource']) self.assertEqual(quota_one['hard_limit'], results[0]['hard_limit']) criterion = dict( tenant_id=quota_two['tenant_id'], resource=quota_two['resource'] ) results = self.storage.find_quotas(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(quota_two['tenant_id'], results[0]['tenant_id']) self.assertEqual(quota_two['resource'], results[0]['resource']) self.assertEqual(quota_two['hard_limit'], results[0]['hard_limit']) def test_get_quota(self): # Create a quota expected = self.create_quota() actual = self.storage.get_quota(self.admin_context, expected['id']) self.assertEqual(expected['tenant_id'], actual['tenant_id']) self.assertEqual(expected['resource'], actual['resource']) self.assertEqual(expected['hard_limit'], actual['hard_limit']) def test_get_quota_missing(self): uuid = 'caf771fc-6b05-4891-bee1-c2a48621f57b' self.assertRaisesRegex( exceptions.QuotaNotFound, 'Could not find Quota', self.storage.get_quota, self.admin_context, uuid ) def test_find_quota_criterion(self): quota_one = self.create_quota() quota_two = self.create_quota(fixture=1) criterion = dict( tenant_id=quota_one['tenant_id'], resource=quota_one['resource'] ) result = self.storage.find_quota(self.admin_context, criterion) self.assertEqual(quota_one['tenant_id'], result['tenant_id']) self.assertEqual(quota_one['resource'], result['resource']) self.assertEqual(quota_one['hard_limit'], result['hard_limit']) criterion = dict( tenant_id=quota_two['tenant_id'], resource=quota_two['resource'] ) result = self.storage.find_quota(self.admin_context, criterion) self.assertEqual(quota_two['tenant_id'], result['tenant_id']) self.assertEqual(quota_two['resource'], result['resource']) self.assertEqual(quota_two['hard_limit'], result['hard_limit']) def test_find_quota_criterion_missing(self): expected = self.create_quota() criterion = dict( tenant_id=expected['tenant_id'] + 'NOT FOUND' ) self.assertRaisesRegex( exceptions.QuotaNotFound, 'Could not find Quota', self.storage.find_quota, self.admin_context, criterion ) def test_update_quota(self): # Create a quota quota = self.create_quota(fixture=1) # Update the Object quota.hard_limit = 5000 # Perform the update quota = self.storage.update_quota(self.admin_context, quota) # Ensure the new value took self.assertEqual(5000, quota.hard_limit) # Ensure the version column was incremented self.assertEqual(2, quota.version) def test_update_quota_duplicate(self): # Create two quotas quota_one = self.create_quota(fixture=0) quota_two = self.create_quota(fixture=1) # Update the Q2 object to be a duplicate of Q1 quota_two.resource = quota_one.resource self.assertRaisesRegex( exceptions.DuplicateQuota, 'Duplicate Quota', self.storage.update_quota, self.admin_context, quota_two ) def test_update_quota_missing(self): quota = objects.Quota( id='caf771fc-6b05-4891-bee1-c2a48621f57b' ) self.assertRaisesRegex( exceptions.QuotaNotFound, 'Could not find Quota', self.storage.update_quota, self.admin_context, quota ) def test_delete_quota(self): quota = self.create_quota() self.storage.delete_quota(self.admin_context, quota['id']) self.assertRaisesRegex( exceptions.QuotaNotFound, 'Could not find Quota', self.storage.get_quota, self.admin_context, quota['id'] ) def test_delete_quota_missing(self): uuid = 'caf771fc-6b05-4891-bee1-c2a48621f57b' self.assertRaisesRegex( exceptions.QuotaNotFound, 'Could not find Quota', self.storage.delete_quota, self.admin_context, uuid ) # TSIG Key Tests def test_create_tsigkey(self): values = self.get_tsigkey_fixture() result = self.storage.create_tsigkey( self.admin_context, tsigkey=objects.TsigKey.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNone(result['updated_at']) self.assertEqual(values['name'], result['name']) self.assertEqual(values['algorithm'], result['algorithm']) self.assertEqual(values['secret'], result['secret']) self.assertEqual(values['scope'], result['scope']) def test_create_tsigkey_duplicate(self): # Create the Initial TsigKey tsigkey_one = self.create_tsigkey() values = self.get_tsigkey_fixture(1) values['name'] = tsigkey_one['name'] exc = self.assertRaises(rpc_dispatcher.ExpectedException, self.create_tsigkey, **values) self.assertEqual(exceptions.DuplicateTsigKey, exc.exc_info[0]) def test_find_tsigkeys(self): actual = self.storage.find_tsigkeys(self.admin_context) self.assertEqual(0, len(actual)) # Create a single tsigkey tsig = self.create_tsigkey() actual = self.storage.find_tsigkeys(self.admin_context) self.assertEqual(1, len(actual)) self.assertEqual(tsig['name'], actual[0]['name']) self.assertEqual(tsig['algorithm'], actual[0]['algorithm']) self.assertEqual(tsig['secret'], actual[0]['secret']) self.assertEqual(tsig['scope'], actual[0]['scope']) def test_find_tsigkey(self): # Create a single tsigkey tsig = self.create_tsigkey() actual = self.storage.find_tsigkeys(self.admin_context) self.assertEqual(1, len(actual)) name = actual[0].name actual = self.storage.find_tsigkey(self.admin_context, {'name': name}) self.assertEqual(tsig['name'], actual['name']) self.assertEqual(tsig['algorithm'], actual['algorithm']) self.assertEqual(tsig['secret'], actual['secret']) self.assertEqual(tsig['scope'], actual['scope']) def test_find_tsigkeys_paging(self): # Create 10 TSIG Keys created = [self.create_tsigkey(name='tsig-%s' % i) for i in range(10)] # Ensure we can page through the results. self._ensure_paging(created, self.storage.find_tsigkeys) def test_find_tsigkeys_criterion(self): tsigkey_one = self.create_tsigkey(fixture=0) tsigkey_two = self.create_tsigkey(fixture=1) criterion = dict( name=tsigkey_one['name'] ) results = self.storage.find_tsigkeys(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(tsigkey_one['name'], results[0]['name']) criterion = dict( name=tsigkey_two['name'] ) results = self.storage.find_tsigkeys(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(tsigkey_two['name'], results[0]['name']) def test_get_tsigkey(self): # Create a tsigkey expected = self.create_tsigkey() actual = self.storage.get_tsigkey(self.admin_context, expected['id']) self.assertEqual(expected['name'], actual['name']) self.assertEqual(expected['algorithm'], actual['algorithm']) self.assertEqual(expected['secret'], actual['secret']) self.assertEqual(expected['scope'], actual['scope']) def test_get_tsigkey_missing(self): uuid = 'caf771fc-6b05-4891-bee1-c2a48621f57b' self.assertRaisesRegex( exceptions.TsigKeyNotFound, 'Could not find TsigKey', self.storage.get_tsigkey, self.admin_context, uuid ) def test_update_tsigkey(self): # Create a tsigkey tsigkey = self.create_tsigkey(name='test-key') # Update the Object tsigkey.name = 'test-key-updated' # Perform the update tsigkey = self.storage.update_tsigkey(self.admin_context, tsigkey) # Ensure the new value took self.assertEqual('test-key-updated', tsigkey.name) # Ensure the version column was incremented self.assertEqual(2, tsigkey.version) def test_update_tsigkey_duplicate(self): # Create two tsigkeys tsigkey_one = self.create_tsigkey(fixture=0) tsigkey_two = self.create_tsigkey(fixture=1) # Update the T2 object to be a duplicate of T1 tsigkey_two.name = tsigkey_one.name self.assertRaisesRegex( exceptions.DuplicateTsigKey, 'Duplicate TsigKey', self.storage.update_tsigkey, self.admin_context, tsigkey_two ) def test_update_tsigkey_missing(self): tsigkey = objects.TsigKey( id='caf771fc-6b05-4891-bee1-c2a48621f57b' ) self.assertRaisesRegex( exceptions.TsigKeyNotFound, 'Could not find TsigKey', self.storage.update_tsigkey, self.admin_context, tsigkey ) def test_delete_tsigkey(self): tsigkey = self.create_tsigkey() self.storage.delete_tsigkey(self.admin_context, tsigkey['id']) self.assertRaisesRegex( exceptions.TsigKeyNotFound, 'Could not find TsigKey', self.storage.get_tsigkey, self.admin_context, tsigkey['id'] ) def test_delete_tsigkey_missing(self): uuid = 'caf771fc-6b05-4891-bee1-c2a48621f57b' self.assertRaisesRegex( exceptions.TsigKeyNotFound, 'Could not find TsigKey', self.storage.delete_tsigkey, self.admin_context, uuid ) # Tenant Tests def test_find_tenants(self): context = self.get_admin_context() one_context = context one_context.project_id = 'One' two_context = context two_context.project_id = 'Two' context.all_tenants = True # create 3 zones in 2 tenants self.create_zone(fixture=0, context=one_context, tenant_id='One') zone = self.create_zone(fixture=1, context=one_context, tenant_id='One') self.create_zone(fixture=2, context=two_context, tenant_id='Two') # Delete one of the zones. self.storage.delete_zone(context, zone['id']) # Ensure we get accurate results result = self.storage.find_tenants(context) result_dict = [dict(t) for t in result] expected = [{ 'id': 'One', 'zone_count': 1, }, { 'id': 'Two', 'zone_count': 1, }] self.assertEqual(expected, result_dict) def test_get_tenant(self): context = self.get_admin_context() one_context = context one_context.project_id = 1 context.all_tenants = True # create 2 zones in a tenant zone_1 = self.create_zone(fixture=0, context=one_context) zone_2 = self.create_zone(fixture=1, context=one_context) zone_3 = self.create_zone(fixture=2, context=one_context) # Delete one of the zones. self.storage.delete_zone(context, zone_3['id']) result = self.storage.get_tenant(context, 1) self.assertEqual(1, result['id']) self.assertEqual(2, result['zone_count']) self.assertEqual([zone_1['name'], zone_2['name']], sorted(result['zones'])) def test_count_tenants(self): context = self.get_admin_context() one_context = context one_context.project_id = 1 two_context = context two_context.project_id = 2 context.all_tenants = True # in the beginning, there should be nothing tenants = self.storage.count_tenants(context) self.assertEqual(0, tenants) # create 2 zones with 2 tenants self.create_zone(fixture=0, context=one_context, tenant_id=1) self.create_zone(fixture=1, context=two_context, tenant_id=2) zone = self.create_zone(fixture=2, context=two_context, tenant_id=2) # Delete one of the zones. self.storage.delete_zone(context, zone['id']) tenants = self.storage.count_tenants(context) self.assertEqual(2, tenants) def test_count_tenants_no_results(self): tenants = self.storage.count_tenants(self.admin_context) self.assertEqual(0, tenants) @mock.patch('designate.storage.sql.get_read_session') def test_count_tenants_none_result(self, mock_read_session): mock_sql_execute = mock.Mock() mock_sql_fetchone = mock.Mock() mock_read_session().__enter__.return_value = mock_sql_execute mock_sql_execute.execute.return_value = mock_sql_fetchone mock_sql_fetchone.fetchone.return_value = None tenants = self.storage.count_tenants(self.admin_context) mock_read_session.assert_called() self.assertEqual(0, tenants) # Zone Tests def test_create_zone(self): pool_id = cfg.CONF['service:central'].default_pool_id values = { 'tenant_id': self.admin_context.project_id, 'name': 'example.net.', 'email': 'example@example.net', 'pool_id': pool_id } result = self.storage.create_zone( self.admin_context, zone=objects.Zone.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNone(result['updated_at']) self.assertEqual(self.admin_context.project_id, result['tenant_id']) self.assertEqual(values['name'], result['name']) self.assertEqual(values['email'], result['email']) self.assertEqual(pool_id, result['pool_id']) self.assertIn('status', result) def test_create_zone_duplicate(self): # Create the Initial Zone self.create_zone() exc = self.assertRaises(rpc_dispatcher.ExpectedException, self.create_zone) self.assertEqual(exceptions.DuplicateZone, exc.exc_info[0]) def test_find_zones(self): self.config(quota_zones=20) actual = self.storage.find_zones(self.admin_context) self.assertEqual(0, len(actual)) # Create a single zone zone = self.create_zone() actual = self.storage.find_zones(self.admin_context) self.assertEqual(1, len(actual)) self.assertEqual(zone['name'], actual[0]['name']) self.assertEqual(zone['email'], actual[0]['email']) def test_find_zones_paging(self): # Create 10 zones created = [self.create_zone(name='example-%d.org.' % i) for i in range(10)] # Ensure we can page through the results. self._ensure_paging(created, self.storage.find_zones) def test_find_zones_criterion(self): zone_one = self.create_zone() zone_two = self.create_zone(fixture=1) criterion = dict( name=zone_one['name'] ) results = self.storage.find_zones(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(zone_one['name'], results[0]['name']) self.assertEqual(zone_one['email'], results[0]['email']) self.assertIn('status', zone_one) criterion = dict( name=zone_two['name'] ) results = self.storage.find_zones(self.admin_context, criterion) self.assertEqual(len(results), 1) self.assertEqual(zone_two['name'], results[0]['name']) self.assertEqual(zone_two['email'], results[0]['email']) self.assertIn('status', zone_two) def test_find_zones_all_tenants(self): # Create two contexts with different tenant_id's one_context = self.get_admin_context() one_context.project_id = 1 two_context = self.get_admin_context() two_context.project_id = 2 # Create normal and all_tenants context objects nm_context = self.get_admin_context() at_context = self.get_admin_context() at_context.all_tenants = True # Create two zones in different tenants self.create_zone(fixture=0, context=one_context) self.create_zone(fixture=1, context=two_context) # Ensure the all_tenants context see's two zones results = self.storage.find_zones(at_context) self.assertEqual(2, len(results)) # Ensure the normal context see's no zones results = self.storage.find_zones(nm_context) self.assertEqual(0, len(results)) # Ensure the tenant 1 context see's 1 zone results = self.storage.find_zones(one_context) self.assertEqual(1, len(results)) # Ensure the tenant 2 context see's 1 zone results = self.storage.find_zones(two_context) self.assertEqual(1, len(results)) def test_find_zones_shared(self): # Create an admin context admin_context = self.get_admin_context() # Create a zone in the admin context zone = self.create_zone(context=admin_context) # Share the zone with two other projects self.share_zone( zone_id=zone['id'], target_project_id=1, context=admin_context) self.share_zone( zone_id=zone['id'], target_project_id=2, context=admin_context) # Ensure that one zone record is returned from find_zones (LP 2025295) results = self.storage.find_zones(admin_context) self.assertEqual(1, len(results)) def test_get_zone(self): # Create a zone expected = self.create_zone() actual = self.storage.get_zone(self.admin_context, expected['id']) self.assertEqual(expected['name'], actual['name']) self.assertEqual(expected['email'], actual['email']) self.assertIn('status', actual) def test_get_zone_missing(self): uuid = 'caf771fc-6b05-4891-bee1-c2a48621f57b' self.assertRaisesRegex( exceptions.ZoneNotFound, 'Could not find Zone', self.storage.get_zone, self.admin_context, uuid ) def test_get_deleted_zone(self): context = self.get_admin_context() context.show_deleted = True zone = self.create_zone(context=context) self.storage.delete_zone(context, zone['id']) self.storage.get_zone(context, zone['id']) def test_find_zone_criterion(self): zone_one = self.create_zone() zone_two = self.create_zone(fixture=1) criterion = dict( name=zone_one['name'] ) result = self.storage.find_zone(self.admin_context, criterion) self.assertEqual(zone_one['name'], result['name']) self.assertEqual(zone_one['email'], result['email']) self.assertIn('status', zone_one) criterion = dict( name=zone_two['name'] ) result = self.storage.find_zone(self.admin_context, criterion) self.assertEqual(zone_two['name'], result['name']) self.assertEqual(zone_two['email'], result['email']) self.assertIn('status', zone_one) self.assertIn('status', zone_two) def test_find_zone_criterion_missing(self): expected = self.create_zone() criterion = dict( name=expected['name'] + 'NOT FOUND' ) self.assertRaisesRegex( exceptions.ZoneNotFound, 'Could not find Zone', self.storage.find_zone, self.admin_context, criterion ) def test_find_zone_criterion_lessthan(self): zone = self.create_zone() # Test Finding No Results (serial is not < serial) criterion = dict( name=zone['name'], serial='<%s' % zone['serial'], ) self.assertRaisesRegex( exceptions.ZoneNotFound, 'Could not find Zone', self.storage.find_zone, self.admin_context, criterion ) # Test Finding 1 Result (serial is < serial + 1) criterion = dict( name=zone['name'], serial='<%s' % (zone['serial'] + 1), ) result = self.storage.find_zone(self.admin_context, criterion) self.assertEqual(zone['name'], result['name']) def test_find_zone_criterion_greaterthan(self): zone = self.create_zone() # Test Finding No Results (serial is not > serial) criterion = dict( name=zone['name'], serial='>%s' % zone['serial'], ) self.assertRaisesRegex( exceptions.ZoneNotFound, 'Could not find Zone', self.storage.find_zone, self.admin_context, criterion ) # Test Finding 1 Result (serial is > serial - 1) criterion = dict( name=zone['name'], serial='>%s' % (zone['serial'] - 1), ) result = self.storage.find_zone(self.admin_context, criterion) self.assertEqual(zone['name'], result['name']) def test_update_zone(self): # Create a zone zone = self.create_zone(name='example.org.') # Update the Object zone.name = 'example.net.' zone.recordsets = objects.RecordSetList(objects=[]) zone.attributes = objects.ZoneAttributeList( objects=[objects.ZoneAttribute(key='foo', value='bar')] ) zone.masters = objects.ZoneMasterList( objects=[objects.ZoneMaster(host='127.0.0.1', port=80)] ) # Perform the update zone = self.storage.update_zone(self.admin_context, zone) # Ensure the new valie took self.assertEqual('example.net.', zone.name) # Ensure the version column was incremented self.assertEqual(2, zone.version) def test_update_zone_secondary(self): # Create a zone fixture = self.get_zone_fixture('SECONDARY', 1) fixture['email'] = 'root@example.com' zone = self.create_zone(**fixture) # Update the Object zone.name = 'example.net.' zone.recordsets = objects.RecordSetList() # Perform the update zone = self.storage.update_zone(self.admin_context, zone) # Ensure the new valie took self.assertEqual('example.net.', zone.name) # Ensure the version column was incremented self.assertEqual(2, zone.version) def test_update_zone_new_recordset_with_existing(self): zone = self.create_zone(name='example.org.') recordset1 = self.create_recordset(zone) recordset2 = objects.RecordSet( name='www.example.org.', type='A', records=objects.RecordList(objects=[ objects.Record(data='192.0.2.1'), ]) ) zone.name = 'example.net.' zone.recordsets = objects.RecordSetList( objects=[recordset1, recordset2] ) # Perform the update self.storage.update_zone(self.admin_context, zone) recordsets = self.storage.find_recordsets( self.admin_context, {'zone_id': zone['id']} ) self.assertEqual(4, len(recordsets)) def test_update_zone_new_recordset(self): zone = self.create_zone(name='example.org.') recordset = objects.RecordSet( name='www.example.org.', type='A', records=objects.RecordList(objects=[ objects.Record(data='192.0.2.1'), ]) ) zone.name = 'example.net.' zone.recordsets = objects.RecordSetList(objects=[recordset]) # Perform the update self.storage.update_zone(self.admin_context, zone) recordsets = self.storage.find_recordsets( self.admin_context, {'zone_id': zone['id']} ) self.assertEqual(3, len(recordsets)) def test_update_zone_duplicate(self): # Create two zones zone_one = self.create_zone(fixture=0) zone_two = self.create_zone(fixture=1) # Update the D2 object to be a duplicate of D1 zone_two.name = zone_one.name self.assertRaisesRegex( exceptions.DuplicateZone, 'Duplicate Zone', self.storage.update_zone, self.admin_context, zone_two ) def test_update_zone_missing(self): zone = objects.Zone( id='caf771fc-6b05-4891-bee1-c2a48621f57b' ) self.assertRaisesRegex( exceptions.ZoneNotFound, 'Could not find Zone', self.storage.update_zone, self.admin_context, zone ) def test_delete_zone(self): zone = self.create_zone() self.storage.delete_zone(self.admin_context, zone['id']) self.assertRaisesRegex( exceptions.ZoneNotFound, 'Could not find Zone', self.storage.get_zone, self.admin_context, zone['id'] ) def test_delete_zone_missing(self): uuid = 'caf771fc-6b05-4891-bee1-c2a48621f57b' self.assertRaisesRegex( exceptions.ZoneNotFound, 'Could not find Zone', self.storage.delete_zone, self.admin_context, uuid ) def test_count_zones(self): # in the beginning, there should be nothing zones = self.storage.count_zones(self.admin_context) self.assertEqual(0, zones) # Create a single zone self.create_zone() # count 'em up zones = self.storage.count_zones(self.admin_context) # well, did we get 1? self.assertEqual(1, zones) @mock.patch('designate.storage.sql.get_read_session') def test_count_zones_none_result(self, mock_read_session): mock_sql_execute = mock.Mock() mock_sql_fetchone = mock.Mock() mock_read_session().__enter__.return_value = mock_sql_execute mock_sql_execute.execute.return_value = mock_sql_fetchone mock_sql_fetchone.fetchone.return_value = None zones = self.storage.count_zones(self.admin_context) mock_read_session.assert_called() self.assertEqual(0, zones) def test_create_recordset(self): zone = self.create_zone() values = { 'name': 'www.%s' % zone['name'], 'type': 'A' } result = self.storage.create_recordset( self.admin_context, zone['id'], recordset=objects.RecordSet.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNone(result['updated_at']) self.assertEqual(values['name'], result['name']) self.assertEqual(values['type'], result['type']) def test_create_recordset_duplicate(self): zone = self.create_zone() # Create the First RecordSet self.create_recordset(zone) exc = self.assertRaises(rpc_dispatcher.ExpectedException, self.create_recordset, zone) self.assertEqual(exceptions.DuplicateRecordSet, exc.exc_info[0]) def test_create_recordset_with_records(self): zone = self.create_zone() recordset = objects.RecordSet( name='www.%s' % zone['name'], type='A', records=objects.RecordList(objects=[ objects.Record(data='192.0.2.1'), objects.Record(data='192.0.2.2'), ]) ) recordset = self.storage.create_recordset( self.admin_context, zone['id'], recordset) # Ensure recordset.records is a RecordList instance self.assertIsInstance(recordset.records, objects.RecordList) # Ensure two Records are attached to the RecordSet correctly self.assertEqual(2, len(recordset.records)) self.assertIsInstance(recordset.records[0], objects.Record) self.assertIsInstance(recordset.records[1], objects.Record) # Ensure the Records have been saved by checking they have an ID self.assertIsNotNone(recordset.records[0].id) self.assertIsNotNone(recordset.records[1].id) def test_find_recordsets_axfr(self): zone = self.create_zone() self.create_recordset(zone) result = self.storage.find_recordsets_axfr( self.admin_context, {'zone_id': zone['id']} ) self.assertEqual(3, len(result)) def test_find_recordsets(self): zone = self.create_zone() criterion = {'zone_id': zone['id']} actual = self.storage.find_recordsets(self.admin_context, criterion) self.assertEqual(2, len(actual)) # Create a single recordset recordset_one = self.create_recordset(zone) actual = self.storage.find_recordsets(self.admin_context, criterion) self.assertEqual(3, len(actual)) self.assertEqual(recordset_one['name'], actual[2]['name']) self.assertEqual(recordset_one['type'], actual[2]['type']) def test_find_recordsets_paging(self): zone = self.create_zone(name='example.org.') # Create 10 RecordSets created = [self.create_recordset(zone, name='r-%d.example.org.' % i) for i in range(10)] # Add in the SOA and NS recordsets that are automatically created soa = self.storage.find_recordset(self.admin_context, criterion={'zone_id': zone['id'], 'type': 'SOA'}) ns = self.storage.find_recordset(self.admin_context, criterion={'zone_id': zone['id'], 'type': 'NS'}) created.insert(0, ns) created.insert(0, soa) # Ensure we can page through the results. self._ensure_paging(created, self.storage.find_recordsets) def test_find_recordsets_criterion(self): zone = self.create_zone() recordset_one = self.create_recordset(zone, type='A', fixture=0) self.create_recordset(zone, fixture=1) criterion = dict( zone_id=zone['id'], name=recordset_one['name'], ) results = self.storage.find_recordsets(self.admin_context, criterion) self.assertEqual(1, len(results)) criterion = dict( zone_id=zone['id'], type='A', ) results = self.storage.find_recordsets(self.admin_context, criterion) self.assertEqual(2, len(results)) def test_find_recordsets_criterion_wildcard(self): zone = self.create_zone() values = {'name': 'one.%s' % zone['name']} self.create_recordset(zone, **values) criterion = dict( zone_id=zone['id'], name='%%%(name)s' % {'name': zone['name']}, ) results = self.storage.find_recordsets(self.admin_context, criterion) # Should be 3, as SOA and NS recordsets are automiatcally created self.assertEqual(3, len(results)) def test_find_recordsets_with_records(self): zone = self.create_zone() records = [ objects.Record.from_dict({'data': '192.0.2.1'}), objects.Record.from_dict({'data': '192.0.2.2'}), objects.Record.from_dict({'data': '192.0.2.3'}) ] recordset = self.create_recordset(zone, records=records) criterion = dict( id=recordset.id, ) # Find the RecordSet results = self.storage.find_recordsets(self.admin_context, criterion) # Ensure we only have one result self.assertEqual(1, len(results)) recordset = results[0] # Ensure recordset.records is a RecordList instance self.assertIsInstance(recordset.records, objects.RecordList) # Ensure two Records are attached to the RecordSet correctly self.assertEqual(3, len(recordset.records)) records = [] for record in recordset.records: self.assertIsInstance(record, objects.Record) self.assertNotIn(record, records) records.append(record) def test_find_recordset_criterion(self): zone = self.create_zone() expected = self.create_recordset(zone) criterion = dict( zone_id=zone['id'], name=expected['name'], ) actual = self.storage.find_recordset(self.admin_context, criterion) self.assertEqual(expected['name'], actual['name']) self.assertEqual(expected['type'], actual['type']) def test_find_recordset_criterion_missing(self): zone = self.create_zone() expected = self.create_recordset(zone) criterion = dict( name=expected['name'] + 'NOT FOUND' ) self.assertRaisesRegex( exceptions.RecordSetNotFound, 'Could not find RecordSet', self.storage.find_recordset, self.admin_context, criterion ) def test_find_recordset_criterion_with_records(self): zone = self.create_zone() records = [ objects.Record.from_dict(self.get_record_fixture('A', fixture=0)), objects.Record.from_dict(self.get_record_fixture('A', fixture=1)) ] recordset = self.create_recordset(zone, records=records) criterion = dict( id=recordset.id, ) # Fetch the RecordSet again recordset = self.storage.find_recordset(self.admin_context, criterion) # Ensure recordset.records is a RecordList instance self.assertIsInstance(recordset.records, objects.RecordList) # Ensure two Records are attached to the RecordSet correctly self.assertEqual(2, len(recordset.records)) self.assertIsInstance(recordset.records[0], objects.Record) self.assertIsInstance(recordset.records[1], objects.Record) def test_update_recordset(self): zone = self.create_zone() # Create a recordset recordset = self.create_recordset(zone) # Update the Object recordset.ttl = 1800 # Change records as well recordset.records.append(objects.Record(data='192.0.2.2')) # Perform the update recordset = self.storage.update_recordset(self.admin_context, recordset) # Ensure the new value took self.assertEqual(1800, recordset.ttl) # Ensure the version column was incremented self.assertEqual(2, recordset.version) def test_update_recordset_duplicate(self): zone = self.create_zone() # Create two recordsets recordset_one = self.create_recordset(zone, type='A') recordset_two = self.create_recordset(zone, type='A', fixture=1) # Update the R2 object to be a duplicate of R1 recordset_two.name = recordset_one.name self.assertRaisesRegex( exceptions.DuplicateRecordSet, 'Duplicate RecordSet', self.storage.update_recordset, self.admin_context, recordset_two ) def test_update_recordset_missing(self): recordset = objects.RecordSet( id='caf771fc-6b05-4891-bee1-c2a48621f57b' ) self.assertRaisesRegex( exceptions.RecordSetNotFound, 'Could not find RecordSet', self.storage.update_recordset, self.admin_context, recordset ) def test_update_recordset_with_record_create(self): zone = self.create_zone() # Create a RecordSet recordset = self.create_recordset(zone, 'A', records=[]) # Append two new Records recordset.records.append(objects.Record(data='192.0.2.1')) recordset.records.append(objects.Record(data='192.0.2.2')) # Perform the update self.storage.update_recordset(self.admin_context, recordset) # Fetch the RecordSet again recordset = self.storage.find_recordset(self.admin_context, {'id': recordset.id}) # Ensure two Records are attached to the RecordSet correctly self.assertEqual(2, len(recordset.records)) self.assertIsInstance(recordset.records[0], objects.Record) self.assertIsInstance(recordset.records[1], objects.Record) # Ensure the Records have been saved by checking they have an ID self.assertIsNotNone(recordset.records[0].id) self.assertIsNotNone(recordset.records[1].id) def test_update_recordset_with_record_delete(self): zone = self.create_zone() # Create a RecordSet and two Records records = [ objects.Record.from_dict(self.get_record_fixture('A', fixture=0)), objects.Record.from_dict(self.get_record_fixture('A', fixture=1)) ] recordset = self.create_recordset(zone, records=records) # Fetch the RecordSet again recordset = self.storage.find_recordset(self.admin_context, {'id': recordset.id}) # Remove one of the Records recordset.records.pop(0) # Ensure only one Record is attached to the RecordSet self.assertEqual(1, len(recordset.records)) # Perform the update self.storage.update_recordset(self.admin_context, recordset) # Fetch the RecordSet again recordset = self.storage.find_recordset(self.admin_context, {'id': recordset.id}) # Ensure only one Record is attached to the RecordSet self.assertEqual(1, len(recordset.records)) self.assertIsInstance(recordset.records[0], objects.Record) def test_update_recordset_with_record_update(self): zone = self.create_zone() # Create a RecordSet and two Records records = [ objects.Record.from_dict(self.get_record_fixture('A', fixture=0)), objects.Record.from_dict(self.get_record_fixture('A', fixture=1)) ] recordset = self.create_recordset(zone, records=records) # Fetch the RecordSet again recordset = self.storage.find_recordset(self.admin_context, {'id': recordset.id}) # Update one of the Records updated_record_id = recordset.records[0].id recordset.records[0].data = '192.0.2.255' # Perform the update self.storage.update_recordset(self.admin_context, recordset) # Fetch the RecordSet again recordset = self.storage.find_recordset(self.admin_context, {'id': recordset.id}) # Ensure the Record has been updated for record in recordset.records: if record.id != updated_record_id: continue self.assertEqual('192.0.2.255', record.data) return # Exits this test early as we succeeded raise Exception('Updated record not found') def test_delete_recordset(self): zone = self.create_zone() # Create a recordset recordset = self.create_recordset(zone) self.storage.delete_recordset(self.admin_context, recordset['id']) self.assertRaisesRegex( exceptions.RecordSetNotFound, 'Could not find RecordSet', self.storage.find_recordset, self.admin_context, criterion={'id': recordset['id']} ) def test_delete_recordset_missing(self): uuid = 'caf771fc-6b05-4891-bee1-c2a48621f57b' self.assertRaisesRegex( exceptions.RecordSetNotFound, 'Could not find RecordSet', self.storage.delete_recordset, self.admin_context, uuid ) def test_count_recordsets(self): # in the beginning, there should be nothing recordsets = self.storage.count_recordsets(self.admin_context) self.assertEqual(0, recordsets) # Create a single zone & recordset zone = self.create_zone() self.create_recordset(zone) # we should have 3 recordsets now, including SOA & NS recordsets = self.storage.count_recordsets(self.admin_context) self.assertEqual(3, recordsets) # Delete the zone, we should be back to 0 recordsets self.storage.delete_zone(self.admin_context, zone.id) recordsets = self.storage.count_recordsets(self.admin_context) self.assertEqual(0, recordsets) @mock.patch('designate.storage.sql.get_read_session') def test_count_recordsets_none_result(self, mock_read_session): mock_sql_execute = mock.Mock() mock_sql_fetchone = mock.Mock() mock_read_session().__enter__.return_value = mock_sql_execute mock_sql_execute.execute.return_value = mock_sql_fetchone mock_sql_fetchone.fetchone.return_value = None recordsets = self.storage.count_recordsets(self.admin_context) mock_read_session.assert_called() self.assertEqual(0, recordsets) def test_find_records(self): zone = self.create_zone() recordset = self.create_recordset(zone, records=[]) criterion = { 'zone_id': zone['id'], 'recordset_id': recordset['id'] } actual = self.storage.find_records(self.admin_context, criterion) self.assertEqual(0, len(actual)) # Create a single record records = [ objects.Record.from_dict(self.get_record_fixture('A', fixture=0)), ] recordset.records = records self.central_service.update_recordset(self.admin_context, recordset) recordset = self.central_service.get_recordset( self.admin_context, zone['id'], recordset['id'] ) record = recordset.records[0] actual = self.storage.find_records(self.admin_context, criterion) self.assertEqual(1, len(actual)) self.assertEqual(record['data'], actual[0]['data']) def test_find_records_paging(self): zone = self.create_zone() records = [] for i in range(10): records.append( objects.Record.from_dict(({'data': '192.0.2.%d' % i})) ) self.create_recordset(zone, type='A', records=records) # Add in the SOA and NS records that are automatically created soa = self.storage.find_recordset(self.admin_context, criterion={'zone_id': zone['id'], 'type': 'SOA'}) ns = self.storage.find_recordset(self.admin_context, criterion={'zone_id': zone['id'], 'type': 'NS'}) for r in ns['records']: records.insert(0, r) records.insert(0, soa['records'][0]) # Ensure we can page through the results. self._ensure_paging(records, self.storage.find_records) def test_find_records_criterion(self): zone = self.create_zone() record_one = objects.Record.from_dict( self.get_record_fixture('A', fixture=0) ) records = [ record_one, objects.Record.from_dict(self.get_record_fixture('A', fixture=1)) ] recordset = self.create_recordset(zone, records=records) criterion = dict( data=record_one['data'], zone_id=zone['id'], recordset_id=recordset['id'], ) results = self.storage.find_records(self.admin_context, criterion) self.assertEqual(1, len(results)) criterion = dict( zone_id=zone['id'], recordset_id=recordset['id'], ) results = self.storage.find_records(self.admin_context, criterion) self.assertEqual(2, len(results)) def test_find_records_criterion_wildcard(self): zone = self.create_zone() records = [objects.Record.from_dict({'data': '127.0.0.1'})] recordset = self.create_recordset(zone, type='A', records=records) criterion = dict( zone_id=zone['id'], recordset_id=recordset['id'], data='%.0.0.1', ) results = self.storage.find_records(self.admin_context, criterion) self.assertEqual(1, len(results)) def test_get_record(self): zone = self.create_zone() recordset = self.create_recordset(zone) expected = recordset.records[0] actual = self.storage.get_record(self.admin_context, expected['id']) self.assertEqual(expected['data'], actual['data']) self.assertIn('status', actual) def test_get_record_missing(self): uuid = 'caf771fc-6b05-4891-bee1-c2a48621f57b' self.assertRaisesRegex( exceptions.RecordNotFound, 'Could not find Record', self.storage.get_record, self.admin_context, uuid ) def test_find_record_criterion(self): zone = self.create_zone() recordset = self.create_recordset(zone) expected = recordset.records[0] criterion = dict( zone_id=zone['id'], recordset_id=recordset['id'], data=expected['data'], ) actual = self.storage.find_record(self.admin_context, criterion) self.assertEqual(expected['data'], actual['data']) self.assertIn('status', actual) def test_find_record_criterion_missing(self): zone = self.create_zone() recordset = self.create_recordset(zone) expected = recordset.records[0] criterion = dict( zone_id=zone['id'], data=expected['data'] + 'NOT FOUND', ) self.assertRaisesRegex( exceptions.RecordNotFound, 'Could not find Record', self.storage.find_record, self.admin_context, criterion ) def test_update_record(self): zone = self.create_zone() recordset = self.create_recordset(zone, type='A') record = recordset.records[0] # Update the Object record.data = '192.0.2.255' # Perform the update record = self.storage.update_record(self.admin_context, record) # Ensure the new value took self.assertEqual('192.0.2.255', record.data) # Ensure the version column was incremented self.assertEqual(2, record.version) def test_update_record_duplicate(self): zone = self.create_zone() record_one = objects.Record.from_dict( self.get_record_fixture('A', fixture=0) ) record_two = objects.Record.from_dict( self.get_record_fixture('A', fixture=1) ) records = [ record_one, record_two ] self.create_recordset(zone, records=records) # Update the R2 object to be a duplicate of R1 record_two.data = record_one.data self.assertRaisesRegex( exceptions.DuplicateRecord, 'Duplicate Record', self.storage.update_record, self.admin_context, record_two ) def test_update_record_missing(self): record = objects.Record( id='caf771fc-6b05-4891-bee1-c2a48621f57b' ) self.assertRaisesRegex( exceptions.RecordNotFound, 'Could not find Record', self.storage.update_record, self.admin_context, record ) def test_delete_record(self): zone = self.create_zone() recordset = self.create_recordset(zone) record = recordset.records[0] self.storage.delete_record(self.admin_context, record['id']) self.assertRaisesRegex( exceptions.RecordNotFound, 'Could not find Record', self.storage.get_record, self.admin_context, record['id'] ) def test_delete_record_missing(self): uuid = 'caf771fc-6b05-4891-bee1-c2a48621f57b' self.assertRaisesRegex( exceptions.RecordNotFound, 'Could not find Record', self.storage.delete_record, self.admin_context, uuid ) def test_count_records(self): # in the beginning, there should be nothing records = self.storage.count_records(self.admin_context) self.assertEqual(0, records) # Create a single zone & record zone = self.create_zone() self.create_recordset(zone) # we should have 3 records now, including NS and SOA records = self.storage.count_records(self.admin_context) self.assertEqual(3, records) # Delete the zone, we should be back to 0 records self.storage.delete_zone(self.admin_context, zone.id) records = self.storage.count_records(self.admin_context) self.assertEqual(0, records) @mock.patch('designate.storage.sql.get_read_session') def test_count_records_none_result(self, mock_read_session): mock_sql_execute = mock.Mock() mock_sql_fetchone = mock.Mock() mock_read_session().__enter__.return_value = mock_sql_execute mock_sql_execute.execute.return_value = mock_sql_fetchone mock_sql_fetchone.fetchone.return_value = None records = self.storage.count_records(self.admin_context) mock_read_session.assert_called() self.assertEqual(0, records) # TLD Tests def test_create_tld(self): values = { 'name': 'com', 'description': 'This is a comment.' } result = self.storage.create_tld( self.admin_context, objects.Tld.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNone(result['updated_at']) self.assertIsNotNone(result['version']) self.assertEqual(values['name'], result['name']) self.assertEqual(values['description'], result['description']) def test_create_tld_with_duplicate(self): # Create the First Tld self.create_tld(fixture=0) exc = self.assertRaises(rpc_dispatcher.ExpectedException, self.create_tld, fixture=0) self.assertEqual(exceptions.DuplicateTld, exc.exc_info[0]) def test_find_tlds(self): actual = self.storage.find_tlds(self.admin_context) self.assertEqual(0, len(actual)) # Create a single Tld tld = self.create_tld(fixture=0) actual = self.storage.find_tlds(self.admin_context) self.assertEqual(1, len(actual)) self.assertEqual(tld['name'], actual[0]['name']) self.assertEqual(tld['description'], actual[0]['description']) def test_find_tlds_paging(self): # Create 10 Tlds created = [self.create_tld(name='org%d' % i) for i in range(10)] # Ensure we can page through the results. self._ensure_paging(created, self.storage.find_tlds) def test_find_tlds_with_criterion(self): tld_one = self.create_tld(fixture=0) tld_two = self.create_tld(fixture=1) criterion_one = dict(name=tld_one['name']) results = self.storage.find_tlds(self.admin_context, criterion_one) self.assertEqual(1, len(results)) self.assertEqual(tld_one['name'], results[0]['name']) criterion_two = dict(name=tld_two['name']) results = self.storage.find_tlds(self.admin_context, criterion_two) self.assertEqual(len(results), 1) self.assertEqual(tld_two['name'], results[0]['name']) def test_get_tld(self): # Create a tld expected = self.create_tld() actual = self.storage.get_tld(self.admin_context, expected['id']) self.assertEqual(expected['name'], actual['name']) def test_get_tld_missing(self): uuid = '4c8e7f82-3519-4bf7-8940-a66a4480f223' self.assertRaisesRegex( exceptions.TldNotFound, 'Could not find Tld', self.storage.get_tld, self.admin_context, uuid ) def test_find_tld_criterion(self): # Create two tlds tld_one = self.create_tld(fixture=0) tld_two = self.create_tld(fixture=1) criterion = dict(name=tld_one['name']) # Find tld_one using its name as criterion result = self.storage.find_tld(self.admin_context, criterion) # Assert names match self.assertEqual(tld_one['name'], result['name']) # Repeat with tld_two criterion = dict(name=tld_two['name']) result = self.storage.find_tld(self.admin_context, criterion) self.assertEqual(tld_two['name'], result['name']) def test_find_tld_criterion_missing(self): expected = self.create_tld() criterion = dict(name=expected['name'] + 'NOT FOUND') self.assertRaisesRegex( exceptions.TldNotFound, 'Could not find Tld', self.storage.find_tld, self.admin_context, criterion ) def test_update_tld(self): # Create a tld tld = self.create_tld(name='net') # Update the tld tld.name = 'org' # Update storage tld = self.storage.update_tld(self.admin_context, tld) # Verify the new value self.assertEqual('org', tld.name) # Ensure the version column was incremented self.assertEqual(2, tld.version) def test_update_tld_duplicate(self): # Create two tlds tld_one = self.create_tld(fixture=0) tld_two = self.create_tld(fixture=1) # Update tld_two to be a duplicate of tld_ond tld_two.name = tld_one.name self.assertRaisesRegex( exceptions.DuplicateTld, 'Duplicate Tld', self.storage.update_tld, self.admin_context, tld_two ) def test_update_tld_missing(self): tld = objects.Tld( id='486f9cbe-b8b6-4d8c-8275-1a6e47b13e00' ) self.assertRaisesRegex( exceptions.TldNotFound, 'Could not find Tld', self.storage.update_tld, self.admin_context, tld ) def test_delete_tld(self): # Create a tld tld = self.create_tld() # Delete the tld self.storage.delete_tld(self.admin_context, tld['id']) # Verify that it's deleted self.assertRaisesRegex( exceptions.TldNotFound, 'Could not find Tld', self.storage.get_tld, self.admin_context, tld['id'] ) def test_delete_tld_missing(self): uuid = 'cac1fc02-79b2-4e62-a1a4-427b6790bbe6' self.assertRaisesRegex( exceptions.TldNotFound, 'Could not find Tld', self.storage.delete_tld, self.admin_context, uuid ) # Blacklist tests def test_create_blacklist(self): values = { 'pattern': '^([A-Za-z0-9_\\-]+\\.)*example\\.com\\.$', 'description': 'This is a comment.' } result = self.storage.create_blacklist( self.admin_context, objects.Blacklist.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNotNone(result['version']) self.assertIsNone(result['updated_at']) self.assertEqual(values['pattern'], result['pattern']) self.assertEqual(values['description'], result['description']) def test_create_blacklist_duplicate(self): # Create the initial Blacklist self.create_blacklist(fixture=0) exc = self.assertRaises(rpc_dispatcher.ExpectedException, self.create_blacklist, fixture=0) self.assertEqual(exceptions.DuplicateBlacklist, exc.exc_info[0]) def test_find_blacklists(self): # Verify that there are no blacklists created actual = self.storage.find_blacklists(self.admin_context) self.assertEqual(0, len(actual)) # Create a Blacklist blacklist = self.create_blacklist(fixture=0) actual = self.storage.find_blacklists(self.admin_context) self.assertEqual(1, len(actual)) self.assertEqual(blacklist['pattern'], actual[0]['pattern']) def test_find_blacklists_paging(self): # Create 10 Blacklists created = [self.create_blacklist(pattern='^example-%d.org.' % i) for i in range(10)] # Ensure we can page through the results. self._ensure_paging(created, self.storage.find_blacklists) def test_find_blacklists_with_criterion(self): # Create two blacklists blacklist_one = self.create_blacklist(fixture=0) blacklist_two = self.create_blacklist(fixture=1) # Verify blacklist_one criterion = dict(pattern=blacklist_one['pattern']) results = self.storage.find_blacklists(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(blacklist_one['pattern'], results[0]['pattern']) # Verify blacklist_two criterion = dict(pattern=blacklist_two['pattern']) results = self.storage.find_blacklists(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(blacklist_two['pattern'], results[0]['pattern']) def test_get_blacklist(self): expected = self.create_blacklist(fixture=0) actual = self.storage.get_blacklist(self.admin_context, expected['id']) self.assertEqual(expected['pattern'], actual['pattern']) def test_get_blacklist_missing(self): uuid = '2c102ffd-7146-4b4e-ad62-b530ee0873fb' self.assertRaisesRegex( exceptions.BlacklistNotFound, 'Could not find Blacklist', self.storage.get_blacklist, self.admin_context, uuid ) def test_find_blacklist_criterion(self): blacklist_one = self.create_blacklist(fixture=0) blacklist_two = self.create_blacklist(fixture=1) criterion = dict(pattern=blacklist_one['pattern']) result = self.storage.find_blacklist(self.admin_context, criterion) self.assertEqual(blacklist_one['pattern'], result['pattern']) criterion = dict(pattern=blacklist_two['pattern']) result = self.storage.find_blacklist(self.admin_context, criterion) self.assertEqual(blacklist_two['pattern'], result['pattern']) def test_find_blacklist_criterion_missing(self): expected = self.create_blacklist(fixture=0) criterion = dict(pattern=expected['pattern'] + 'NOT FOUND') self.assertRaisesRegex( exceptions.BlacklistNotFound, 'Could not find Blacklist', self.storage.find_blacklist, self.admin_context, criterion ) def test_update_blacklist(self): blacklist = self.create_blacklist(pattern='^example.uk.') # Update the blacklist blacklist.pattern = '^example.uk.co.' blacklist = self.storage.update_blacklist(self.admin_context, blacklist) # Verify the new values self.assertEqual('^example.uk.co.', blacklist.pattern) # Ensure the version column was incremented self.assertEqual(2, blacklist.version) def test_update_blacklist_duplicate(self): # Create two blacklists blacklist_one = self.create_blacklist(fixture=0) blacklist_two = self.create_blacklist(fixture=1) # Update the second one to be a duplicate of the first blacklist_two.pattern = blacklist_one.pattern self.assertRaisesRegex( exceptions.DuplicateBlacklist, 'Duplicate Blacklist', self.storage.update_blacklist, self.admin_context, blacklist_two ) def test_update_blacklist_missing(self): blacklist = objects.Blacklist( id='e8cee063-3a26-42d6-b181-bdbdc2c99d08' ) self.assertRaisesRegex( exceptions.BlacklistNotFound, 'Could not find Blacklist', self.storage.update_blacklist, self.admin_context, blacklist ) def test_delete_blacklist(self): blacklist = self.create_blacklist(fixture=0) self.storage.delete_blacklist(self.admin_context, blacklist['id']) self.assertRaisesRegex( exceptions.BlacklistNotFound, 'Could not find Blacklist', self.storage.get_blacklist, self.admin_context, blacklist['id'] ) def test_delete_blacklist_missing(self): uuid = '97f57960-f41b-4e93-8e22-8fd6c7e2c183' self.assertRaisesRegex( exceptions.BlacklistNotFound, 'Could not find Blacklist', self.storage.delete_blacklist, self.admin_context, uuid ) # Pool Tests def test_create_pool(self): values = { 'name': 'test1', 'tenant_id': self.admin_context.project_id, 'provisioner': 'UNMANAGED' } result = self.storage.create_pool( self.admin_context, objects.Pool.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNone(result['updated_at']) self.assertEqual(values['name'], result['name']) self.assertEqual(values['tenant_id'], result['tenant_id']) self.assertEqual(values['provisioner'], result['provisioner']) def test_create_pool_with_all_relations(self): values = { 'name': 'Pool', 'description': 'Pool description', 'attributes': [{'key': 'scope', 'value': 'public'}], 'ns_records': [{'priority': 1, 'hostname': 'ns1.example.org.'}], 'nameservers': [{'host': '192.0.2.1', 'port': 53}], 'targets': [{ 'type': 'fake', 'description': 'FooBar', 'masters': [{'host': '192.0.2.2', 'port': DEFAULT_MDNS_PORT}], 'options': [{'key': 'fake_option', 'value': 'fake_value'}], }], 'also_notifies': [{'host': '192.0.2.3', 'port': 53}] } # Create the Pool, and check all values are OK result = self.storage.create_pool( self.admin_context, objects.Pool.from_dict(values)) self.assertNestedDictContainsSubset(values, result.to_dict()) # Re-Fetch the pool, and check everything is still OK result = self.storage.get_pool(self.admin_context, result.id) self.assertNestedDictContainsSubset(values, result.to_dict()) def test_create_pool_duplicate(self): # Create the first pool self.create_pool(fixture=0) # Create the second pool and should get exception exc = self.assertRaises(rpc_dispatcher.ExpectedException, self.create_pool, fixture=0) self.assertEqual(exceptions.DuplicatePool, exc.exc_info[0]) def test_find_pools(self): # Verify that there are no pools, except for default pool actual = self.storage.find_pools(self.admin_context) self.assertEqual(1, len(actual)) # Create a Pool pool = self.create_pool(fixture=0) actual = self.storage.find_pools(self.admin_context) self.assertEqual(2, len(actual)) # Test against the second pool, since the first is the default pool self.assertEqual(pool['name'], actual[1]['name']) def test_find_pools_paging(self): # Get any pools that are already created, including default pools = self.storage.find_pools(self.admin_context) # Create 10 Pools created = [self.create_pool(name='test%d' % i) for i in range(10)] # Add in the existing pools for p in pools: created.insert(0, p) # Ensure we can page through the results self._ensure_paging(created, self.storage.find_pools) def test_find_pools_criterion(self): # Create two pools pool_one = self.create_pool(fixture=0) pool_two = self.create_pool(fixture=1) # Verify pool_one criterion = dict(name=pool_one['name']) results = self.storage.find_pools(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(pool_one['name'], results[0]['name']) self.assertEqual(pool_one['provisioner'], results[0]['provisioner']) criterion = dict(name=pool_two['name']) results = self.storage.find_pools(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(pool_two['name'], results[0]['name']) self.assertEqual(pool_two['provisioner'], results[0]['provisioner']) def test_get_pool(self): # Create a pool expected = self.create_pool() actual = self.storage.get_pool(self.admin_context, expected['id']) self.assertEqual(expected['name'], actual['name']) self.assertEqual(expected['provisioner'], actual['provisioner']) def test_get_pool_missing(self): uuid = 'c28893e3-eb87-4562-aa29-1f0e835d749b' self.assertRaisesRegex( exceptions.PoolNotFound, 'Could not find Pool', self.storage.get_pool, self.admin_context, uuid ) def test_find_pool_criterion(self): pool_one = self.create_pool(fixture=0) pool_two = self.create_pool(fixture=1) criterion = dict(name=pool_one['name']) result = self.storage.find_pool(self.admin_context, criterion) self.assertEqual(pool_one['name'], result['name']) self.assertEqual(pool_one['provisioner'], result['provisioner']) criterion = dict(name=pool_two['name']) result = self.storage.find_pool(self.admin_context, criterion) self.assertEqual(pool_two['name'], result['name']) self.assertEqual(pool_two['provisioner'], result['provisioner']) def test_find_pool_criterion_missing(self): expected = self.create_pool() criterion = dict(name=expected['name'] + 'NOT FOUND') self.assertRaisesRegex( exceptions.PoolNotFound, 'Could not find Pool', self.storage.find_pool, self.admin_context, criterion ) def test_update_pool(self): # Create a pool pool = self.create_pool(name='test1') # Update the Pool pool.name = 'test3' # Perform the update pool = self.storage.update_pool(self.admin_context, pool) # Verify the new value is there self.assertEqual('test3', pool.name) def test_update_pool_duplicate(self): # Create two pools pool_one = self.create_pool(fixture=0) pool_two = self.create_pool(fixture=1) # Update pool_two to be a duplicate of pool_one pool_two.name = pool_one.name self.assertRaisesRegex( exceptions.DuplicatePool, 'Duplicate Pool', self.storage.update_pool, self.admin_context, pool_two ) def test_update_pool_missing(self): pool = objects.Pool( id='8806f871-5140-43f4-badd-2bbc5715b013' ) self.assertRaisesRegex( exceptions.PoolNotFound, 'Could not find Pool', self.storage.update_pool, self.admin_context, pool ) def test_update_pool_with_all_relations(self): values = { 'name': 'Pool-A', 'description': 'Pool-A description', 'attributes': [{'key': 'scope', 'value': 'public'}], 'ns_records': [{'priority': 1, 'hostname': 'ns1.example.org.'}], 'nameservers': [{'host': '192.0.2.1', 'port': 53}], 'targets': [{ 'type': 'fake', 'description': 'FooBar', 'masters': [{'host': '192.0.2.2', 'port': DEFAULT_MDNS_PORT}], 'options': [{'key': 'fake_option', 'value': 'fake_value'}], }], 'also_notifies': [{'host': '192.0.2.3', 'port': 53}] } # Create the Pool result = self.storage.create_pool( self.admin_context, objects.Pool.from_dict(values)) created_pool_id = result.id # Prepare a new set of data for the Pool, copying over the ID so # we trigger an update rather than a create. values = { 'id': created_pool_id, 'name': 'Pool-B', 'description': 'Pool-B description', 'attributes': [{'key': 'scope', 'value': 'private'}], 'ns_records': [{'priority': 1, 'hostname': 'ns2.example.org.'}], 'nameservers': [{'host': '192.0.2.5', 'port': 53}], 'targets': [{ 'type': 'fake', 'description': 'NewFooBar', 'masters': [{'host': '192.0.2.2', 'port': DEFAULT_MDNS_PORT}], 'options': [{'key': 'fake_option', 'value': 'fake_value'}], }, { 'type': 'fake', 'description': 'FooBar2', 'masters': [{'host': '192.0.2.7', 'port': 5355}], 'options': [{'key': 'fake_option', 'value': 'new_fake_value'}], }], 'also_notifies': [] } # Update the pool, and check everything is OK result = self.storage.update_pool( self.admin_context, objects.Pool.from_dict(values)) self.assertNestedDictContainsSubset(values, result.to_dict()) # Re-Fetch the pool, and check everything is still OK result = self.storage.get_pool(self.admin_context, created_pool_id) self.assertNestedDictContainsSubset(values, result.to_dict()) def test_delete_pool(self): pool = self.create_pool() self.storage.delete_pool(self.admin_context, pool['id']) self.assertRaisesRegex( exceptions.PoolNotFound, 'Could not find Pool', self.storage.delete_pool, self.admin_context, pool['id'] ) def test_delete_pool_missing(self): uuid = '203ca44f-c7e7-4337-9a02-0d735833e6aa' self.assertRaisesRegex( exceptions.PoolNotFound, 'Could not find Pool', self.storage.delete_pool, self.admin_context, uuid ) def test_create_pool_ns_record_duplicate(self): # Create a pool pool = self.create_pool(name='test1') ns = objects.PoolNsRecord(priority=1, hostname='ns.example.io.') self.storage.create_pool_ns_record( self.admin_context, pool.id, ns) ns2 = objects.PoolNsRecord(priority=2, hostname='ns.example.io.') self.assertRaisesRegex( exceptions.DuplicatePoolNsRecord, 'Duplicate PoolNsRecord', self.storage.create_pool_ns_record, self.admin_context, pool.id, ns2 ) def test_update_pool_ns_record_duplicate(self): # Create a pool pool = self.create_pool(name='test1') ns1 = objects.PoolNsRecord(priority=1, hostname='ns1.example.io.') self.storage.create_pool_ns_record( self.admin_context, pool.id, ns1) ns2 = objects.PoolNsRecord(priority=2, hostname='ns2.example.io.') self.storage.create_pool_ns_record( self.admin_context, pool.id, ns2) ns2.hostname = ns1.hostname self.assertRaisesRegex( exceptions.DuplicatePoolNsRecord, 'Duplicate PoolNsRecord', self.storage.update_pool_ns_record, self.admin_context, ns2 ) # PoolAttribute tests def test_create_pool_attribute(self): values = { 'pool_id': 'd5d10661-0312-4ae1-8664-31188a4310b7', 'key': 'test-attribute', 'value': 'test-value' } result = self.storage.create_pool_attribute( self.admin_context, values['pool_id'], objects.PoolAttribute.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNotNone(result['version']) self.assertIsNone(result['updated_at']) self.assertEqual(values['pool_id'], result['pool_id']) self.assertEqual(values['key'], result['key']) self.assertEqual(values['value'], result['value']) def test_find_pool_attribute(self): # Verify that there are no Pool Attributes created actual = self.storage.find_pool_attributes(self.admin_context) self.assertEqual(0, len(actual)) # Create a Pool Attribute pool_attribute = self.create_pool_attribute(fixture=0) actual = self.storage.find_pool_attributes(self.admin_context) self.assertEqual(1, len(actual)) self.assertEqual(pool_attribute['pool_id'], actual[0]['pool_id']) self.assertEqual(pool_attribute['key'], actual[0]['key']) self.assertEqual(pool_attribute['value'], actual[0]['value']) def test_find_pool_attributes_paging(self): # Create 10 Pool Attributes created = [self.create_pool_attribute(value='^ns%d.example.com.' % i) for i in range(10)] # Ensure we can page through the results. self._ensure_paging(created, self.storage.find_pool_attributes) def test_find_pool_attributes_with_criterion(self): # Create two pool attributes pool_attribute_one = self.create_pool_attribute(fixture=0) pool_attribute_two = self.create_pool_attribute(fixture=1) # Verify pool_attribute_one criterion = dict(key=pool_attribute_one['key']) results = self.storage.find_pool_attributes(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(pool_attribute_one['pool_id'], results[0]['pool_id']) self.assertEqual(pool_attribute_one['key'], results[0]['key']) self.assertEqual(pool_attribute_one['value'], results[0]['value']) # Verify pool_attribute_two criterion = dict(key=pool_attribute_two['key']) LOG.debug('Criterion is %r ' % criterion) results = self.storage.find_pool_attributes(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(pool_attribute_two['pool_id'], results[0]['pool_id']) self.assertEqual(pool_attribute_two['key'], results[0]['key']) self.assertEqual(pool_attribute_two['value'], results[0]['value']) def test_get_pool_attribute(self): expected = self.create_pool_attribute(fixture=0) actual = self.storage.get_pool_attribute(self.admin_context, expected['id']) self.assertEqual(expected['pool_id'], actual['pool_id']) self.assertEqual(expected['key'], actual['key']) self.assertEqual(expected['value'], actual['value']) def test_get_pool_attribute_missing(self): uuid = '2c102ffd-7146-4b4e-ad62-b530ee0873fb' self.assertRaisesRegex( exceptions.PoolAttributeNotFound, 'Could not find PoolAttribute', self.storage.get_pool_attribute, self.admin_context, uuid ) def test_find_pool_attribute_criterion(self): pool_attribute_one = self.create_pool_attribute(fixture=0) pool_attribute_two = self.create_pool_attribute(fixture=1) criterion = dict(key=pool_attribute_one['key']) result = self.storage.find_pool_attribute(self.admin_context, criterion) self.assertEqual(pool_attribute_one['pool_id'], result['pool_id']) self.assertEqual(pool_attribute_one['key'], result['key']) self.assertEqual(pool_attribute_one['value'], result['value']) criterion = dict(key=pool_attribute_two['key']) result = self.storage.find_pool_attribute(self.admin_context, criterion) self.assertEqual(pool_attribute_two['pool_id'], result['pool_id']) self.assertEqual(pool_attribute_two['key'], result['key']) self.assertEqual(pool_attribute_two['value'], result['value']) def test_find_pool_attribute_criterion_missing(self): expected = self.create_pool_attribute(fixture=0) criterion = dict(key=expected['key'] + 'NOT FOUND') self.assertRaisesRegex( exceptions.PoolAttributeNotFound, 'Could not find PoolAttribute', self.storage.find_pool_attribute, self.admin_context, criterion ) def test_update_pool_attribute(self): pool_attribute = self.create_pool_attribute(value='ns1.example.org') # Update the Pool Attribute pool_attribute.value = 'ns5.example.org' pool_attribute = self.storage.update_pool_attribute(self.admin_context, pool_attribute) # Verify the new values self.assertEqual('ns5.example.org', pool_attribute.value) # Ensure the version column was incremented self.assertEqual(2, pool_attribute.version) def test_update_pool_attribute_missing(self): pool_attribute = objects.PoolAttribute( id='728a329a-83b1-4573-82dc-45dceab435d4' ) self.assertRaisesRegex( exceptions.PoolAttributeNotFound, 'Could not find PoolAttribute', self.storage.update_pool_attribute, self.admin_context, pool_attribute ) def test_update_pool_attribute_duplicate(self): # Create two PoolAttributes pool_attribute_one = self.create_pool_attribute(fixture=0) pool_attribute_two = self.create_pool_attribute(fixture=1) # Update the second one to be a duplicate of the first pool_attribute_two.pool_id = pool_attribute_one.pool_id pool_attribute_two.key = pool_attribute_one.key pool_attribute_two.value = pool_attribute_one.value self.assertRaisesRegex( exceptions.DuplicatePoolAttribute, 'Duplicate PoolAttribute', self.storage.update_pool_attribute, self.admin_context, pool_attribute_two ) def test_delete_pool_attribute(self): pool_attribute = self.create_pool_attribute(fixture=0) self.storage.delete_pool_attribute(self.admin_context, pool_attribute['id']) self.assertRaisesRegex( exceptions.PoolAttributeNotFound, 'Could not find PoolAttribute', self.storage.get_pool_attribute, self.admin_context, pool_attribute['id'] ) def test_delete_oool_attribute_missing(self): uuid = '464e9250-4fe0-4267-9993-da639390bb04' self.assertRaisesRegex( exceptions.PoolAttributeNotFound, 'Could not find PoolAttribute', self.storage.delete_pool_attribute, self.admin_context, uuid ) def test_create_pool_attribute_duplicate(self): # Create the initial PoolAttribute self.create_pool_attribute(fixture=0) self.assertRaisesRegex( exceptions.DuplicatePoolAttribute, 'Duplicate PoolAttribute', self.create_pool_attribute, fixture=0 ) # PoolNameserver tests def test_create_pool_nameserver(self): pool = self.create_pool(fixture=0) values = { 'pool_id': pool.id, 'host': '192.0.2.1', 'port': 53 } result = self.storage.create_pool_nameserver( self.admin_context, pool.id, objects.PoolNameserver.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNotNone(result['version']) self.assertIsNone(result['updated_at']) self.assertEqual(values['pool_id'], result['pool_id']) self.assertEqual(values['host'], result['host']) self.assertEqual(values['port'], result['port']) def test_create_pool_nameserver_duplicate(self): pool = self.create_pool(fixture=0) # Create the initial PoolNameserver self.create_pool_nameserver(pool, fixture=0) self.assertRaisesRegex( exceptions.DuplicatePoolNameserver, 'Duplicate PoolNameserver', self.create_pool_nameserver, pool, fixture=0 ) def test_find_pool_nameservers(self): pool = self.create_pool(fixture=0) # Verify that there are no pool_nameservers created actual = self.storage.find_pool_nameservers(self.admin_context) self.assertEqual(0, len(actual)) # Create a PoolNameserver pool_nameserver = self.create_pool_nameserver(pool, fixture=0) # Fetch the PoolNameservers and ensure only 1 exists actual = self.storage.find_pool_nameservers(self.admin_context) self.assertEqual(1, len(actual)) self.assertEqual(pool_nameserver['pool_id'], actual[0]['pool_id']) self.assertEqual(pool_nameserver['host'], actual[0]['host']) self.assertEqual(pool_nameserver['port'], actual[0]['port']) def test_find_pool_nameservers_paging(self): pool = self.create_pool(fixture=0) # Create 10 PoolNameservers created = [self.create_pool_nameserver(pool, host='192.0.2.%d' % i) for i in range(10)] # Ensure we can page through the results. self._ensure_paging(created, self.storage.find_pool_nameservers) def test_find_pool_nameservers_with_criterion(self): pool = self.create_pool(fixture=0) # Create two pool_nameservers pool_nameserver_one = self.create_pool_nameserver(pool, fixture=0) pool_nameserver_two = self.create_pool_nameserver(pool, fixture=1) # Verify pool_nameserver_one criterion = dict(host=pool_nameserver_one['host']) results = self.storage.find_pool_nameservers( self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(pool_nameserver_one['host'], results[0]['host']) # Verify pool_nameserver_two criterion = dict(host=pool_nameserver_two['host']) results = self.storage.find_pool_nameservers(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(pool_nameserver_two['host'], results[0]['host']) def test_get_pool_nameserver(self): pool = self.create_pool(fixture=0) expected = self.create_pool_nameserver(pool, fixture=0) actual = self.storage.get_pool_nameserver( self.admin_context, expected['id']) self.assertEqual(expected['host'], actual['host']) def test_get_pool_nameserver_missing(self): uuid = '2c102ffd-7146-4b4e-ad62-b530ee0873fb' self.assertRaisesRegex( exceptions.PoolNameserverNotFound, 'Could not find PoolNameserver', self.storage.get_pool_nameserver, self.admin_context, uuid ) def test_find_pool_nameserver_criterion(self): pool = self.create_pool(fixture=0) # Create two pool_nameservers pool_nameserver_one = self.create_pool_nameserver(pool, fixture=0) pool_nameserver_two = self.create_pool_nameserver(pool, fixture=1) # Verify pool_nameserver_one criterion = dict(host=pool_nameserver_one['host']) result = self.storage.find_pool_nameserver( self.admin_context, criterion) self.assertEqual(pool_nameserver_one['host'], result['host']) # Verify pool_nameserver_two criterion = dict(host=pool_nameserver_two['host']) result = self.storage.find_pool_nameserver( self.admin_context, criterion) self.assertEqual(pool_nameserver_two['host'], result['host']) def test_find_pool_nameserver_criterion_missing(self): pool = self.create_pool(fixture=0) expected = self.create_pool_nameserver(pool, fixture=0) criterion = dict(host=expected['host'] + 'NOT FOUND') self.assertRaisesRegex( exceptions.PoolNameserverNotFound, 'Could not find PoolNameserver', self.storage.find_pool_nameserver, self.admin_context, criterion ) def test_update_pool_nameserver(self): pool = self.create_pool(fixture=0) pool_nameserver = self.create_pool_nameserver(pool, host='192.0.2.1') # Update the pool_nameserver pool_nameserver.host = '192.0.2.2' pool_nameserver = self.storage.update_pool_nameserver( self.admin_context, pool_nameserver) # Verify the new values self.assertEqual('192.0.2.2', pool_nameserver.host) # Ensure the version column was incremented self.assertEqual(2, pool_nameserver.version) def test_update_pool_nameserver_duplicate(self): pool = self.create_pool(fixture=0) # Create two pool_nameservers pool_nameserver_one = self.create_pool_nameserver( pool, fixture=0, host='192.0.2.1') pool_nameserver_two = self.create_pool_nameserver( pool, fixture=0, host='192.0.2.2') # Update the second one to be a duplicate of the first pool_nameserver_two.host = pool_nameserver_one.host self.assertRaisesRegex( exceptions.DuplicatePoolNameserver, 'Duplicate PoolNameserver', self.storage.update_pool_nameserver, self.admin_context, pool_nameserver_two ) def test_update_pool_nameserver_missing(self): pool_nameserver = objects.PoolNameserver( id='e8cee063-3a26-42d6-b181-bdbdc2c99d08' ) self.assertRaisesRegex( exceptions.PoolNameserverNotFound, 'Could not find PoolNameserver', self.storage.update_pool_nameserver, self.admin_context, pool_nameserver ) def test_delete_pool_nameserver(self): pool = self.create_pool(fixture=0) pool_nameserver = self.create_pool_nameserver(pool, fixture=0) self.storage.delete_pool_nameserver( self.admin_context, pool_nameserver['id']) self.assertRaisesRegex( exceptions.PoolNameserverNotFound, 'Could not find PoolNameserver', self.storage.get_pool_nameserver, self.admin_context, pool_nameserver['id'] ) def test_delete_pool_nameserver_missing(self): uuid = '97f57960-f41b-4e93-8e22-8fd6c7e2c183' self.assertRaisesRegex( exceptions.PoolNameserverNotFound, 'Could not find PoolNameserver', self.storage.delete_pool_nameserver, self.admin_context, uuid ) # PoolTarget tests def test_create_pool_target(self): pool = self.create_pool(fixture=0) values = { 'pool_id': pool.id, 'type': 'fake' } result = self.storage.create_pool_target( self.admin_context, pool.id, objects.PoolTarget.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNotNone(result['version']) self.assertIsNone(result['updated_at']) self.assertEqual(values['pool_id'], result['pool_id']) self.assertEqual(values['type'], result['type']) def test_find_pool_targets(self): pool = self.create_pool(fixture=0) # Verify that there are no new pool_targets created actual = self.storage.find_pool_targets( self.admin_context, criterion={'pool_id': pool.id}) self.assertEqual(0, len(actual)) # Create a PoolTarget pool_target = self.create_pool_target(pool, fixture=0) # Fetch the PoolTargets and ensure only 2 exist actual = self.storage.find_pool_targets( self.admin_context, criterion={'pool_id': pool.id}) self.assertEqual(1, len(actual)) self.assertEqual(pool_target['pool_id'], actual[0]['pool_id']) self.assertEqual(pool_target['type'], actual[0]['type']) def test_find_pool_targets_paging(self): pool = self.create_pool(fixture=0) # Create 10 PoolTargets created = [self.create_pool_target(pool, description='Target %d' % i) for i in range(10)] # Ensure we can page through the results. self._ensure_paging(created, self.storage.find_pool_targets, criterion={'pool_id': pool.id}) def test_find_pool_targets_with_criterion(self): pool = self.create_pool(fixture=0) # Create two pool_targets pool_target_one = self.create_pool_target( pool, fixture=0, description='One') pool_target_two = self.create_pool_target( pool, fixture=1, description='Two') # Verify pool_target_one criterion = dict(description=pool_target_one['description']) results = self.storage.find_pool_targets( self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual( pool_target_one['description'], results[0]['description']) # Verify pool_target_two criterion = dict(description=pool_target_two['description']) results = self.storage.find_pool_targets(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual( pool_target_two['description'], results[0]['description']) def test_get_pool_target(self): pool = self.create_pool(fixture=0) expected = self.create_pool_target(pool, fixture=0) actual = self.storage.get_pool_target( self.admin_context, expected['id']) self.assertEqual(expected['type'], actual['type']) def test_get_pool_target_missing(self): uuid = '2c102ffd-7146-4b4e-ad62-b530ee0873fb' self.assertRaisesRegex( exceptions.PoolTargetNotFound, 'Could not find PoolTarget', self.storage.get_pool_target, self.admin_context, uuid ) def test_find_pool_target_criterion(self): pool = self.create_pool(fixture=0) # Create two pool_targets pool_target_one = self.create_pool_target( pool, fixture=0, description='One') pool_target_two = self.create_pool_target( pool, fixture=1, description='Two') # Verify pool_target_one criterion = dict(description=pool_target_one['description']) result = self.storage.find_pool_target( self.admin_context, criterion) self.assertEqual(pool_target_one['description'], result['description']) # Verify pool_target_two criterion = dict(description=pool_target_two['description']) result = self.storage.find_pool_target( self.admin_context, criterion) self.assertEqual(pool_target_two['description'], result['description']) def test_find_pool_target_criterion_missing(self): pool = self.create_pool(fixture=0) expected = self.create_pool_target(pool, fixture=0) criterion = dict(description=expected['description'] + 'NOT FOUND') self.assertRaisesRegex( exceptions.PoolTargetNotFound, 'Could not find PoolTarget', self.storage.find_pool_target, self.admin_context, criterion ) def test_update_pool_target(self): pool = self.create_pool(fixture=0) pool_target = self.create_pool_target(pool, description='One') # Update the pool_target pool_target.description = 'Two' pool_target.masters = objects.PoolTargetMasterList( objects=[objects.PoolTargetMaster(host='127.0.0.1', port=80)] ) pool_target.options = objects.PoolTargetOptionList( objects=[objects.PoolTargetOption(key='foo', value='bar')] ) pool_target = self.storage.update_pool_target( self.admin_context, pool_target) # Verify the new values self.assertEqual('Two', pool_target.description) # Ensure the version column was incremented self.assertEqual(2, pool_target.version) def test_update_pool_target_missing(self): pool_target = objects.PoolTarget( id='e8cee063-3a26-42d6-b181-bdbdc2c99d08' ) self.assertRaisesRegex( exceptions.PoolTargetNotFound, 'Could not find PoolTarget', self.storage.update_pool_target, self.admin_context, pool_target ) def test_delete_pool_target(self): pool = self.create_pool(fixture=0) pool_target = self.create_pool_target(pool, fixture=0) self.storage.delete_pool_target( self.admin_context, pool_target['id']) self.assertRaisesRegex( exceptions.PoolTargetNotFound, 'Could not find PoolTarget', self.storage.get_pool_target, self.admin_context, pool_target['id'] ) def test_delete_pool_target_missing(self): uuid = '97f57960-f41b-4e93-8e22-8fd6c7e2c183' self.assertRaisesRegex( exceptions.PoolTargetNotFound, 'Could not find PoolTarget', self.storage.delete_pool_target, self.admin_context, uuid ) def test_create_pool_target_option(self): pool = self.create_pool(fixture=0) pool_target = self.create_pool_target(pool, fixture=0) target = self.storage.create_pool_target_option( self.admin_context, pool_target['id'], objects.PoolTargetOption(key='foo', value='bar') ) result = self.storage._find_pool_target_options( self.admin_context, {'id': target['id']} ) self.assertEqual(1, len(result)) def test_update_pool_target_option(self): pool = self.create_pool(fixture=0) pool_target = self.create_pool_target(pool, fixture=0) target = self.storage.create_pool_target_option( self.admin_context, pool_target['id'], objects.PoolTargetOption(key='foo', value='bar') ) target.value = 'baz' self.storage.update_pool_target_option(self.admin_context, target) result = self.storage._find_pool_target_options( self.admin_context, {'id': target['id']} ) self.assertEqual('baz', result[0].value) def test_delete_pool_target_option(self): pool = self.create_pool(fixture=0) pool_target = self.create_pool_target(pool, fixture=0) target = self.storage.create_pool_target_option( self.admin_context, pool_target['id'], objects.PoolTargetOption(key='foo', value='bar') ) self.storage.delete_pool_target_option( self.admin_context, target['id'] ) result = self.storage._find_pool_target_options( self.admin_context, {'id': target['id']} ) self.assertEqual(0, len(result)) def test_create_pool_target_master(self): pool = self.create_pool(fixture=0) pool_target = self.create_pool_target(pool, fixture=0) target = self.storage.create_pool_target_master( self.admin_context, pool_target['id'], objects.PoolTargetMaster(host='127.0.0.1', port=80) ) result = self.storage._find_pool_target_masters( self.admin_context, {'id': target['id']} ) self.assertEqual(1, len(result)) def test_update_pool_target_master(self): pool = self.create_pool(fixture=0) pool_target = self.create_pool_target(pool, fixture=0) target = self.storage.create_pool_target_master( self.admin_context, pool_target['id'], objects.PoolTargetMaster(host='127.0.0.1', port=80) ) target.port = 443 self.storage.update_pool_target_master(self.admin_context, target) result = self.storage._find_pool_target_masters( self.admin_context, {'id': target['id']} ) self.assertEqual(443, result[0].port) def test_delete_pool_target_master(self): pool = self.create_pool(fixture=0) pool_target = self.create_pool_target(pool, fixture=0) target = self.storage.create_pool_target_master( self.admin_context, pool_target['id'], objects.PoolTargetMaster(host='127.0.0.1', port=80) ) self.storage.delete_pool_target_master( self.admin_context, target['id'] ) result = self.storage._find_pool_target_masters( self.admin_context, {'id': target['id']} ) self.assertEqual(0, len(result)) def test_create_zone_attribute(self): zone = self.create_zone() zone_attribute = self.storage.create_zone_attribute( self.admin_context, zone['id'], objects.ZoneAttribute(key='foo', value='bar') ) result = self.storage.find_zone_attributes( self.admin_context, {'id': zone_attribute['id']} ) self.assertEqual(1, len(result)) def test_update_zone_attribute(self): zone = self.create_zone() zone_attribute = self.storage.create_zone_attribute( self.admin_context, zone['id'], objects.ZoneAttribute(key='foo', value='bar') ) zone_attribute.value = 'baz' self.storage.update_zone_attribute( self.admin_context, zone_attribute ) result = self.storage.get_zone_attributes( self.admin_context, zone_attribute['id'] ) self.assertEqual('baz', result.value) def test_delete_zone_attribute(self): zone = self.create_zone() zone_attribute = self.storage.create_zone_attribute( self.admin_context, zone['id'], objects.ZoneAttribute(key='foo', value='bar') ) self.storage.delete_zone_attribute( self.admin_context, zone_attribute['id'] ) result = self.storage.find_zone_attributes( self.admin_context, {'id': zone_attribute['id']} ) self.assertEqual(0, len(result)) def test_create_zone_master(self): zone = self.create_zone() zone_master = self.storage.create_zone_master( self.admin_context, zone['id'], objects.ZoneMaster(host='127.0.0.1', port='80') ) result = self.storage._find_zone_masters( self.admin_context, {'id': zone_master['id']} ) self.assertEqual(1, len(result)) def test_update_zone_master(self): zone = self.create_zone() zone_master = self.storage.create_zone_master( self.admin_context, zone['id'], objects.ZoneMaster(host='127.0.0.1', port='80') ) zone_master.port = 443 self.storage.update_zone_master( self.admin_context, zone_master ) result = self.storage._find_zone_masters( self.admin_context, {'id': zone_master['id']} ) self.assertEqual(443, result[0].port) def test_delete_zone_master(self): zone = self.create_zone() zone_master = self.storage.create_zone_master( self.admin_context, zone['id'], objects.ZoneMaster(host='127.0.0.1', port='80') ) self.storage.delete_zone_master( self.admin_context, zone_master['id'] ) result = self.storage._find_zone_masters( self.admin_context, {'id': zone_master['id']} ) self.assertEqual(0, len(result)) def test_create_zone_export(self): zone_export = self.storage.create_zone_export( self.admin_context, objects.ZoneExport(status='ACTIVE', task_type='EXPORT') ) result = self.storage.find_zone_exports( self.admin_context, {'id': zone_export['id']} ) self.assertEqual(1, len(result)) def test_find_zone_exports_with_no_criterion(self): self.storage.create_zone_export( self.admin_context, objects.ZoneExport(status='ACTIVE', task_type='EXPORT') ) result = self.storage._find_zone_exports( self.admin_context, None ) self.assertEqual(1, len(result)) def test_update_zone_export(self): zone_export = self.storage.create_zone_export( self.admin_context, objects.ZoneExport(status='ACTIVE', task_type='EXPORT') ) zone_export.message = 'foo' self.storage.update_zone_export( self.admin_context, zone_export ) result = self.storage.find_zone_export( self.admin_context, {'id': zone_export['id']} ) self.assertEqual('foo', result.message) def test_delete_zone_export(self): zone_export = self.storage.create_zone_export( self.admin_context, objects.ZoneExport(status='ACTIVE', task_type='EXPORT') ) self.storage.delete_zone_export( self.admin_context, zone_export['id'] ) result = self.storage.find_zone_exports( self.admin_context, {'id': zone_export['id']} ) self.assertEqual(0, len(result)) # PoolAlsoNotify tests def test_create_pool_also_notify(self): pool = self.create_pool(fixture=0) values = { 'pool_id': pool.id, 'host': '192.0.2.1', 'port': 53 } result = self.storage.create_pool_also_notify( self.admin_context, pool.id, objects.PoolAlsoNotify.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNotNone(result['version']) self.assertIsNone(result['updated_at']) self.assertEqual(values['pool_id'], result['pool_id']) self.assertEqual(values['host'], result['host']) self.assertEqual(values['port'], result['port']) def test_create_pool_also_notify_duplicate(self): pool = self.create_pool(fixture=0) # Create the initial PoolAlsoNotify self.create_pool_also_notify(pool, fixture=0) self.assertRaisesRegex( exceptions.DuplicatePoolAlsoNotify, 'Duplicate PoolAlsoNotify', self.create_pool_also_notify, pool, fixture=0 ) def test_find_pool_also_notifies(self): pool = self.create_pool(fixture=0) # Verify that there are no pool_also_notifies created actual = self.storage.find_pool_also_notifies(self.admin_context) self.assertEqual(0, len(actual)) # Create a PoolAlsoNotify pool_also_notify = self.create_pool_also_notify(pool, fixture=0) # Fetch the PoolAlsoNotifies and ensure only 1 exists actual = self.storage.find_pool_also_notifies(self.admin_context) self.assertEqual(1, len(actual)) self.assertEqual(pool_also_notify['pool_id'], actual[0]['pool_id']) self.assertEqual(pool_also_notify['host'], actual[0]['host']) self.assertEqual(pool_also_notify['port'], actual[0]['port']) def test_find_pool_also_notifies_paging(self): pool = self.create_pool(fixture=0) # Create 10 PoolAlsoNotifies created = [self.create_pool_also_notify(pool, host='192.0.2.%d' % i) for i in range(10)] # Ensure we can page through the results. self._ensure_paging(created, self.storage.find_pool_also_notifies) def test_find_pool_also_notifies_with_criterion(self): pool = self.create_pool(fixture=0) # Create two pool_also_notifies pool_also_notify_one = self.create_pool_also_notify(pool, fixture=0) pool_also_notify_two = self.create_pool_also_notify(pool, fixture=1) # Verify pool_also_notify_one criterion = dict(host=pool_also_notify_one['host']) results = self.storage.find_pool_also_notifies( self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(pool_also_notify_one['host'], results[0]['host']) # Verify pool_also_notify_two criterion = dict(host=pool_also_notify_two['host']) results = self.storage.find_pool_also_notifies(self.admin_context, criterion) self.assertEqual(1, len(results)) self.assertEqual(pool_also_notify_two['host'], results[0]['host']) def test_get_pool_also_notify(self): pool = self.create_pool(fixture=0) expected = self.create_pool_also_notify(pool, fixture=0) actual = self.storage.get_pool_also_notify( self.admin_context, expected['id']) self.assertEqual(expected['host'], actual['host']) def test_get_pool_also_notify_missing(self): uuid = '2c102ffd-7146-4b4e-ad62-b530ee0873fb' self.assertRaisesRegex( exceptions.PoolAlsoNotifyNotFound, 'Could not find PoolAlsoNotify', self.storage.get_pool_also_notify, self.admin_context, uuid ) def test_find_pool_also_notify_criterion(self): pool = self.create_pool(fixture=0) # Create two pool_also_notifies pool_also_notify_one = self.create_pool_also_notify(pool, fixture=0) pool_also_notify_two = self.create_pool_also_notify(pool, fixture=1) # Verify pool_also_notify_one criterion = dict(host=pool_also_notify_one['host']) result = self.storage.find_pool_also_notify( self.admin_context, criterion) self.assertEqual(pool_also_notify_one['host'], result['host']) # Verify pool_also_notify_two criterion = dict(host=pool_also_notify_two['host']) result = self.storage.find_pool_also_notify( self.admin_context, criterion) self.assertEqual(pool_also_notify_two['host'], result['host']) def test_find_pool_also_notify_criterion_missing(self): pool = self.create_pool(fixture=0) expected = self.create_pool_also_notify(pool, fixture=0) criterion = dict(host=expected['host'] + 'NOT FOUND') self.assertRaisesRegex( exceptions.PoolAlsoNotifyNotFound, 'Could not find PoolAlsoNotify', self.storage.find_pool_also_notify, self.admin_context, criterion ) def test_update_pool_also_notify(self): pool = self.create_pool(fixture=0) pool_also_notify = self.create_pool_also_notify(pool, host='192.0.2.1') # Update the pool_also_notify pool_also_notify.host = '192.0.2.2' pool_also_notify = self.storage.update_pool_also_notify( self.admin_context, pool_also_notify) # Verify the new values self.assertEqual('192.0.2.2', pool_also_notify.host) # Ensure the version column was incremented self.assertEqual(2, pool_also_notify.version) def test_update_pool_also_notify_duplicate(self): pool = self.create_pool(fixture=0) # Create two pool_also_notifies pool_also_notify_one = self.create_pool_also_notify( pool, fixture=0, host='192.0.2.1') pool_also_notify_two = self.create_pool_also_notify( pool, fixture=0, host='192.0.2.2') # Update the second one to be a duplicate of the first pool_also_notify_two.host = pool_also_notify_one.host self.assertRaisesRegex( exceptions.DuplicatePoolAlsoNotify, 'Duplicate PoolAlsoNotify', self.storage.update_pool_also_notify, self.admin_context, pool_also_notify_two ) def test_update_pool_also_notify_missing(self): pool_also_notify = objects.PoolAlsoNotify( id='e8cee063-3a26-42d6-b181-bdbdc2c99d08' ) self.assertRaisesRegex( exceptions.PoolAlsoNotifyNotFound, 'Could not find PoolAlsoNotify', self.storage.update_pool_also_notify, self.admin_context, pool_also_notify ) def test_delete_pool_also_notify(self): pool = self.create_pool(fixture=0) pool_also_notify = self.create_pool_also_notify(pool, fixture=0) self.storage.delete_pool_also_notify( self.admin_context, pool_also_notify['id']) self.assertRaisesRegex( exceptions.PoolAlsoNotifyNotFound, 'Could not find PoolAlsoNotify', self.storage.get_pool_also_notify, self.admin_context, pool_also_notify['id'] ) def test_delete_pool_also_notify_missing(self): uuid = '97f57960-f41b-4e93-8e22-8fd6c7e2c183' self.assertRaisesRegex( exceptions.PoolAlsoNotifyNotFound, 'Could not find PoolAlsoNotify', self.storage.delete_pool_also_notify, self.admin_context, uuid ) def test_create_service_status_duplicate(self): values = self.get_service_status_fixture(fixture=0) self.storage.create_service_status( self.admin_context, objects.ServiceStatus.from_dict(values)) self.assertRaisesRegex( exceptions.DuplicateServiceStatus, 'Duplicate ServiceStatus', self.storage.create_service_status, self.admin_context, objects.ServiceStatus.from_dict(values) ) # Zone Transfer Accept tests def test_create_zone_transfer_request(self): zone = self.create_zone() values = { 'tenant_id': self.admin_context.project_id, 'zone_id': zone.id, 'key': 'qwertyuiop' } result = self.storage.create_zone_transfer_request( self.admin_context, objects.ZoneTransferRequest.from_dict(values)) self.assertEqual(self.admin_context.project_id, result['tenant_id']) self.assertIn('status', result) def test_create_zone_transfer_request_scoped(self): zone = self.create_zone() tenant_2_context = self.get_context(project_id='2') tenant_3_context = self.get_context(project_id='3') values = { 'tenant_id': self.admin_context.project_id, 'zone_id': zone.id, 'key': 'qwertyuiop', 'target_tenant_id': tenant_2_context.project_id, } result = self.storage.create_zone_transfer_request( self.admin_context, objects.ZoneTransferRequest.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNone(result['updated_at']) self.assertEqual(self.admin_context.project_id, result['tenant_id']) self.assertEqual( tenant_2_context.project_id, result['target_tenant_id'] ) self.assertIn('status', result) stored_ztr = self.storage.get_zone_transfer_request( tenant_2_context, result.id) self.assertEqual( self.admin_context.project_id, stored_ztr['tenant_id'] ) self.assertEqual(stored_ztr['id'], result['id']) self.assertRaisesRegex( exceptions.ZoneTransferRequestNotFound, 'Could not find ZoneTransferRequest', self.storage.get_zone_transfer_request, tenant_3_context, result.id ) def test_find_zone_transfer_requests(self): zone = self.create_zone() values = { 'tenant_id': self.admin_context.project_id, 'zone_id': zone.id, 'key': 'qwertyuiop' } self.storage.create_zone_transfer_request( self.admin_context, objects.ZoneTransferRequest.from_dict(values)) requests = self.storage.find_zone_transfer_requests( self.admin_context, {'tenant_id': self.admin_context.project_id}) self.assertEqual(1, len(requests)) def test_delete_zone_transfer_request(self): zone = self.create_zone() zt_request = self.create_zone_transfer_request(zone) self.storage.delete_zone_transfer_request( self.admin_context, zt_request.id) self.assertRaisesRegex( exceptions.ZoneTransferRequestNotFound, 'Could not find ZoneTransferRequest', self.storage.get_zone_transfer_request, self.admin_context, zt_request.id ) def test_update_zone_transfer_request(self): zone = self.create_zone() zt_request = self.create_zone_transfer_request(zone) zt_request.description = 'New description' result = self.storage.update_zone_transfer_request( self.admin_context, zt_request) self.assertEqual('New description', result.description) def test_get_zone_transfer_request(self): zone = self.create_zone() zt_request = self.create_zone_transfer_request(zone) result = self.storage.get_zone_transfer_request( self.admin_context, zt_request.id) self.assertEqual(zt_request.id, result.id) self.assertEqual(zt_request.zone_id, result.zone_id) def test_get_zone_transfer_request_no_project_id(self): context1 = self.get_context(project_id='1', roles=['member', 'reader']) context2 = self.get_context(roles=['member', 'reader']) zone = self.create_zone(context=context1) zt_request = self.create_zone_transfer_request(zone, context=context1) result = self.storage.get_zone_transfer_request(context2, zt_request.id) self.assertEqual(objects.ZoneTransferRequest(), result) def test_find_zone_transfer_requests_no_project_id(self): context1 = self.get_context(project_id='1', roles=['member', 'reader']) context2 = self.get_context(roles=['member', 'reader']) zone = self.create_zone(context=context1) zt_request = self.create_zone_transfer_request(zone, context=context1) result = self.storage.find_zone_transfer_requests(context2, zt_request.id) self.assertEqual(objects.ZoneTransferRequestList(), result) self.assertEqual(0, len(result)) # Zone Transfer Accept tests def test_create_zone_transfer_accept(self): zone = self.create_zone() zt_request = self.create_zone_transfer_request(zone) values = { 'tenant_id': self.admin_context.project_id, 'zone_transfer_request_id': zt_request.id, 'zone_id': zone.id, 'key': zt_request.key } result = self.storage.create_zone_transfer_accept( self.admin_context, objects.ZoneTransferAccept.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNone(result['updated_at']) self.assertEqual(self.admin_context.project_id, result['tenant_id']) self.assertIn('status', result) def test_find_zone_transfer_accepts(self): zone = self.create_zone() zt_request = self.create_zone_transfer_request(zone) values = { 'tenant_id': self.admin_context.project_id, 'zone_transfer_request_id': zt_request.id, 'zone_id': zone.id, 'key': zt_request.key } self.storage.create_zone_transfer_accept( self.admin_context, objects.ZoneTransferAccept.from_dict(values)) accepts = self.storage.find_zone_transfer_accepts( self.admin_context, {'tenant_id': self.admin_context.project_id}) self.assertEqual(1, len(accepts)) def test_find_zone_transfer_accept(self): zone = self.create_zone() zt_request = self.create_zone_transfer_request(zone) values = { 'tenant_id': self.admin_context.project_id, 'zone_transfer_request_id': zt_request.id, 'zone_id': zone.id, 'key': zt_request.key } result = self.storage.create_zone_transfer_accept( self.admin_context, objects.ZoneTransferAccept.from_dict(values)) accept = self.storage.find_zone_transfer_accept( self.admin_context, {'id': result.id}) self.assertEqual(result.id, accept.id) def test_transfer_zone_ownership(self): tenant_1_context = self.get_context(project_id='1', roles=['member', 'reader']) tenant_2_context = self.get_context(project_id='2', roles=['member', 'reader']) admin_context = self.get_admin_context() admin_context.all_tenants = True zone = self.create_zone(context=tenant_1_context) recordset = self.create_recordset(zone, context=tenant_1_context) record = recordset.records[0] updated_zone = zone updated_zone.tenant_id = tenant_2_context.project_id self.storage.update_zone( admin_context, updated_zone) saved_zone = self.storage.get_zone( admin_context, zone.id) saved_recordset = self.storage.find_recordset( admin_context, criterion={'id': recordset.id}) saved_record = self.storage.get_record( admin_context, record.id) self.assertEqual(tenant_2_context.project_id, saved_zone.tenant_id) self.assertEqual( tenant_2_context.project_id, saved_recordset.tenant_id ) self.assertEqual(tenant_2_context.project_id, saved_record.tenant_id) def test_delete_zone_transfer_accept(self): zone = self.create_zone() zt_request = self.create_zone_transfer_request(zone) zt_accept = self.create_zone_transfer_accept(zt_request) self.storage.delete_zone_transfer_accept( self.admin_context, zt_accept.id) self.assertRaisesRegex( exceptions.ZoneTransferAcceptNotFound, 'Could not find ZoneTransferAccept', self.storage.get_zone_transfer_accept, self.admin_context, zt_accept.id ) def test_update_zone_transfer_accept(self): zone = self.create_zone() zt_request = self.create_zone_transfer_request(zone) zt_accept = self.create_zone_transfer_accept(zt_request) zt_accept.status = 'COMPLETE' result = self.storage.update_zone_transfer_accept( self.admin_context, zt_accept) self.assertEqual('COMPLETE', result.status) def test_get_zone_transfer_accept(self): zone = self.create_zone() zt_request = self.create_zone_transfer_request(zone) zt_accept = self.create_zone_transfer_accept(zt_request) result = self.storage.get_zone_transfer_accept( self.admin_context, zt_accept.id) self.assertEqual(zt_accept.id, result.id) self.assertEqual(zt_accept.zone_id, result.zone_id) def test_count_zone_tasks(self): # in the beginning, there should be nothing zones = self.storage.count_zone_tasks(self.admin_context) self.assertEqual(0, zones) values = { 'status': 'PENDING', 'task_type': 'IMPORT' } self.storage.create_zone_import( self.admin_context, objects.ZoneImport.from_dict(values)) # count imported zones zones = self.storage.count_zone_tasks(self.admin_context) # well, did we get 1? self.assertEqual(1, zones) @mock.patch('designate.storage.sql.get_read_session') def test_count_zone_tasks_none_result(self, mock_read_session): mock_sql_execute = mock.Mock() mock_sql_fetchone = mock.Mock() mock_read_session().__enter__.return_value = mock_sql_execute mock_sql_execute.execute.return_value = mock_sql_fetchone mock_sql_fetchone.fetchone.return_value = None zone_tasks = self.storage.count_zone_tasks(self.admin_context) mock_read_session.assert_called() self.assertEqual(0, zone_tasks) @mock.patch('designate.storage.sql.get_read_session') def test_count_zone_transfer_accept_none_result(self, mock_read_session): mock_sql_execute = mock.Mock() mock_sql_fetchone = mock.Mock() mock_read_session().__enter__.return_value = mock_sql_execute mock_sql_execute.execute.return_value = mock_sql_fetchone mock_sql_fetchone.fetchone.return_value = None zone_transfer_accepts = self.storage.count_zone_transfer_accept( self.admin_context ) mock_read_session.assert_called() self.assertEqual(0, zone_transfer_accepts) # Zone Import Tests def test_create_zone_import(self): values = { 'status': 'PENDING', 'task_type': 'IMPORT' } result = self.storage.create_zone_import( self.admin_context, objects.ZoneImport.from_dict(values)) self.assertIsNotNone(result['id']) self.assertIsNotNone(result['created_at']) self.assertIsNone(result['updated_at']) self.assertIsNotNone(result['version']) self.assertEqual(values['status'], result['status']) self.assertIsNone(result['zone_id']) self.assertIsNone(result['message']) def test_find_zone_imports(self): actual = self.storage.find_zone_imports(self.admin_context) self.assertEqual(0, len(actual)) # Create a single ZoneImport zone_import = self.create_zone_import(fixture=0) actual = self.storage.find_zone_imports(self.admin_context) self.assertEqual(1, len(actual)) self.assertEqual(zone_import['status'], actual[0]['status']) self.assertEqual(zone_import['message'], actual[0]['message']) self.assertEqual(zone_import['zone_id'], actual[0]['zone_id']) def test_find_zone_imports_paging(self): # Create 10 ZoneImports created = [self.create_zone_import() for i in range(10)] # Ensure we can page through the results. self._ensure_paging(created, self.storage.find_zone_imports) def test_find_zone_imports_with_criterion(self): zone_import_one = self.create_zone_import(fixture=0) zone_import_two = self.create_zone_import(fixture=1) criterion_one = dict(status=zone_import_one['status']) results = self.storage.find_zone_imports(self.admin_context, criterion_one) self.assertEqual(1, len(results)) self.assertEqual(zone_import_one['status'], results[0]['status']) criterion_two = dict(status=zone_import_two['status']) results = self.storage.find_zone_imports(self.admin_context, criterion_two) self.assertEqual(1, len(results)) self.assertEqual(zone_import_two['status'], results[0]['status']) def test_get_zone_import(self): # Create a zone_import expected = self.create_zone_import() actual = self.storage.get_zone_import(self.admin_context, expected['id']) self.assertEqual(expected['status'], actual['status']) def test_get_zone_import_missing(self): uuid = '4c8e7f82-3519-4bf7-8940-a66a4480f223' self.assertRaisesRegex( exceptions.ZoneImportNotFound, 'Could not find ZoneImport', self.storage.get_zone_import, self.admin_context, uuid ) def test_find_zone_import_criterion_missing(self): expected = self.create_zone_import() criterion = dict(status=expected['status'] + 'NOT FOUND') self.assertRaisesRegex( exceptions.ZoneImportNotFound, 'Could not find ZoneImport', self.storage.find_zone_import, self.admin_context, criterion ) def test_update_zone_import(self): # Create a zone_import zone_import = self.create_zone_import(status='PENDING', task_type='IMPORT') # Update the zone_import zone_import.status = 'COMPLETE' # Update storage zone_import = self.storage.update_zone_import(self.admin_context, zone_import) # Verify the new value self.assertEqual('COMPLETE', zone_import.status) # Ensure the version column was incremented self.assertEqual(2, zone_import.version) def test_update_zone_import_missing(self): zone_import = objects.ZoneImport( id='486f9cbe-b8b6-4d8c-8275-1a6e47b13e00' ) self.assertRaisesRegex( exceptions.ZoneImportNotFound, 'Could not find ZoneImport', self.storage.update_zone_import, self.admin_context, zone_import ) def test_delete_zone_import(self): # Create a zone_import zone_import = self.create_zone_import() # Delete the zone_import self.storage.delete_zone_import(self.admin_context, zone_import['id']) # Verify that it's deleted self.assertRaisesRegex( exceptions.ZoneImportNotFound, 'Could not find ZoneImport', self.storage.get_zone_import, self.admin_context, zone_import['id'] ) def test_delete_zone_import_missing(self): uuid = 'cac1fc02-79b2-4e62-a1a4-427b6790bbe6' self.assertRaisesRegex( exceptions.ZoneImportNotFound, 'Could not find ZoneImport', self.storage.delete_zone_import, self.admin_context, uuid ) def test_schema_table_names(self): table_names = [ 'blacklists', 'pool_also_notifies', 'pool_attributes', 'pool_nameservers', 'pool_ns_records', 'pool_target_masters', 'pool_target_options', 'pool_targets', 'pools', 'quotas', 'records', 'recordsets', 'service_statuses', 'shared_zones', 'tlds', 'tsigkeys', 'zone_attributes', 'zone_masters', 'zone_tasks', 'zone_transfer_accepts', 'zone_transfer_requests', 'zones' ] inspector = self.storage.get_inspector() actual_table_names = inspector.get_table_names() # We have transitioned database schema migration tools. # They use different tracking tables. Accomidate that one or both # may exist in this test. migration_table_found = False if ('migrate_version' in actual_table_names or 'alembic_version' in actual_table_names): migration_table_found = True self.assertTrue( migration_table_found, 'A DB migration table was not found.' ) try: actual_table_names.remove('migrate_version') except ValueError: pass try: actual_table_names.remove('alembic_version') except ValueError: pass self.assertEqual(table_names, actual_table_names) def test_schema_table_indexes(self): with sql.get_read_session() as session: indexes_t = session.execute( text(""SELECT * FROM sqlite_master WHERE type = 'index';"")) indexes = {} # table name -> index names -> cmd for _, index_name, table_name, num, cmd in indexes_t: if index_name.startswith(""sqlite_""): continue # ignore sqlite-specific indexes if table_name not in indexes: indexes[table_name] = {} indexes[table_name][index_name] = cmd expected = { ""records"": { ""record_created_at"": ""CREATE INDEX record_created_at ON records (created_at)"", # noqa ""records_tenant"": ""CREATE INDEX records_tenant ON records (tenant_id)"", # noqa ""update_status_index"": ""CREATE INDEX update_status_index ON records (status, zone_id, tenant_id, created_at, serial)"", # noqa }, ""recordsets"": { ""recordset_created_at"": ""CREATE INDEX recordset_created_at ON recordsets (created_at)"", # noqa ""recordset_type_name"": ""CREATE INDEX recordset_type_name ON recordsets (type, name)"", # noqa ""reverse_name_dom_id"": ""CREATE INDEX reverse_name_dom_id ON recordsets (reverse_name, zone_id)"", # noqa ""rrset_type_domainid"": ""CREATE INDEX rrset_type_domainid ON recordsets (type, zone_id)"", # noqa ""rrset_updated_at"": ""CREATE INDEX rrset_updated_at ON recordsets (updated_at)"", # noqa ""rrset_zoneid"": ""CREATE INDEX rrset_zoneid ON recordsets (zone_id)"", # noqa ""rrset_type"": ""CREATE INDEX rrset_type ON recordsets (type)"", # noqa ""rrset_ttl"": ""CREATE INDEX rrset_ttl ON recordsets (ttl)"", # noqa ""rrset_tenant_id"": ""CREATE INDEX rrset_tenant_id ON recordsets (tenant_id)"", # noqa }, ""zones"": { ""delayed_notify"": ""CREATE INDEX delayed_notify ON zones (delayed_notify)"", # noqa ""increment_serial"": ""CREATE INDEX increment_serial ON zones (increment_serial)"", # noqa ""reverse_name_deleted"": ""CREATE INDEX reverse_name_deleted ON zones (reverse_name, deleted)"", # noqa ""zone_created_at"": ""CREATE INDEX zone_created_at ON zones (created_at)"", # noqa ""zone_deleted"": ""CREATE INDEX zone_deleted ON zones (deleted)"", ""zone_tenant_deleted"": ""CREATE INDEX zone_tenant_deleted ON zones (tenant_id, deleted)"", # noqa } } self.assertDictEqual(expected, indexes) >>>>>>> CHANGE (011ebe Fix list zones if shared with multiple projects) ",,6377,0
openstack%2Fdesignate~master~I8af9b5cf8c1473bbf7db71a1fb848fb64509db84,openstack/designate,master,I8af9b5cf8c1473bbf7db71a1fb848fb64509db84,Fix list zones if shared with multiple projects,MERGED,2023-06-30 18:53:42.000000000,2023-07-05 16:34:01.000000000,2023-07-05 16:32:53.000000000,"[{'_account_id': 11628}, {'_account_id': 22348}, {'_account_id': 22623}]","[{'number': 1, 'created': '2023-06-30 18:53:42.000000000', 'files': ['designate/storage/sqlalchemy/__init__.py', 'releasenotes/notes/Fix-zone-list-when-zone-shared-more-than-once-288b57cafeba82df.yaml', 'designate/tests/test_storage/test_storage.py'], 'web_link': 'https://opendev.org/openstack/designate/commit/011ebe2e7cd0df0c7f0869f0c7abbce79434821a', 'message': 'Fix list zones if shared with multiple projects\n\nThis patch fixes a bug when listing zones or updating recordsets in\nzones that are shared with more than one project.\n\nCloses-Bug: #2025295\nChange-Id: I8af9b5cf8c1473bbf7db71a1fb848fb64509db84\n'}]",1,887425,011ebe2e7cd0df0c7f0869f0c7abbce79434821a,8,3,1,11628,,,0,"Fix list zones if shared with multiple projects

This patch fixes a bug when listing zones or updating recordsets in
zones that are shared with more than one project.

Closes-Bug: #2025295
Change-Id: I8af9b5cf8c1473bbf7db71a1fb848fb64509db84
",git fetch https://review.opendev.org/openstack/designate refs/changes/25/887425/1 && git format-patch -1 --stdout FETCH_HEAD,"['designate/storage/sqlalchemy/__init__.py', 'releasenotes/notes/Fix-zone-list-when-zone-shared-more-than-once-288b57cafeba82df.yaml', 'designate/tests/test_storage/test_storage.py']",3,011ebe2e7cd0df0c7f0869f0c7abbce79434821a,," def test_find_zones_shared(self): # Create an admin context admin_context = self.get_admin_context() # Create a zone in the admin context zone = self.create_zone(context=admin_context) # Share the zone with two other projects self.share_zone( zone_id=zone['id'], target_project_id=1, context=admin_context) self.share_zone( zone_id=zone['id'], target_project_id=2, context=admin_context) # Ensure that one zone record is returned from find_zones (LP 2025295) results = self.storage.find_zones(admin_context) self.assertEqual(1, len(results)) ",,24,1
openstack%2Ftripleo-common~stable%2Fwallaby~I04f6ac171b10af7a294819d6248eac641090cc49,openstack/tripleo-common,stable/wallaby,I04f6ac171b10af7a294819d6248eac641090cc49,Only modify the container manifest if the mediaType has changed.,MERGED,2023-06-15 13:46:01.000000000,2023-07-05 16:23:41.000000000,2023-07-05 16:22:36.000000000,"[{'_account_id': 8833}, {'_account_id': 9816}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-15 13:46:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/b9b97350150c13eef3a2b13e01fa2f703be131d6', 'message': 'Only modify the container manifest if the mediaType has changed.\n\nOnly modify the manifest if the mediaType has actually changed.  This is a\npartial fix for https://bugzilla.redhat.com/show_bug.cgi?id=2213672 where the\njson pretty print with indent=3 causes the SHA256 of the manifest to change\nsince the manifest is not pretty printed the same way in the source registry.\n\nWhen the SHA256 changes, the above bug is triggered since the code does\nnot account for the fact that the httpd type-map file also needs to be\nupdated to use the changed SHA256.\n\nChange-Id: I04f6ac171b10af7a294819d6248eac641090cc49\nSigned-off-by: James Slagle <jslagle@redhat.com>\n'}, {'number': 2, 'created': '2023-06-15 13:46:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/95dbdbe260dbab5ef0d9deeb48ce44e055343cef', 'message': 'Only modify the container manifest if the mediaType has changed.\n\nOnly modify the manifest if the mediaType has actually changed.  This is a\npartial fix for https://bugzilla.redhat.com/show_bug.cgi?id=2213672 where the\njson pretty print with indent=3 causes the SHA256 of the manifest to change\nsince the manifest is not pretty printed the same way in the source registry.\n\nWhen the SHA256 changes, the above bug is triggered since the code does\nnot account for the fact that the httpd type-map file also needs to be\nupdated to use the changed SHA256.\n\nThis will need to be backported to stable/train as well.\n\nChange-Id: I04f6ac171b10af7a294819d6248eac641090cc49\nSigned-off-by: James Slagle <jslagle@redhat.com>\n'}, {'number': 3, 'created': '2023-06-15 17:11:54.000000000', 'files': ['tripleo_common/image/image_uploader.py'], 'web_link': 'https://opendev.org/openstack/tripleo-common/commit/b0962d2ba09fbb4da33daa328e6a50cac5e3ba05', 'message': 'Only modify the container manifest if the mediaType has changed.\n\nOnly modify the manifest if the mediaType has actually changed.  This is a\npartial fix for https://bugzilla.redhat.com/show_bug.cgi?id=2213672 where the\njson pretty print with indent=3 causes the SHA256 of the manifest to change\nsince the manifest is not pretty printed the same way in the source registry.\n\nWhen the SHA256 changes, the above bug is triggered since the code does\nnot account for the fact that the httpd type-map file also needs to be\nupdated to use the changed SHA256.\n\nThis will need to be backported to stable/train as well.\n\nChange-Id: I04f6ac171b10af7a294819d6248eac641090cc49\nSigned-off-by: James Slagle <jslagle@redhat.com>\n'}]",1,886179,b0962d2ba09fbb4da33daa328e6a50cac5e3ba05,13,3,3,7144,,,0,"Only modify the container manifest if the mediaType has changed.

Only modify the manifest if the mediaType has actually changed.  This is a
partial fix for https://bugzilla.redhat.com/show_bug.cgi?id=2213672 where the
json pretty print with indent=3 causes the SHA256 of the manifest to change
since the manifest is not pretty printed the same way in the source registry.

When the SHA256 changes, the above bug is triggered since the code does
not account for the fact that the httpd type-map file also needs to be
updated to use the changed SHA256.

This will need to be backported to stable/train as well.

Change-Id: I04f6ac171b10af7a294819d6248eac641090cc49
Signed-off-by: James Slagle <jslagle@redhat.com>
",git fetch https://review.opendev.org/openstack/tripleo-common refs/changes/79/886179/3 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_common/image/image_uploader.py'],1,b9b97350150c13eef3a2b13e01fa2f703be131d6,," new_manifest_type = manifest_type new_manifest_type = MEDIA_MANIFEST_V2 new_manifest_type = MEDIA_MANIFEST_V2 elif manifest_type == MEDIA_OCI_INDEX_V1: new_manifest_type = MEDIA_MANIFEST_V2_LIST # Only modify the manifest if the mediaType has actually changed. # This is a partial fix for # https://bugzilla.redhat.com/show_bug.cgi?id=2213672 # where the json pretty print with indent=3 causes the SHA256 of # the manifest to change since the manifest is not pretty printed # the same way in the source registry. if manifest_type != new_manifest_type: manifest['mediaType'] = manifest_type manifest_str = json.dumps(manifest, indent=3)"," manifest_type = MEDIA_MANIFEST_V2 manifest_type = MEDIA_MANIFEST_V2 elif manifest_type == MEDIA_OCI_INDEX_V1: manifest_type = MEDIA_MANIFEST_V2_LIST manifest['mediaType'] = manifest_type manifest_str = json.dumps(manifest, indent=3)",13,5
openstack%2Fnova-specs~master~Ie3f89730128fdb8beca8bb02312d11516affcbbc,openstack/nova-specs,master,Ie3f89730128fdb8beca8bb02312d11516affcbbc,Remove sphinxcontrib.seqdiag,MERGED,2023-07-05 14:24:47.000000000,2023-07-05 15:57:32.000000000,2023-07-05 15:56:26.000000000,"[{'_account_id': 7166}, {'_account_id': 11604}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-05 14:24:47.000000000', 'files': ['doc/source/_media/stein/reshape-provider-tree.seqdiag', 'doc/source/_media/rocky/reshape-provider-tree.svg', 'doc/source/_media/stein/reshape-provider-tree.svg', 'doc/source/_media/train/nova-cyborg-interaction.svg', 'specs/rocky/approved/reshape-provider-tree.rst', 'doc/source/conf.py', 'specs/train/approved/nova-cyborg-interaction.rst', 'doc/source/_media/rocky/reshape-provider-tree.seqdiag', 'doc/source/_media/ussuri/nova-cyborg-interaction.seqdiag', 'doc/source/_media/ussuri/nova-cyborg-interaction.svg', 'specs/stein/approved/reshape-provider-tree.rst', 'doc/source/_media/train/nova-cyborg-interaction.seqdiag', 'doc/requirements.txt', 'specs/ussuri/implemented/nova-cyborg-interaction.rst'], 'web_link': 'https://opendev.org/openstack/nova-specs/commit/00347ca9b7ea4e52b1833340c08869b72eedbb61', 'message': 'Remove sphinxcontrib.seqdiag\n\nThis was a great tool but it seems it is no longer maintained, with no\ncommits since late 2021 [1]. Remove it, capturing static copies of the\nSVGs it was generating so we don\'t lose information. The original\n""source"" is retained in case we ever want to revive our use of the tool\nbut that seems unlikely at this point.\n\n[1] https://github.com/blockdiag/blockdiag\n\nChange-Id: Ie3f89730128fdb8beca8bb02312d11516affcbbc\nSigned-off-by: Stephen Finucane <stephenfin@redhat.com>\n'}]",0,887702,00347ca9b7ea4e52b1833340c08869b72eedbb61,8,3,1,15334,,,0,"Remove sphinxcontrib.seqdiag

This was a great tool but it seems it is no longer maintained, with no
commits since late 2021 [1]. Remove it, capturing static copies of the
SVGs it was generating so we don't lose information. The original
""source"" is retained in case we ever want to revive our use of the tool
but that seems unlikely at this point.

[1] https://github.com/blockdiag/blockdiag

Change-Id: Ie3f89730128fdb8beca8bb02312d11516affcbbc
Signed-off-by: Stephen Finucane <stephenfin@redhat.com>
",git fetch https://review.opendev.org/openstack/nova-specs refs/changes/02/887702/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/_media/stein/reshape-provider-tree.seqdiag', 'doc/source/_media/rocky/reshape-provider-tree.svg', 'doc/source/_media/stein/reshape-provider-tree.svg', 'doc/source/_media/train/nova-cyborg-interaction.svg', 'specs/rocky/approved/reshape-provider-tree.rst', 'doc/source/conf.py', 'specs/train/approved/nova-cyborg-interaction.rst', 'doc/source/_media/rocky/reshape-provider-tree.seqdiag', 'doc/source/_media/ussuri/nova-cyborg-interaction.seqdiag', 'doc/source/_media/ussuri/nova-cyborg-interaction.svg', 'specs/stein/approved/reshape-provider-tree.rst', 'doc/source/_media/train/nova-cyborg-interaction.seqdiag', 'doc/requirements.txt', 'specs/ussuri/implemented/nova-cyborg-interaction.rst']",14,00347ca9b7ea4e52b1833340c08869b72eedbb61,remove-seqdiag,.. image:: /_media/ussuri/nova-cyborg-interaction.svg,".. seqdiag:: seqdiag { edge_length = 200; span_height = 15; activation = none; default_note_color = white; 'Nova Controller'; 'Placement'; 'Cyborg'; 'Nova Compute'; 'Nova Controller' -> 'Cyborg' [label = ""GET /v2/device_profiles?name=mydp""]; 'Nova Controller' <- 'Cyborg' [label = '{""device_profiles"": $device_profile}']; 'Nova Controller' -> 'Nova Controller' [label= 'Merge request groups into request_spec']; 'Nova Controller' -> 'Placement' [label= 'Get /allocation_candidates']; 'Nova Controller' <- 'Placement' [label= 'allocation candidates with nested RPs']; 'Nova Controller' -> 'Nova Controller' [label= 'Select a candidate']; 'Nova Controller' -> 'Nova Compute' [label= 'build_and_run_instances()']; 'Nova Compute' -> 'Cyborg' [label= 'POST /v2/accelerator_requests']; 'Nova Compute' <- 'Cyborg' [label= '{""arqs"": [$arq, ...]']; 'Nova Compute' -> 'Cyborg' [label= 'PATCH /v2/accelerator_requests']; 'Nova Compute' <- 'Cyborg' [label= '{""arqs"": [$arq, ...]']; 'Cyborg' -> 'Nova Controller' [label= 'POST /os-server-external-events']; 'Nova Compute' -> 'Nova Compute' [label= 'Wait for notification from Cyborg']; 'Nova Compute' -> 'Cyborg' [label= 'GET /v2/accelerator_requests? instance=$uuid&bind_state=resolved']; 'Nova Compute' <- 'Cyborg' [label= '{""arqs"": [$arq, ....]}']; } ",538,172
openstack%2Fmistral~master~I415bb3fbefa91da0e7aaca9fe5d3714989ffda69,openstack/mistral,master,I415bb3fbefa91da0e7aaca9fe5d3714989ffda69,Fix test not waiting for action to be created,MERGED,2023-06-27 21:30:52.000000000,2023-07-05 15:40:50.000000000,2023-07-05 15:39:49.000000000,"[{'_account_id': 8731}, {'_account_id': 11583}, {'_account_id': 15895}, {'_account_id': 19134}, {'_account_id': 21970}, {'_account_id': 22348}, {'_account_id': 29124}, {'_account_id': 35600}]","[{'number': 1, 'created': '2023-06-27 21:30:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/mistral/commit/b20213fdb35bf08cabd04fada1fd4a67e1a2fca0', 'message': 'Fix test not waiting for action to be created\n\nSigned-off-by: Oleg Ovcharuk <vgvoleg@gmail.com>\nChange-Id: I415bb3fbefa91da0e7aaca9fe5d3714989ffda69\n'}, {'number': 2, 'created': '2023-06-27 23:50:28.000000000', 'files': ['mistral/tests/unit/engine/base.py', 'mistral/tests/unit/engine/test_task_cancel.py'], 'web_link': 'https://opendev.org/openstack/mistral/commit/5cb9cb74e535bec21bda766fd6d34098a5b1e496', 'message': 'Fix test not waiting for action to be created\n\nChange-Id: I415bb3fbefa91da0e7aaca9fe5d3714989ffda69\nSigned-off-by: Oleg Ovcharuk <vgvoleg@gmail.com>\n'}]",4,887119,5cb9cb74e535bec21bda766fd6d34098a5b1e496,17,8,2,29124,,,0,"Fix test not waiting for action to be created

Change-Id: I415bb3fbefa91da0e7aaca9fe5d3714989ffda69
Signed-off-by: Oleg Ovcharuk <vgvoleg@gmail.com>
",git fetch https://review.opendev.org/openstack/mistral refs/changes/19/887119/2 && git format-patch -1 --stdout FETCH_HEAD,"['mistral/tests/unit/engine/base.py', 'mistral/tests/unit/engine/test_task_cancel.py']",2,b20213fdb35bf08cabd04fada1fd4a67e1a2fca0,fix/tests_stabilization, self.await_task_creates_action(task_1_ex.id),,14,0
openstack%2Ftripleo-upgrade~stable%2Fwallaby~Ia3051418f4cddcf5bfc7692d1b46c3eabc6a5442,openstack/tripleo-upgrade,stable/wallaby,Ia3051418f4cddcf5bfc7692d1b46c3eabc6a5442,[collect log] Fix wrong permission when collecting vm information.,ABANDONED,2023-07-05 15:28:59.000000000,2023-07-05 15:33:13.000000000,,[],"[{'number': 1, 'created': '2023-07-05 15:28:59.000000000', 'files': ['templates/collect_logs.yaml.j2'], 'web_link': 'https://opendev.org/openstack/tripleo-upgrade/commit/4ac4d2fd0841228746811d39bf4b293d4cee97dd', 'message': ""[collect log] Fix wrong permission when collecting vm information.\n\nWe must become root to be able to write in `/var/log/extra`.\n\n'/etc/openstack/clouds.yaml' is correctly populated in upstream and\ndownstream jobs meaning that we can use the `openstack --os-cloud`\ncommand with the root user.\n\nChange-Id: Ia3051418f4cddcf5bfc7692d1b46c3eabc6a5442\n""}]",0,887721,4ac4d2fd0841228746811d39bf4b293d4cee97dd,2,0,1,8297,,,0,"[collect log] Fix wrong permission when collecting vm information.

We must become root to be able to write in `/var/log/extra`.

'/etc/openstack/clouds.yaml' is correctly populated in upstream and
downstream jobs meaning that we can use the `openstack --os-cloud`
command with the root user.

Change-Id: Ia3051418f4cddcf5bfc7692d1b46c3eabc6a5442
",git fetch https://review.opendev.org/openstack/tripleo-upgrade refs/changes/21/887721/1 && git format-patch -1 --stdout FETCH_HEAD,['templates/collect_logs.yaml.j2'],1,4ac4d2fd0841228746811d39bf4b293d4cee97dd,, openstack --os-cloud {{ overcloud_stack_name }} server show $i -f shell > /var/log/extra/oc-server-$i-{% raw %}{{ current_stage }}{% endraw %}.txt;, openstack --os-cloud {{ overcloud_stack_name }} server show $i > /var/log/extra/oc-server-$i-{% raw %}{{ current_stage }}{% endraw %}.txt;,1,1
openstack%2Fcinder~master~I9ee8bb76e5b97b0493fc4647d2655ec74765097e,openstack/cinder,master,I9ee8bb76e5b97b0493fc4647d2655ec74765097e,Tatlin Unified driver - simplify code,NEW,2022-10-24 12:32:07.000000000,2023-07-05 15:00:07.000000000,,"[{'_account_id': 9236}, {'_account_id': 22348}, {'_account_id': 27615}, {'_account_id': 30615}, {'_account_id': 32029}, {'_account_id': 32238}, {'_account_id': 32761}]","[{'number': 1, 'created': '2022-10-24 12:32:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/ff372ff10660a9bac041ad0ad01895d3fa1056e6', 'message': 'Tatlin Unified driver - simplify code\n\nImplements: blueprint yadro-tatlin-unified-refactoring\nChange-Id: I9ee8bb76e5b97b0493fc4647d2655ec74765097e\n'}, {'number': 2, 'created': '2023-01-31 12:23:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2a55ff4e8c5b91a5098c1be7a8b8b96b989e6083', 'message': 'Tatlin Unified driver - simplify code\n\nImplements: blueprint yadro-tatlin-unified-refactoring\nChange-Id: I9ee8bb76e5b97b0493fc4647d2655ec74765097e\n'}, {'number': 3, 'created': '2023-02-15 15:21:12.000000000', 'files': ['cinder/volume/drivers/yadro/tatlin_common.py', 'cinder/tests/unit/volume/drivers/yadro/test_tatlin_client.py', 'cinder/volume/drivers/yadro/tatlin_client.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/90478c726e3a476e69031b79a58ec63b94da5634', 'message': 'Tatlin Unified driver - simplify code\n\nImplements: blueprint yadro-tatlin-unified-refactoring\nChange-Id: I9ee8bb76e5b97b0493fc4647d2655ec74765097e\n'}]",4,862491,90478c726e3a476e69031b79a58ec63b94da5634,83,7,3,13671,,,0,"Tatlin Unified driver - simplify code

Implements: blueprint yadro-tatlin-unified-refactoring
Change-Id: I9ee8bb76e5b97b0493fc4647d2655ec74765097e
",git fetch https://review.opendev.org/openstack/cinder refs/changes/91/862491/2 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/yadro/tatlin_common.py', 'cinder/tests/unit/volume/drivers/yadro/test_tatlin_client.py', 'cinder/volume/drivers/yadro/tatlin_client.py']",3,ff372ff10660a9bac041ad0ad01895d3fa1056e6,tatlin-unified-simplify," raise exception.VolumeBackendAPIException(message=message) if not ports: result = [p['port'] for p in ports] LOG.debug('Volume %s port list %s', volume_id, result) return result"," if not self.is_volume_exists(vol_id): message = _('Unable to get volume info %s' % vol_id) LOG.error(message) return {} return {} if ports == {}: res = [] for p in ports: res.append(p['port']) LOG.debug('Volume %s port list %s', volume_id, res) return res",60,38
openstack%2Frequirements~master~Ic5c748d60e902b55a4af9587c951a4520f5977db,openstack/requirements,master,Ic5c748d60e902b55a4af9587c951a4520f5977db,Upgrade XStatic-jQuery to 2.2.4.1 for Horizon,ABANDONED,2023-07-04 14:02:45.000000000,2023-07-05 14:51:55.000000000,,"[{'_account_id': 6914}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 14:02:45.000000000', 'files': ['global-requirements.txt', 'upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/82b4da69cd80c7053d3662f488dcbd733283a444', 'message': ""Upgrade XStatic-jQuery to 2.2.4.1 for Horizon\n\nSince we upgraded XStatic-JQuery-Migrate to 3.3.2.1 in patch\nhttps://review.opendev.org/c/openstack/requirements/+/883402 we now\nneed newer jquery, because that version of jquery-migrate won't\nwork with jquery 1.2.\n\nChange-Id: Ic5c748d60e902b55a4af9587c951a4520f5977db\n""}]",0,887607,82b4da69cd80c7053d3662f488dcbd733283a444,4,2,1,8648,,,0,"Upgrade XStatic-jQuery to 2.2.4.1 for Horizon

Since we upgraded XStatic-JQuery-Migrate to 3.3.2.1 in patch
https://review.opendev.org/c/openstack/requirements/+/883402 we now
need newer jquery, because that version of jquery-migrate won't
work with jquery 1.2.

Change-Id: Ic5c748d60e902b55a4af9587c951a4520f5977db
",git fetch https://review.opendev.org/openstack/requirements refs/changes/07/887607/1 && git format-patch -1 --stdout FETCH_HEAD,"['global-requirements.txt', 'upper-constraints.txt']",2,82b4da69cd80c7053d3662f488dcbd733283a444,,XStatic-jQuery===2.2.4.1,XStatic-jQuery===1.12.4.1,2,2
openstack%2Fcharm-ceph-dashboard~stable%2Fquincy.2~Iec2f200963c3ed1e4d9bff86d62b49ddc91d0578,openstack/charm-ceph-dashboard,stable/quincy.2,Iec2f200963c3ed1e4d9bff86d62b49ddc91d0578,Adds skip logic to non-leader units,MERGED,2023-07-05 07:50:29.000000000,2023-07-05 14:45:20.000000000,2023-07-05 14:45:20.000000000,"[{'_account_id': 20648}, {'_account_id': 22348}, {'_account_id': 33717}, {'_account_id': 34952}]","[{'number': 1, 'created': '2023-07-05 07:50:29.000000000', 'files': ['tests/bundles/kinetic-zed.yaml', 'tests/bundles/lunar-antelope.yaml', 'tests/bundles/focal-yoga.yaml', 'tests/bundles/focal.yaml', 'tests/bundles/jammy-antelope.yaml', 'src/charm.py', 'tests/bundles/jammy-yoga.yaml', 'tests/bundles/jammy-zed.yaml'], 'web_link': 'https://opendev.org/openstack/charm-ceph-dashboard/commit/28a0ce89e81d803e832144d711b518f245c76fa3', 'message': 'Adds skip logic to non-leader units\n\nNon leader units will skip the event handling if Ceph dashboard\nis not enabled by the leader yet.\n\nSome test bundle fixes.\n\nPartial-Bug: #1952282\ncherry-pick from I743e50663ee85c91af4962d7d100e2fd48efa48c\n\nChange-Id: Iec2f200963c3ed1e4d9bff86d62b49ddc91d0578\n'}]",1,887654,28a0ce89e81d803e832144d711b518f245c76fa3,8,4,1,15382,,,0,"Adds skip logic to non-leader units

Non leader units will skip the event handling if Ceph dashboard
is not enabled by the leader yet.

Some test bundle fixes.

Partial-Bug: #1952282
cherry-pick from I743e50663ee85c91af4962d7d100e2fd48efa48c

Change-Id: Iec2f200963c3ed1e4d9bff86d62b49ddc91d0578
",git fetch https://review.opendev.org/openstack/charm-ceph-dashboard refs/changes/54/887654/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/bundles/kinetic-zed.yaml', 'tests/bundles/lunar-antelope.yaml', 'tests/bundles/focal-yoga.yaml', 'tests/bundles/focal.yaml', 'tests/bundles/jammy-antelope.yaml', 'src/charm.py', 'tests/bundles/jammy-yoga.yaml', 'tests/bundles/jammy-zed.yaml']",8,28a0ce89e81d803e832144d711b518f245c76fa3,," osd-devices: 'cinder,10G,2' channel: 8.0/edge"," osd-devices: 'cinder,10G' channel: latest/edge",27,22
openstack%2Fcharm-vault~master~Ide581e82f53710d1d9f1e6e4a5232260a7ccabed,openstack/charm-vault,master,Ide581e82f53710d1d9f1e6e4a5232260a7ccabed,Add support for lunar,ABANDONED,2023-07-05 14:14:36.000000000,2023-07-05 14:15:30.000000000,,[],"[{'number': 1, 'created': '2023-07-05 14:14:36.000000000', 'files': ['osci.yaml', 'src/tests/bundles/lunar-mysql8.yaml', 'src/wheelhouse.txt', 'src/tests/bundles/lunar-raft.yaml', 'src/tests/tests.yaml', 'src/tests/bundles/lunar-raft-cluster.yaml'], 'web_link': 'https://opendev.org/openstack/charm-vault/commit/c36555576d12cc4ce49cc32cc13e3b189bb722a6', 'message': 'Add support for lunar\n\nAs lunar introduces python3.11, psycopg2 version needs to be at least\nversion 2.9.5 to support it.\nModify the tests to run on Lunar and remove the Kinetic ones.\n\nCloses-Bug: #2025983\n\nChange-Id: Ide581e82f53710d1d9f1e6e4a5232260a7ccabed\nSigned-off-by: David Negreira <david.negreira@canonical.com>\n'}]",0,887701,c36555576d12cc4ce49cc32cc13e3b189bb722a6,2,0,1,32371,,,0,"Add support for lunar

As lunar introduces python3.11, psycopg2 version needs to be at least
version 2.9.5 to support it.
Modify the tests to run on Lunar and remove the Kinetic ones.

Closes-Bug: #2025983

Change-Id: Ide581e82f53710d1d9f1e6e4a5232260a7ccabed
Signed-off-by: David Negreira <david.negreira@canonical.com>
",git fetch https://review.opendev.org/openstack/charm-vault refs/changes/01/887701/1 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', 'src/tests/bundles/lunar-mysql8.yaml', 'src/wheelhouse.txt', 'src/tests/bundles/lunar-raft.yaml', 'src/tests/tests.yaml', 'src/tests/bundles/lunar-raft-cluster.yaml']",6,c36555576d12cc4ce49cc32cc13e3b189bb722a6,lp2025983,series: lunar,series: kinetic,21,19
openstack%2Fcharm-vault~master~I57047d140cf1bc0e9c503db1c82fe46d05bb9a0b,openstack/charm-vault,master,I57047d140cf1bc0e9c503db1c82fe46d05bb9a0b,Add support for lunar,ABANDONED,2023-07-05 14:11:55.000000000,2023-07-05 14:12:20.000000000,,[],"[{'number': 1, 'created': '2023-07-05 14:11:55.000000000', 'files': ['osci.yaml', 'src/tests/bundles/lunar-mysql8.yaml', 'src/wheelhouse.txt', 'src/tests/bundles/lunar-raft.yaml', 'src/tests/tests.yaml', 'src/tests/bundles/lunar-raft-cluster.yaml'], 'web_link': 'https://opendev.org/openstack/charm-vault/commit/09b5ea787c0ba0c950c0508c8f90d4d0bce58542', 'message': 'Add support for lunar\n\nAs lunar introduces python3.11, psycopg2 version needs to be at least\nversion 2.9.5 to support it.\nModify the tests to run on Lunar and remove the Kinetic ones.\n\nCloses-Bug: #2025983\n\nChange-Id: I57047d140cf1bc0e9c503db1c82fe46d05bb9a0b\nSigned-off-by: David Negreira <david.negreira@canonical.com>\n'}]",0,887700,09b5ea787c0ba0c950c0508c8f90d4d0bce58542,2,0,1,32371,,,0,"Add support for lunar

As lunar introduces python3.11, psycopg2 version needs to be at least
version 2.9.5 to support it.
Modify the tests to run on Lunar and remove the Kinetic ones.

Closes-Bug: #2025983

Change-Id: I57047d140cf1bc0e9c503db1c82fe46d05bb9a0b
Signed-off-by: David Negreira <david.negreira@canonical.com>
",git fetch https://review.opendev.org/openstack/charm-vault refs/changes/00/887700/1 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', 'src/tests/bundles/lunar-mysql8.yaml', 'src/wheelhouse.txt', 'src/tests/bundles/lunar-raft.yaml', 'src/tests/tests.yaml', 'src/tests/bundles/lunar-raft-cluster.yaml']",6,09b5ea787c0ba0c950c0508c8f90d4d0bce58542,,series: lunar,series: kinetic,21,19
openstack%2Fbifrost~stable%2Fyoga~I208182e65884d63548d78c68f676b899c562a2dc,openstack/bifrost,stable/yoga,I208182e65884d63548d78c68f676b899c562a2dc,CI: Update cached cirros image to 0.5.3,MERGED,2023-07-05 08:07:24.000000000,2023-07-05 14:05:40.000000000,2023-07-05 14:04:43.000000000,"[{'_account_id': 22348}, {'_account_id': 23851}]","[{'number': 1, 'created': '2023-07-05 08:07:24.000000000', 'files': ['playbooks/test-bifrost.yaml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/835bd27861ed22fd011ae8a933897432f202c919', 'message': ""CI: Update cached cirros image to 0.5.3\n\nBifrost CI is currently failing to fetch the cirros image from cache:\n\n    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'\n\nThis may be caused by the removal of cirros-0.5.1 images from cache in\nchange Ibada405e0c1183559f428c749d0e54d0a45a2223.\n\nSwitch to cirros version 0.5.3 image instead.\n\nChange-Id: I208182e65884d63548d78c68f676b899c562a2dc\n""}]",1,887656,835bd27861ed22fd011ae8a933897432f202c919,11,2,1,10239,,,0,"CI: Update cached cirros image to 0.5.3

Bifrost CI is currently failing to fetch the cirros image from cache:

    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'

This may be caused by the removal of cirros-0.5.1 images from cache in
change Ibada405e0c1183559f428c749d0e54d0a45a2223.

Switch to cirros version 0.5.3 image instead.

Change-Id: I208182e65884d63548d78c68f676b899c562a2dc
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/56/887656/1 && git format-patch -1 --stdout FETCH_HEAD,['playbooks/test-bifrost.yaml'],1,835bd27861ed22fd011ae8a933897432f202c919,, cirros_deploy_image_upstream_url: file:///opt/cache/files/cirros-0.5.3-x86_64-disk.img, cirros_deploy_image_upstream_url: file:///opt/cache/files/cirros-0.5.1-x86_64-disk.img,1,1
openstack%2Fcharm-vault~master~I3185f26e243bf02e8a0dcdd8ad5c7abd1a5729a1,openstack/charm-vault,master,I3185f26e243bf02e8a0dcdd8ad5c7abd1a5729a1,Add support for lunar,ABANDONED,2023-07-05 14:04:14.000000000,2023-07-05 14:04:55.000000000,,[],"[{'number': 1, 'created': '2023-07-05 14:04:14.000000000', 'files': ['osci.yaml', 'src/tests/bundles/kinetic-raft-cluster.yaml', 'src/wheelhouse.txt', 'src/tests/bundles/kinetic-mysql8.yaml', 'src/tests/tests.yaml', 'src/tests/bundles/kinetic-raft.yaml'], 'web_link': 'https://opendev.org/openstack/charm-vault/commit/cedf9e84017764ff366d929a629ad13622ed08d1', 'message': 'Add support for lunar\n\nAs lunar introduces python3.11, psycopg2 version needs to be at least\nversion 2.9.5 to support it.\nModify the tests to run on Lunar and remove the Kinetic ones.\n\nCloses-Bug: #2025983\n\nChange-Id: I3185f26e243bf02e8a0dcdd8ad5c7abd1a5729a1\nSigned-off-by: David Negreira <david.negreira@canonical.com>\n'}]",0,887698,cedf9e84017764ff366d929a629ad13622ed08d1,2,0,1,32371,,,0,"Add support for lunar

As lunar introduces python3.11, psycopg2 version needs to be at least
version 2.9.5 to support it.
Modify the tests to run on Lunar and remove the Kinetic ones.

Closes-Bug: #2025983

Change-Id: I3185f26e243bf02e8a0dcdd8ad5c7abd1a5729a1
Signed-off-by: David Negreira <david.negreira@canonical.com>
",git fetch https://review.opendev.org/openstack/charm-vault refs/changes/98/887698/1 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', 'src/tests/bundles/kinetic-raft-cluster.yaml', 'src/wheelhouse.txt', 'src/tests/bundles/kinetic-mysql8.yaml', 'src/tests/tests.yaml', 'src/tests/bundles/kinetic-raft.yaml']",6,cedf9e84017764ff366d929a629ad13622ed08d1,lp2025983,,"variables: openstack-origin: &openstack-origin distro local_overlay_enabled: False series: kinetic comment: - 'machines section to decide order of deployment. database sooner = faster' machines: '0': constraints: mem=3072M '1': constraints: mem=3072M '2': constraints: mem=3072M '3': '4': '5': '6': '7': '8': '9': '10': applications: keystone-mysql-router: charm: ch:mysql-router channel: latest/edge mysql-innodb-cluster: charm: ch:mysql-innodb-cluster num_units: 3 options: source: *openstack-origin to: - '0' - '1' - '2' channel: latest/edge vault: num_units: 1 charm: ../../../vault.charm to: - '3' keystone: charm: ch:keystone num_units: 1 options: admin-password: openstack openstack-origin: *openstack-origin to: - '4' channel: latest/edge ceph-mon: charm: ch:ceph-mon num_units: 3 options: source: *openstack-origin to: - '5' - '6' - '7' channel: latest/edge ceph-osd: charm: ch:ceph-osd num_units: 3 options: osd-encrypt: true osd-encrypt-keymanager: vault source: *openstack-origin storage: osd-devices: 10G,2 to: - '8' - '9' - '10' channel: latest/edge relations: - - 'keystone:shared-db' - 'keystone-mysql-router:shared-db' - - 'keystone-mysql-router:db-router' - 'mysql-innodb-cluster:db-router' - - 'vault:certificates' - 'keystone:certificates' - - 'vault:secrets' - 'ceph-osd:secrets-storage' - - 'ceph-mon:osd' - 'ceph-osd:mon' ",18,323
openstack%2Fcharm-vault~master~I651d3d5018eb1e79f01578e668aa5beb25726c52,openstack/charm-vault,master,I651d3d5018eb1e79f01578e668aa5beb25726c52,Add support for lunar,ABANDONED,2023-07-05 13:57:53.000000000,2023-07-05 13:58:11.000000000,,[],"[{'number': 1, 'created': '2023-07-05 13:57:53.000000000', 'files': ['osci.yaml', 'src/tests/bundles/lunar-mysql8.yaml', 'src/wheelhouse.txt', 'src/tests/bundles/lunar-raft.yaml', 'src/tests/tests.yaml', 'src/tests/bundles/lunar-raft-cluster.yaml'], 'web_link': 'https://opendev.org/openstack/charm-vault/commit/cfda53cab70198b4bf5eba9fe9bbbdf2b62fcd5c', 'message': 'Add support for lunar\n\nAs lunar introduces python3.11, psycopg2 version needs to be at least\nversion 2.9.5 to support it.\nModify the tests to run on Lunar and remove the Kinetic ones.\n\nCloses-Bug: #2025983\n\nChange-Id: I651d3d5018eb1e79f01578e668aa5beb25726c52\nSigned-off-by: David Negreira <david.negreira@canonical.com>\n'}]",0,887697,cfda53cab70198b4bf5eba9fe9bbbdf2b62fcd5c,2,0,1,32371,,,0,"Add support for lunar

As lunar introduces python3.11, psycopg2 version needs to be at least
version 2.9.5 to support it.
Modify the tests to run on Lunar and remove the Kinetic ones.

Closes-Bug: #2025983

Change-Id: I651d3d5018eb1e79f01578e668aa5beb25726c52
Signed-off-by: David Negreira <david.negreira@canonical.com>
",git fetch https://review.opendev.org/openstack/charm-vault refs/changes/97/887697/1 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', 'src/tests/bundles/lunar-mysql8.yaml', 'src/wheelhouse.txt', 'src/tests/bundles/lunar-raft.yaml', 'src/tests/tests.yaml', 'src/tests/bundles/lunar-raft-cluster.yaml']",6,cfda53cab70198b4bf5eba9fe9bbbdf2b62fcd5c,lp2025983,series: lunar,series: kinetic,21,19
openstack%2Fcharm-mysql-innodb-cluster~stable%2Fjammy~Ic6130a450bd1cf2fe3632145686fb1304b467afc,openstack/charm-mysql-innodb-cluster,stable/jammy,Ic6130a450bd1cf2fe3632145686fb1304b467afc,Add service user password rotation feature,NEW,2023-06-02 13:44:25.000000000,2023-07-05 13:56:44.000000000,,"[{'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-02 13:44:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-mysql-innodb-cluster/commit/2fa726c568d7a238a46229aa56986cdef609cf3e', 'message': 'Add service user password rotation feature\n\nThis patch adds the service user rotation feature, which provides two\nactions:\n\n - list-service-usernames\n - rotate-service-user-password\n\n The first lists the possible usernames that can be rotated.  The\n second action rotates the service.  The patch is dependent on the\n charm-helpers change [1], and is tested via the func-test-pr.\n\n [1] https://github.com/juju/charm-helpers/pull/765\n\nChange-Id: Ic6130a450bd1cf2fe3632145686fb1304b467afc\nfunc-test-pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/1027\n(cherry picked from commit ba6c62021ad008a8ca6b6b3542428c206853c51a)\n'}, {'number': 2, 'created': '2023-07-05 13:37:17.000000000', 'files': ['src/actions/rotate-service-user-password', 'src/actions.yaml', 'unit_tests/test_lib_charm_openstack_mysql_innodb_cluster.py', 'src/actions/list-service-usernames', 'src/lib/charm/openstack/exceptions.py', 'src/lib/charm/openstack/mysql_innodb_cluster.py', 'src/actions/actions.py', 'src/tests/tests.yaml'], 'web_link': 'https://opendev.org/openstack/charm-mysql-innodb-cluster/commit/d0b3e1a383bc01bc9cb591994a615adbd02508c9', 'message': 'Add service user password rotation feature\n\nThis patch adds the service user rotation feature, which provides two\nactions:\n\n - list-service-usernames\n - rotate-service-user-password\n\n The first lists the possible usernames that can be rotated.  The\n second action rotates the service.  The patch is dependent on the\n charm-helpers change [1], and is tested via the func-test-pr.\n\n [1] https://github.com/juju/charm-helpers/pull/765\n\nChange-Id: Ic6130a450bd1cf2fe3632145686fb1304b467afc\nfunc-test-pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/1027\n(cherry picked from commit ba6c62021ad008a8ca6b6b3542428c206853c51a)\n'}]",0,885021,d0b3e1a383bc01bc9cb591994a615adbd02508c9,6,2,2,20870,,,0,"Add service user password rotation feature

This patch adds the service user rotation feature, which provides two
actions:

 - list-service-usernames
 - rotate-service-user-password

 The first lists the possible usernames that can be rotated.  The
 second action rotates the service.  The patch is dependent on the
 charm-helpers change [1], and is tested via the func-test-pr.

 [1] https://github.com/juju/charm-helpers/pull/765

Change-Id: Ic6130a450bd1cf2fe3632145686fb1304b467afc
func-test-pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/1027
(cherry picked from commit ba6c62021ad008a8ca6b6b3542428c206853c51a)
",git fetch https://review.opendev.org/openstack/charm-mysql-innodb-cluster refs/changes/21/885021/2 && git format-patch -1 --stdout FETCH_HEAD,"['src/actions/rotate-service-user-password', 'src/actions.yaml', 'unit_tests/test_lib_charm_openstack_mysql_innodb_cluster.py', 'src/actions/list-service-usernames', 'src/lib/charm/openstack/exceptions.py', 'src/lib/charm/openstack/mysql_innodb_cluster.py', 'src/actions/actions.py', 'src/tests/tests.yaml']",8,2fa726c568d7a238a46229aa56986cdef609cf3e,service-password-rotation-stable/jammy,- zaza.openstack.charm_tests.mysql.tests.MySQLInnoDBClusterRotatePasswordTests - zaza.openstack.charm_tests.mysql.tests.MySQLInnoDBClusterRotatePasswordTests - zaza.openstack.charm_tests.mysql.tests.MySQLInnoDBClusterRotatePasswordTests,,329,13
openstack%2Fcharm-vault~master~Ia8acfd5d1cdf627e085a42291e38742d93f90b96,openstack/charm-vault,master,Ia8acfd5d1cdf627e085a42291e38742d93f90b96,Add support for lunar,ABANDONED,2023-07-05 13:44:29.000000000,2023-07-05 13:45:23.000000000,,[],"[{'number': 1, 'created': '2023-07-05 13:44:29.000000000', 'files': ['osci.yaml', 'src/tests/bundles/lunar-mysql8.yaml', 'src/wheelhouse.txt', 'src/tests/bundles/lunar-raft.yaml', 'src/tests/tests.yaml', 'src/tests/bundles/lunar-raft-cluster.yaml'], 'web_link': 'https://opendev.org/openstack/charm-vault/commit/9238cebd644b33d9e54c181a653a26f00478cf83', 'message': 'Add support for lunar\n\nAs lunar introduces python3.11, psycopg2 version needs to be at least\nversion 2.9.5 to support it.\nModify the tests to run on Lunar and remove the Kinetic ones.\n\nCloses-Bug: #2025983\n\nChange-Id: Ia8acfd5d1cdf627e085a42291e38742d93f90b96\nSigned-off-by: David Negreira <david.negreira@canonical.com>\n'}]",0,887696,9238cebd644b33d9e54c181a653a26f00478cf83,2,0,1,32371,,,0,"Add support for lunar

As lunar introduces python3.11, psycopg2 version needs to be at least
version 2.9.5 to support it.
Modify the tests to run on Lunar and remove the Kinetic ones.

Closes-Bug: #2025983

Change-Id: Ia8acfd5d1cdf627e085a42291e38742d93f90b96
Signed-off-by: David Negreira <david.negreira@canonical.com>
",git fetch https://review.opendev.org/openstack/charm-vault refs/changes/96/887696/1 && git format-patch -1 --stdout FETCH_HEAD,"['osci.yaml', 'src/tests/bundles/lunar-mysql8.yaml', 'src/wheelhouse.txt', 'src/tests/bundles/lunar-raft.yaml', 'src/tests/tests.yaml', 'src/tests/bundles/lunar-raft-cluster.yaml']",6,9238cebd644b33d9e54c181a653a26f00478cf83,lp2025983,series: lunar,series: kinetic,21,19
openstack%2Fnova~master~Ia198f712e2ad277743aed08e27e480208f463ac7,openstack/nova,master,Ia198f712e2ad277743aed08e27e480208f463ac7,enable validations in nova-lvm,MERGED,2023-07-04 15:44:14.000000000,2023-07-05 13:02:13.000000000,2023-07-05 13:00:20.000000000,"[{'_account_id': 7166}, {'_account_id': 9708}, {'_account_id': 17685}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 15:44:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/a27e0394e1a6d61bc1cf3e40ef80810c61c84d39', 'message': 'enable validations in nova-lvm\n\nAs of I8ca059a4702471d4d30ea5a06079859eba3f5a81 validations\nare now requried for test_rebuild_volume_backed_server.\nValidations are also required for any volume attach/detach based test\nin general due to know qemu issues.\n\nThis patch just turns them back on to unblock the gate.\n\nChange-Id: Ia198f712e2ad277743aed08e27e480208f463ac7\n'}, {'number': 2, 'created': '2023-07-04 15:49:11.000000000', 'files': ['.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/nova/commit/6f56c5c9fd60ee1d53376a9100a9580cb2b38dc3', 'message': 'enable validations in nova-lvm\n\nAs of I8ca059a4702471d4d30ea5a06079859eba3f5a81 validations\nare now requried for test_rebuild_volume_backed_server.\nValidations are also required for any volume attach/detach based test\nin general due to know qemu issues.\n\nThis patch just turns them back on to unblock the gate.\n\nCloses-Bug: #2025813\nChange-Id: Ia198f712e2ad277743aed08e27e480208f463ac7\n'}]",3,887632,6f56c5c9fd60ee1d53376a9100a9580cb2b38dc3,19,4,2,11604,,,0,"enable validations in nova-lvm

As of I8ca059a4702471d4d30ea5a06079859eba3f5a81 validations
are now requried for test_rebuild_volume_backed_server.
Validations are also required for any volume attach/detach based test
in general due to know qemu issues.

This patch just turns them back on to unblock the gate.

Closes-Bug: #2025813
Change-Id: Ia198f712e2ad277743aed08e27e480208f463ac7
",git fetch https://review.opendev.org/openstack/nova refs/changes/32/887632/2 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.yaml'],1,a27e0394e1a6d61bc1cf3e40ef80810c61c84d39,bug/2025813,, # Disable SSH validation in tests to save time. TEMPEST_RUN_VALIDATION: false,0,2
openstack%2Fcinder~master~I20ec0b2edbba2aeb28881d58102d964bb35222d7,openstack/cinder,master,I20ec0b2edbba2aeb28881d58102d964bb35222d7,Fujitsu Driver: Add format when deleting volume,NEW,2022-08-04 09:40:20.000000000,2023-07-05 13:00:08.000000000,,"[{'_account_id': 22348}, {'_account_id': 29122}, {'_account_id': 30615}]","[{'number': 1, 'created': '2022-08-04 09:40:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/de41d957727bf9c984368a57f1d1b94db4644ff0', 'message': ""Fujitsu Driver: Add volume format when deleting volume.\n\nIf parameter 'type:delete_with_volume_format' is specified in the volume type when creating a volume, formatting will be performed before deleting the volume\n\nAdd the following file:\neternus_dx_utils\n\nChange-Id: I20ec0b2edbba2aeb28881d58102d964bb35222d7\n""}, {'number': 2, 'created': '2022-11-21 01:03:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a89f95b0f13d8ed2fe628e36ee6de427a4301157', 'message': ""Fujitsu Driver: Add volume format when deleting volume.\n\nIf parameter 'type:delete_with_volume_format' is specified in the volume type when creating a volume, formatting will be performed before deleting the volume\n\nAdd the following file:\neternus_dx_utils\n\nChange-Id: I20ec0b2edbba2aeb28881d58102d964bb35222d7\n""}, {'number': 3, 'created': '2022-11-21 06:54:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/8f6ace6878ef37ded8143221fa0aab73af61d13e', 'message': ""Fujitsu Driver: Add volume format when deleting volume.\n\nIf parameter 'type:delete_with_volume_format' is specified in the volume type when creating a volume, formatting will be performed before deleting the volume\n\nAdd the following file:\neternus_dx_utils\n\nChange-Id: I20ec0b2edbba2aeb28881d58102d964bb35222d7\n""}, {'number': 4, 'created': '2023-03-24 09:22:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/200c80ea989b489e56e94134781b75d60f4c070f', 'message': ""Fujitsu Driver: Add volume format when deleting volume.\n\nIf parameter 'type:delete_with_volume_format' is specified in the volume type when creating a volume, formatting will be performed before deleting the volume\n\nAdd the following file:\neternus_dx_utils\n\nChange-Id: I20ec0b2edbba2aeb28881d58102d964bb35222d7\n""}, {'number': 5, 'created': '2023-03-27 03:48:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/35a0c0d46b92950b2cdcd291b4a78cf638b8a598', 'message': ""Fujitsu Driver: Add volume format when deleting volume.\n\nIf parameter 'type:delete_with_volume_format' is specified in the volume type when creating a volume, formatting will be performed before deleting the volume\n\nAdd the following file:\neternus_dx_utils\n\nChange-Id: I20ec0b2edbba2aeb28881d58102d964bb35222d7\n""}, {'number': 6, 'created': '2023-03-27 06:24:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/d3e1af9574e01e96baeb095867862966f03a5b1a', 'message': ""Fujitsu Driver: Add volume format when deleting volume.\n\nIf parameter 'type:delete_with_volume_format' is specified in the volume type when creating a volume, formatting will be performed before deleting the volume\n\nAdd the following file:\neternus_dx_utils\n\nChange-Id: I20ec0b2edbba2aeb28881d58102d964bb35222d7\n""}, {'number': 7, 'created': '2023-03-31 02:00:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/288c9ee114154dabf043d5afe9092345f909a339', 'message': ""Fujitsu Driver: Add volume format when deleting volume.\n\nIf parameter 'type:delete_with_volume_format' is specified in the volume type when creating a volume, formatting will be performed before deleting the volume\n\nAdd the following file:\neternus_dx_utils\n\nChange-Id: I20ec0b2edbba2aeb28881d58102d964bb35222d7\n""}, {'number': 8, 'created': '2023-04-07 08:43:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/09cca1aab9fea680a3ef8d445609593ebcfd1b57', 'message': ""Fujitsu Driver: Add volume format when deleting volume\n\nIf parameter 'type:delete_with_volume_format' is specified in the\nvolume type when creating a volume, formatting will be performed\nbefore deleting the volume.\n\nAdd the following file:\neternus_dx_utils\n\nChange-Id: I20ec0b2edbba2aeb28881d58102d964bb35222d7\n""}, {'number': 9, 'created': '2023-04-07 08:48:04.000000000', 'files': ['cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_cli.py', 'releasenotes/notes/fujitsu-add-volume-format-40ec9cc8ef9ba8f6.yaml', 'cinder/tests/unit/volume/drivers/test_fujitsu_dx.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_fc.py', 'doc/source/configuration/block-storage/drivers/fujitsu-eternus-dx-driver.rst', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_iscsi.py', 'cinder/volume/drivers/fujitsu/eternus_dx/constants.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_common.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_utils.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/f576c31603cbc6826bd9263f24bfa6a95dd2d57b', 'message': ""Fujitsu Driver: Add format when deleting volume\n\nIf parameter 'type:delete_with_volume_format' is specified in the\nvolume type when creating a volume, formatting will be performed\nbefore deleting the volume.\n\nAdd the following file:\neternus_dx_utils\n\nChange-Id: I20ec0b2edbba2aeb28881d58102d964bb35222d7\n""}]",31,852003,f576c31603cbc6826bd9263f24bfa6a95dd2d57b,201,3,9,33609,,,0,"Fujitsu Driver: Add format when deleting volume

If parameter 'type:delete_with_volume_format' is specified in the
volume type when creating a volume, formatting will be performed
before deleting the volume.

Add the following file:
eternus_dx_utils

Change-Id: I20ec0b2edbba2aeb28881d58102d964bb35222d7
",git fetch https://review.opendev.org/openstack/cinder refs/changes/03/852003/4 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_cli.py', 'cinder/tests/unit/volume/drivers/test_fujitsu_dx.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_fc.py', 'cinder/volume/drivers/fujitsu/eternus_dx/constants.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_common.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_iscsi.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_utils.py']",7,de41d957727bf9c984368a57f1d1b94db4644ff0,fujitsu-driver-update,"# Copyright (c) 2021 FUJITSU LIMITED # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. # from oslo_log import log as logging from cinder import context from cinder.volume import volume_types LOG = logging.getLogger(__name__) def get_metadata(volume): """"""Get metadata using volume information."""""" LOG.debug('get_metadata, volume id: %s.', volume['id']) d_metadata = {} metadata = volume.get('volume_metadata') # value = {} enters the if branch, value = None enters the else. if metadata is not None: d_metadata = { data['key']: data['value'] for data in metadata } else: metadata = volume.get('metadata') if metadata: d_metadata = { key: metadata[key] for key in metadata } LOG.debug('get_metadata, metadata is: %s.', d_metadata) return d_metadata def get_format_property(volume, format_key): format_in_type = get_extra_specs(volume, key=format_key, default='False') format_in_vol = get_metadata(volume).get(format_key, 'False') if 'true' in [format_in_type.lower(), format_in_vol.lower()]: return 'True' else: return 'False' def get_extra_specs(volume, key=None, default=None): """"""Get extra specs from volume. If key is none, return all extra specs. """""" LOG.debug('get_extra_specs, ' 'volume id: %(v_id)s, ' 'key: %(key)s, ' 'default: %(default)s.', {'v_id': volume['id'], 'key': key, 'default': default}) extra_specs = None ret = default volume_type_id = volume.get('volume_type_id') if volume_type_id: ctxt = context.get_admin_context() volume_type = volume_types.get_volume_type(ctxt, volume_type_id) extra_specs = volume_type.get('extra_specs') if extra_specs: if not key: ret = extra_specs else: ret = extra_specs.get(key, default) LOG.debug('get_extra_specs: %s.', extra_specs) return ret ",,899,353
openstack%2Ftripleo-ansible~stable%2Fwallaby~I4e8ea9591a3193fb0078c0c3581cbf59fcc20f01,openstack/tripleo-ansible,stable/wallaby,I4e8ea9591a3193fb0078c0c3581cbf59fcc20f01,Add ceph keyring check,MERGED,2023-06-14 10:18:24.000000000,2023-07-05 12:59:54.000000000,2023-07-05 12:59:54.000000000,"[{'_account_id': 18002}, {'_account_id': 22348}, {'_account_id': 25402}]","[{'number': 1, 'created': '2023-06-14 10:18:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/b4352122a563a975ef59ff194b80927a41aefe0a', 'message': ""Add ceph keyring check\n\nThis patch adds a additional task in export.yaml\nto check if keyring is found in ceph cluster.\n\nAlso updaed 'seuser' in selinux.yaml\n\nChange-Id: I4e8ea9591a3193fb0078c0c3581cbf59fcc20f01\n""}, {'number': 2, 'created': '2023-06-14 11:16:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/2f55c465b1b8b033639c78085d9ca8d361ef915a', 'message': ""Add ceph keyring check\n\nThis patch adds a additional task in export.yaml\nto check if keyring is found in ceph cluster.\n\nAlso added 'seuser' for sefcontext in selinux.yaml\n\nChange-Id: I4e8ea9591a3193fb0078c0c3581cbf59fcc20f01\n""}, {'number': 3, 'created': '2023-06-14 15:04:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/59c33250360adfb8fb8912c6779c0ad54a23532d', 'message': ""Add ceph keyring check\n\nThis patch adds a additional task in export.yaml\nto check if keyring is found in ceph cluster.\n\nAlso added 'seuser' for sefcontext in selinux.yaml\n\nChange-Id: I4e8ea9591a3193fb0078c0c3581cbf59fcc20f01\n""}, {'number': 4, 'created': '2023-06-14 18:59:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/5b3fdc92fb53ecb959ef8dbd7fca583235f73446', 'message': ""Add ceph keyring check\n\nThis patch adds a additional task in export.yaml\nto check if keyring is found in ceph cluster.\n\nAlso added 'seuser' for sefcontext in selinux.yaml\n\nChange-Id: I4e8ea9591a3193fb0078c0c3581cbf59fcc20f01\n""}, {'number': 5, 'created': '2023-06-15 07:37:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/e99ef9748c2e47abc89e046986bf152c1d17ca60', 'message': ""Add ceph keyring check\n\nThis patch adds a additional task in export.yaml\nto check if keyring is found in ceph cluster.\n\nAlso added 'seuser' for sefcontext in selinux.yaml\n\nChange-Id: I4e8ea9591a3193fb0078c0c3581cbf59fcc20f01\n""}, {'number': 6, 'created': '2023-06-23 15:40:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/699d303a3215bdcbe20f45bd3f800228ef712e0d', 'message': ""Add ceph keyring check\n\nThis patch adds a additional task in export.yaml\nto check if keyring is found in ceph cluster.\n\nAlso added 'seuser' for sefcontext in selinux.yaml\n\nChange-Id: I4e8ea9591a3193fb0078c0c3581cbf59fcc20f01\n""}, {'number': 7, 'created': '2023-06-27 07:53:57.000000000', 'files': ['tripleo_ansible/roles/tripleo_cephadm/tasks/export.yaml', 'tripleo_ansible/roles/tripleo_cephadm/tasks/check_keyring.yaml', 'tripleo_ansible/roles/tripleo_cephadm/tasks/selinux.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/a1218c56246b74b1173d9c45774ddefb6c242a76', 'message': ""Add ceph keyring check\n\nThis patch adds a additional task in export.yaml\nto check if keyring is found in ceph cluster.\n\nAlso added 'seuser' for sefcontext in selinux.yaml\n\nChange-Id: I4e8ea9591a3193fb0078c0c3581cbf59fcc20f01\n""}]",20,885975,a1218c56246b74b1173d9c45774ddefb6c242a76,47,3,7,34598,,,0,"Add ceph keyring check

This patch adds a additional task in export.yaml
to check if keyring is found in ceph cluster.

Also added 'seuser' for sefcontext in selinux.yaml

Change-Id: I4e8ea9591a3193fb0078c0c3581cbf59fcc20f01
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/75/885975/7 && git format-patch -1 --stdout FETCH_HEAD,"['tripleo_ansible/roles/tripleo_cephadm/tasks/export.yaml', 'tripleo_ansible/roles/tripleo_cephadm/tasks/check_keyring.yaml', 'tripleo_ansible/roles/tripleo_cephadm/tasks/selinux.yaml']",3,b4352122a563a975ef59ff194b80927a41aefe0a,cix_followup, seuser: system_u,,29,0
openstack%2Fnova~stable%2Fvictoria~I1348cca8cbd8b1142dab8507c8aa1b9baf01e73c,openstack/nova,stable/victoria,I1348cca8cbd8b1142dab8507c8aa1b9baf01e73c,Remove mentions of removed scheduler filters,MERGED,2022-09-16 09:23:55.000000000,2023-07-05 12:22:12.000000000,2023-07-05 12:21:09.000000000,"[{'_account_id': 11604}, {'_account_id': 15197}, {'_account_id': 17685}, {'_account_id': 20733}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-09-16 09:23:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/4a67a40adf72cd16f21c06a593345705f564c45a', 'message': 'Remove mentions of removed scheduler filters\n\nChange-Id: I1348cca8cbd8b1142dab8507c8aa1b9baf01e73c\n(cherry picked from commit 4fb4f6832c156907b786571f214984894703bf16)\n(cherry picked from commit b9d10c45790dfccb645e02db957d1e3015aed306)\n(cherry picked from commit 1fa73cb76d455cfb220823b1483c40e348ab97c7)\n(cherry picked from commit 39d8fffc1caa149157b322c9cdd42b3cfcec14e5)\n'}, {'number': 2, 'created': '2023-06-14 07:41:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/88207057f8d3275ee08c5f8d99182ff9d9c7f60b', 'message': 'Remove mentions of removed scheduler filters\n\nChange-Id: I1348cca8cbd8b1142dab8507c8aa1b9baf01e73c\n(cherry picked from commit 4fb4f6832c156907b786571f214984894703bf16)\n(cherry picked from commit b9d10c45790dfccb645e02db957d1e3015aed306)\n(cherry picked from commit 1fa73cb76d455cfb220823b1483c40e348ab97c7)\n(cherry picked from commit 39d8fffc1caa149157b322c9cdd42b3cfcec14e5)\n'}, {'number': 3, 'created': '2023-06-22 11:54:10.000000000', 'files': ['nova/conf/compute.py', 'doc/source/contributor/development-environment.rst'], 'web_link': 'https://opendev.org/openstack/nova/commit/85365c028b52a6bb27c28094e79bcc4bdba0ba65', 'message': 'Remove mentions of removed scheduler filters\n\nChange-Id: I1348cca8cbd8b1142dab8507c8aa1b9baf01e73c\n(cherry picked from commit 4fb4f6832c156907b786571f214984894703bf16)\n(cherry picked from commit c3489ed5cc21a9fa968949e04f1c7762f09b5606)\n(cherry picked from commit 37129b4b4423ef8ce932ae506f0e9a6ae771ec0c)\n(cherry picked from commit 5f1e83cfbc52fb88dc8f38af5ef96a2d2893cecf)\n(cherry picked from commit 6143aec8a46642f0285c080067b886422b3a9bc0)\n'}]",6,858051,85365c028b52a6bb27c28094e79bcc4bdba0ba65,22,5,3,15334,,,0,"Remove mentions of removed scheduler filters

Change-Id: I1348cca8cbd8b1142dab8507c8aa1b9baf01e73c
(cherry picked from commit 4fb4f6832c156907b786571f214984894703bf16)
(cherry picked from commit c3489ed5cc21a9fa968949e04f1c7762f09b5606)
(cherry picked from commit 37129b4b4423ef8ce932ae506f0e9a6ae771ec0c)
(cherry picked from commit 5f1e83cfbc52fb88dc8f38af5ef96a2d2893cecf)
(cherry picked from commit 6143aec8a46642f0285c080067b886422b3a9bc0)
",git fetch https://review.opendev.org/openstack/nova refs/changes/51/858051/3 && git format-patch -1 --stdout FETCH_HEAD,"['nova/conf/compute.py', 'doc/source/contributor/development-environment.rst']",2,4a67a40adf72cd16f21c06a593345705f564c45a,docs-removed-filters,"that will limit the number of instances per compute, such as ``NumInstancesFilter``.","that will limit the number of instances per compute, such as ``AggregateCoreFilter``.",4,10
openstack%2Freleases~master~I6b5393ff35310d7470ca2d1b714d778c804dbe53,openstack/releases,master,I6b5393ff35310d7470ca2d1b714d778c804dbe53,"Add 2024.1/""cantaloupe"" election events to the 2023.2/bobcat schedule",MERGED,2023-07-03 11:08:30.000000000,2023-07-05 12:17:42.000000000,2023-07-05 12:17:42.000000000,"[{'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-03 11:08:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/1c524d931fd40a3724b128aeff008e0bf5aad64a', 'message': 'Add 2024.1/""cantaloupe"" election events to the 2023.2/bobcat schedule\n\nChange-Id: I6b5393ff35310d7470ca2d1b714d778c804dbe53\n'}, {'number': 2, 'created': '2023-07-04 12:12:41.000000000', 'files': ['doc/source/bobcat/schedule.yaml', 'doc/source/bobcat/schedule.rst'], 'web_link': 'https://opendev.org/openstack/releases/commit/a66455f3a318f71ac566c317c94170c640986c4f', 'message': 'Add 2024.1/""cantaloupe"" election events to the 2023.2/bobcat schedule\n\nChange-Id: I6b5393ff35310d7470ca2d1b714d778c804dbe53\n'}]",3,887508,a66455f3a318f71ac566c317c94170c640986c4f,10,3,2,12898,,,0,"Add 2024.1/""cantaloupe"" election events to the 2023.2/bobcat schedule

Change-Id: I6b5393ff35310d7470ca2d1b714d778c804dbe53
",git fetch https://review.opendev.org/openstack/releases refs/changes/08/887508/1 && git format-patch -1 --stdout FETCH_HEAD,"['doc/source/bobcat/schedule.yaml', 'doc/source/bobcat/schedule.rst']",2,1c524d931fd40a3724b128aeff008e0bf5aad64a,," 2024.1 TC and PTL Elections --------------------------- 2024.1 Election Email Deadline ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Contributors that will be in the electorate for the upcoming election should confirm their gerrit email addresses by this date (August 30th, 2023 at 00:00 UTC). Electorate rolls are generated after this date and ballots will be sent to the listed gerrit email address. .. _c-election-nominations: 2024.1 Election Nomination Begins ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Candidates interested in serving for the next calendar year (TC), or development cycle (PTL) should announce their candidacies and platforms during this week. Please see the `Election site`_ for specific timing information. .. _c-election-campaigning: 2024.1 Election Campaigning Begins ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ The electorate has time to ask candidates questions about their platforms and debate topics before polling begins. Please see the `Election site`_ for specific timing information. .. _c-election-voting: 2024.1 Election Polling Begins ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Election polling for open seats on the TC and any required PTL elections. Please see the `Election site`_ for specific timing information. .. _c-election-close: 2024.1 Election Polling Ends ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ All polls close in the 2024.1 Election and results announced. Please see the `Election site`_ for specific timing information. .. _Election site: https://governance.openstack.org/election/",,50,0
openstack%2Freleases~master~I6247f603688c017664ad806d51419238806eba1a,openstack/releases,master,I6247f603688c017664ad806d51419238806eba1a,Run format-yaml over doc/source/bobcat/schedule.yaml,MERGED,2023-07-03 11:08:30.000000000,2023-07-05 12:17:40.000000000,2023-07-05 12:17:40.000000000,"[{'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-03 11:08:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/51ad70b071268d006cc4ecc26de22d39707dde81', 'message': 'Run format-yaml over doc/source/bobcat/schedule.yaml\n\nChange-Id: I6247f603688c017664ad806d51419238806eba1a\n'}, {'number': 2, 'created': '2023-07-04 12:12:41.000000000', 'files': ['doc/source/bobcat/schedule.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/de4a8bb4102bfaefd164f9dcc7f1c6308c550643', 'message': 'Run format-yaml over doc/source/bobcat/schedule.yaml\n\nChange-Id: I6247f603688c017664ad806d51419238806eba1a\n'}]",1,887507,de4a8bb4102bfaefd164f9dcc7f1c6308c550643,9,3,2,12898,,,0,"Run format-yaml over doc/source/bobcat/schedule.yaml

Change-Id: I6247f603688c017664ad806d51419238806eba1a
",git fetch https://review.opendev.org/openstack/releases refs/changes/07/887507/2 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/bobcat/schedule.yaml'],1,51ad70b071268d006cc4ecc26de22d39707dde81,, - end: '2023-03-24' start: '2023-03-20' x-project: - a-final - end: '2023-03-31' name: R-27 start: '2023-03-27' x-project: - b-vptg - end: '2023-04-07' name: R-26 start: '2023-04-03' - end: '2023-04-14' name: R-25 start: '2023-04-10' - end: '2023-04-21' name: R-24 start: '2023-04-17' - end: '2023-04-28' name: R-23 start: '2023-04-24' - end: '2023-05-05' name: R-22 start: '2023-05-01' - end: '2023-05-12' name: R-21 start: '2023-05-08' x-project: - b-1 project-specific: - b-nova-stable-review-day - end: '2023-05-19' name: R-20 start: '2023-05-15' - end: '2023-05-26' name: R-19 start: '2023-05-22' project-specific: - b-manila-hackathon - end: '2023-06-02' name: R-18 start: '2023-05-29' x-project: - b-cycle-trail project-specific: - b-cinder-mid-cycle-ptg-1 - end: '2023-06-09' name: R-17 start: '2023-06-05' project-specific: - b-nova-spec-review-day - end: '2023-06-16' name: R-16 start: '2023-06-12' x-project: - b-summit - end: '2023-06-23' name: R-15 start: '2023-06-19' project-specific: - b-cinder-spec-freeze - end: '2023-06-30' name: R-14 start: '2023-06-26' project-specific: - b-manila-spec-freeze - b-nova-spec-review-day - end: '2023-07-07' name: R-13 start: '2023-07-03' x-project: - b-2 - b-mf project-specific: - b-cinder-driver-deadline - b-cinder-target-driver-deadline - b-nova-spec-freeze - b-nova-review-day - end: '2023-07-14' name: R-12 start: '2023-07-10' - end: '2023-07-21' name: R-11 start: '2023-07-17' - end: '2023-07-28' name: R-10 start: '2023-07-24' project-specific: - b-nova-review-day - b-manila-new-driver-deadline - end: '2023-08-04' name: R-9 start: '2023-07-31' project-specific: - b-cinder-feature-checkpoint - b-manila-fpfreeze - end: '2023-08-11' name: R-8 start: '2023-08-07' project-specific: - b-cinder-driver-features-declaration - end: '2023-08-18' name: R-7 start: '2023-08-14' x-project: - b-extra-atc-freeze project-specific: - b-oslo-feature-freeze - end: '2023-08-25' name: R-6 start: '2023-08-21' x-project: - b-final-lib - end: '2023-09-01' name: R-5 start: '2023-08-28' x-project: - b-3 - b-ff - b-final-clientlib - b-soft-sf - b-rf - b-cycle-highlights - end: '2023-09-08' name: R-4 start: '2023-09-04' project-specific: - b-cinder-ci-checkpoint - end: '2023-09-15' name: R-3 start: '2023-09-11' x-project: - b-rc1 - b-hard-sf - end: '2023-09-22' name: R-2 start: '2023-09-18' project-specific: - b-manila-bugsquash - end: '2023-09-29' name: R-1 start: '2023-09-25' x-project: - b-finalrc - end: '2023-10-06' name: R+0 start: '2023-10-02' x-project: - b-final,- end: '2023-03-24' start: '2023-03-20' x-project: - a-final - end: '2023-03-31' name: R-27 start: '2023-03-27' x-project: - b-vptg - end: '2023-04-07' name: R-26 start: '2023-04-03' - end: '2023-04-14' name: R-25 start: '2023-04-10' - end: '2023-04-21' name: R-24 start: '2023-04-17' - end: '2023-04-28' name: R-23 start: '2023-04-24' - end: '2023-05-05' name: R-22 start: '2023-05-01' - end: '2023-05-12' name: R-21 start: '2023-05-08' x-project: - b-1 project-specific: - b-nova-stable-review-day - end: '2023-05-19' name: R-20 start: '2023-05-15' - end: '2023-05-26' name: R-19 start: '2023-05-22' project-specific: - b-manila-hackathon - end: '2023-06-02' name: R-18 start: '2023-05-29' x-project: - b-cycle-trail project-specific: - b-cinder-mid-cycle-ptg-1 - end: '2023-06-09' name: R-17 start: '2023-06-05' project-specific: - b-nova-spec-review-day - end: '2023-06-16' name: R-16 start: '2023-06-12' x-project: - b-summit - end: '2023-06-23' name: R-15 start: '2023-06-19' project-specific: - b-cinder-spec-freeze - end: '2023-06-30' name: R-14 start: '2023-06-26' project-specific: - b-manila-spec-freeze - b-nova-spec-review-day - end: '2023-07-07' name: R-13 start: '2023-07-03' x-project: - b-2 - b-mf project-specific: - b-cinder-driver-deadline - b-cinder-target-driver-deadline - b-nova-spec-freeze - b-nova-review-day - end: '2023-07-14' name: R-12 start: '2023-07-10' - end: '2023-07-21' name: R-11 start: '2023-07-17' - end: '2023-07-28' name: R-10 start: '2023-07-24' project-specific: - b-nova-review-day - b-manila-new-driver-deadline - end: '2023-08-04' name: R-9 start: '2023-07-31' project-specific: - b-cinder-feature-checkpoint - b-manila-fpfreeze - end: '2023-08-11' name: R-8 start: '2023-08-07' project-specific: - b-cinder-driver-features-declaration - end: '2023-08-18' name: R-7 start: '2023-08-14' x-project: - b-extra-atc-freeze project-specific: - b-oslo-feature-freeze - end: '2023-08-25' name: R-6 start: '2023-08-21' x-project: - b-final-lib - end: '2023-09-01' name: R-5 start: '2023-08-28' x-project: - b-3 - b-ff - b-final-clientlib - b-soft-sf - b-rf - b-cycle-highlights - end: '2023-09-08' name: R-4 start: '2023-09-04' project-specific: - b-cinder-ci-checkpoint - end: '2023-09-15' name: R-3 start: '2023-09-11' x-project: - b-rc1 - b-hard-sf - end: '2023-09-22' name: R-2 start: '2023-09-18' project-specific: - b-manila-bugsquash - end: '2023-09-29' name: R-1 start: '2023-09-25' x-project: - b-finalrc - end: '2023-10-06' name: R+0 start: '2023-10-02' x-project: - b-final,149,149
openstack%2Freleases~master~Iae5a79e362ea30396c70708a48005b44ec67fd62,openstack/releases,master,Iae5a79e362ea30396c70708a48005b44ec67fd62,Update list_weeks to use the release yamlutils,MERGED,2023-07-03 11:08:30.000000000,2023-07-05 12:14:27.000000000,2023-07-05 12:14:27.000000000,"[{'_account_id': 12898}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-07-03 11:08:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/8d67c30b8329a22c7e979c2a703e06ffc3cfd5e7', 'message': 'Update list_weeks to use the release yamlutils\n\nIt\'s a little more verbose but this is essentially the same as running\nformat-yaml over the generated file.  While this isn\'t ""needed"" it\nisn\'t a functional change and makes it easier to write a tool for the\nelections repo to generate changes like:\n  https://review.opendev.org/q/I954afb4aad4286a8a6641ef8800541a8822a38f2\n\nChange-Id: Iae5a79e362ea30396c70708a48005b44ec67fd62\n'}, {'number': 2, 'created': '2023-07-04 12:12:41.000000000', 'files': ['tools/list_weeks.py'], 'web_link': 'https://opendev.org/openstack/releases/commit/318fcbde221ae49bfa98b2032aca14c88625ba78', 'message': 'Update list_weeks to use the release yamlutils\n\nIt\'s a little more verbose but this is essentially the same as running\nformat-yaml over the generated file.  While this isn\'t ""needed"" it\nisn\'t a functional change and makes it easier to write a tool for the\nelections repo to generate changes like:\n  https://review.opendev.org/q/I954afb4aad4286a8a6641ef8800541a8822a38f2\n\nChange-Id: Iae5a79e362ea30396c70708a48005b44ec67fd62\n'}]",4,887506,318fcbde221ae49bfa98b2032aca14c88625ba78,11,4,2,12898,,,0,"Update list_weeks to use the release yamlutils

It's a little more verbose but this is essentially the same as running
format-yaml over the generated file.  While this isn't ""needed"" it
isn't a functional change and makes it easier to write a tool for the
elections repo to generate changes like:
  https://review.opendev.org/q/I954afb4aad4286a8a6641ef8800541a8822a38f2

Change-Id: Iae5a79e362ea30396c70708a48005b44ec67fd62
",git fetch https://review.opendev.org/openstack/releases refs/changes/06/887506/1 && git format-patch -1 --stdout FETCH_HEAD,['tools/list_weeks.py'],1,8d67c30b8329a22c7e979c2a703e06ffc3cfd5e7,,"from openstack_releases import yamlutils 'start-week': '{:%Y-%m-%d}'.format(weeks[0]), 'release-week': '{:%Y-%m-%d}'.format(next_release_date),print(yamlutils.dumps(data))","import yamlprint('start-week: {:%Y-%m-%d}'.format(weeks[0])) print('release-week: {:%Y-%m-%d}'.format(next_release_date)) print(yaml.dump(data, default_flow_style=False, explicit_start=False))",4,4
openstack%2Freleases~master~Ic1083a959d8ca21ac093fd23784148e87617653e,openstack/releases,master,Ic1083a959d8ca21ac093fd23784148e87617653e,Fix duplicate key in bobcat schedule,MERGED,2023-06-30 06:45:18.000000000,2023-07-05 12:10:29.000000000,2023-07-05 12:10:29.000000000,"[{'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-06-30 06:45:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/03b90f990aa72f0bcbfb53ef6e25cc8bd7d2488b', 'message': 'Fix duplicate key in bobcat schedule\n\nSomehow we have to project-specific keys in the R-10 schedule.\n\nFix that\n\nChange-Id: Ic1083a959d8ca21ac093fd23784148e87617653e\n'}, {'number': 2, 'created': '2023-07-03 11:08:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/eb323859997a3a91736f469decf186c1902bf4fe', 'message': 'Fix duplicate key in bobcat schedule\n\nSomehow we have to project-specific keys in the R-10 schedule.\n\nFix that\n\nChange-Id: Ic1083a959d8ca21ac093fd23784148e87617653e\n'}, {'number': 3, 'created': '2023-07-04 12:12:41.000000000', 'files': ['doc/source/bobcat/schedule.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/e5019ca0a3dab34948df5cf07664714505f91347', 'message': 'Fix duplicate key in bobcat schedule\n\nSomehow we have to project-specific keys in the R-10 schedule.\n\nFix that\n\nChange-Id: Ic1083a959d8ca21ac093fd23784148e87617653e\n'}]",1,887355,e5019ca0a3dab34948df5cf07664714505f91347,11,3,3,12898,,,0,"Fix duplicate key in bobcat schedule

Somehow we have to project-specific keys in the R-10 schedule.

Fix that

Change-Id: Ic1083a959d8ca21ac093fd23784148e87617653e
",git fetch https://review.opendev.org/openstack/releases refs/changes/55/887355/2 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/bobcat/schedule.yaml'],1,03b90f990aa72f0bcbfb53ef6e25cc8bd7d2488b,, - b-nova-review-day, project-specific: - b-nova-review-day,1,3
openstack%2Fcharm-openstack-dashboard~stable%2Ftrain~I3f956ae811cb6c46b5e2ab31f1353678a35e623a,openstack/charm-openstack-dashboard,stable/train,I3f956ae811cb6c46b5e2ab31f1353678a35e623a,Handle JSON data from dashboard plugins,MERGED,2023-05-30 18:38:03.000000000,2023-07-05 11:23:43.000000000,2023-07-05 11:23:43.000000000,"[{'_account_id': 11805}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-05-30 18:38:03.000000000', 'files': ['hooks/horizon_contexts.py', 'unit_tests/test_horizon_contexts.py'], 'web_link': 'https://opendev.org/openstack/charm-openstack-dashboard/commit/997279ad349ddc0ba30f83d6f150a1105a10a750', 'message': 'Handle JSON data from dashboard plugins\n\nThe dashboard-plugin interface sends relation data json encoded but\nthe charm does not decode the local-settings key. This change decodes\nthe data. I have not been able to find any classic plugins that\nrely on sending raw data but to maintain backwards compatability\njust incase the charm will fallback to the old behaviour if the\nrelation data is not json encoded.\n\nChange-Id: I3f956ae811cb6c46b5e2ab31f1353678a35e623a\nCloses-Bug: #1986538\n(cherry picked from commit 123b8447ed40a9c5339bd181ce707d726d09a565)\n'}]",0,884734,997279ad349ddc0ba30f83d6f150a1105a10a750,10,4,1,2424,,,0,"Handle JSON data from dashboard plugins

The dashboard-plugin interface sends relation data json encoded but
the charm does not decode the local-settings key. This change decodes
the data. I have not been able to find any classic plugins that
rely on sending raw data but to maintain backwards compatability
just incase the charm will fallback to the old behaviour if the
relation data is not json encoded.

Change-Id: I3f956ae811cb6c46b5e2ab31f1353678a35e623a
Closes-Bug: #1986538
(cherry picked from commit 123b8447ed40a9c5339bd181ce707d726d09a565)
",git fetch https://review.opendev.org/openstack/charm-openstack-dashboard refs/changes/34/884734/1 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/horizon_contexts.py', 'unit_tests/test_horizon_contexts.py']",2,997279ad349ddc0ba30f83d6f150a1105a10a750,bug/1986538," def test_LocalSettingsContextJSON(self): self.relation_ids.return_value = ['plugin:0', 'plugin-too:0'] self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] # One JSON and one raw relation self.relation_get.side_effect = [{'priority': ""99"", 'local-settings': '""FOO = True""'}, {'priority': 60, 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': ['# horizon-plugin-too/0\n' 'BAR = False', '# horizon-plugin/0\n' 'FOO = True']}) ",,37,1
openstack%2Fcharm-openstack-dashboard~stable%2Fussuri~I3f956ae811cb6c46b5e2ab31f1353678a35e623a,openstack/charm-openstack-dashboard,stable/ussuri,I3f956ae811cb6c46b5e2ab31f1353678a35e623a,Handle JSON data from dashboard plugins,MERGED,2023-05-30 16:34:54.000000000,2023-07-05 11:23:41.000000000,2023-07-05 11:23:41.000000000,"[{'_account_id': 11805}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-05-30 16:34:54.000000000', 'files': ['hooks/horizon_contexts.py', 'unit_tests/test_horizon_contexts.py'], 'web_link': 'https://opendev.org/openstack/charm-openstack-dashboard/commit/d1f310131f9d1a5bb27dcdb529641992745b6a2a', 'message': 'Handle JSON data from dashboard plugins\n\nThe dashboard-plugin interface sends relation data json encoded but\nthe charm does not decode the local-settings key. This change decodes\nthe data. I have not been able to find any classic plugins that\nrely on sending raw data but to maintain backwards compatability\njust incase the charm will fallback to the old behaviour if the\nrelation data is not json encoded.\n\nChange-Id: I3f956ae811cb6c46b5e2ab31f1353678a35e623a\nCloses-Bug: #1986538\n(cherry picked from commit 123b8447ed40a9c5339bd181ce707d726d09a565)\n'}]",0,884732,d1f310131f9d1a5bb27dcdb529641992745b6a2a,10,4,1,2424,,,0,"Handle JSON data from dashboard plugins

The dashboard-plugin interface sends relation data json encoded but
the charm does not decode the local-settings key. This change decodes
the data. I have not been able to find any classic plugins that
rely on sending raw data but to maintain backwards compatability
just incase the charm will fallback to the old behaviour if the
relation data is not json encoded.

Change-Id: I3f956ae811cb6c46b5e2ab31f1353678a35e623a
Closes-Bug: #1986538
(cherry picked from commit 123b8447ed40a9c5339bd181ce707d726d09a565)
",git fetch https://review.opendev.org/openstack/charm-openstack-dashboard refs/changes/32/884732/1 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/horizon_contexts.py', 'unit_tests/test_horizon_contexts.py']",2,d1f310131f9d1a5bb27dcdb529641992745b6a2a,bug/1986538," def test_LocalSettingsContextJSON(self): self.relation_ids.return_value = ['plugin:0', 'plugin-too:0'] self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] # One JSON and one raw relation self.relation_get.side_effect = [{'priority': ""99"", 'local-settings': '""FOO = True""'}, {'priority': 60, 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': ['# horizon-plugin-too/0\n' 'BAR = False', '# horizon-plugin/0\n' 'FOO = True']}) ",,37,1
openstack%2Fcharm-openstack-dashboard~stable%2Fvictoria~I3f956ae811cb6c46b5e2ab31f1353678a35e623a,openstack/charm-openstack-dashboard,stable/victoria,I3f956ae811cb6c46b5e2ab31f1353678a35e623a,Handle JSON data from dashboard plugins,MERGED,2023-05-30 16:34:39.000000000,2023-07-05 11:23:40.000000000,2023-07-05 11:23:40.000000000,"[{'_account_id': 11805}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-05-30 16:34:39.000000000', 'files': ['hooks/horizon_contexts.py', 'unit_tests/test_horizon_contexts.py'], 'web_link': 'https://opendev.org/openstack/charm-openstack-dashboard/commit/46e98662f1acf0efce5ed95008e01ced9caffb9d', 'message': 'Handle JSON data from dashboard plugins\n\nThe dashboard-plugin interface sends relation data json encoded but\nthe charm does not decode the local-settings key. This change decodes\nthe data. I have not been able to find any classic plugins that\nrely on sending raw data but to maintain backwards compatability\njust incase the charm will fallback to the old behaviour if the\nrelation data is not json encoded.\n\nChange-Id: I3f956ae811cb6c46b5e2ab31f1353678a35e623a\nCloses-Bug: #1986538\n(cherry picked from commit 123b8447ed40a9c5339bd181ce707d726d09a565)\n'}]",0,884731,46e98662f1acf0efce5ed95008e01ced9caffb9d,10,4,1,2424,,,0,"Handle JSON data from dashboard plugins

The dashboard-plugin interface sends relation data json encoded but
the charm does not decode the local-settings key. This change decodes
the data. I have not been able to find any classic plugins that
rely on sending raw data but to maintain backwards compatability
just incase the charm will fallback to the old behaviour if the
relation data is not json encoded.

Change-Id: I3f956ae811cb6c46b5e2ab31f1353678a35e623a
Closes-Bug: #1986538
(cherry picked from commit 123b8447ed40a9c5339bd181ce707d726d09a565)
",git fetch https://review.opendev.org/openstack/charm-openstack-dashboard refs/changes/31/884731/1 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/horizon_contexts.py', 'unit_tests/test_horizon_contexts.py']",2,46e98662f1acf0efce5ed95008e01ced9caffb9d,bug/1986538," def test_LocalSettingsContextJSON(self): self.relation_ids.return_value = ['plugin:0', 'plugin-too:0'] self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] # One JSON and one raw relation self.relation_get.side_effect = [{'priority': ""99"", 'local-settings': '""FOO = True""'}, {'priority': 60, 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': ['# horizon-plugin-too/0\n' 'BAR = False', '# horizon-plugin/0\n' 'FOO = True']}) ",,37,1
openstack%2Fironic-python-agent~master~I1a7c01d16eb8d2a2e372bd0d2474a601d3d2c1f4,openstack/ironic-python-agent,master,I1a7c01d16eb8d2a2e372bd0d2474a601d3d2c1f4,Remove netifaces usage,NEW,2023-04-10 15:47:18.000000000,2023-07-05 11:13:06.000000000,,"[{'_account_id': 4571}, {'_account_id': 10239}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-04-10 15:47:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/2eb4b73dfa27be0b72d1c0d70b522bdd5a868d38', 'message': 'WIP: Remove netutils usage\n\nChange-Id: I1a7c01d16eb8d2a2e372bd0d2474a601d3d2c1f4\n'}, {'number': 2, 'created': '2023-04-10 15:47:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/50adec0fc1dbfcdac91dfefb184f156add0158ef', 'message': 'WIP: Remove netifaces usage\n\nChange-Id: I1a7c01d16eb8d2a2e372bd0d2474a601d3d2c1f4\n'}, {'number': 3, 'created': '2023-04-10 18:43:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/d767d0be8400d21dc2480c5b8d922e2a3a7c5eca', 'message': 'WIP: Remove netifaces usage\n\nChange-Id: I1a7c01d16eb8d2a2e372bd0d2474a601d3d2c1f4\n'}, {'number': 4, 'created': '2023-04-17 14:25:04.000000000', 'files': ['ironic_python_agent/tests/unit/hardware_managers/test_mlnx.py', 'releasenotes/notes/remove-netifaces-dependency-63fcc599602820d8.yaml', 'requirements.txt', 'ironic_python_agent/netutils.py', 'ironic_python_agent/tests/unit/test_hardware.py'], 'web_link': 'https://opendev.org/openstack/ironic-python-agent/commit/30b360d5438b85d43356ede831284b33b9dd172b', 'message': 'Remove netifaces usage\n\nThe Ironic project recently learned that the netifaces library was\nno longer being maintained. As the underlying dependency was minimal,\nthe dependency has been removed from the ironic-python-agent.\n\nChange-Id: I1a7c01d16eb8d2a2e372bd0d2474a601d3d2c1f4\n'}]",25,879984,30b360d5438b85d43356ede831284b33b9dd172b,16,3,4,11655,,,0,"Remove netifaces usage

The Ironic project recently learned that the netifaces library was
no longer being maintained. As the underlying dependency was minimal,
the dependency has been removed from the ironic-python-agent.

Change-Id: I1a7c01d16eb8d2a2e372bd0d2474a601d3d2c1f4
",git fetch https://review.opendev.org/openstack/ironic-python-agent refs/changes/84/879984/2 && git format-patch -1 --stdout FETCH_HEAD,['ironic_python_agent/netutils.py'],1,2eb4b73dfa27be0b72d1c0d70b522bdd5a868d38,,"def _extract_address(lines): """"""Extract an usable IP address from the supplied lines. :param lines: The split lines of an ""ip addr show $interface"" command. :returns: The first found global/non-temporary IP address. """""" for line in lines: # Get a global, non-temporary (i.e. v6 privacy) address if 'global' in line and 'temporary' not in line line = line.lstrip() # line is the list of addresses address = line.split('/')[1] # strip tailing / off of the result, as v4 and v6 addresses return address[0] try: out, _ = utils.execute('ip', '-4', 'addr', 'show', interface_id) except OSError: LOG.error('The IP command is required. Could not get IPv4 address.') return None return _extract_address(out.splitlines()) try: out, _ = utils.execute('ip', '-6', 'addr', 'show', interface_id) except OSError: LOG.error('The IP command is required. Could not get IPv4 address.') return None return _extract_address(out.splitlines()) path = '/sys/class/net/{}/address'.format(interface_id) try: with open(path, 'rt') as fp: return fp.read().strip() except OSError as e: LOG.debug('Encountered error while attempting to access the address ' 'for %s. Error: %s', interface_id, e)) LOG.debug('No MAC Address found for interface %s.', interface_id) return None","import netifacesdef get_default_ip_addr(type, interface_id): """"""Retrieve default IPv4 or IPv6 address."""""" try: addrs = netifaces.ifaddresses(interface_id) return addrs[type][0]['addr'] except (ValueError, IndexError, KeyError): # No default IP address found return None return get_default_ip_addr(netifaces.AF_INET, interface_id) return get_default_ip_addr(netifaces.AF_INET6, interface_id) try: addrs = netifaces.ifaddresses(interface_id) return addrs[netifaces.AF_LINK][0]['addr'] except (ValueError, IndexError, KeyError): # No mac address found",34,15
openstack%2Fpython-openstackclient~master~Ib1069ce7a441c1ff10d2dca05095eb6bf53e7fb6,openstack/python-openstackclient,master,Ib1069ce7a441c1ff10d2dca05095eb6bf53e7fb6,Allow multiple `--remove-tag` in `project set`,MERGED,2023-06-08 06:08:38.000000000,2023-07-05 11:09:00.000000000,2023-07-05 11:07:48.000000000,"[{'_account_id': 15334}, {'_account_id': 22348}, {'_account_id': 27900}]","[{'number': 1, 'created': '2023-06-08 06:08:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/c7d92acf3f4e419db7702dfcea2c8a0bc3ce3a20', 'message': ""Fix `project --remove-tag` behaviour\n\nHelp text says 'repeat option to delete multiple tags'. Fix to reflect\nwhat docs says.\n\nAlso add missing test for this argument.\n\nChange-Id: Ib1069ce7a441c1ff10d2dca05095eb6bf53e7fb6\n""}, {'number': 2, 'created': '2023-06-08 08:24:06.000000000', 'files': ['openstackclient/tests/unit/identity/v3/test_project.py', 'openstackclient/identity/v3/tag.py'], 'web_link': 'https://opendev.org/openstack/python-openstackclient/commit/b4ba04698b8a9fa5461dcb9f39656e9753e52215', 'message': ""Allow multiple `--remove-tag` in `project set`\n\nThe help text for this arg says 'repeat option to delete multiple tags'.\nFix to reflect what docs says.\n\nAlso add missing test for this argument.\n\nChange-Id: Ib1069ce7a441c1ff10d2dca05095eb6bf53e7fb6\n""}]",2,885566,b4ba04698b8a9fa5461dcb9f39656e9753e52215,12,3,2,8064,,,0,"Allow multiple `--remove-tag` in `project set`

The help text for this arg says 'repeat option to delete multiple tags'.
Fix to reflect what docs says.

Also add missing test for this argument.

Change-Id: Ib1069ce7a441c1ff10d2dca05095eb6bf53e7fb6
",git fetch https://review.opendev.org/openstack/python-openstackclient refs/changes/66/885566/2 && git format-patch -1 --stdout FETCH_HEAD,"['openstackclient/identity/v3/tag.py', 'openstackclient/tests/unit/identity/v3/test_project.py']",2,c7d92acf3f4e419db7702dfcea2c8a0bc3ce3a20,," attrs={ 'domain_id': domain.id, 'tags': ['tag1', 'tag2', 'tag3'] } # Set expected values. new tag is added to original tags for update. kwargs = {'name': 'qwerty', 'tags': list(set(['tag1', 'tag2', 'tag3', 'foo']))} def test_project_remove_tags(self): arglist = [ '--remove-tag', 'tag1', '--remove-tag', 'tag2', self.project.name, ] verifylist = [ ('enable', False), ('disable', False), ('project', self.project.name), ('remove_tag', ['tag1', 'tag2']), ] parsed_args = self.check_parser(self.cmd, arglist, verifylist) result = self.cmd.take_action(parsed_args) kwargs = {'tags': list(set(['tag3']))} self.projects_mock.update.assert_called_with(self.project.id, **kwargs) self.assertIsNone(result) "," attrs={'domain_id': domain.id} # Set expected values kwargs = {'name': 'qwerty', 'tags': ['foo']}",30,6
openstack%2Fcharm-openstack-dashboard~stable%2Fwallaby~I3f956ae811cb6c46b5e2ab31f1353678a35e623a,openstack/charm-openstack-dashboard,stable/wallaby,I3f956ae811cb6c46b5e2ab31f1353678a35e623a,Handle JSON data from dashboard plugins,MERGED,2023-05-30 16:34:28.000000000,2023-07-05 10:55:43.000000000,2023-07-05 10:55:43.000000000,"[{'_account_id': 11805}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-05-30 16:34:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-openstack-dashboard/commit/db56626f0f548fced3a940da77a808a6549b0c2f', 'message': 'Handle JSON data from dashboard plugins\n\nThe dashboard-plugin interface sends relation data json encoded but\nthe charm does not decode the local-settings key. This change decodes\nthe data. I have not been able to find any classic plugins that\nrely on sending raw data but to maintain backwards compatability\njust incase the charm will fallback to the old behaviour if the\nrelation data is not json encoded.\n\nChange-Id: I3f956ae811cb6c46b5e2ab31f1353678a35e623a\nCloses-Bug: #1986538\n(cherry picked from commit 123b8447ed40a9c5339bd181ce707d726d09a565)\n'}, {'number': 2, 'created': '2023-05-30 20:07:29.000000000', 'files': ['hooks/horizon_contexts.py', 'unit_tests/test_horizon_contexts.py'], 'web_link': 'https://opendev.org/openstack/charm-openstack-dashboard/commit/4f27f0b89396cbf34007b517fac9e9e4e356dc5f', 'message': 'Handle JSON data from dashboard plugins\n\nThe dashboard-plugin interface sends relation data json encoded but\nthe charm does not decode the local-settings key. This change decodes\nthe data. I have not been able to find any classic plugins that\nrely on sending raw data but to maintain backwards compatability\njust incase the charm will fallback to the old behaviour if the\nrelation data is not json encoded.\n\nChange-Id: I3f956ae811cb6c46b5e2ab31f1353678a35e623a\nCloses-Bug: #1986538\n(cherry picked from commit 123b8447ed40a9c5339bd181ce707d726d09a565)\n'}]",0,884730,4f27f0b89396cbf34007b517fac9e9e4e356dc5f,13,4,2,2424,,,0,"Handle JSON data from dashboard plugins

The dashboard-plugin interface sends relation data json encoded but
the charm does not decode the local-settings key. This change decodes
the data. I have not been able to find any classic plugins that
rely on sending raw data but to maintain backwards compatability
just incase the charm will fallback to the old behaviour if the
relation data is not json encoded.

Change-Id: I3f956ae811cb6c46b5e2ab31f1353678a35e623a
Closes-Bug: #1986538
(cherry picked from commit 123b8447ed40a9c5339bd181ce707d726d09a565)
",git fetch https://review.opendev.org/openstack/charm-openstack-dashboard refs/changes/30/884730/2 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/horizon_contexts.py', 'unit_tests/test_horizon_contexts.py']",2,db56626f0f548fced3a940da77a808a6549b0c2f,bug/1986538," def test_LocalSettingsContextJSON(self): self.relation_ids.return_value = ['plugin:0', 'plugin-too:0'] self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] # One JSON and one raw relation self.relation_get.side_effect = [{'priority': ""99"", 'local-settings': '""FOO = True""'}, {'priority': 60, 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': ['# horizon-plugin-too/0\n' 'BAR = False', '# horizon-plugin/0\n' 'FOO = True']}) ",,37,1
openstack%2Fcharm-openstack-dashboard~stable%2Fwallaby~I58f0f8c0506c0146e64d10f5c46dabe379d9a1dc,openstack/charm-openstack-dashboard,stable/wallaby,I58f0f8c0506c0146e64d10f5c46dabe379d9a1dc,Pin tox to < 4.0.0,MERGED,2023-01-13 20:12:34.000000000,2023-07-05 10:54:09.000000000,2023-07-05 10:54:09.000000000,"[{'_account_id': 2424}, {'_account_id': 11805}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-01-13 20:12:34.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/charm-openstack-dashboard/commit/d17ada60878963ebbec7259da19f2e711c67b8f5', 'message': ""Pin tox to < 4.0.0\n\nTox 4.0.0 was recently released and it has several breaking changes.\nWe pin to < 4.0.0 here. We are planning to move forward only on the\nmaster charm branches.\n\nTox is also pinned to < 4.0.0 for stable branches in upstream\nopenstack-zuul-jobs as well as in zosci-config. However, the\nrequires= section in the charm's tox.ini file ends up installing\ntox>4, wiping out the zuul-pinned tox<4 that was already installed\ninstalled. This patch fixes that.\n\nRelated-Bug: #2002788\nChange-Id: I58f0f8c0506c0146e64d10f5c46dabe379d9a1dc\n""}]",3,870269,d17ada60878963ebbec7259da19f2e711c67b8f5,14,5,1,11805,,,0,"Pin tox to < 4.0.0

Tox 4.0.0 was recently released and it has several breaking changes.
We pin to < 4.0.0 here. We are planning to move forward only on the
master charm branches.

Tox is also pinned to < 4.0.0 for stable branches in upstream
openstack-zuul-jobs as well as in zosci-config. However, the
requires= section in the charm's tox.ini file ends up installing
tox>4, wiping out the zuul-pinned tox<4 that was already installed
installed. This patch fixes that.

Related-Bug: #2002788
Change-Id: I58f0f8c0506c0146e64d10f5c46dabe379d9a1dc
",git fetch https://review.opendev.org/openstack/charm-openstack-dashboard refs/changes/69/870269/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,d17ada60878963ebbec7259da19f2e711c67b8f5,pin-tox-wallaby, tox < 4.0.0,,1,0
openstack%2Fcharm-openstack-dashboard~stable%2Fxena~I3f956ae811cb6c46b5e2ab31f1353678a35e623a,openstack/charm-openstack-dashboard,stable/xena,I3f956ae811cb6c46b5e2ab31f1353678a35e623a,Handle JSON data from dashboard plugins,MERGED,2023-05-30 16:34:15.000000000,2023-07-05 10:52:29.000000000,2023-07-05 10:52:29.000000000,"[{'_account_id': 11805}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-05-30 16:34:15.000000000', 'files': ['hooks/horizon_contexts.py', 'unit_tests/test_horizon_contexts.py'], 'web_link': 'https://opendev.org/openstack/charm-openstack-dashboard/commit/d2803bfbc36507ae59c68fbf4f2faa408664f213', 'message': 'Handle JSON data from dashboard plugins\n\nThe dashboard-plugin interface sends relation data json encoded but\nthe charm does not decode the local-settings key. This change decodes\nthe data. I have not been able to find any classic plugins that\nrely on sending raw data but to maintain backwards compatability\njust incase the charm will fallback to the old behaviour if the\nrelation data is not json encoded.\n\nChange-Id: I3f956ae811cb6c46b5e2ab31f1353678a35e623a\nCloses-Bug: #1986538\n(cherry picked from commit 123b8447ed40a9c5339bd181ce707d726d09a565)\n'}]",0,884549,d2803bfbc36507ae59c68fbf4f2faa408664f213,10,4,1,2424,,,0,"Handle JSON data from dashboard plugins

The dashboard-plugin interface sends relation data json encoded but
the charm does not decode the local-settings key. This change decodes
the data. I have not been able to find any classic plugins that
rely on sending raw data but to maintain backwards compatability
just incase the charm will fallback to the old behaviour if the
relation data is not json encoded.

Change-Id: I3f956ae811cb6c46b5e2ab31f1353678a35e623a
Closes-Bug: #1986538
(cherry picked from commit 123b8447ed40a9c5339bd181ce707d726d09a565)
",git fetch https://review.opendev.org/openstack/charm-openstack-dashboard refs/changes/49/884549/1 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/horizon_contexts.py', 'unit_tests/test_horizon_contexts.py']",2,d2803bfbc36507ae59c68fbf4f2faa408664f213,bug/1986538," def test_LocalSettingsContextJSON(self): self.relation_ids.return_value = ['plugin:0', 'plugin-too:0'] self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] # One JSON and one raw relation self.relation_get.side_effect = [{'priority': ""99"", 'local-settings': '""FOO = True""'}, {'priority': 60, 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': ['# horizon-plugin-too/0\n' 'BAR = False', '# horizon-plugin/0\n' 'FOO = True']}) ",,37,1
openstack%2Fneutron-tempest-plugin~master~Id49654aa5ca59eeb0585646d3fd5aa0de22337b5,openstack/neutron-tempest-plugin,master,Id49654aa5ca59eeb0585646d3fd5aa0de22337b5,"Add ""-d 1"" option to the ncat client.",MERGED,2023-07-04 10:15:59.000000000,2023-07-05 10:30:20.000000000,2023-07-05 10:30:20.000000000,"[{'_account_id': 13861}, {'_account_id': 16688}, {'_account_id': 19118}, {'_account_id': 22348}, {'_account_id': 28245}, {'_account_id': 29350}, {'_account_id': 33341}, {'_account_id': 34777}, {'_account_id': 35308}, {'_account_id': 36141}]","[{'number': 1, 'created': '2023-07-04 10:15:59.000000000', 'files': ['neutron_tempest_plugin/scenario/base.py'], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/fddcd186a496b4510ca29952e1529430a2072033', 'message': 'Add ""-d 1"" option to the ncat client.\n\nIn nc run as client, when it is provided by the nmap (nmap-ncat package\nin Centos/RHEL for example) it could happend that client which was\nstarted with input string given through pipe (echo ""test"" | nc ... ) was\nclosed sooner than it received response from the nc server. In such case\nnc client was finished without error (exit code 0) but also without\nprinting any message from server and that causes tests failures.\n\nTo avoid that there is option ""-d 1"" (--delay) added. According to the\nnc man page [1] this option can be used to configure ""Wait between\nread/writes"".\n\n[1] https://man7.org/linux/man-pages/man1/ncat.1.html\n\nChange-Id: Id49654aa5ca59eeb0585646d3fd5aa0de22337b5\n'}]",1,887591,fddcd186a496b4510ca29952e1529430a2072033,10,10,1,11975,,,0,"Add ""-d 1"" option to the ncat client.

In nc run as client, when it is provided by the nmap (nmap-ncat package
in Centos/RHEL for example) it could happend that client which was
started with input string given through pipe (echo ""test"" | nc ... ) was
closed sooner than it received response from the nc server. In such case
nc client was finished without error (exit code 0) but also without
printing any message from server and that causes tests failures.

To avoid that there is option ""-d 1"" (--delay) added. According to the
nc man page [1] this option can be used to configure ""Wait between
read/writes"".

[1] https://man7.org/linux/man-pages/man1/ncat.1.html

Change-Id: Id49654aa5ca59eeb0585646d3fd5aa0de22337b5
",git fetch https://review.opendev.org/openstack/neutron-tempest-plugin refs/changes/91/887591/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron_tempest_plugin/scenario/base.py'],1,fddcd186a496b4510ca29952e1529430a2072033,bz/2219470, ncat_version = get_ncat_version(ssh_client=ssh_client) if ncat_version > packaging_version.Version('7.60'): cmd += '-d 1 ', ncat_version = get_ncat_version(ssh_client=ssh_client),3,1
openstack%2Fetcd3gw~master~Id7fa0a79ac0fef367e8f44b02a15913d19781d1f,openstack/etcd3gw,master,Id7fa0a79ac0fef367e8f44b02a15913d19781d1f,Implement version discovery,NEW,2023-07-05 09:48:33.000000000,2023-07-05 10:00:50.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-05 09:48:33.000000000', 'files': ['etcd3gw/watch.py', 'releasenotes/notes/api-version-discovery-086ba24ff2b19ab7.yaml', 'etcd3gw/exceptions.py', 'etcd3gw/client.py', 'etcd3gw/tests/test_etcd3gw.py'], 'web_link': 'https://opendev.org/openstack/etcd3gw/commit/9db45b34abacc5c139a54b66270623b8f0f0b7d8', 'message': 'Implement version discovery\n\nWhen api_path=/auto/ try to discover etcd api version.\nVersion map contains 3.2 - 3.5 etcd protocol versions.\nVersion is re-discovered on 404 API errors.\n\nChange-Id: Id7fa0a79ac0fef367e8f44b02a15913d19781d1f\n'}]",0,887662,9db45b34abacc5c139a54b66270623b8f0f0b7d8,2,1,1,9542,,,0,"Implement version discovery

When api_path=/auto/ try to discover etcd api version.
Version map contains 3.2 - 3.5 etcd protocol versions.
Version is re-discovered on 404 API errors.

Change-Id: Id7fa0a79ac0fef367e8f44b02a15913d19781d1f
",git fetch https://review.opendev.org/openstack/etcd3gw refs/changes/62/887662/1 && git format-patch -1 --stdout FETCH_HEAD,"['etcd3gw/watch.py', 'releasenotes/notes/api-version-discovery-086ba24ff2b19ab7.yaml', 'etcd3gw/exceptions.py', 'etcd3gw/client.py', 'etcd3gw/tests/test_etcd3gw.py']",5,9db45b34abacc5c139a54b66270623b8f0f0b7d8,api-discover," with mock.patch.object(self.client, 'do_post', return_value=mocked_response) as mock_method: try: res = self.client.watch_once('/some/key', timeout=1) except exceptions.WatchTimedOut: self.fail(""watch timed out when server responded with unicode"") self.assertEqual(res, {'kv': {'key': b'value'}}) mock_method.assert_called_once_with( 'http://localhost:2379/v3alpha/watch', json={'create_request': {'key': 'L3NvbWUva2V5'}}, stream=True)"," try: res = self.client.watch_once('/some/key', timeout=1) except exceptions.WatchTimedOut: self.fail(""watch timed out when server responded with unicode"") self.assertEqual(res, {'kv': {'key': b'value'}})",104,27
openstack%2Fkeystone~master~I66626ba8771ef2aa8b3580fd3f5d15fd4b58ab48,openstack/keystone,master,I66626ba8771ef2aa8b3580fd3f5d15fd4b58ab48,db: Drop redundant index on application_credentials table,NEW,2023-06-07 13:00:59.000000000,2023-07-05 09:58:40.000000000,,"[{'_account_id': 7414}, {'_account_id': 7973}, {'_account_id': 14250}, {'_account_id': 15334}, {'_account_id': 16137}, {'_account_id': 22348}, {'_account_id': 32761}]","[{'number': 1, 'created': '2023-06-07 13:00:59.000000000', 'files': ['keystone/application_credential/backends/sql.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/4c54243a989d68bd624cac2a2cc1c934ed8d0c76', 'message': 'db: Drop redundant index on application_credentials table\n\n * There already exists a unique constraint on the external_id, making\n   an additional index redundant.\n\nCloses-Bug: #1988297\nChange-Id: I66626ba8771ef2aa8b3580fd3f5d15fd4b58ab48\n'}]",10,885463,4c54243a989d68bd624cac2a2cc1c934ed8d0c76,19,7,1,32755,,,0,"db: Drop redundant index on application_credentials table

 * There already exists a unique constraint on the external_id, making
   an additional index redundant.

Closes-Bug: #1988297
Change-Id: I66626ba8771ef2aa8b3580fd3f5d15fd4b58ab48
",git fetch https://review.opendev.org/openstack/keystone refs/changes/63/885463/1 && git format-patch -1 --stdout FETCH_HEAD,['keystone/application_credential/backends/sql.py'],1,4c54243a989d68bd624cac2a2cc1c934ed8d0c76,acidxdrop," external_id = sql.Column(sql.String(64), unique=True)"," external_id = sql.Column(sql.String(64), index=True, unique=True)",1,1
openstack%2Fmagnum~master~Ic086dd147b432c5d1b21061d04f801844e6f5dbf,openstack/magnum,master,Ic086dd147b432c5d1b21061d04f801844e6f5dbf,Add flatcar as os for cluster api driver,NEW,2023-07-04 02:53:51.000000000,2023-07-05 09:44:10.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-04 02:53:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/1fb038f88fc02fe0200a7d2ae3d56819df02a405', 'message': 'Add flatcar as os for cluster api driver\n\n* Use image os_distro to add flatcar option to template\n\nChange-Id: Ic086dd147b432c5d1b21061d04f801844e6f5dbf\n'}, {'number': 2, 'created': '2023-07-04 18:46:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/b37b1a8095c9208a2f985f96678026824a78d1fc', 'message': 'Add flatcar as os for cluster api driver\n\n* Use image os_distro to add flatcar option to template\n\nChange-Id: Ic086dd147b432c5d1b21061d04f801844e6f5dbf\n'}, {'number': 3, 'created': '2023-07-04 19:15:39.000000000', 'files': ['magnum/conf/capi_driver.py', 'devstack/lib/magnum', 'magnum/drivers/cluster_api/driver.py', 'magnum/drivers/cluster_api/helm.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/4ef65f690d894818e5eec400e6debbaf728f6d8c', 'message': 'Add flatcar as os for cluster api driver\n\n* Use image os_distro to add flatcar option to template\n* Using a fork of capi-helm-chart to include ignitionBasedOS configuration\n\nChange-Id: Ic086dd147b432c5d1b21061d04f801844e6f5dbf\n'}]",5,887545,4ef65f690d894818e5eec400e6debbaf728f6d8c,9,1,3,35921,,,0,"Add flatcar as os for cluster api driver

* Use image os_distro to add flatcar option to template
* Using a fork of capi-helm-chart to include ignitionBasedOS configuration

Change-Id: Ic086dd147b432c5d1b21061d04f801844e6f5dbf
",git fetch https://review.opendev.org/openstack/magnum refs/changes/45/887545/1 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/conf/capi_driver.py', 'devstack/lib/magnum', 'magnum/drivers/cluster_api/driver.py', 'magnum/drivers/cluster_api/helm.py']",4,1fb038f88fc02fe0200a7d2ae3d56819df02a405,flatcar-ignition-bootstrap," # ""--repo"", # repo,"," ""--repo"", repo,",26,4
openstack%2Fneutron-tempest-plugin~master~Id00f833d2d24b45b0f52af01ede3be681d22644a,openstack/neutron-tempest-plugin,master,Id00f833d2d24b45b0f52af01ede3be681d22644a,Alternative configuration for ssh user in test,ABANDONED,2023-07-03 20:43:17.000000000,2023-07-05 08:46:58.000000000,,"[{'_account_id': 11975}, {'_account_id': 13861}, {'_account_id': 16688}, {'_account_id': 19118}, {'_account_id': 22348}, {'_account_id': 28245}, {'_account_id': 29088}, {'_account_id': 29350}, {'_account_id': 31291}, {'_account_id': 34777}, {'_account_id': 35308}]","[{'number': 1, 'created': '2023-07-03 20:43:17.000000000', 'files': ['neutron_tempest_plugin/scenario/test_security_groups.py'], 'web_link': 'https://opendev.org/openstack/neutron-tempest-plugin/commit/dff43fe484e80a2f5b9d15b4286340f6857b0bee', 'message': 'Alternative configuration for ssh user in test\n\nSpecific security group logging test changed to rely on ssh username\nfrom different tempest configuration section.\n\nCurrent setting needs to be dropped when tempest and rally used,\notherwise it breaks the rally run.\n\nChange-Id: Id00f833d2d24b45b0f52af01ede3be681d22644a\n'}]",4,887540,dff43fe484e80a2f5b9d15b4286340f6857b0bee,7,11,1,33341,,,0,"Alternative configuration for ssh user in test

Specific security group logging test changed to rely on ssh username
from different tempest configuration section.

Current setting needs to be dropped when tempest and rally used,
otherwise it breaks the rally run.

Change-Id: Id00f833d2d24b45b0f52af01ede3be681d22644a
",git fetch https://review.opendev.org/openstack/neutron-tempest-plugin refs/changes/40/887540/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron_tempest_plugin/scenario/test_security_groups.py'],1,dff43fe484e80a2f5b9d15b4286340f6857b0bee,fix_rally_tempest_fragmented_failure," CONF.validation.image_ssh_user,"," CONF.neutron_plugin_options.advanced_image_ssh_user,",1,1
openstack%2Fcinder~master~I9c37ada5e6af1f3c28ebd5c3c2a8baf2d88d0a96,openstack/cinder,master,I9c37ada5e6af1f3c28ebd5c3c2a8baf2d88d0a96,Hitachi: Fix to use correct pool on secondary storage,NEW,2023-03-17 06:08:24.000000000,2023-07-05 07:00:10.000000000,,"[{'_account_id': 22348}, {'_account_id': 28403}, {'_account_id': 29122}, {'_account_id': 30615}]","[{'number': 1, 'created': '2023-03-17 06:08:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/38d5d9fe5ddce8e33421c85ee69acd9a529519a9', 'message': 'Hitachi: Fix to use correct pool number for secondary storage on GAD\n\nThis patch fixes to use correct pool number for a secondary storage on\nGAD environment[*1].\nEven though the option ``hitachi_mirror_pool`` is to set pool number for\nsecondary storage, the option does not work and a pool number for\nprimary storage would be used on secondary storage.\nThe bug should be fixed and delete the risk to be used unexpected pool\nnumber.\n\n[1] GAD is the storage mirroring product on Hitachi storage. The feature\nwas implied as the patch\n https://review.opendev.org/c/openstack/cinder/+/796170 and was merged\ninto Antelope.\n\nCloses-Bug: #2011810\nChange-Id: I9c37ada5e6af1f3c28ebd5c3c2a8baf2d88d0a96\n'}, {'number': 2, 'created': '2023-03-31 03:15:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2fd6ad04501cb64939f8b7ab35b9124d7da61341', 'message': 'Hitachi: Fix to use correct pool number for secondary storage on GAD\n\nThis patch fixes to use correct pool number for a secondary storage on\nGAD environment[*1].\nEven though the option ``hitachi_mirror_pool`` is to set pool number for\nsecondary storage, the option does not work and a pool number for\nprimary storage would be used on secondary storage.\nThe bug should be fixed and delete the risk to be used unexpected pool\nnumber.\n\n[1] GAD is the storage mirroring product on Hitachi storage. The feature\nwas implied as the patch\n https://review.opendev.org/c/openstack/cinder/+/796170 and was merged\ninto Antelope.\n\nCloses-Bug: #2011810\nChange-Id: I9c37ada5e6af1f3c28ebd5c3c2a8baf2d88d0a96\n'}, {'number': 3, 'created': '2023-05-08 07:32:47.000000000', 'files': ['releasenotes/notes/hitachi-vsp-fix-to-use-correct-pool-in-GAD-9413a343dcc98029.yaml', 'cinder/volume/drivers/hitachi/hbsd_replication.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/2270611507720ba5c05bb6acaf0a5825711ba992', 'message': 'Hitachi: Fix to use correct pool on secondary storage\n\nThis patch fixes to use correct pool number for a secondary storage on\nGAD environment[*1].\nEven though the option ``hitachi_mirror_pool`` is to set pool number for\nsecondary storage, the option does not work and a pool number for\nprimary storage would be used on secondary storage.\nThe bug should be fixed and delete the risk to be used unexpected pool\nnumber.\n\n[1] GAD is the storage mirroring product on Hitachi storage. The feature\nwas implied as the patch\n https://review.opendev.org/c/openstack/cinder/+/796170 and was merged\ninto Antelope.\n\nCloses-Bug: #2011810\nChange-Id: I9c37ada5e6af1f3c28ebd5c3c2a8baf2d88d0a96\n'}]",15,877672,2270611507720ba5c05bb6acaf0a5825711ba992,96,4,3,33473,,,0,"Hitachi: Fix to use correct pool on secondary storage

This patch fixes to use correct pool number for a secondary storage on
GAD environment[*1].
Even though the option ``hitachi_mirror_pool`` is to set pool number for
secondary storage, the option does not work and a pool number for
primary storage would be used on secondary storage.
The bug should be fixed and delete the risk to be used unexpected pool
number.

[1] GAD is the storage mirroring product on Hitachi storage. The feature
was implied as the patch
 https://review.opendev.org/c/openstack/cinder/+/796170 and was merged
into Antelope.

Closes-Bug: #2011810
Change-Id: I9c37ada5e6af1f3c28ebd5c3c2a8baf2d88d0a96
",git fetch https://review.opendev.org/openstack/cinder refs/changes/72/877672/1 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/hitachi-vsp-fix-to-use-correct-pool-in-GAD-9413a343dcc98029.yaml', 'cinder/volume/drivers/hitachi/hbsd_replication.py']",2,38d5d9fe5ddce8e33421c85ee69acd9a529519a9,bug/2011810," if opt.name == 'hitachi_mirror_pool': if conf.safe_get('hitachi_mirror_pool'): name = 'hitachi_pools' value = [getattr(conf, opt.name)] else: raise ValueError() else: value = getattr(conf, opt.name) setattr(conf, name, value)"," setattr(conf, name, getattr(conf, opt.name))",15,1
openstack%2Fcinder~master~I743ccce00d5d27ba07dfbabe01af5f95b857d80b,openstack/cinder,master,I743ccce00d5d27ba07dfbabe01af5f95b857d80b,Tatlin unified driver - rename tatlin_api object,ABANDONED,2023-07-04 08:55:13.000000000,2023-07-05 05:47:44.000000000,,[],"[{'number': 1, 'created': '2023-07-04 08:55:13.000000000', 'files': ['cinder/volume/drivers/yadro/tatlin_iscsi.py', 'cinder/tests/unit/volume/drivers/yadro/test_tatlin_common.py', 'cinder/volume/drivers/yadro/tatlin_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/f5f225fa125f1a6244094c689dffa17cbe42d525', 'message': 'Tatlin unified driver - rename tatlin_api object\n\nIn Tatlin Unified driver client object is incorrectly called\ntatlin_api. TatlinAPI is a different type and does not have\nTatlin*Client methods. Renamed to tatlin_client.\n\nPartially Implements: blueprint yadro-tatlin-unified-refactoring\nChange-Id: I10135877889a17c6ae6274f8be1b992294bf65d3\n\nChange-Id: I743ccce00d5d27ba07dfbabe01af5f95b857d80b\n'}]",0,887587,f5f225fa125f1a6244094c689dffa17cbe42d525,5,0,1,13671,,,0,"Tatlin unified driver - rename tatlin_api object

In Tatlin Unified driver client object is incorrectly called
tatlin_api. TatlinAPI is a different type and does not have
Tatlin*Client methods. Renamed to tatlin_client.

Partially Implements: blueprint yadro-tatlin-unified-refactoring
Change-Id: I10135877889a17c6ae6274f8be1b992294bf65d3

Change-Id: I743ccce00d5d27ba07dfbabe01af5f95b857d80b
",git fetch https://review.opendev.org/openstack/cinder refs/changes/87/887587/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/yadro/tatlin_iscsi.py', 'cinder/tests/unit/volume/drivers/yadro/test_tatlin_common.py', 'cinder/volume/drivers/yadro/tatlin_common.py']",3,f5f225fa125f1a6244094c689dffa17cbe42d525,rename_tatlin_api," self.tatlin_client = self._get_tatlin_client() self.tatlin_client.get_resource_count(self.pool_id) LOG.debug('Current pool %(pool)s has %(pool_res)s resourses. ' if pool_res_count >= self._max_pool_resource_count: message = _( 'Create volume failed. Too many resources per pool: %s' ) % pool_res_count if cluster_res_count >= self.MAX_ALLOWED_RESOURCES: message = _( 'Create volume failed. Too many resources per cluster: %s' ) % cluster_res_count self.tatlin_client.create_volume(volume.name_id, name, size, self.pool_id, lbaFormat=self._lba_format) if self.tatlin_client.is_volume_ready(volume.name_id): if (self.tatlin_client.get_volume_status(volume.name_id) == 'online'): if not self.tatlin_client.is_volume_exists(volume.name_id): self.tatlin_client.delete_volume(volume.name_id) if not self.tatlin_client.is_volume_exists(volume.name_id): if self.tatlin_client.is_volume_exists(volume.name_id): self.tatlin_client.extend_volume(volume.name_id, size) self.tatlin_client.update_qos( pool_stat = self.tatlin_client.get_pool_detail(self.pool_id) sys_stat = self.tatlin_client.get_sys_statistic() self.tatlin_client.get_resource_count(self.pool_id) result = self.tatlin_client.get_volume_info(source_name) result = self.tatlin_client.get_volume_info(source_name) self.tatlin_client.add_vol_to_host(volume.name_id, host_id) self.tatlin_client.remove_vol_from_host(volume.name_id, host_id) cur_ports = self.tatlin_client.get_resource_ports_array(volume_id) result = self.tatlin_client.get_resource_mapping() self.tatlin_client.export_volume(volume.name_id, ports) self._pool_id = self.tatlin_client.get_pool_id_by_name("," self.tatlin_api = self._get_tatlin_client() self.tatlin_api.get_resource_count(self.pool_id) LOG.debug('Current pool %(pool)s has %(pool_res)s res.' if pool_res_count > 255: message = _('TatlinVolumeDriver create volume failed. ' 'Too many resources per pool created') if cluster_res_count + 1 > self.MAX_ALLOWED_RESOURCES: message = _('TatlinVolumeDriver create volume failed. ' 'Too many resources per cluster created') self.tatlin_api.create_volume(volume.name_id, name, size, self.pool_id, lbaFormat=self._lba_format) if self.tatlin_api.is_volume_ready(volume.name_id): if self.tatlin_api.get_volume_status(volume.name_id) == 'online': if not self.tatlin_api.is_volume_exists(volume.name_id): self.tatlin_api.delete_volume(volume.name_id) if not self.tatlin_api.is_volume_exists(volume.name_id): if self.tatlin_api.is_volume_exists(volume.name_id): self.tatlin_api.extend_volume(volume.name_id, size) self.tatlin_api.update_qos( pool_stat = self.tatlin_api.get_pool_detail(self.pool_id) sys_stat = self.tatlin_api.get_sys_statistic() self.tatlin_api.get_resource_count(self.pool_id) result = self.tatlin_api.get_volume_info(source_name) result = self.tatlin_api.get_volume_info(source_name) self.tatlin_api.add_vol_to_host(volume.name_id, host_id) self.tatlin_api.remove_vol_from_host(volume.name_id, host_id) cur_ports = self.tatlin_api.get_resource_ports_array(volume_id) result = self.tatlin_api.get_resource_mapping() self.tatlin_api.export_volume(volume.name_id, ports) self._pool_id = self.tatlin_api.get_pool_id_by_name(",49,46
openstack%2Fcinder~master~I2bfba5207fa6bf785d0b660e35a0f2355db7058d,openstack/cinder,master,I2bfba5207fa6bf785d0b660e35a0f2355db7058d,SC: Add unit test for mismatched case WWNs,NEW,2023-06-29 14:43:26.000000000,2023-07-05 05:47:30.000000000,,"[{'_account_id': 5997}, {'_account_id': 9236}, {'_account_id': 11904}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 14:43:26.000000000', 'files': ['cinder/tests/unit/volume/drivers/dell_emc/sc/test_scapi.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/4675aa9c6b17d48cd41b54961001085a1a738486', 'message': 'SC: Add unit test for mismatched case WWNs\n\nThis adds a unit test for the fix made in I6a767e9a.\n\nRelated-Bug: #2016840\nChange-Id: I2bfba5207fa6bf785d0b660e35a0f2355db7058d\n'}]",2,887290,4675aa9c6b17d48cd41b54961001085a1a738486,23,4,1,4523,,,0,"SC: Add unit test for mismatched case WWNs

This adds a unit test for the fix made in I6a767e9a.

Related-Bug: #2016840
Change-Id: I2bfba5207fa6bf785d0b660e35a0f2355db7058d
",git fetch https://review.opendev.org/openstack/cinder refs/changes/90/887290/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/tests/unit/volume/drivers/dell_emc/sc/test_scapi.py'],1,4675aa9c6b17d48cd41b54961001085a1a738486,DSM_WWN_string_changes," WWNS_UC = [u'21000024FF30441C', u'21000024FF30441D'] '_find_controller_port', return_value=FC_CTRLR_PORT) @mock.patch.object(storagecenter_api.SCApi, '_find_mappings', return_value=FC_MAPPINGS) @mock.patch.object(storagecenter_api.SCApi, '_find_initiators', return_value=WWNS_UC) def test_find_wwns_case_mismatch(self, mock_find_initiators, mock_find_mappings, mock_find_controller_port, mock_close_connection, mock_open_connection, mock_init): lun, wwns, itmap = self.scapi.find_wwns(self.VOLUME, self.SCSERVER) self.assertTrue(mock_find_initiators.called) self.assertTrue(mock_find_mappings.called) self.assertTrue(mock_find_controller_port.called) # The _find_controller_port is Mocked, so all mapping pairs # will have the same WWN for the ScControllerPort itmapCompare = {u'21000024ff30441c': [u'5000d31000fcbe36'], u'21000024ff30441d': [u'5000d31000fcbe36', u'5000d31000fcbe36']} self.assertEqual(1, lun, 'Incorrect LUN') self.assertIsNotNone(wwns, 'WWNs is None') self.assertEqual(itmapCompare, itmap, 'WWN mapping incorrect') @mock.patch.object(storagecenter_api.SCApi,",,34,0
openstack%2Fcinder~master~Ia906eb70e64d05fc5202f797b32a2195104cfedd,openstack/cinder,master,Ia906eb70e64d05fc5202f797b32a2195104cfedd,Tatlin unified driver - rename tatlin_api object,ABANDONED,2023-07-04 08:53:29.000000000,2023-07-05 04:56:06.000000000,,[],"[{'number': 1, 'created': '2023-07-04 08:53:29.000000000', 'files': ['cinder/volume/drivers/yadro/tatlin_iscsi.py', 'cinder/tests/unit/volume/drivers/yadro/test_tatlin_common.py', 'cinder/volume/drivers/yadro/tatlin_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/1e17546c3f73e9ad06161a159a9547c5a24f7906', 'message': 'Tatlin unified driver - rename tatlin_api object\n\nIn Tatlin Unified driver client object is incorrectly called\ntatlin_api. TatlinAPI is a different type and does not have\nTatlin*Client methods. Renamed to tatlin_client.\n\nPartially Implements: blueprint yadro-tatlin-unified-refactoring\nChange-Id: I10135877889a17c6ae6274f8be1b992294bf65d3\n\nChange-Id: Ia906eb70e64d05fc5202f797b32a2195104cfedd\n'}]",0,887586,1e17546c3f73e9ad06161a159a9547c5a24f7906,4,0,1,13671,,,0,"Tatlin unified driver - rename tatlin_api object

In Tatlin Unified driver client object is incorrectly called
tatlin_api. TatlinAPI is a different type and does not have
Tatlin*Client methods. Renamed to tatlin_client.

Partially Implements: blueprint yadro-tatlin-unified-refactoring
Change-Id: I10135877889a17c6ae6274f8be1b992294bf65d3

Change-Id: Ia906eb70e64d05fc5202f797b32a2195104cfedd
",git fetch https://review.opendev.org/openstack/cinder refs/changes/86/887586/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/yadro/tatlin_iscsi.py', 'cinder/tests/unit/volume/drivers/yadro/test_tatlin_common.py', 'cinder/volume/drivers/yadro/tatlin_common.py']",3,1e17546c3f73e9ad06161a159a9547c5a24f7906,," self.tatlin_client = self._get_tatlin_client() self.tatlin_client.get_resource_count(self.pool_id) LOG.debug('Current pool %(pool)s has %(pool_res)s resourses. ' if pool_res_count >= self._max_pool_resource_count: message = _( 'Create volume failed. Too many resources per pool: %s' ) % pool_res_count if cluster_res_count >= self.MAX_ALLOWED_RESOURCES: message = _( 'Create volume failed. Too many resources per cluster: %s' ) % cluster_res_count self.tatlin_client.create_volume(volume.name_id, name, size, self.pool_id, lbaFormat=self._lba_format) if self.tatlin_client.is_volume_ready(volume.name_id): if (self.tatlin_client.get_volume_status(volume.name_id) == 'online'): if not self.tatlin_client.is_volume_exists(volume.name_id): self.tatlin_client.delete_volume(volume.name_id) if not self.tatlin_client.is_volume_exists(volume.name_id): if self.tatlin_client.is_volume_exists(volume.name_id): self.tatlin_client.extend_volume(volume.name_id, size) self.tatlin_client.update_qos( pool_stat = self.tatlin_client.get_pool_detail(self.pool_id) sys_stat = self.tatlin_client.get_sys_statistic() self.tatlin_client.get_resource_count(self.pool_id) result = self.tatlin_client.get_volume_info(source_name) result = self.tatlin_client.get_volume_info(source_name) self.tatlin_client.add_vol_to_host(volume.name_id, host_id) self.tatlin_client.remove_vol_from_host(volume.name_id, host_id) cur_ports = self.tatlin_client.get_resource_ports_array(volume_id) result = self.tatlin_client.get_resource_mapping() self.tatlin_client.export_volume(volume.name_id, ports) self._pool_id = self.tatlin_client.get_pool_id_by_name("," self.tatlin_api = self._get_tatlin_client() self.tatlin_api.get_resource_count(self.pool_id) LOG.debug('Current pool %(pool)s has %(pool_res)s res.' if pool_res_count > 255: message = _('TatlinVolumeDriver create volume failed. ' 'Too many resources per pool created') if cluster_res_count + 1 > self.MAX_ALLOWED_RESOURCES: message = _('TatlinVolumeDriver create volume failed. ' 'Too many resources per cluster created') self.tatlin_api.create_volume(volume.name_id, name, size, self.pool_id, lbaFormat=self._lba_format) if self.tatlin_api.is_volume_ready(volume.name_id): if self.tatlin_api.get_volume_status(volume.name_id) == 'online': if not self.tatlin_api.is_volume_exists(volume.name_id): self.tatlin_api.delete_volume(volume.name_id) if not self.tatlin_api.is_volume_exists(volume.name_id): if self.tatlin_api.is_volume_exists(volume.name_id): self.tatlin_api.extend_volume(volume.name_id, size) self.tatlin_api.update_qos( pool_stat = self.tatlin_api.get_pool_detail(self.pool_id) sys_stat = self.tatlin_api.get_sys_statistic() self.tatlin_api.get_resource_count(self.pool_id) result = self.tatlin_api.get_volume_info(source_name) result = self.tatlin_api.get_volume_info(source_name) self.tatlin_api.add_vol_to_host(volume.name_id, host_id) self.tatlin_api.remove_vol_from_host(volume.name_id, host_id) cur_ports = self.tatlin_api.get_resource_ports_array(volume_id) result = self.tatlin_api.get_resource_mapping() self.tatlin_api.export_volume(volume.name_id, ports) self._pool_id = self.tatlin_api.get_pool_id_by_name(",49,46
openstack%2Fcinder~master~I8070aa4829ae55efd648325558be50a0b9a10f8c,openstack/cinder,master,I8070aa4829ae55efd648325558be50a0b9a10f8c,Fix NFS cloned volume,NEW,2023-01-27 21:44:59.000000000,2023-07-05 04:20:14.000000000,,"[{'_account_id': 4523}, {'_account_id': 20813}, {'_account_id': 22348}, {'_account_id': 30615}, {'_account_id': 32425}, {'_account_id': 35193}]","[{'number': 1, 'created': '2023-01-27 21:44:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/593fba43707306c987d265bb5a0229ef01195c97', 'message': 'WIP Create clone from luks inside qcow2 volume\n\nChange-Id: I8070aa4829ae55efd648325558be50a0b9a10f8c\n'}, {'number': 2, 'created': '2023-01-31 16:08:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/55f43b314b01790ccb4c13344af7489bcf94ed0f', 'message': 'WIP Create clone from luks inside qcow2 volume\n\nChange-Id: I8070aa4829ae55efd648325558be50a0b9a10f8c\n'}, {'number': 3, 'created': '2023-01-31 18:26:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c25f2a3315a0184d184552275212ca08935867cb', 'message': 'WIP Create clone from luks inside qcow2 volume\n\nChange-Id: I8070aa4829ae55efd648325558be50a0b9a10f8c\n'}, {'number': 4, 'created': '2023-02-03 15:47:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/666f10f085b972afc0e3c9ae1375156380d43536', 'message': 'WIP Create clone from luks inside qcow2 volume\n\nChange-Id: I8070aa4829ae55efd648325558be50a0b9a10f8c\n'}, {'number': 5, 'created': '2023-02-07 13:59:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/03e53dab84ae58661cd0d8df2869c0f484fc140f', 'message': 'WIP Create clone from luks inside qcow2 volume\n\nChange-Id: I8070aa4829ae55efd648325558be50a0b9a10f8c\n'}, {'number': 6, 'created': '2023-02-09 14:50:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/3f2783f392d09fa3ca4a278e5a2e439ccb4debd4', 'message': 'WIP Create clone from luks inside qcow2 volume\n\nChange-Id: I8070aa4829ae55efd648325558be50a0b9a10f8c\n'}, {'number': 7, 'created': '2023-02-13 13:20:38.000000000', 'files': ['cinder/tests/unit/volume/drivers/test_nfs.py', 'cinder/volume/drivers/nfs.py', 'releasenotes/notes/bug-2007139-9e6b6d69143f7546.yaml', 'cinder/volume/drivers/remotefs.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/58c91c5ca50a39f87db60141e769580abbcffdea', 'message': 'Fix NFS cloned volume\n\nThis patch adds code to the _copy_volume_from_snapshot method to enable\ncloning of encrypted NFS volumes. Currently, when attempting to clone a\nnfs volume, a snapshot of the volume is created and then converted into\na volume.\n\nIn particular, when using encryption, qemu requires the backingfile with\nthe encryption key in order to convert the snapshot into an independent\nvolume.\n\nCloses-Bug: #2007139\nChange-Id: I8070aa4829ae55efd648325558be50a0b9a10f8c\n'}]",16,872052,58c91c5ca50a39f87db60141e769580abbcffdea,148,6,7,20813,,,0,"Fix NFS cloned volume

This patch adds code to the _copy_volume_from_snapshot method to enable
cloning of encrypted NFS volumes. Currently, when attempting to clone a
nfs volume, a snapshot of the volume is created and then converted into
a volume.

In particular, when using encryption, qemu requires the backingfile with
the encryption key in order to convert the snapshot into an independent
volume.

Closes-Bug: #2007139
Change-Id: I8070aa4829ae55efd648325558be50a0b9a10f8c
",git fetch https://review.opendev.org/openstack/cinder refs/changes/52/872052/5 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/image/image_utils.py', 'cinder/volume/drivers/nfs.py', 'cinder/volume/drivers/remotefs.py']",3,593fba43707306c987d265bb5a0229ef01195c97,bp/nfs-volume-encryption," if not getattr(self.configuration, self.driver_prefix + '_qcow2_volumes', False): msg = _('Encryption requires qcow2 volumes.') raise exception.VolumeDriverException(message=msg) ",,64,27
openstack%2Fcinder~master~I19be8ffd1bd2f012cc08df2c2241e97cdbb5a4d2,openstack/cinder,master,I19be8ffd1bd2f012cc08df2c2241e97cdbb5a4d2,mypy: cinder/api/v3/volume_metadata.py,NEW,2023-03-24 14:18:49.000000000,2023-07-05 04:13:07.000000000,,"[{'_account_id': 597}, {'_account_id': 22348}, {'_account_id': 30615}]","[{'number': 1, 'created': '2023-03-24 14:18:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/cc80ff14209dc09655e7fa5bd2e795b90abae9ec', 'message': 'mypy: cinder/api/v3/volume_metadata.py\n\nCover volume_metadata.py and some files it\nimports.\n\nChange-Id: I19be8ffd1bd2f012cc08df2c2241e97cdbb5a4d2\n'}, {'number': 2, 'created': '2023-04-12 16:01:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/36dccf60c54268e82ff641e00b2fb195e514ffe5', 'message': 'mypy: cinder/api/v3/volume_metadata.py\n\nCover volume_metadata.py and some files it\nimports.\n\nChange-Id: I19be8ffd1bd2f012cc08df2c2241e97cdbb5a4d2\n'}, {'number': 3, 'created': '2023-04-13 14:36:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/ec10ce2bad68eab4eb6df96a7928957517b9f326', 'message': 'mypy: cinder/api/v3/volume_metadata.py\n\nCover volume_metadata.py and some files it\nimports.\n\nChange-Id: I19be8ffd1bd2f012cc08df2c2241e97cdbb5a4d2\n'}, {'number': 4, 'created': '2023-04-13 19:53:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/76b08a70c5943021ab88ea37fde4650825a7e1b2', 'message': 'mypy: cinder/api/v3/volume_metadata.py\n\nCover volume_metadata.py and some files it\nimports.\n\nChange-Id: I19be8ffd1bd2f012cc08df2c2241e97cdbb5a4d2\n'}, {'number': 5, 'created': '2023-05-04 14:37:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1e198df9c201ae82b23403c8040a431034e7134c', 'message': 'mypy: cinder/api/v3/volume_metadata.py\n\nCover volume_metadata.py and some files it\nimports.\n\nChange-Id: I19be8ffd1bd2f012cc08df2c2241e97cdbb5a4d2\n'}, {'number': 6, 'created': '2023-06-05 14:31:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/fd41cfe521bf57d003df91e3f4c9dbef513d8371', 'message': 'mypy: cinder/api/v3/volume_metadata.py\n\nCover volume_metadata.py and some files it\nimports.\n\nChange-Id: I19be8ffd1bd2f012cc08df2c2241e97cdbb5a4d2\n'}, {'number': 7, 'created': '2023-06-06 12:14:09.000000000', 'files': ['cinder/api/v3/volume_metadata.py', 'cinder/api/openstack/api_version_request.py', 'mypy-files.txt', 'cinder/api/microversions.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/62cfb210d5df2efd83e9fcdb98bf8aa631af30cd', 'message': 'mypy: cinder/api/v3/volume_metadata.py\n\nCover volume_metadata.py and some files it\nimports.\n\nChange-Id: I19be8ffd1bd2f012cc08df2c2241e97cdbb5a4d2\n'}]",6,878534,62cfb210d5df2efd83e9fcdb98bf8aa631af30cd,105,3,7,4523,,,0,"mypy: cinder/api/v3/volume_metadata.py

Cover volume_metadata.py and some files it
imports.

Change-Id: I19be8ffd1bd2f012cc08df2c2241e97cdbb5a4d2
",git fetch https://review.opendev.org/openstack/cinder refs/changes/34/878534/7 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/api/v3/volume_metadata.py', 'cinder/api/openstack/api_version_request.py', 'mypy-files.txt', 'cinder/api/microversions.py']",4,cc80ff14209dc09655e7fa5bd2e795b90abae9ec,,"from typing import Any def get_mv_header(version: str) -> dict[str, Any]: :return: A dict containing the microversion header with thedef get_api_version(version: str) -> api_version.APIVersionRequest:def get_prior_version(version: str) -> str:",def get_mv_header(version): :return: A tuple containing the microversion header with thedef get_api_version(version):def get_prior_version(version):,37,18
openstack%2Fcinder~master~Ia10bd1cc13170c22d89f78124c714c2278353f0e,openstack/cinder,master,Ia10bd1cc13170c22d89f78124c714c2278353f0e,target/spdknvmf: Add trtype configuration parameter,NEW,2022-07-07 12:20:50.000000000,2023-07-05 04:12:38.000000000,,"[{'_account_id': 13425}, {'_account_id': 22348}, {'_account_id': 30615}, {'_account_id': 35075}]","[{'number': 1, 'created': '2022-07-07 12:20:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/3e630acf4ed1888f8052f0a7cec4223e6972126d', 'message': 'target/spdknvmf: Add trtype configuration parameter\n\nAdds a choice of rdma or tcp as type of created\ntransport in SPDK.\n\nChange-Id: Ia10bd1cc13170c22d89f78124c714c2278353f0e\nSigned-off-by: Krzysztof Karas <krzysztof.karas@intel.com>\n'}, {'number': 2, 'created': '2022-07-11 10:31:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/3e822d459afdf64bc56af71a2407a36f770063a7', 'message': 'target/spdknvmf: Add trtype configuration parameter\n\nAdds a choice of rdma or tcp as type of created\ntransport in SPDK.\n\nChange-Id: Ia10bd1cc13170c22d89f78124c714c2278353f0e\nSigned-off-by: Krzysztof Karas <krzysztof.karas@intel.com>\n'}, {'number': 3, 'created': '2022-07-11 13:19:15.000000000', 'files': ['cinder/volume/targets/spdknvmf.py', 'releasenotes/notes/spdk-add-config-parameter-b828194feae74aef.yaml'], 'web_link': 'https://opendev.org/openstack/cinder/commit/034c65c3cc6b61eb625d01c5cf9944e8de1cd0d5', 'message': 'target/spdknvmf: Add trtype configuration parameter\n\nAdds a choice of rdma or tcp as type of created\ntransport in SPDK.\n\nChange-Id: Ia10bd1cc13170c22d89f78124c714c2278353f0e\nSigned-off-by: Krzysztof Karas <krzysztof.karas@intel.com>\n'}]",8,848429,034c65c3cc6b61eb625d01c5cf9944e8de1cd0d5,48,4,3,34596,,,0,"target/spdknvmf: Add trtype configuration parameter

Adds a choice of rdma or tcp as type of created
transport in SPDK.

Change-Id: Ia10bd1cc13170c22d89f78124c714c2278353f0e
Signed-off-by: Krzysztof Karas <krzysztof.karas@intel.com>
",git fetch https://review.opendev.org/openstack/cinder refs/changes/29/848429/2 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/targets/spdknvmf.py', 'releasenotes/notes/spdk-add-config-parameter-b828194feae74aef.yaml']",2,3e630acf4ed1888f8052f0a7cec4223e6972126d,,upgrade: - | Added a new configuration option spdk_rpc_trtype for SPDK NVMe-oF target. It lets user choose transport type between 'rdma' and 'tcp'. ,,10,1
openstack%2Fcinder~master~I790ee022227674acf308c40a7595fd0c2a1175ae,openstack/cinder,master,I790ee022227674acf308c40a7595fd0c2a1175ae,Add tox testenv for no-paramiko tests,NEW,2023-04-12 16:42:57.000000000,2023-07-05 04:06:12.000000000,,"[{'_account_id': 22348}, {'_account_id': 30615}]","[{'number': 1, 'created': '2023-04-12 16:42:57.000000000', 'files': ['no-paramiko-excludes.txt', 'tools/no-paramiko.sh', 'tox.ini'], 'web_link': 'https://opendev.org/openstack/cinder/commit/d07b4e05c8580e128bb92101e2fa7eacbb594b7a', 'message': 'Add tox testenv for no-paramiko tests\n\nRun unit tests without paramiko present, as would happen in a FIPS\ncompliant cinder installation.  (The tests that explicitly use\nparamiko are excluded.)\n\nChange-Id: I790ee022227674acf308c40a7595fd0c2a1175ae\n'}]",0,880191,d07b4e05c8580e128bb92101e2fa7eacbb594b7a,23,2,1,5314,,,0,"Add tox testenv for no-paramiko tests

Run unit tests without paramiko present, as would happen in a FIPS
compliant cinder installation.  (The tests that explicitly use
paramiko are excluded.)

Change-Id: I790ee022227674acf308c40a7595fd0c2a1175ae
",git fetch https://review.opendev.org/openstack/cinder refs/changes/91/880191/1 && git format-patch -1 --stdout FETCH_HEAD,"['no-paramiko-excludes.txt', 'tools/no-paramiko.sh', 'tox.ini']",3,d07b4e05c8580e128bb92101e2fa7eacbb594b7a,,"[testenv:no-paramiko] # 'stestr run' always does discovery unless you specify --no-discover (which # takes a single test as an argument) or --load-list. Thus we need to generate # the list of tests, and this has to be done before un-installing paramiko, # which is why this testenv is complicated. install_command = {[testenv:py3]install_command} allowlist_externals = {toxinidir}/tools/no-paramiko.sh commands_pre = {toxinidir}/tools/no-paramiko.sh {env_python} {env_tmp_dir} commands = python -m pip uninstall -y paramiko stestr run --load-list {env_tmp_dir}/all-tests.txt --exclude-list ./no-paramiko-excludes.txt --random {posargs} commands_post = python -m pip install -c{env:TOX_CONSTRAINTS_FILE:https://releases.openstack.org/constraints/upper/master} paramiko ",,40,0
openstack%2Fcinder-tempest-plugin~master~I8e539f58cc7c8b2a1d2c3036520cad9092a48638,openstack/cinder-tempest-plugin,master,I8e539f58cc7c8b2a1d2c3036520cad9092a48638,Add test for Ceph backup snapshot cleanup option,NEW,2022-09-05 12:17:30.000000000,2023-07-05 04:05:56.000000000,,"[{'_account_id': 4523}, {'_account_id': 5314}, {'_account_id': 9535}, {'_account_id': 22348}, {'_account_id': 27615}, {'_account_id': 30615}, {'_account_id': 33634}]","[{'number': 1, 'created': '2022-09-05 12:17:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/a78ce07791869f44cb73c9de63a4c5da121f660a', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 2, 'created': '2022-09-05 12:29:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/ebce6ecebc50f4982eac2c75061293217d5927a6', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 3, 'created': '2022-09-05 15:11:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/3cb4ea2d0153bb44692db96cfa5e25f7568e1ae4', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 4, 'created': '2022-10-04 08:11:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/55cff877d3ec1fe0a0bef0568c9c4ef97743d127', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 5, 'created': '2022-10-04 08:26:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/c0062743564504884fa6fa26063aa232909c6b25', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 6, 'created': '2022-10-04 08:55:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/13e0595269d342b0054d992fcc2b41e3ae68c143', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 7, 'created': '2022-10-04 14:58:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/dfffcd5819968d896b3728b1d91500b09765c8ee', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 8, 'created': '2022-10-04 15:30:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/9d20d2757a21fa2f3371e248f3848599500beddb', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 9, 'created': '2022-10-04 15:50:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/52a10b59d1bd9b43211cd77e117f6cc9687c4c14', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 10, 'created': '2022-10-04 16:10:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/9b5a3ca2e42f5df798a06d403cd1c63723c9d927', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 11, 'created': '2022-10-06 10:39:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/6e3ab1bc9f20b54626d0af3343093fa32630c8c0', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 12, 'created': '2022-10-06 13:25:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/54d020e27c69792c657426da15acf1a44ffa554e', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 13, 'created': '2022-10-07 12:56:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/eba6302166eff5e525c18faa78f27248309e9302', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 14, 'created': '2022-10-10 08:48:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/604d180f4ace7709fa53d201ceb9606f6c2a158f', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nDepends-On: https://review.opendev.org/c/openstack/devstack/+/860812\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 15, 'created': '2022-10-10 10:02:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/99d23b9859ed1e82b1e89829124adb0bbbce0882', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nDepends-On: https://review.opendev.org/c/openstack/devstack/+/860812\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 16, 'created': '2022-10-10 11:21:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/f26f95408fc034d2a84da86093e7d0170e07e9a6', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nDepends-On: https://review.opendev.org/c/openstack/devstack/+/860812\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 17, 'created': '2022-10-10 13:02:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/4af9c89907248b92307fd63474cfa8eb3edd6c53', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nDepends-On: https://review.opendev.org/c/openstack/devstack/+/860812\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 18, 'created': '2022-10-10 14:24:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/1dd5b11bc079625f7e5b0f537db43dd477f73c3d', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 19, 'created': '2022-10-10 15:49:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/bafc0d0906bb0cd6d82b5384ad8bd01b01620b20', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nDepends-On: https://review.opendev.org/c/openstack/devstack/+/860812\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 20, 'created': '2022-10-11 07:33:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/8ace92b3f73c80570e34fce1c0516f380b4ab86e', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nDepends-On: https://review.opendev.org/c/openstack/devstack/+/860812\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}, {'number': 21, 'created': '2023-04-12 13:26:21.000000000', 'files': ['cinder_tempest_plugin/scenario/test_ceph_backup_keep_snapshots_option.py', 'cinder_tempest_plugin/config.py', '.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/f5893ef1ddea6844cb273b2694365317ff336025', 'message': 'Add test for Ceph backup snapshot cleanup option\n\nThis adds a new test for the Ceph backup snapshot cleanup\noption. It tests a special situation in which a requested\nincremental backup should become a full backup instead.\n\nDepends-On: https://review.opendev.org/c/openstack/cinder/+/810457\nDepends-On: https://review.opendev.org/c/openstack/devstack/+/860812\nSigned-off-by: Jan Hartkopf <jhartkopf@inovex.de>\nChange-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638\n'}]",17,855876,f5893ef1ddea6844cb273b2694365317ff336025,42,7,21,33634,,,0,"Add test for Ceph backup snapshot cleanup option

This adds a new test for the Ceph backup snapshot cleanup
option. It tests a special situation in which a requested
incremental backup should become a full backup instead.

Depends-On: https://review.opendev.org/c/openstack/cinder/+/810457
Depends-On: https://review.opendev.org/c/openstack/devstack/+/860812
Signed-off-by: Jan Hartkopf <jhartkopf@inovex.de>
Change-Id: I8e539f58cc7c8b2a1d2c3036520cad9092a48638
",git fetch https://review.opendev.org/openstack/cinder-tempest-plugin refs/changes/76/855876/18 && git format-patch -1 --stdout FETCH_HEAD,['cinder_tempest_plugin/scenario/test_ceph_backup_keep_snapshots_option.py'],1,a78ce07791869f44cb73c9de63a4c5da121f660a,ceph_keep_snapshots,"# Copyright (c) 2022 inovex GmbH # All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from tempest.common import waiters from tempest import config from tempest.lib.common.utils import data_utils from tempest.lib import decorators from cinder_tempest_plugin.api.volume import base CONF = config.CONF class VolumesBackupsTest(base.BaseVolumeTest): @classmethod def skip_checks(cls): super(VolumesBackupsTest, cls).skip_checks() @decorators.idempotent_id('10c43668-808e-48b6-bd37-92531f59183b') def test_backup_snapshot_cleanup_incremental_forces_full_backup(self): """"""Test incremental backup forces full backup after manually deleting backups."""""" # Assume backup_ceph_keep_snapshots_count is set to 1 # Create volume volume = self.create_volume() # Create base backup self.create_backup(volume_id=volume['id']) # Create 2 incremental backup snapshots snapshots = [] for _ in range(2): backup_incr = self.create_backup( volume_id=volume['id'], incremental=True) snapshots.append(backup_incr) # Snapshots are returned in reverse order of creation time listed_snaps = self.backups_client.list_backups(detail=True) self.assertEqual(listed_snaps['backups'][0]['id'], snapshots[1]['id']) self.assertEqual(listed_snaps['backups'][1]['id'], snapshots[0]['id']) self.assertTrue(listed_snaps['backups'][0]['is_incremental']) self.assertFalse(listed_snaps['backups'][0]['has_dependent_backups']) # Manually delete latest backup snapshot; this should force the next incremental # backup to become a full backup. # (At least backup_ceph_keep_snapshots_count backup snapshots must be # deleted to achieve this situation, which is 1 in our case, so 1 # deletion is sufficient.) self.backups_client.delete_backup(snapshots[1]['id']) self.backups_client.wait_for_resource_deletion(snapshots[1]['id']) # Try to create incremental backup. As there are no base backups available # anymore (one deleted by us, one by Ceph driver), this should # automatically become a full backup. backup_incr = self.create_backup( volume_id=volume['id'], incremental=True) listed_snaps = self.backups_client.list_backups(detail=True) self.assertEqual(listed_snaps['backups'][0]['id'], backup_incr['id']) # Assert that, even though incremental backup was requested, # a full backup was created self.assertFalse(listed_snaps['backups'][0]['is_incremental']) ",,78,0
openstack%2Fnova~master~I89f74af4b4dda6b926e348e257273ed107af43a4,openstack/nova,master,I89f74af4b4dda6b926e348e257273ed107af43a4,docs: fix 404 for ironic docs,NEW,2023-07-04 20:43:39.000000000,2023-07-05 04:04:30.000000000,,"[{'_account_id': 8556}, {'_account_id': 11583}, {'_account_id': 22348}, {'_account_id': 34860}]","[{'number': 1, 'created': '2023-07-04 20:43:39.000000000', 'files': ['doc/source/admin/configuration/hypervisor-ironic.rst'], 'web_link': 'https://opendev.org/openstack/nova/commit/c9c3931120efd8951b853676eddf56ca1e775df2', 'message': 'docs: fix 404 for ironic docs\n\nThe Ironic documentation currently seems to be giving 404s at the\nmoment since the URLs contain /latest/ and the ironic-doc line seems\nto add them automatically, ending up with /latest/latest/\n\nThis can be observed at the moment[0] but will likely not be visible\nbreakage once this change merges.\n\n[0]: https://docs.openstack.org/nova/latest/admin/configuration/hypervisor-ironic.html\n\nChange-Id: I89f74af4b4dda6b926e348e257273ed107af43a4\n'}]",2,887643,c9c3931120efd8951b853676eddf56ca1e775df2,6,4,1,1004,,,0,"docs: fix 404 for ironic docs

The Ironic documentation currently seems to be giving 404s at the
moment since the URLs contain /latest/ and the ironic-doc line seems
to add them automatically, ending up with /latest/latest/

This can be observed at the moment[0] but will likely not be visible
breakage once this change merges.

[0]: https://docs.openstack.org/nova/latest/admin/configuration/hypervisor-ironic.html

Change-Id: I89f74af4b4dda6b926e348e257273ed107af43a4
",git fetch https://review.opendev.org/openstack/nova refs/changes/43/887643/1 && git format-patch -1 --stdout FETCH_HEAD,['doc/source/admin/configuration/hypervisor-ironic.rst'],1,c9c3931120efd8951b853676eddf56ca1e775df2,,<install/configure-nova-flavors.html>`).<install/configure-glance-images.html>`). <install/configure-compute.html>`. <install/configure-nova-flavors.html>`. - :ironic-doc:`Conductors Groups <admin/conductor-groups.html>`.,</install/configure-nova-flavors.html>`).</latest/install/configure-glance-images.html>`). </latest/install/configure-compute.html>`. </latest/install/configure-nova-flavors.html>`. - :ironic-doc:`Conductors Groups </admin/conductor-groups.html>`.,5,5
openstack%2Fcinder~master~Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c,openstack/cinder,master,Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c,Query internal project/user id from Keystone,NEW,2021-06-21 09:01:26.000000000,2023-07-05 04:02:26.000000000,,"[{'_account_id': 22348}, {'_account_id': 30615}]","[{'number': 1, 'created': '2021-06-21 09:01:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2e8caa02115b5d8e2149ba451892513b316276be', 'message': 'WIP: Query internal tenant/user id from Keystone\n\nChange-Id: Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c\n'}, {'number': 2, 'created': '2021-06-21 09:07:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a94035af748efb85f12a4bdddedad42171f544a0', 'message': 'WIP: Query internal tenant/user id from Keystone\n\nChange-Id: Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c\n'}, {'number': 3, 'created': '2021-06-21 09:30:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b6d06d8d5d854b8e70c2ea275d35d272f7340091', 'message': 'WIP: Query internal tenant/user id from Keystone\n\nChange-Id: Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c\n'}, {'number': 4, 'created': '2021-06-28 14:49:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/36ba8f98c50934c4a911da34c00a48f1be1e52f5', 'message': 'Query internal project/user id from Keystone\n\nCinder has provided cinder_internal_teant_project/user_id to define\nits internal project and user, but these are not predictable thus\nusers always need to set up keystone and create an internal project\nand user to define these parameters.\n\nThis change introduces the new internal_tenant section in cinder.conf\nwhich accepts keystoneauth parameters. With these parameters, users\nare now allowed to define internal project and user by names and cinder\nautomatically looks up ids from Keystone.\nNote that this change also lets cinder to ensure that its internal\nproject and user actually exists in Keystone.\n\nCloses-Bug: #1933052\nChange-Id: Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c\n'}, {'number': 5, 'created': '2021-06-28 14:53:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/e25d6943e546dcca7b07958aaa6d4fadb69340b6', 'message': 'Query internal project/user id from Keystone\n\nCinder has provided cinder_internal_teant_project/user_id to define\nits internal project and user, but these are not predictable thus\nusers always need to set up keystone and create an internal project\nand user to define these parameters.\n\nThis change introduces the new internal_tenant section in cinder.conf\nwhich accepts keystoneauth parameters. With these parameters, users\nare now allowed to define internal project and user by names and cinder\nautomatically looks up ids from Keystone.\nNote that this change also lets cinder to ensure that its internal\nproject and user actually exists in Keystone.\n\nCloses-Bug: #1933052\nChange-Id: Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c\n'}, {'number': 6, 'created': '2021-06-29 01:57:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/3e676ab55fd0c354d7166313b79ad5fe076985bb', 'message': 'Query internal project/user id from Keystone\n\nCinder has provided cinder_internal_teant_project/user_id to define\nits internal project and user, but these are not predictable thus\nusers always need to set up keystone and create an internal project\nand user to define these parameters.\n\nThis change introduces the new internal_tenant section in cinder.conf\nwhich accepts keystoneauth parameters. With these parameters, users\nare now allowed to define internal project and user by names and cinder\nautomatically looks up ids from Keystone.\nNote that this change also lets cinder to ensure that its internal\nproject and user actually exists in Keystone.\n\nCloses-Bug: #1933052\nChange-Id: Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c\n'}, {'number': 7, 'created': '2021-06-29 02:06:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/04ad70e0d35fe14cc33562ee1c89c56e61c6ebad', 'message': 'Query internal project/user id from Keystone\n\nCinder has provided cinder_internal_teant_project/user_id to define\nits internal project and user, but these are not predictable thus\nusers always need to set up keystone and create an internal project\nand user to define these parameters.\n\nThis change introduces the new internal_tenant section in cinder.conf\nwhich accepts keystoneauth parameters. With these parameters, users\nare now allowed to define internal project and user by names and cinder\nautomatically looks up ids from Keystone.\nNote that this change also lets cinder to ensure that its internal\nproject and user actually exists in Keystone.\n\nCloses-Bug: #1933052\nChange-Id: Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c\n'}, {'number': 8, 'created': '2021-06-29 02:49:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/4f89bc0c9e26bbb3a46935daba008d99a2dd6e29', 'message': 'Query internal project/user id from Keystone\n\nCinder has provided cinder_internal_teant_project/user_id to define\nits internal project and user, but these are not predictable thus\nusers always need to set up keystone and create an internal project\nand user to define these parameters.\n\nThis change introduces the new internal_tenant section in cinder.conf\nwhich accepts keystoneauth parameters. With these parameters, users\nare now allowed to define internal project and user by names and cinder\nautomatically looks up ids from Keystone.\nNote that this change also lets cinder to ensure that its internal\nproject and user actually exists in Keystone.\n\nCloses-Bug: #1933052\nChange-Id: Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c\n'}, {'number': 9, 'created': '2021-06-29 05:24:27.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c483633fbd76120fdc6676e8945a3fc99ea08fd4', 'message': 'Query internal project/user id from Keystone\n\nCinder has provided cinder_internal_teant_project/user_id to define\nits internal project and user, but these are not predictable thus\nusers always need to set up keystone and create an internal project\nand user to define these parameters.\n\nThis change introduces the new internal_tenant section in cinder.conf\nwhich accepts keystoneauth parameters. With these parameters, users\nare now allowed to define internal project and user by names and cinder\nautomatically looks up ids from Keystone.\nNote that this change also lets cinder to ensure that its internal\nproject and user actually exists in Keystone.\n\nCloses-Bug: #1933052\nChange-Id: Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c\n'}, {'number': 10, 'created': '2021-06-30 00:35:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/62ccabb9915f67eee38eab6d310e254897e18c0a', 'message': 'Query internal project/user id from Keystone\n\nCinder has provided cinder_internal_teant_project/user_id to define\nits internal project and user, but these are not predictable thus\nusers always need to set up keystone and create an internal project\nand user to define these parameters.\n\nThis change introduces the new internal_tenant section in cinder.conf\nwhich accepts keystoneauth parameters. With these parameters, users\nare now allowed to define internal project and user by names and cinder\nautomatically looks up ids from Keystone.\nNote that this change also lets cinder to ensure that its internal\nproject and user actually exists in Keystone.\n\nCloses-Bug: #1933052\nChange-Id: Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c\n'}, {'number': 11, 'created': '2022-03-23 12:03:48.000000000', 'files': ['cinder/opts.py', 'cinder/context.py', 'releasenotes/notes/define-internal-tenant-by-names-b3aefe7f67e363ab.yaml', 'cinder/tests/unit/test_context.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/9b755937bcfa607ca85aa90f7b2462e72f1c5c00', 'message': 'Query internal project/user id from Keystone\n\nCinder has provided cinder_internal_teant_project/user_id to define\nits internal project and user, but these are not predictable thus\nusers always need to set up keystone and create an internal project\nand user to define these parameters.\n\nThis change introduces the new internal_tenant section in cinder.conf\nwhich accepts keystoneauth parameters. With these parameters, users\nare now allowed to define internal project and user by names and cinder\nautomatically looks up ids from Keystone.\nNote that this change also lets cinder to ensure that its internal\nproject and user actually exists in Keystone.\n\nCloses-Bug: #1933052\nChange-Id: Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c\n'}]",8,797245,9b755937bcfa607ca85aa90f7b2462e72f1c5c00,127,2,11,9816,,,0,"Query internal project/user id from Keystone

Cinder has provided cinder_internal_teant_project/user_id to define
its internal project and user, but these are not predictable thus
users always need to set up keystone and create an internal project
and user to define these parameters.

This change introduces the new internal_tenant section in cinder.conf
which accepts keystoneauth parameters. With these parameters, users
are now allowed to define internal project and user by names and cinder
automatically looks up ids from Keystone.
Note that this change also lets cinder to ensure that its internal
project and user actually exists in Keystone.

Closes-Bug: #1933052
Change-Id: Ibd62f3176c84e90e1fb3999d9d68681763aa0a5c
",git fetch https://review.opendev.org/openstack/cinder refs/changes/45/797245/2 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/opts.py', 'cinder/context.py']",2,2e8caa02115b5d8e2149ba451892513b316276be,bug/1933052,"from keystoneauth1 import loading as ks_loading deprecated_for_removal=True, deprecated_reason='Internal tenant should be defined by ' 'the parameters in [internal_tenant] ' 'secsion instead.', deprecated_for_removal=True, deprecated_reason='Internal tenant should be defined by ' 'the parameters in [internal_tenant] ' 'secsion instead.',INTERNAL_TENANT_GROUP = 'internal_tenant' internal_tenant_session_opts = ks_loading.get_session_conf_options() internal_tenant_auth_opts = ks_loading.get_auth_common_conf_options() CONF.register_opts(internal_tenant_session_opts, group=INTERNAL_TENANT_GROUP) CONF.register_opts(internal_tenant_auth_opts, group=INTERNAL_TENANT_GROUP) This request context is used for internal Cinder operations like image cache. # Check the deprecated parameters first try: # TODO: It'd be better to cache project/user id here internal_auth = ks_loading.load_auth_from_conf_options( CONF, INTERNAL_TENANT_GROUP) internal_session = ks_loading.load_session_from_conf_options( CONF, INTERNAL_TENANT_GROUP, auth=internal_auth) project_id = session.get_project_id() user_id = session.get_user_id() if project_id and user_id: return RequestContext(user_id=user_id, project_id=project_id, is_admin=True, overwrite=False) else: # If the session doesn't includ either project_id and user_id, # then it is likely that the required parameters are missing LOG.warning('Unable to get internal tenant context: Missing ' 'required config parameters.') return None except Exception: LOG.exception('Unable to get internal tenant context')", This request context will only work for internal Cinder operations. It will not be able to make requests to remote services. To do so it will need to use the keystone client to get an auth_token. else: LOG.warning('Unable to get internal tenant context: Missing ' 'required config parameters.'),48,7
openstack%2Fcinder~master~I51ee7d6d07561157ee764f73520c9f6f7c25eca0,openstack/cinder,master,I51ee7d6d07561157ee764f73520c9f6f7c25eca0,Time comparison filter didn't support time zone offset,NEW,2020-06-09 04:14:04.000000000,2023-07-05 03:54:15.000000000,,"[{'_account_id': 1736}, {'_account_id': 4523}, {'_account_id': 8846}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 11611}, {'_account_id': 11904}, {'_account_id': 12016}, {'_account_id': 12017}, {'_account_id': 12032}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 14384}, {'_account_id': 15386}, {'_account_id': 15670}, {'_account_id': 15941}, {'_account_id': 18120}, {'_account_id': 18883}, {'_account_id': 19933}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 22126}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 22510}, {'_account_id': 23613}, {'_account_id': 24814}, {'_account_id': 24815}, {'_account_id': 24921}, {'_account_id': 25243}, {'_account_id': 25678}, {'_account_id': 26077}, {'_account_id': 26537}, {'_account_id': 28801}, {'_account_id': 29705}, {'_account_id': 30615}, {'_account_id': 30688}, {'_account_id': 31254}, {'_account_id': 35679}]","[{'number': 1, 'created': '2020-06-09 04:14:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/27b49aab58d15e15c247f4a5314030fca94b755e', 'message': ""time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 2, 'created': '2020-06-09 08:20:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b86ec9631d392252dfd17564718813afc1d8725c', 'message': ""time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 3, 'created': '2020-06-12 07:02:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/3a0e713635772f531fb60cced419b6f4c3b3475d', 'message': ""time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 4, 'created': '2020-06-12 07:14:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/57e21ea30cf7fac7a8ad08f030d8073e20658e69', 'message': ""time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 5, 'created': '2020-06-12 10:22:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/68931bc4a0bc18dd0279663a3852a7ded4de7255', 'message': ""time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 6, 'created': '2020-07-01 06:15:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a298b9f53378e65769ee97a3cc48b5625f5b5927', 'message': ""time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 7, 'created': '2020-07-06 07:07:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6ef653e696b13b8d4c63bc8d376c14760883411b', 'message': ""time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 8, 'created': '2020-07-06 07:20:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/bded5114793806c5436cbca8e42aa74dd46127b3', 'message': ""time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 9, 'created': '2020-07-08 09:36:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/de00f1bf9bf885d50c88c1e75ed88ec6149fa1c2', 'message': ""time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 10, 'created': '2020-07-09 03:35:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c900f7b0e1a9a36be14aeca1ba35b6fa4f4a74f8', 'message': ""time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 11, 'created': '2020-07-09 07:05:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9418d717080c6dbaad7ffad735731d909dd390cf', 'message': ""time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 12, 'created': '2020-07-09 09:40:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/81bd4b3f0c833085536b1278db666db010f2f2e7', 'message': ""Time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 13, 'created': '2020-07-10 01:13:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/7f4ac89f261eaa195d679e1217d45a72620ca469', 'message': ""Time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 14, 'created': '2021-06-30 01:14:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9e6dbab7ee7dbb300b59966cb76e672716fc9668', 'message': ""Time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 15, 'created': '2023-02-03 03:10:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/fb78572fa5f0759d323f0c6ed871168d785c19bf', 'message': ""Time comparison filter didn't support time zone offset\n\nIn the U cycle, Cinder support to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 16, 'created': '2023-02-22 01:31:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c0b601d87f86c1ba1656f2cf3613248d197a4308', 'message': ""Time comparison filter didn't support time zone offset\n\nCinder has supported to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 17, 'created': '2023-03-20 02:11:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a9a193d3f7dd41003f85cad0424591deabaadd8c', 'message': ""Time comparison filter didn't support time zone offset\n\nCinder has supported to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 18, 'created': '2023-03-20 03:24:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/3fdd43e2f17335775a3cd2154d1ba4d29e9b9ea9', 'message': ""Time comparison filter didn't support time zone offset\n\nCinder has supported to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}, {'number': 19, 'created': '2023-06-19 16:51:20.000000000', 'files': ['cinder/tests/unit/api/v3/test_volumes.py', 'cinder/api/v3/volumes.py', 'test-requirements.txt', 'releasenotes/notes/support-time-zone-offset-in-time-comparison-filter-a3f58078d129036f.yaml'], 'web_link': 'https://opendev.org/openstack/cinder/commit/52d63d6965d54dd7109eae91ad8ce462c66c854e', 'message': ""Time comparison filter didn't support time zone offset\n\nCinder has supported to filter volumes by\ntime comparison operators. The time format is following\nthe ISO 8601. But there is small bug that didn't consider\nthe time zone offset well in time string.\nThat will impact the result of filtering volumes.\n\nChange-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0\nCloses-bug: #1882622\n""}]",30,734415,52d63d6965d54dd7109eae91ad8ce462c66c854e,458,39,19,8846,,,0,"Time comparison filter didn't support time zone offset

Cinder has supported to filter volumes by
time comparison operators. The time format is following
the ISO 8601. But there is small bug that didn't consider
the time zone offset well in time string.
That will impact the result of filtering volumes.

Change-Id: I51ee7d6d07561157ee764f73520c9f6f7c25eca0
Closes-bug: #1882622
",git fetch https://review.opendev.org/openstack/cinder refs/changes/15/734415/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/unit/api/v3/test_volumes.py', 'cinder/api/v3/volumes.py', 'cinder/db/sqlalchemy/api.py']",3,27b49aab58d15e15c247f4a5314030fca94b755e,bug/1882622, return and_(*conditions), return or_(*conditions),16,3
openstack%2Fcinder~master~I36da4f9b7afca755ae0524f2c5a3af810ce4052a,openstack/cinder,master,I36da4f9b7afca755ae0524f2c5a3af810ce4052a,Support to query snapshot filter by updated_at/created_at,NEW,2020-06-03 07:50:59.000000000,2023-07-05 03:53:39.000000000,,"[{'_account_id': 4523}, {'_account_id': 8846}, {'_account_id': 9008}, {'_account_id': 10118}, {'_account_id': 11611}, {'_account_id': 12016}, {'_account_id': 12032}, {'_account_id': 12033}, {'_account_id': 12369}, {'_account_id': 12988}, {'_account_id': 14384}, {'_account_id': 15386}, {'_account_id': 15670}, {'_account_id': 15941}, {'_account_id': 18120}, {'_account_id': 18883}, {'_account_id': 21863}, {'_account_id': 21884}, {'_account_id': 22248}, {'_account_id': 22348}, {'_account_id': 23613}, {'_account_id': 24236}, {'_account_id': 24921}, {'_account_id': 25243}, {'_account_id': 25678}, {'_account_id': 26077}, {'_account_id': 26537}, {'_account_id': 28801}, {'_account_id': 29705}, {'_account_id': 30615}, {'_account_id': 31254}, {'_account_id': 31868}, {'_account_id': 32071}]","[{'number': 1, 'created': '2020-06-03 07:50:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/179a98bc7c946267c559048389f396b27decbf02', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 2, 'created': '2020-06-04 07:06:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2e784b18d7ebf8ad6ae8b0ac65c986308c45ef15', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 3, 'created': '2020-06-04 09:33:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/530e63b4efee39317afa1e1cf891945906886356', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 4, 'created': '2020-06-05 03:06:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/074007bf2a6ba8aa1d4e3586b3306ee99274dfb7', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 5, 'created': '2020-06-05 04:40:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/14c205885c6a3b802eab3af3759689e7a9527d4d', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 6, 'created': '2020-06-05 10:05:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/500e5c13822faba403aa02a65ea5d0378ada159d', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 7, 'created': '2020-06-06 02:45:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/146b5532c34024d5841076be166ea555907a309b', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 8, 'created': '2020-06-08 03:32:07.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/f40c0b3aa6e60be7c23a96f05ff1a86fa6f17442', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 9, 'created': '2020-06-08 06:53:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/13edc9baca2e4e9b80975990f481d893baa1617f', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 10, 'created': '2020-06-08 07:19:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/225ffd848d4c27d53e54595d9267abd7db422162', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 11, 'created': '2020-06-08 09:53:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/cf632fed4879a3bff04d490b0729c7e936ec4e92', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 12, 'created': '2020-06-08 09:58:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/ab5f4f332a3565ff0c3fb1ecf0fac4aef98289f7', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 13, 'created': '2020-06-09 01:44:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/5981a21bf550c1e6816c239b3d17421b9b1aa18a', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 14, 'created': '2020-08-31 01:59:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a27468fe26a3957793a2e896e48bf412bbb3844f', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 15, 'created': '2020-10-13 02:08:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/31f9761f0b11028581ee1b607040fa678608b0db', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 16, 'created': '2020-10-23 05:49:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/36dee65226acabbec5d6667fbefe9d9cc0c07b25', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 17, 'created': '2020-12-21 03:17:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/38ad2611d312b20a1102a5908360ee2293555d3e', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 18, 'created': '2021-02-04 07:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1965e24c3225e8a35775317a8dbf15d31553ca5d', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 19, 'created': '2021-03-15 00:58:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/4981194999e525f581531888ac5069f7951e53ad', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 20, 'created': '2021-06-30 01:57:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/84e081e9b12ed8e9e1bf964ddc8cc573090f5e69', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 21, 'created': '2021-07-28 02:56:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/be9dc065081ad4e911f1ba097fa14bafc3d8bced', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 22, 'created': '2021-09-07 07:43:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a4c9246b7e5d28c4cec6583c5ecb6649a6c2ae70', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 23, 'created': '2021-09-09 05:54:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/198bd93f87a632df501e548118ebacb756af35b3', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 24, 'created': '2021-11-23 06:07:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a9cb9f134e54911fb029ccb503ee1f4df89b0235', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 25, 'created': '2021-11-23 06:14:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/6ffb09cb13dffda47d5ca4f65e6627c650c2a7be', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 26, 'created': '2021-12-01 03:07:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b2350dbf24ed35c869c71f0954ada89d79cf5684', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 27, 'created': '2021-12-01 03:09:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b9d1a1c92f7203f2978a80c55706b99d0bb6e2f3', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 28, 'created': '2021-12-02 01:24:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/c56646220c7d9ae7a499b056eb1ebc9d26dad692', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 29, 'created': '2022-03-08 03:28:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2cddd357ffcaacbafa48eceea33deddf238bb3a4', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 30, 'created': '2022-06-07 07:44:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/824d263bcd4dfcc997965b8232b3d13dd838c2dc', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 31, 'created': '2022-08-01 06:46:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/764179af4189e3d1dfc8de40ab945239b08c0d30', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 32, 'created': '2022-08-04 01:13:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/d4fa9b5269fcff3af665bbee4957e007b701832c', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 33, 'created': '2022-08-08 01:26:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9ba354b3d637f817063dd9dd6e295c49eda76c17', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nPartial-Implements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 34, 'created': '2022-08-15 06:35:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/da2758f9d91a2e6a63a4e26f422d8cd55d7c5bdb', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nImplements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 35, 'created': '2022-09-01 06:34:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/f4bc6783d816893f47ac5e8d1c4c5ce80be58da5', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nImplements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 36, 'created': '2022-09-29 01:29:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b761ffe6034c038e1645d18b3e8f8a8011d20746', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nImplements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 37, 'created': '2023-02-02 08:38:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/0e05d137292519c0a745775426fbd768759551fe', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query resources by specifying the time\nthat resources are created at or updated at with time\ncomparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resource in U cycle, now\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nImplements: blueprint support-to-query-cinder-resources-filter-by-time-comparison-operators\n'}, {'number': 38, 'created': '2023-03-09 01:47:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/f3600c108eee02c012cb83516b15c4f4505703c2', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query snapshot resources by specifying\nthe time that snapshots are created at or updated at with\ntime comparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resources,\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nImplements: blueprint support-to-query-snapshot-filter-by-updated-at-created-at\n'}, {'number': 39, 'created': '2023-03-20 02:51:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/ea4509dc975a14dc23e86b832d864afc754aa9d7', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query snapshot resources by specifying\nthe time that snapshots are created at or updated at with\ntime comparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resources,\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nImplements: blueprint support-to-query-snapshot-filter-by-updated-at-created-at\n'}, {'number': 40, 'created': '2023-06-19 16:50:21.000000000', 'files': ['releasenotes/notes/support-to-query-snapshots-by-time-comparison-filters-cd2eab06dd6d6f70.yaml', 'api-ref/source/v3/samples/versions/versions-response.json', 'cinder/tests/unit/api/v3/test_volumes.py', 'cinder/api/v3/volumes.py', 'cinder/api/common.py', 'cinder/api/v3/snapshots.py', 'cinder/api/openstack/api_version_request.py', 'etc/cinder/resource_filters.json', 'api-ref/source/v3/volumes-v3-snapshots.inc', 'cinder/api/openstack/rest_api_version_history.rst', 'cinder/db/sqlalchemy/api.py', 'cinder/tests/unit/api/v3/test_snapshots.py', 'api-ref/source/v3/samples/versions/version-show-response.json', 'cinder/api/microversions.py', 'cinder/tests/unit/api/test_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/0fea52d40dd17a60df74faad23fae3fa26c5dc3b', 'message': 'Support to query snapshot filter by updated_at/created_at\n\nSupport users can query snapshot resources by specifying\nthe time that snapshots are created at or updated at with\ntime comparison operators: ""gt/gte/eq/neq/lt/lte"",\nand cinder will return all which match the condition.\n\nCinder has supported volume resources,\nit\'s time for other resources.\n\nAdd two filters updated_at and created_at in querying API.\n""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""\n\nChange-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a\nImplements: blueprint support-to-query-snapshot-filter-by-updated-at-created-at\n'}]",26,733062,0fea52d40dd17a60df74faad23fae3fa26c5dc3b,813,33,40,8846,,,0,"Support to query snapshot filter by updated_at/created_at

Support users can query snapshot resources by specifying
the time that snapshots are created at or updated at with
time comparison operators: ""gt/gte/eq/neq/lt/lte"",
and cinder will return all which match the condition.

Cinder has supported volume resources,
it's time for other resources.

Add two filters updated_at and created_at in querying API.
""snapshots/detail?created_at=gt:2016-01-01T01:00:00,lt:2016-12-31T01:00:00""

Change-Id: I36da4f9b7afca755ae0524f2c5a3af810ce4052a
Implements: blueprint support-to-query-snapshot-filter-by-updated-at-created-at
",git fetch https://review.opendev.org/openstack/cinder refs/changes/62/733062/21 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/support-to-query-snapshots-by-time-comparison-filters-cd2eab06dd6d6f70.yaml', 'api-ref/source/v3/samples/versions/versions-response.json', 'cinder/api/v3/volumes.py', 'cinder/api/common.py', 'cinder/api/v3/snapshots.py', 'cinder/api/openstack/api_version_request.py', 'etc/cinder/resource_filters.json', 'api-ref/source/v3/volumes-v3-snapshots.inc', 'cinder/api/openstack/rest_api_version_history.rst', 'cinder/db/sqlalchemy/api.py', 'cinder/tests/unit/api/v3/test_snapshots.py', 'api-ref/source/v3/samples/versions/version-show-response.json', 'cinder/api/microversions.py']",13,179a98bc7c946267c559048389f396b27decbf02,support-to-query-snapshot-filter-by-timestamp,SNAPSHOT_TIME_COMPARISON_FILTER = '3.61' ,,196,35
openstack%2Fcinder~master~Ifbb1d7d5271ef62cc12f4de8d916661bdba67640,openstack/cinder,master,Ifbb1d7d5271ef62cc12f4de8d916661bdba67640,Fujitsu Driver: Add update migrated volume support,NEW,2022-07-25 08:00:16.000000000,2023-07-05 03:52:33.000000000,,"[{'_account_id': 22348}, {'_account_id': 29122}, {'_account_id': 30615}]","[{'number': 1, 'created': '2022-07-25 08:00:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2c745216fdfb2f2bf8d9379ebde1351548d5b3ea', 'message': 'Fujitsu Driver: Add volume migration support\n\nAdded support for volume migration in the Fujitsu driver.\n\nAdded below function:\n\nupdate_migrated_volume\n\nChange-Id: Ifbb1d7d5271ef62cc12f4de8d916661bdba67640\n'}, {'number': 2, 'created': '2022-07-26 01:46:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/8503a1a2c081833cc6dbdaf7178caf4471de9f0e', 'message': 'Fujitsu Driver: Add volume migration support\n\nAdded support for volume migration in the Fujitsu driver.\n\nAdded below function:\nupdate_migrated_volume\n\nChange-Id: Ifbb1d7d5271ef62cc12f4de8d916661bdba67640\n'}, {'number': 3, 'created': '2022-07-26 06:44:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/b3c590e1730ca87fd494c5ea76fe58794734a3ff', 'message': 'Fujitsu Driver: Add volume migration support\n\nAdded support for volume migration in the Fujitsu driver.\n\nAdded below function:\nupdate_migrated_volume\n\nChange-Id: Ifbb1d7d5271ef62cc12f4de8d916661bdba67640\n'}, {'number': 4, 'created': '2022-11-21 00:57:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/965397fbf7f533cbad978d649caebf2083a248f7', 'message': 'Fujitsu Driver: Add volume migration support\n\nAdded support for volume migration in the Fujitsu driver.\n\nAdded below function:\nupdate_migrated_volume\n\nChange-Id: Ifbb1d7d5271ef62cc12f4de8d916661bdba67640\n'}, {'number': 5, 'created': '2022-11-22 03:59:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/45dc514ee9e25ddd45acfaa074459b7cc8cbd0f7', 'message': 'Fujitsu Driver: Add volume migration support\n\nAdded support for volume migration in the Fujitsu driver.\n\nAdded below function:\nupdate_migrated_volume\n\nChange-Id: Ifbb1d7d5271ef62cc12f4de8d916661bdba67640\n'}, {'number': 6, 'created': '2023-03-24 02:39:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/98a16eafd9ffe1a300785d43a14c8becee7604d1', 'message': 'Fujitsu Driver: Add update migrated volume support\n\nAfter migration of a volume to fujitsu eternus backend,\nupdate the back-end name in provider locationto its original\nvolume name.\n\nAdded below function:\nupdate_migrated_volume\n\nChange-Id: Ifbb1d7d5271ef62cc12f4de8d916661bdba67640\n'}, {'number': 7, 'created': '2023-06-07 08:40:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/005f5659dccb970a08829a9f364bf1fd16465d0a', 'message': 'Fujitsu Driver: Add update migrated volume support\n\nAfter migration of a volume to fujitsu eternus backend,\nupdate the back-end name in provider location to its original\nvolume name.\n\nChange-Id: Ifbb1d7d5271ef62cc12f4de8d916661bdba67640\n'}, {'number': 8, 'created': '2023-06-07 09:17:50.000000000', 'files': ['cinder/tests/unit/volume/drivers/test_fujitsu_dx.py', 'releasenotes/notes/fujitsu-add-migration-b75afc331456e663.yaml', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_fc.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_iscsi.py', 'cinder/volume/drivers/fujitsu/eternus_dx/constants.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/469a4989dbd6e6183eb7a2b07827ce617010b5ed', 'message': 'Fujitsu Driver: Add update migrated volume support\n\nAfter migration of a volume to fujitsu eternus backend,\nupdate the back-end name in provider location to its original\nvolume name.\n\nChange-Id: Ifbb1d7d5271ef62cc12f4de8d916661bdba67640\n'}]",42,850840,469a4989dbd6e6183eb7a2b07827ce617010b5ed,201,3,8,33609,,,0,"Fujitsu Driver: Add update migrated volume support

After migration of a volume to fujitsu eternus backend,
update the back-end name in provider location to its original
volume name.

Change-Id: Ifbb1d7d5271ef62cc12f4de8d916661bdba67640
",git fetch https://review.opendev.org/openstack/cinder refs/changes/40/850840/3 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_fc.py', 'cinder/volume/drivers/fujitsu/eternus_dx/constants.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_common.py', 'cinder/volume/drivers/fujitsu/eternus_dx/eternus_dx_iscsi.py']",4,2c745216fdfb2f2bf8d9379ebde1351548d5b3ea,fujitsu-migrate," model_update = self.common.create_volume(volume) return model_update return {'provider_location': str(element_path), 'metadata': metadata} return {'provider_location': str(element_path), 'metadata': metadata} self.common.delete_volume(volume) return {'provider_location': str(element_path)} self.common.delete_snapshot(snapshot) self.common.terminate_connection(volume, connector) 'pool name: %s.', pool_name) self.common.extend_volume(volume, new_size) def update_migrated_volume(self, ctxt, volume, new_volume, original_volume_status): """"""Update migrated volume."""""" LOG.debug('update_migrated_volume, ' 'source volume id: %(s_id)s, ' 'target volume id: %(t_id)s, Enter method.', {'s_id': volume['id'], 't_id': new_volume['id']}) model_update = self.common.update_migrated_volume( ctxt, volume, new_volume) LOG.debug('update_migrated_volume, ' 'target volume meta: %s, Exit method.', model_update) ","import six LOG.info('create_volume, volume id: %s, Enter method.', volume['id']) element_path, metadata = self.common.create_volume(volume) v_metadata = volume.get('volume_metadata') if v_metadata: for data in v_metadata: metadata[data['key']] = data['value'] else: v_metadata = volume.get('metadata', {}) metadata.update(v_metadata) LOG.info('create_volume, info: %s, Exit method.', metadata) return {'provider_location': six.text_type(element_path), 'metadata': metadata} LOG.info('create_volume_from_snapshot, ' 'volume id: %(vid)s, snap id: %(sid)s, Enter method.', {'vid': volume['id'], 'sid': snapshot['id']}) LOG.info('create_volume_from_snapshot, ' 'info: %s, Exit method.', metadata) return {'provider_location': six.text_type(element_path), 'metadata': metadata} LOG.info('create_cloned_volume, ' 'target volume id: %(tid)s, ' 'source volume id: %(sid)s, Enter method.', {'tid': volume['id'], 'sid': src_vref['id']}) LOG.info('create_cloned_volume, info: %s, Exit method.', metadata) return {'provider_location': six.text_type(element_path), 'metadata': metadata} LOG.info('delete_volume, volume id: %s, Enter method.', volume['id']) vol_exist = self.common.delete_volume(volume) LOG.info('delete_volume, delete: %s, Exit method.', vol_exist) return LOG.info('create_snapshot, snap id: %(sid)s, volume id: %(vid)s, ' 'Enter method.', {'sid': snapshot['id'], 'vid': snapshot['volume_id']}) LOG.info('create_snapshot, info: %s, Exit method.', metadata) return {'provider_location': six.text_type(element_path)} LOG.info('delete_snapshot, snap id: %(sid)s, volume id: %(vid)s, ' 'Enter method.', {'sid': snapshot['id'], 'vid': snapshot['volume_id']}) vol_exist = self.common.delete_snapshot(snapshot) LOG.info('delete_snapshot, delete: %s, Exit method.', vol_exist) return LOG.info('initialize_connection, volume id: %(vid)s, ' 'initiator: %(initiator)s, Enter method.', {'vid': volume['id'], 'initiator': connector['initiator']}) LOG.info('initialize_connection, info: %s, Exit method.', info) initiator = connector.get('initiator') if connector else None LOG.info('terminate_connection, volume id: %(vid)s, ' 'initiator: %(initiator)s, Enter method.', {'vid': volume['id'], 'initiator': initiator}) map_exist = self.common.terminate_connection(volume, connector) LOG.info('terminate_connection, unmap: %s, Exit method.', map_exist) return LOG.debug('get_volume_stats, refresh: %s, Enter method.', refresh) 'pool name: %s, Exit method.', pool_name) LOG.info('extend_volume, volume id: %s, Enter method.', volume['id']) used_pool_name = self.common.extend_volume(volume, new_size) LOG.info('extend_volume, used pool name: %s, Exit method.', used_pool_name)",411,304
openstack%2Fcinder~master~I1c9677112caa0808dff17cbde2db6afbe25a2129,openstack/cinder,master,I1c9677112caa0808dff17cbde2db6afbe25a2129,HPE XP and NEC V: Host group name is not correct,NEW,2023-04-07 01:38:00.000000000,2023-07-05 03:51:36.000000000,,"[{'_account_id': 22348}, {'_account_id': 28403}, {'_account_id': 30615}, {'_account_id': 35063}, {'_account_id': 35075}]","[{'number': 1, 'created': '2023-04-07 01:38:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/da2cb5ff7db6e8041dd3f0857191385eef6bec53', 'message': 'HPE XP and NEC V: Host group name is not correct\n\nHost group[1] name formats, which is managed by cinder driver,\nfor each storage models must be:\n``HBSD-xxx`` for Hitachi storage(``xxx`` is WWPN of target host on FC,\nor IP address of target host)\n``HPEXP-xxx`` for HPE-XP storage\n``NEC-xxx`` for NEC V storage , but the format `HBSD-xxx` is used for\nOEM storage models because a bug in the merged patch\nhttps://review.opendev.org/c/openstack/cinder/+/866526 overwrites the\nprefix for Hitachi storage.\n\nIt should be fixed to use original prefixes.\n\n[1] ``Host group``(or ``iSCSI target``), which is one of Hitachi and\nOEM storage parameter,\nis a group of multiple server hosts connecting to same storage port.\n\nFollowing site would be help to know what host group is:\nhttps://knowledge.hitachivantara.com/Documents/Management_Software/SVOS/9.1.x/Volume_Management_-_VSP_G130%2C_G%2F%2FF350%2C_G%2F%2FF370%2C_G%2F%2FF700%2C_G%2F%2FF900/Provisioning/11_About_LUN_Manager%2C_logical_units_(LUs)%2C_and_host_groups\n\nCloses-Bug; #2012515\n\nChange-Id: I1c9677112caa0808dff17cbde2db6afbe25a2129\n'}, {'number': 2, 'created': '2023-04-07 04:09:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9d27ac7d31ddbde3dd075e341aa18679e49533d5', 'message': 'HPE XP and NEC V: Host group name is not correct\n\nHost group[1] name formats, which is managed by cinder driver,\nfor each storage models must be:\n``HBSD-xxx`` for Hitachi storage(``xxx`` is WWPN of target host on FC,\nor IP address of target host)\n``HPEXP-xxx`` for HPE-XP storage\n``NEC-xxx`` for NEC V storage , but the format `HBSD-xxx` is used for\nOEM storage models because a bug in the merged patch\nhttps://review.opendev.org/c/openstack/cinder/+/866526 overwrites the\nprefix for Hitachi storage.\n\nIt should be fixed to use original prefixes.\n\n[1] ``Host group``(or ``iSCSI target``), which is one of Hitachi and\nOEM storage parameter,\nis a group of multiple server hosts connecting to same storage port.\n\nFollowing site would be help to know what host group is:\nhttps://knowledge.hitachivantara.com/Documents/Management_Software/SVOS/9.1.x/Volume_Management_-_VSP_G130%2C_G%2F%2FF350%2C_G%2F%2FF370%2C_G%2F%2FF700%2C_G%2F%2FF900/Provisioning/11_About_LUN_Manager%2C_logical_units_(LUs)%2C_and_host_groups\n\nCloses-Bug; #2012515\n\nChange-Id: I1c9677112caa0808dff17cbde2db6afbe25a2129\n'}, {'number': 3, 'created': '2023-04-07 07:21:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/1b69d3ddd03779b0916e7033bc2be0dc43dc8a34', 'message': 'HPE XP and NEC V: Host group name is not correct\n\nHost group[1] name formats, which is managed by cinder driver,\nfor each storage models must be:\n``HBSD-xxx`` for Hitachi storage(``xxx`` is WWPN of target host on FC,\nor IP address of target host)\n``HPEXP-xxx`` for HPE-XP storage\n``NEC-xxx`` for NEC V storage , but the format `HBSD-xxx` is used for\nOEM storage models because a bug in the merged patch\nhttps://review.opendev.org/c/openstack/cinder/+/866526 overwrites the\nprefix for Hitachi storage.\n\nIt should be fixed to use original prefixes.\n\n[1] ``Host group``(or ``iSCSI target``), which is one of Hitachi and\nOEM storage parameter,\nis a group of multiple server hosts connecting to same storage port.\n\nFollowing site would be help to know what host group is:\nhttps://knowledge.hitachivantara.com/Documents/Management_Software/SVOS/9.1.x/Volume_Management_-_VSP_G130%2C_G%2F%2FF350%2C_G%2F%2FF370%2C_G%2F%2FF700%2C_G%2F%2FF900/Provisioning/11_About_LUN_Manager%2C_logical_units_(LUs)%2C_and_host_groups\n\nCloses-Bug: #2012515\nChange-Id: I1c9677112caa0808dff17cbde2db6afbe25a2129\n'}, {'number': 4, 'created': '2023-04-07 09:46:48.000000000', 'files': ['releasenotes/notes/hitachi-vsp-fix-to-use-correct-HGname-78c3c47dcf984ddf.yaml', 'cinder/tests/unit/volume/drivers/hpe/xp/test_hpe_xp_rest_iscsi.py', 'cinder/volume/drivers/hitachi/hbsd_common.py', 'cinder/tests/unit/volume/drivers/hpe/xp/test_hpe_xp_rest_fc.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/bfac2eed1057f57d56a8ae3d780a0b48fbd405aa', 'message': 'HPE XP and NEC V: Host group name is not correct\n\nHost group[1] name formats, which is managed by cinder driver,\nfor each storage models must be:\n``HBSD-xxx`` for Hitachi storage(``xxx`` is WWPN of target host on FC,\nor IP address of target host)\n``HPEXP-xxx`` for HPE-XP storage\n``NEC-xxx`` for NEC V storage , but the format `HBSD-xxx` is used for\nOEM storage models because a bug in the merged patch\nhttps://review.opendev.org/c/openstack/cinder/+/866526 overwrites the\nprefix for Hitachi storage.\n\nIt should be fixed to use original prefixes.\n\n[1] ``Host group``(or ``iSCSI target``), which is one of Hitachi and\nOEM storage parameter,\nis a group of multiple server hosts connecting to same storage port.\n\nFollowing site would be help to know what host group is:\nhttps://knowledge.hitachivantara.com/Documents/Management_Software/SVOS/9.1.x/Volume_Management_-_VSP_G130%2C_G%2F%2FF350%2C_G%2F%2FF370%2C_G%2F%2FF700%2C_G%2F%2FF900/Provisioning/11_About_LUN_Manager%2C_logical_units_(LUs)%2C_and_host_groups\n\nCloses-Bug: #2012515\nChange-Id: I1c9677112caa0808dff17cbde2db6afbe25a2129\n'}]",10,879830,bfac2eed1057f57d56a8ae3d780a0b48fbd405aa,86,5,4,33473,,,0,"HPE XP and NEC V: Host group name is not correct

Host group[1] name formats, which is managed by cinder driver,
for each storage models must be:
``HBSD-xxx`` for Hitachi storage(``xxx`` is WWPN of target host on FC,
or IP address of target host)
``HPEXP-xxx`` for HPE-XP storage
``NEC-xxx`` for NEC V storage , but the format `HBSD-xxx` is used for
OEM storage models because a bug in the merged patch
https://review.opendev.org/c/openstack/cinder/+/866526 overwrites the
prefix for Hitachi storage.

It should be fixed to use original prefixes.

[1] ``Host group``(or ``iSCSI target``), which is one of Hitachi and
OEM storage parameter,
is a group of multiple server hosts connecting to same storage port.

Following site would be help to know what host group is:
https://knowledge.hitachivantara.com/Documents/Management_Software/SVOS/9.1.x/Volume_Management_-_VSP_G130%2C_G%2F%2FF350%2C_G%2F%2FF370%2C_G%2F%2FF700%2C_G%2F%2FF900/Provisioning/11_About_LUN_Manager%2C_logical_units_(LUs)%2C_and_host_groups

Closes-Bug: #2012515
Change-Id: I1c9677112caa0808dff17cbde2db6afbe25a2129
",git fetch https://review.opendev.org/openstack/cinder refs/changes/30/879830/4 && git format-patch -1 --stdout FETCH_HEAD,"['releasenotes/notes/hitachi-vsp-fix-to-use-correct-HGname-78c3c47dcf984ddf.yaml', 'cinder/tests/unit/volume/drivers/hpe/xp/test_hpe_xp_rest_iscsi.py', 'cinder/volume/drivers/hitachi/hbsd_common.py', 'cinder/tests/unit/volume/drivers/hpe/xp/test_hpe_xp_rest_fc.py']",4,da2cb5ff7db6e8041dd3f0857191385eef6bec53,hitachi_fix_HGprefix," self.assertEqual(CONFIG_MAP['host_grp_name'], drv.common.format_info['group_name_format'].format( wwn=min(DEFAULT_CONNECTOR['wwpns'])))",,37,26
openstack%2Fcinder~master~I3ab790e4687f45264fc316a72a8e131147a73cac,openstack/cinder,master,I3ab790e4687f45264fc316a72a8e131147a73cac,Ceph backup: Fix Incremental Restore Using Wrong Container,NEW,2021-04-27 15:49:47.000000000,2023-07-05 03:50:19.000000000,,"[{'_account_id': 22348}, {'_account_id': 30615}, {'_account_id': 35075}]","[{'number': 1, 'created': '2021-04-27 15:49:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/0134e8a7f338c47169ec9503b034d9d36a52d5ca', 'message': 'Fix RBD incremental restore\n\nWhen using RBD incremental restores with more than one pool, aka\na non-default container, Cinder still tries to get the restore\npoint from the pool/container configured in configuration which\nresults in a failure.\n\nChange-Id: I3ab790e4687f45264fc316a72a8e131147a73cac\n'}, {'number': 2, 'created': '2023-06-20 17:25:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/9fa4df12fe165d609fef8ce627accf981ad5246e', 'message': ""Ceph backup: Fix Incremental Restore Using Wrong Container\n\nWhen using RBD incremental restores with more than one pool, aka\na non-default container, Cinder still tries to get the restore\npoint from the pool/container configured in configuration which\nresults in a failure.\n\nIn the current scenario, when using the RBD incremental restore function\nin Ceph backup for more than one pool, or non-default containers, Cinder\nis incorrectly attempting to fetch the restore point from the\npool/container specified in the configuration, leading to a failure.\n\nTo resolve this issue, the implemented fix suggests replacing the use of\nself._ceph_backup_pool, which restricts users to the pool/container\nconfigured in the configuration, with the backup.container attribute.\nThis change will ensure that the user's preference for the container, as\nspecified during backup creation, is honored during the restore process.\n\nCloses-Bug: #2024484\nChange-Id: I3ab790e4687f45264fc316a72a8e131147a73cac\n""}, {'number': 3, 'created': '2023-06-21 09:04:28.000000000', 'files': ['cinder/tests/unit/backup/drivers/test_backup_ceph.py', 'releasenotes/notes/bug-2024484-ceph-backup-incremental-restore-17ad88bd60f6202f.yaml', 'cinder/backup/drivers/ceph.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/d2ed5b17bfd32ecc4b7a92b076e6d0d46741971b', 'message': ""Ceph backup: Fix Incremental Restore Using Wrong Container\n\nWhen using RBD incremental restores with more than one pool, aka\na non-default container, Cinder still tries to get the restore\npoint from the pool/container configured in configuration which\nresults in a failure.\n\nIn the current scenario, when using the RBD incremental restore function\nin Ceph backup for more than one pool, or non-default containers, Cinder\nis incorrectly attempting to fetch the restore point from the\npool/container specified in the configuration, leading to a failure.\n\nTo resolve this issue, the implemented fix suggests replacing the use of\nself._ceph_backup_pool, which restricts users to the pool/container\nconfigured in the configuration, with the backup.container attribute.\nThis change will ensure that the user's preference for the container, as\nspecified during backup creation, is honored during the restore process.\n\nCloses-Bug: #2024484\nChange-Id: I3ab790e4687f45264fc316a72a8e131147a73cac\n""}]",2,788319,d2ed5b17bfd32ecc4b7a92b076e6d0d46741971b,52,3,3,1004,,,0,"Ceph backup: Fix Incremental Restore Using Wrong Container

When using RBD incremental restores with more than one pool, aka
a non-default container, Cinder still tries to get the restore
point from the pool/container configured in configuration which
results in a failure.

In the current scenario, when using the RBD incremental restore function
in Ceph backup for more than one pool, or non-default containers, Cinder
is incorrectly attempting to fetch the restore point from the
pool/container specified in the configuration, leading to a failure.

To resolve this issue, the implemented fix suggests replacing the use of
self._ceph_backup_pool, which restricts users to the pool/container
configured in the configuration, with the backup.container attribute.
This change will ensure that the user's preference for the container, as
specified during backup creation, is honored during the restore process.

Closes-Bug: #2024484
Change-Id: I3ab790e4687f45264fc316a72a8e131147a73cac
",git fetch https://review.opendev.org/openstack/cinder refs/changes/19/788319/1 && git format-patch -1 --stdout FETCH_HEAD,['cinder/backup/drivers/ceph.py'],1,0134e8a7f338c47169ec9503b034d9d36a52d5ca,fix-cinder-backup," def _get_restore_point(self, base_name, backup): restore point associated with backup.id is returned. backup.container)) as client: backup.id) restore_point = self._get_restore_point(base_name, backup)"," def _get_restore_point(self, base_name, backup_id): restore point associated with backup_id is returned. self._ceph_backup_pool)) as client: backup_id) restore_point = self._get_restore_point(base_name, backup.id)",5,5
openstack%2Fcinder~master~Ib0a157af28479b9ae15bf0855c8b993269f96b3f,openstack/cinder,master,Ib0a157af28479b9ae15bf0855c8b993269f96b3f,Tatlin unified driver - rename tatlin_api object,ABANDONED,2023-07-04 09:12:45.000000000,2023-07-05 01:06:37.000000000,,[],"[{'number': 1, 'created': '2023-07-04 09:12:45.000000000', 'files': ['cinder/volume/drivers/yadro/tatlin_iscsi.py', 'cinder/tests/unit/volume/drivers/yadro/test_tatlin_common.py', 'cinder/volume/drivers/yadro/tatlin_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/c052ab61001989c1fd07b615cc06d0aa1094d019', 'message': 'Tatlin unified driver - rename tatlin_api object\n\nIn Tatlin Unified driver client object is incorrectly called\ntatlin_api. TatlinAPI is a different type and does not have\nTatlin*Client methods. Renamed to tatlin_client.\n\nPartially Implements: blueprint yadro-tatlin-unified-refactoring\nChange-Id: I10135877889a17c6ae6274f8be1b992294bf65d3\n\nChange-Id: Ib0a157af28479b9ae15bf0855c8b993269f96b3f\n'}]",0,887588,c052ab61001989c1fd07b615cc06d0aa1094d019,6,0,1,13671,,,0,"Tatlin unified driver - rename tatlin_api object

In Tatlin Unified driver client object is incorrectly called
tatlin_api. TatlinAPI is a different type and does not have
Tatlin*Client methods. Renamed to tatlin_client.

Partially Implements: blueprint yadro-tatlin-unified-refactoring
Change-Id: I10135877889a17c6ae6274f8be1b992294bf65d3

Change-Id: Ib0a157af28479b9ae15bf0855c8b993269f96b3f
",git fetch https://review.opendev.org/openstack/cinder refs/changes/88/887588/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/yadro/tatlin_iscsi.py', 'cinder/tests/unit/volume/drivers/yadro/test_tatlin_common.py', 'cinder/volume/drivers/yadro/tatlin_common.py']",3,c052ab61001989c1fd07b615cc06d0aa1094d019,rename_tatlin_api," self.tatlin_client = self._get_tatlin_client() self.tatlin_client.get_resource_count(self.pool_id) LOG.debug('Current pool %(pool)s has %(pool_res)s resourses. ' if pool_res_count >= self._max_pool_resource_count: message = _( 'Create volume failed. Too many resources per pool: %s' ) % pool_res_count if cluster_res_count >= self.MAX_ALLOWED_RESOURCES: message = _( 'Create volume failed. Too many resources per cluster: %s' ) % cluster_res_count self.tatlin_client.create_volume(volume.name_id, name, size, self.pool_id, lbaFormat=self._lba_format) if self.tatlin_client.is_volume_ready(volume.name_id): if (self.tatlin_client.get_volume_status(volume.name_id) == 'online'): if not self.tatlin_client.is_volume_exists(volume.name_id): self.tatlin_client.delete_volume(volume.name_id) if not self.tatlin_client.is_volume_exists(volume.name_id): if self.tatlin_client.is_volume_exists(volume.name_id): self.tatlin_client.extend_volume(volume.name_id, size) self.tatlin_client.update_qos( pool_stat = self.tatlin_client.get_pool_detail(self.pool_id) sys_stat = self.tatlin_client.get_sys_statistic() self.tatlin_client.get_resource_count(self.pool_id) result = self.tatlin_client.get_volume_info(source_name) result = self.tatlin_client.get_volume_info(source_name) self.tatlin_client.add_vol_to_host(volume.name_id, host_id) self.tatlin_client.remove_vol_from_host(volume.name_id, host_id) cur_ports = self.tatlin_client.get_resource_ports_array(volume_id) result = self.tatlin_client.get_resource_mapping() self.tatlin_client.export_volume(volume.name_id, ports) self._pool_id = self.tatlin_client.get_pool_id_by_name("," self.tatlin_api = self._get_tatlin_client() self.tatlin_api.get_resource_count(self.pool_id) LOG.debug('Current pool %(pool)s has %(pool_res)s res.' if pool_res_count > 255: message = _('TatlinVolumeDriver create volume failed. ' 'Too many resources per pool created') if cluster_res_count + 1 > self.MAX_ALLOWED_RESOURCES: message = _('TatlinVolumeDriver create volume failed. ' 'Too many resources per cluster created') self.tatlin_api.create_volume(volume.name_id, name, size, self.pool_id, lbaFormat=self._lba_format) if self.tatlin_api.is_volume_ready(volume.name_id): if self.tatlin_api.get_volume_status(volume.name_id) == 'online': if not self.tatlin_api.is_volume_exists(volume.name_id): self.tatlin_api.delete_volume(volume.name_id) if not self.tatlin_api.is_volume_exists(volume.name_id): if self.tatlin_api.is_volume_exists(volume.name_id): self.tatlin_api.extend_volume(volume.name_id, size) self.tatlin_api.update_qos( pool_stat = self.tatlin_api.get_pool_detail(self.pool_id) sys_stat = self.tatlin_api.get_sys_statistic() self.tatlin_api.get_resource_count(self.pool_id) result = self.tatlin_api.get_volume_info(source_name) result = self.tatlin_api.get_volume_info(source_name) self.tatlin_api.add_vol_to_host(volume.name_id, host_id) self.tatlin_api.remove_vol_from_host(volume.name_id, host_id) cur_ports = self.tatlin_api.get_resource_ports_array(volume_id) result = self.tatlin_api.get_resource_mapping() self.tatlin_api.export_volume(volume.name_id, ports) self._pool_id = self.tatlin_api.get_pool_id_by_name(",49,46
openstack%2Fpython-ironicclient~master~Ie66bd3a6d68ff4051f3d52dfb2bca26b1053187e,openstack/python-ironicclient,master,Ie66bd3a6d68ff4051f3d52dfb2bca26b1053187e,Node sharding support,MERGED,2023-07-03 18:14:52.000000000,2023-07-04 23:28:51.000000000,2023-07-04 23:27:56.000000000,"[{'_account_id': 4571}, {'_account_id': 10239}, {'_account_id': 11655}, {'_account_id': 15519}, {'_account_id': 22348}, {'_account_id': 23851}]","[{'number': 1, 'created': '2023-07-03 18:14:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/c7c1bbdab549b9a508e6cab80022e39a516b1a55', 'message': 'Node sharding support\n\n- Bumps known API version high enough for shards to exist\n- Implements support for querying for sharded nodes, or by a list\n  of shards\n- Adds basic unit testing.\n\nChange-Id: Ie66bd3a6d68ff4051f3d52dfb2bca26b1053187e\n'}, {'number': 2, 'created': '2023-07-03 19:53:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/822313e47ce32fb963b2629184b3c2d3440a00c3', 'message': 'Node sharding support\n\n- Bumps known API version high enough for shards to exist\n- Implements support for querying for sharded nodes, or by a list\n  of shards\n- Adds basic unit testing.\n- Adds support for all of this to OSC\n\nChange-Id: Ie66bd3a6d68ff4051f3d52dfb2bca26b1053187e\n'}, {'number': 3, 'created': '2023-07-03 19:56:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/1d35faf68e24420cd48e031f9c4bec1626a3d433', 'message': 'Node sharding support\n\n- Bumps known API version high enough for shards to exist\n- Implements support for querying for sharded nodes, or by a list\n  of shards\n- Adds basic unit testing.\n- Adds support for all of this to OSC\n\nChange-Id: Ie66bd3a6d68ff4051f3d52dfb2bca26b1053187e\n'}, {'number': 4, 'created': '2023-07-03 21:22:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/3f805f575634f9780c85d44c59210404486809a4', 'message': 'Node sharding support\n\n- Bumps known API version high enough for shards to exist\n- Implements support for querying for sharded nodes, or by a list\n  of shards\n- Adds basic unit testing.\n- Adds support for all of this to OSC\n\nChange-Id: Ie66bd3a6d68ff4051f3d52dfb2bca26b1053187e\n'}, {'number': 5, 'created': '2023-07-04 00:14:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/75bffcffa9f7c42b917b6b9a44d5f8055fd8e442', 'message': 'Node sharding support\n\n- Bumps known API version high enough for shards to exist\n- Implements support for querying for sharded nodes, or by a list\n  of shards\n- Adds basic unit testing.\n- Adds support for all of this to OSC\n\nChange-Id: Ie66bd3a6d68ff4051f3d52dfb2bca26b1053187e\n'}, {'number': 6, 'created': '2023-07-04 17:39:34.000000000', 'files': ['ironicclient/v1/node.py', 'ironicclient/common/http.py', 'ironicclient/tests/unit/v1/test_node.py', 'ironicclient/tests/unit/osc/v1/test_baremetal_node.py', 'ironicclient/osc/v1/baremetal_node.py', 'releasenotes/notes/node-shard-support-774ebfe6719fc7c2.yaml'], 'web_link': 'https://opendev.org/openstack/python-ironicclient/commit/0e9a476610dcc54be88413b5f642c01c47606b3e', 'message': 'Node sharding support\n\n- Bumps known API version high enough for shards to exist\n- Implements support for querying for sharded nodes, or by a list\n  of shards\n- Adds basic unit testing.\n- Adds support for all of this to OSC\n\nChange-Id: Ie66bd3a6d68ff4051f3d52dfb2bca26b1053187e\n'}]",10,887533,0e9a476610dcc54be88413b5f642c01c47606b3e,23,6,6,10342,,,0,"Node sharding support

- Bumps known API version high enough for shards to exist
- Implements support for querying for sharded nodes, or by a list
  of shards
- Adds basic unit testing.
- Adds support for all of this to OSC

Change-Id: Ie66bd3a6d68ff4051f3d52dfb2bca26b1053187e
",git fetch https://review.opendev.org/openstack/python-ironicclient refs/changes/33/887533/1 && git format-patch -1 --stdout FETCH_HEAD,"['ironicclient/v1/node.py', 'ironicclient/common/http.py', 'ironicclient/tests/unit/v1/test_node.py', 'releasenotes/notes/node-shard-support-774ebfe6719fc7c2.yaml']",4,c7c1bbdab549b9a508e6cab80022e39a516b1a55,,features: - Add support for Node shard field. - Add support for listing nodes by one or more shards. - Add support for filtering nodes by if they are sharded.,,51,6
openstack%2Ftempest~master~I9cc4c2b7deefa55edc26ba215f18b590fa8e448d,openstack/tempest,master,I9cc4c2b7deefa55edc26ba215f18b590fa8e448d,Add tests to list hidden images,MERGED,2023-05-29 11:02:57.000000000,2023-07-04 22:15:43.000000000,2023-07-04 22:14:38.000000000,"[{'_account_id': 8556}, {'_account_id': 10459}, {'_account_id': 19262}, {'_account_id': 22348}, {'_account_id': 30674}]","[{'number': 1, 'created': '2023-05-29 11:02:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/730a6a0cace2be43b14ef0d131a74cc9e83148ce', 'message': 'Add tests to list hidden images\n\nGlance have hidden image property.\nIf os_hidden set to true, image will not appear in default image list response.\n\nChange-Id: I9cc4c2b7deefa55edc26ba215f18b590fa8e448d\n'}, {'number': 2, 'created': '2023-05-30 09:11:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/921a92e1f0df5980fcb2de0544fff1da3fe6b9ee', 'message': 'Add tests to list hidden images\n\nGlance have hidden image property.\nIf os_hidden set to true, image will not appear in default image list response.\n\nChange-Id: I9cc4c2b7deefa55edc26ba215f18b590fa8e448d\n'}, {'number': 3, 'created': '2023-05-30 11:20:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/632de509e9f1cd42db0e6c7d5be1992b20c4cf0b', 'message': 'Add tests to list hidden images\n\nGlance have hidden image property.\nIf os_hidden set to true, image will not appear in default image list response.\n\nChange-Id: I9cc4c2b7deefa55edc26ba215f18b590fa8e448d\n'}, {'number': 4, 'created': '2023-06-13 08:40:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/da789c9936e365a52dc8178176ed339b55b19731', 'message': 'Add tests to list hidden images\n\nGlance have hidden image property.\nIf os_hidden set to true, image will not appear in default image list response.\n\nChange-Id: I9cc4c2b7deefa55edc26ba215f18b590fa8e448d\n'}, {'number': 5, 'created': '2023-06-13 11:32:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/46ab45e0f550050dc6b7bc93353839a4fd69df41', 'message': 'Add tests to list hidden images\n\nGlance have hidden image property.\nIf os_hidden set to true, image will not appear in default image list response.\n\nChange-Id: I9cc4c2b7deefa55edc26ba215f18b590fa8e448d\n'}, {'number': 6, 'created': '2023-06-15 11:46:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/1c9aecac1da39cea6b8a45876700cb78e552513c', 'message': 'Add tests to list hidden images\n\nGlance have hidden image property.\nIf os_hidden set to true, image will not appear in default image list response.\n\nChange-Id: I9cc4c2b7deefa55edc26ba215f18b590fa8e448d\n'}, {'number': 7, 'created': '2023-06-15 11:51:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tempest/commit/4a5cc82eb652ab69886ff6268ba4c0de2243ce00', 'message': 'Add tests to list hidden images\n\nGlance have hidden image property.\nIf os_hidden set to true, image will not appear in default image list response.\n\nChange-Id: I9cc4c2b7deefa55edc26ba215f18b590fa8e448d\n'}, {'number': 8, 'created': '2023-06-18 10:32:27.000000000', 'files': ['tempest/api/image/v2/test_images.py'], 'web_link': 'https://opendev.org/openstack/tempest/commit/1fe102a62165cfa1220d03592602b4153cd137f3', 'message': 'Add tests to list hidden images\n\nGlance have hidden image property.\nIf os_hidden set to true, image will not appear in default image list response.\n\nChange-Id: I9cc4c2b7deefa55edc26ba215f18b590fa8e448d\n'}]",29,884610,1fe102a62165cfa1220d03592602b4153cd137f3,58,5,8,34510,,,0,"Add tests to list hidden images

Glance have hidden image property.
If os_hidden set to true, image will not appear in default image list response.

Change-Id: I9cc4c2b7deefa55edc26ba215f18b590fa8e448d
",git fetch https://review.opendev.org/openstack/tempest refs/changes/10/884610/3 && git format-patch -1 --stdout FETCH_HEAD,['tempest/api/image/v2/test_images.py'],1,730a6a0cace2be43b14ef0d131a74cc9e83148ce,migrate-storage-downstream-glance-api-tests," @decorators.idempotent_id('d43f3efc-da4c-4af9-b636-868f0c6acedb') def test_list_hidden_image(self): image = self.client.create_image(os_hidden=True) images_list = self.client.list_images()['images'] fetched_images_id = [img['id'] for img in images_list] self.assertNotIn(image['id'], fetched_images_id) @decorators.idempotent_id('fdb96b81-257b-42ac-978b-ddeefa3760e4') def test_list_update_hidden_image(self): image = self.create_image() images_list = self.client.list_images()['images'] fetched_images_id = [img['id'] for img in images_list] self.assertIn(image['id'], fetched_images_id) self.client.update_image(image['id'], [dict(replace='/os_hidden', value=True)]) images_list = self.client.list_images()['images'] fetched_images_id = [img['id'] for img in images_list] self.assertNotIn(image['id'], fetched_images_id) ",,21,0
openstack%2Fopenstack-helm~master~I75b8fde3ec4e3355319b1c3f257e2d76c36f6aa4,openstack/openstack-helm,master,I75b8fde3ec4e3355319b1c3f257e2d76c36f6aa4,Add 2023.1 test jobs,MERGED,2023-06-30 19:07:57.000000000,2023-07-04 21:21:24.000000000,2023-07-04 21:19:31.000000000,"[{'_account_id': 22348}, {'_account_id': 33330}]","[{'number': 1, 'created': '2023-06-30 19:07:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/cbacf30c5281339214105cec61b7cafc4f412257', 'message': 'WIP: Add 2023.1 test jobs\n\nChange-Id: I75b8fde3ec4e3355319b1c3f257e2d76c36f6aa4\n'}, {'number': 2, 'created': '2023-07-03 19:46:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/f9bccb0d5041732ce8ff3ad7a2a56ad59f1fedcf', 'message': 'WIP: Add 2023.1 test jobs\n\nChange-Id: I75b8fde3ec4e3355319b1c3f257e2d76c36f6aa4\n'}, {'number': 3, 'created': '2023-07-03 21:18:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/ca91a03786d084ed0f6292264b0ea6dcde1835ff', 'message': 'WIP: Add 2023.1 test jobs\n\nChange-Id: I75b8fde3ec4e3355319b1c3f257e2d76c36f6aa4\n'}, {'number': 4, 'created': '2023-07-03 23:12:31.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/e842b5254e05419607c6621897736e4201a2ffe7', 'message': 'WIP: Add 2023.1 test jobs\n\nChange-Id: I75b8fde3ec4e3355319b1c3f257e2d76c36f6aa4\n'}, {'number': 5, 'created': '2023-07-04 01:46:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/81a7711bc0747d73734174422660490f5da20aa4', 'message': 'Add 2023.1 test jobs\n\n- Also run last two test scripts in compute-kit job\n  sequentially. This is handy since it allows to see\n  what is happening during the test run. Both these\n  test scripts usually take just few minutes. But if\n  we run them using ansible async feature and one of\n  the scripts fails then we are forced to wait for\n  a long timeout.\n\nChange-Id: I75b8fde3ec4e3355319b1c3f257e2d76c36f6aa4\n'}, {'number': 6, 'created': '2023-07-04 15:34:38.000000000', 'files': ['glance/values_overrides/2023.1-ubuntu_focal.yaml', 'releasenotes/notes/heat.yaml', 'heat/Chart.yaml', 'horizon/Chart.yaml', 'releasenotes/notes/manila.yaml', 'releasenotes/notes/neutron.yaml', 'zuul.d/jobs-openstack-helm.yaml', 'keystone/values_overrides/2023.1-ubuntu_focal.yaml', 'nova/values_overrides/2023.1-ubuntu_focal.yaml', 'heat/values_overrides/2023.1-ubuntu_focal.yaml', 'barbican/Chart.yaml', 'cinder/Chart.yaml', 'barbican/values_overrides/2023.1-ubuntu_focal.yaml', 'releasenotes/notes/cinder.yaml', 'releasenotes/notes/nova.yaml', 'releasenotes/notes/glance.yaml', 'releasenotes/notes/placement.yaml', 'horizon/values_overrides/2023.1-ubuntu_focal.yaml', 'manila/values_overrides/2023.1-ubuntu_focal.yaml', 'releasenotes/notes/keystone.yaml', 'releasenotes/notes/horizon.yaml', 'neutron/Chart.yaml', 'neutron/values_overrides/2023.1-ubuntu_focal.yaml', 'nova/Chart.yaml', 'placement/values_overrides/2023.1-ubuntu_focal.yaml', 'releasenotes/notes/barbican.yaml', 'glance/Chart.yaml', 'placement/Chart.yaml', 'zuul.d/project.yaml', 'keystone/Chart.yaml', 'manila/Chart.yaml', 'cinder/values_overrides/2023.1-ubuntu_focal.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/b1f74a351abdd200613b1e14a7699880740099b5', 'message': 'Add 2023.1 test jobs\n\n- Also run last two test scripts in compute-kit job\n  sequentially. This is handy since it allows to see\n  what is happening during the test run. Both these\n  test scripts usually take just few minutes. But if\n  we run them using ansible async feature and one of\n  the scripts fails then we are forced to wait for\n  a long timeout.\n\nChange-Id: I75b8fde3ec4e3355319b1c3f257e2d76c36f6aa4\n'}]",3,887427,b1f74a351abdd200613b1e14a7699880740099b5,19,2,6,3009,,,0,"Add 2023.1 test jobs

- Also run last two test scripts in compute-kit job
  sequentially. This is handy since it allows to see
  what is happening during the test run. Both these
  test scripts usually take just few minutes. But if
  we run them using ansible async feature and one of
  the scripts fails then we are forced to wait for
  a long timeout.

Change-Id: I75b8fde3ec4e3355319b1c3f257e2d76c36f6aa4
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/27/887427/6 && git format-patch -1 --stdout FETCH_HEAD,"['glance/values_overrides/2023.1-ubuntu_focal.yaml', 'horizon/values_overrides/2023.1-ubuntu_focal.yaml', 'manila/values_overrides/2023.1-ubuntu_focal.yaml', 'zuul.d/jobs-openstack-helm.yaml', 'keystone/values_overrides/2023.1-ubuntu_focal.yaml', 'neutron/values_overrides/2023.1-ubuntu_focal.yaml', 'nova/values_overrides/2023.1-ubuntu_focal.yaml', 'placement/values_overrides/2023.1-ubuntu_focal.yaml', 'heat/values_overrides/2023.1-ubuntu_focal.yaml', 'zuul.d/project.yaml', 'barbican/values_overrides/2023.1-ubuntu_focal.yaml', 'cinder/values_overrides/2023.1-ubuntu_focal.yaml']",12,cbacf30c5281339214105cec61b7cafc4f412257,2023_jobs,"--- images: tags: db_init: ""docker.io/openstackhelm/heat:periodic-weekly_2023.1-ubuntu_focal-20230617"" cinder_db_sync: ""docker.io/openstackhelm/cinder:periodic-weekly_2023.1-ubuntu_focal-20230617"" db_drop: ""docker.io/openstackhelm/heat:periodic-weekly_2023.1-ubuntu_focal-20230617"" ks_user: ""docker.io/openstackhelm/heat:periodic-weekly_2023.1-ubuntu_focal-20230617"" ks_service: ""docker.io/openstackhelm/heat:periodic-weekly_2023.1-ubuntu_focal-20230617"" ks_endpoints: ""docker.io/openstackhelm/heat:periodic-weekly_2023.1-ubuntu_focal-20230617"" cinder_api: ""docker.io/openstackhelm/cinder:periodic-weekly_2023.1-ubuntu_focal-20230617"" bootstrap: ""docker.io/openstackhelm/heat:periodic-weekly_2023.1-ubuntu_focal-20230617"" cinder_scheduler: ""docker.io/openstackhelm/cinder:periodic-weekly_2023.1-ubuntu_focal-20230617"" cinder_volume: ""docker.io/openstackhelm/cinder:periodic-weekly_2023.1-ubuntu_focal-20230617"" cinder_volume_usage_audit: ""docker.io/openstackhelm/cinder:periodic-weekly_2023.1-ubuntu_focal-20230617"" cinder_storage_init: ""docker.io/openstackhelm/ceph-config-helper:latest-ubuntu_focal"" cinder_backup: ""docker.io/openstackhelm/cinder:periodic-weekly_2023.1-ubuntu_focal-20230617"" cinder_backup_storage_init: ""docker.io/openstackhelm/ceph-config-helper:latest-ubuntu_focal"" ... ",,205,16
openstack%2Fopenstack-helm-images~master~Id01c093ef22e289f0baacf7889795b9ece01d897,openstack/openstack-helm-images,master,Id01c093ef22e289f0baacf7889795b9ece01d897,Add ovs jammy,MERGED,2023-07-03 15:30:57.000000000,2023-07-04 21:14:59.000000000,2023-07-04 21:12:55.000000000,"[{'_account_id': 3009}, {'_account_id': 22348}, {'_account_id': 33330}]","[{'number': 1, 'created': '2023-07-03 15:30:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/c26bdbbe4f6c82dde9f7ec94da7ac790d438da14', 'message': 'Add ovs jammy\n\nChange-Id: Id01c093ef22e289f0baacf7889795b9ece01d897\n'}, {'number': 2, 'created': '2023-07-04 08:27:59.000000000', 'files': ['openvswitch/Dockerfile.ubuntu_jammy', 'zuul.d/openvswitch.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/bfaac24bd0300304b8e98794ff905a147d353642', 'message': 'Add ovs jammy\n\nChange-Id: Id01c093ef22e289f0baacf7889795b9ece01d897\n'}]",3,887522,bfaac24bd0300304b8e98794ff905a147d353642,12,3,2,35691,,,0,"Add ovs jammy

Change-Id: Id01c093ef22e289f0baacf7889795b9ece01d897
",git fetch https://review.opendev.org/openstack/openstack-helm-images refs/changes/22/887522/1 && git format-patch -1 --stdout FETCH_HEAD,"['openvswitch/Dockerfile.ubuntu_jammy', 'zuul.d/openvswitch.yaml']",2,c26bdbbe4f6c82dde9f7ec94da7ac790d438da14,ovs-jammy," - context: openvswitch repository: openstackhelm/openvswitch dockerfile: Dockerfile.ubuntu_jammy tags: - latest-ubuntu_jammy - ""ubuntu_jammy-{{ currentdate }}""",,13,0
openstack%2Fcinder~master~I3f0c5ba38cdf1ee542762242e8e5ab6e3f5c157b,openstack/cinder,master,I3f0c5ba38cdf1ee542762242e8e5ab6e3f5c157b,Tatlin unified driver - rename tatlin_api object,ABANDONED,2023-07-04 09:13:46.000000000,2023-07-04 21:00:07.000000000,,[],"[{'number': 1, 'created': '2023-07-04 09:13:46.000000000', 'files': ['cinder/volume/drivers/yadro/tatlin_iscsi.py', 'cinder/tests/unit/volume/drivers/yadro/test_tatlin_common.py', 'cinder/volume/drivers/yadro/tatlin_common.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/d22023ef13df6877ef36b0c36333dbaf5ab7078d', 'message': 'Tatlin unified driver - rename tatlin_api object\n\nIn Tatlin Unified driver client object is incorrectly called\ntatlin_api. TatlinAPI is a different type and does not have\nTatlin*Client methods. Renamed to tatlin_client.\n\nPartially Implements: blueprint yadro-tatlin-unified-refactoring\nChange-Id: I10135877889a17c6ae6274f8be1b992294bf65d3\n\nChange-Id: I3f0c5ba38cdf1ee542762242e8e5ab6e3f5c157b\n'}]",0,887589,d22023ef13df6877ef36b0c36333dbaf5ab7078d,5,0,1,13671,,,0,"Tatlin unified driver - rename tatlin_api object

In Tatlin Unified driver client object is incorrectly called
tatlin_api. TatlinAPI is a different type and does not have
Tatlin*Client methods. Renamed to tatlin_client.

Partially Implements: blueprint yadro-tatlin-unified-refactoring
Change-Id: I10135877889a17c6ae6274f8be1b992294bf65d3

Change-Id: I3f0c5ba38cdf1ee542762242e8e5ab6e3f5c157b
",git fetch https://review.opendev.org/openstack/cinder refs/changes/89/887589/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/yadro/tatlin_iscsi.py', 'cinder/tests/unit/volume/drivers/yadro/test_tatlin_common.py', 'cinder/volume/drivers/yadro/tatlin_common.py']",3,d22023ef13df6877ef36b0c36333dbaf5ab7078d,rename_tatlin_api," self.tatlin_client = self._get_tatlin_client() self.tatlin_client.get_resource_count(self.pool_id) LOG.debug('Current pool %(pool)s has %(pool_res)s resourses. ' if pool_res_count >= self._max_pool_resource_count: message = _( 'Create volume failed. Too many resources per pool: %s' ) % pool_res_count if cluster_res_count >= self.MAX_ALLOWED_RESOURCES: message = _( 'Create volume failed. Too many resources per cluster: %s' ) % cluster_res_count self.tatlin_client.create_volume(volume.name_id, name, size, self.pool_id, lbaFormat=self._lba_format) if self.tatlin_client.is_volume_ready(volume.name_id): if (self.tatlin_client.get_volume_status(volume.name_id) == 'online'): if not self.tatlin_client.is_volume_exists(volume.name_id): self.tatlin_client.delete_volume(volume.name_id) if not self.tatlin_client.is_volume_exists(volume.name_id): if self.tatlin_client.is_volume_exists(volume.name_id): self.tatlin_client.extend_volume(volume.name_id, size) self.tatlin_client.update_qos( pool_stat = self.tatlin_client.get_pool_detail(self.pool_id) sys_stat = self.tatlin_client.get_sys_statistic() self.tatlin_client.get_resource_count(self.pool_id) result = self.tatlin_client.get_volume_info(source_name) result = self.tatlin_client.get_volume_info(source_name) self.tatlin_client.add_vol_to_host(volume.name_id, host_id) self.tatlin_client.remove_vol_from_host(volume.name_id, host_id) cur_ports = self.tatlin_client.get_resource_ports_array(volume_id) result = self.tatlin_client.get_resource_mapping() self.tatlin_client.export_volume(volume.name_id, ports) self._pool_id = self.tatlin_client.get_pool_id_by_name("," self.tatlin_api = self._get_tatlin_client() self.tatlin_api.get_resource_count(self.pool_id) LOG.debug('Current pool %(pool)s has %(pool_res)s res.' if pool_res_count > 255: message = _('TatlinVolumeDriver create volume failed. ' 'Too many resources per pool created') if cluster_res_count + 1 > self.MAX_ALLOWED_RESOURCES: message = _('TatlinVolumeDriver create volume failed. ' 'Too many resources per cluster created') self.tatlin_api.create_volume(volume.name_id, name, size, self.pool_id, lbaFormat=self._lba_format) if self.tatlin_api.is_volume_ready(volume.name_id): if self.tatlin_api.get_volume_status(volume.name_id) == 'online': if not self.tatlin_api.is_volume_exists(volume.name_id): self.tatlin_api.delete_volume(volume.name_id) if not self.tatlin_api.is_volume_exists(volume.name_id): if self.tatlin_api.is_volume_exists(volume.name_id): self.tatlin_api.extend_volume(volume.name_id, size) self.tatlin_api.update_qos( pool_stat = self.tatlin_api.get_pool_detail(self.pool_id) sys_stat = self.tatlin_api.get_sys_statistic() self.tatlin_api.get_resource_count(self.pool_id) result = self.tatlin_api.get_volume_info(source_name) result = self.tatlin_api.get_volume_info(source_name) self.tatlin_api.add_vol_to_host(volume.name_id, host_id) self.tatlin_api.remove_vol_from_host(volume.name_id, host_id) cur_ports = self.tatlin_api.get_resource_ports_array(volume_id) result = self.tatlin_api.get_resource_mapping() self.tatlin_api.export_volume(volume.name_id, ports) self._pool_id = self.tatlin_api.get_pool_id_by_name(",49,46
openstack%2Fcinder~master~Id4d6410a699bf650c66a6ed209a459d290d7c68c,openstack/cinder,master,Id4d6410a699bf650c66a6ed209a459d290d7c68c,WIP: Add retention policy support,NEW,2023-06-20 06:29:09.000000000,2023-07-04 19:58:30.000000000,,"[{'_account_id': 11904}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-20 06:29:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/09670f1a7f8cdef3b24fcfbdfa408cd1f8a6c264', 'message': 'Add retention policy support\n\nAdding retention policy support when creating new backup.\nYou can add desired number of retention points by adding\n`retention` metadata.\n\nChange-Id: Id4d6410a699bf650c66a6ed209a459d290d7c68c\nSigned-off-by: Pham Le Gia Dai <daiplg@fpt.com.vn>\n'}, {'number': 2, 'created': '2023-06-20 08:18:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/a63de42d73ca1be1049885cb97ca976d6b879b1f', 'message': ""WIP: Add retention policy support\n\nAdding retention policy support when creating new backup.\nYou can add desired number of retention points by adding\n`retention` metadata.\n\nThis code is not complete, as it's missing:\n- Adding new unit tests\n- Documenting for the new feature\n\nChange-Id: Id4d6410a699bf650c66a6ed209a459d290d7c68c\nSigned-off-by: Pham Le Gia Dai <daiplg@fpt.com.vn>\n""}, {'number': 3, 'created': '2023-06-20 09:01:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/4337eb33c977977872bae7b3195dbe5a1262c777', 'message': ""WIP: Add retention policy support\n\nAdding retention policy support when creating new backup.\nYou can add desired number of retention points by adding\n`retention` metadata.\n\nThis code is not complete, as it's missing:\n- Adding new unit tests\n- Documenting for the new feature\n\nChange-Id: Id4d6410a699bf650c66a6ed209a459d290d7c68c\nSigned-off-by: Pham Le Gia Dai <daiplg@fpt.com.vn>\n""}, {'number': 4, 'created': '2023-06-21 03:02:24.000000000', 'files': ['cinder/backup/manager.py', 'cinder/backup/rpcapi.py', 'cinder/objects/backup.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/f10ea5c0d356803964f5ee515ccc0d424d3fa2f7', 'message': ""WIP: Add retention policy support\n\nAdding retention policy support when creating new backup.\nYou can add desired number of retention points by adding\n`retention` metadata.\n\nThis code is not complete, as it's missing:\n- Adding new unit tests\n- Documenting for the new feature\n\nChange-Id: Id4d6410a699bf650c66a6ed209a459d290d7c68c\nSigned-off-by: Pham Le Gia Dai <daiplg@fpt.com.vn>\n""}]",35,886447,f10ea5c0d356803964f5ee515ccc0d424d3fa2f7,48,2,4,35916,,,0,"WIP: Add retention policy support

Adding retention policy support when creating new backup.
You can add desired number of retention points by adding
`retention` metadata.

This code is not complete, as it's missing:
- Adding new unit tests
- Documenting for the new feature

Change-Id: Id4d6410a699bf650c66a6ed209a459d290d7c68c
Signed-off-by: Pham Le Gia Dai <daiplg@fpt.com.vn>
",git fetch https://review.opendev.org/openstack/cinder refs/changes/47/886447/1 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/backup/manager.py', 'cinder/backup/rpcapi.py', 'cinder/objects/backup.py']",3,09670f1a7f8cdef3b24fcfbdfa408cd1f8a6c264,feature/retention-policy," @classmethod def get_all_disposable_by_volume_id(cls, context: context.RequestContext, project_id: str, volume_id: str, retention_points: int) -> 'BackupList': backups = db.backup_get_all_by_project(context, project_id, sort_keys=[""created_at""], sort_dirs=[""asc""], filters={""volume_id"": volume_id}) backups = backups[:-retention_points] expected_attrs = Backup._get_expected_attrs(context) return base.obj_make_list(context, cls(context), objects.Backup, backups, expected_attrs=expected_attrs) ",,68,2
openstack%2Fcharm-cinder-lvm~stable%2Fyoga~I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc,openstack/charm-cinder-lvm,stable/yoga,I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc,Fix target_helper config for >= Wallaby,NEW,2022-10-14 13:31:21.000000000,2023-07-04 19:48:06.000000000,,"[{'_account_id': 10058}, {'_account_id': 11805}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-10-14 13:31:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/c6536d63646ac9740a7223990b5e3baebcd38fea', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit f9ce4b472100dae79c79d0f2b5ce1e0619ec8a0b)\n""}, {'number': 2, 'created': '2022-10-21 18:55:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/f94d6deabcd32b100a8b27d0f9c8064b53d79653', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit f9ce4b472100dae79c79d0f2b5ce1e0619ec8a0b)\n(cherry picked from commit 0780db3b219ae1b9ae2f92273e578c0f9329ee30)\n""}, {'number': 3, 'created': '2023-03-23 17:56:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/04c180bc49c34142164e1b44fe767c3ca78865e6', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit ae8094a0aebd073caa3152a357cd94b47eb77029)\n(cherry picked from commit 059913ff141609648fbbaa5f50d2bdfdd3a3787e)\n""}, {'number': 4, 'created': '2023-04-25 21:27:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/55c37d3df0d8cf38d475bf75d324eeac61188d16', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit 71718d359e32564c3666c84403cbc8399a8deb4d)\n(cherry picked from commit d11eac5a0e448f09f9dcbe3287cabd35639fafa1)\n""}, {'number': 5, 'created': '2023-06-30 15:42:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/f32575f42f37db2bcc89da9843cc5b38a0ec11c6', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit 3acbbbcf170d76733456156295866ae3470b14e7)\n(cherry picked from commit ec27742fe6e5b0973b777fbb1e3ad5317389d4ef)\n""}, {'number': 6, 'created': '2023-07-02 15:09:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/04eced6ba015ac804e8e78f5bcf0f284a69b6844', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit 2628cacc1ee688833a4b1e892f854eb5aa09b99d)\n(cherry picked from commit 4cdbbebe8c4912e4be704922d70a84ece504eacb)\n""}, {'number': 7, 'created': '2023-07-04 18:23:19.000000000', 'files': ['src/reactive/cinder_lvm_handlers.py', 'src/lib/charm/openstack/cinder_lvm.py', 'unit_tests/test_lib_charm_openstack_cinder_lvm.py'], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/3f6de9c75c81708bfb1e80e24776c9f88efdb649', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit bc942d2d42efecdb7bd22ef61e2ba71fe3d93167)\n(cherry picked from commit 08b5eec158088d210c3833eae44c743db231ef48)\n""}]",6,861379,3f6de9c75c81708bfb1e80e24776c9f88efdb649,34,5,7,10058,,,0,"Fix target_helper config for >= Wallaby

Starting with the release of Wallaby, the 'tgtadm' package is
now the default target helper, which doesn't work for cinder-lvm.
As such, we need to explicitly set the new package to 'lioadm'.

Closes-Bug: #1949074
Change-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc
(cherry picked from commit bc942d2d42efecdb7bd22ef61e2ba71fe3d93167)
(cherry picked from commit 08b5eec158088d210c3833eae44c743db231ef48)
",git fetch https://review.opendev.org/openstack/charm-cinder-lvm refs/changes/79/861379/6 && git format-patch -1 --stdout FETCH_HEAD,"['src/reactive/cinder_lvm_handlers.py', 'src/lib/charm/openstack/cinder_lvm.py', 'unit_tests/test_lib_charm_openstack_cinder_lvm.py']",3,c6536d63646ac9740a7223990b5e3baebcd38fea,fix-target-helper,"import charmhelpers.core.hookenv as ch_hookenv return cinder_lvm.CinderLVMCharmWallaby() cfg_dict = {} _ = [cfg_dict.update({k: v}) for k, v in config] self.assertEqual('3', cfg_dict.get('val')) self.assertIsNone(cfg_dict.get('a')) def test_cinder_wallaby_upgrade(self): charm = self._patch_config_and_charm({}) charm.release = ""wallaby"" config2 = charm.cinder_configuration() self.assertIn(('target_helper', 'lioadm'), config2) self.assertIn(('target_port', 3261), config2) self.patch_object(cinder_lvm, 'juju_log', name='log_mock') charm = self._patch_config_and_charm({ 'config-flags': 'target_helper=tgtadm'}) config3 = charm.cinder_configuration() self.log_mock.assert_any_call( ""Configured target helper '{}' might not be installed as the "" ""default target_helper is lioadm."".format(""tgtadm""), level=ch_hookenv.WARNING) self.assertIn(('target_helper', 'tgtadm'), config3) self.assertIn(('target_port', 3261), config2)"," return cinder_lvm.CinderLVMCharm() self.assertEqual(config[-1][1], '3') self.assertNotIn('a', list(x[0] for x in config))",69,5
openstack%2Fneutron~master~Ie93833538056edeb4da4c38e5139390f1c124724,openstack/neutron,master,Ie93833538056edeb4da4c38e5139390f1c124724,[ovn][ipv6] Add some more tests to skiplist,MERGED,2023-06-28 07:51:36.000000000,2023-07-04 19:38:36.000000000,2023-07-04 19:34:40.000000000,"[{'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-28 07:51:36.000000000', 'files': ['zuul.d/tempest-singlenode.yaml'], 'web_link': 'https://opendev.org/openstack/neutron/commit/add2a6eb8cd9507b666f3d2c289059c4e477e8e8', 'message': '[ovn][ipv6] Add some more tests to skiplist\n\nThis is same to already skipped test in [1][2].\n\n[1] https://review.opendev.org/c/openstack/neutron/+/874112\n[2] https://review.opendev.org/c/openstack/neutron/+/885074\n\nRelated-Bug: #2007166\nChange-Id: Ie93833538056edeb4da4c38e5139390f1c124724\n'}]",0,887142,add2a6eb8cd9507b666f3d2c289059c4e477e8e8,9,4,1,13861,,,0,"[ovn][ipv6] Add some more tests to skiplist

This is same to already skipped test in [1][2].

[1] https://review.opendev.org/c/openstack/neutron/+/874112
[2] https://review.opendev.org/c/openstack/neutron/+/885074

Related-Bug: #2007166
Change-Id: Ie93833538056edeb4da4c38e5139390f1c124724
",git fetch https://review.opendev.org/openstack/neutron refs/changes/42/887142/1 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/tempest-singlenode.yaml'],1,add2a6eb8cd9507b666f3d2c289059c4e477e8e8,bug/2007166," (test_update_router_admin_state)|\ (test_dhcp_stateful_router)|\ (TestSecurityGroupsBasicOps)"""," (test_update_router_admin_state)""",3,1
openstack%2Fneutron~master~Ia3741abe5fa0c8ff65cf36d1ec31c089a1759f05,openstack/neutron,master,Ia3741abe5fa0c8ff65cf36d1ec31c089a1759f05,[FT] Move ``BaseOVSTestCase`` class to concurrency 1 executor,MERGED,2023-07-04 09:51:56.000000000,2023-07-04 19:37:36.000000000,2023-07-04 19:34:36.000000000,"[{'_account_id': 8313}, {'_account_id': 11975}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 09:51:56.000000000', 'files': ['tox.ini'], 'web_link': 'https://opendev.org/openstack/neutron/commit/26a2266cf4e2395b0146902de2eb0a6966a037ec', 'message': '[FT] Move ``BaseOVSTestCase`` class to concurrency 1 executor\n\nMove the ``BaseOVSTestCase`` class tests to the stestr executor with\nconcurrency=1. That will prevent that the minimum bandwidth tests\ninterfere among them.\n\nCloses-Bug: #2025740\nChange-Id: Ia3741abe5fa0c8ff65cf36d1ec31c089a1759f05\n'}]",0,887590,26a2266cf4e2395b0146902de2eb0a6966a037ec,9,3,1,16688,,,0,"[FT] Move ``BaseOVSTestCase`` class to concurrency 1 executor

Move the ``BaseOVSTestCase`` class tests to the stestr executor with
concurrency=1. That will prevent that the minimum bandwidth tests
interfere among them.

Closes-Bug: #2025740
Change-Id: Ia3741abe5fa0c8ff65cf36d1ec31c089a1759f05
",git fetch https://review.opendev.org/openstack/neutron refs/changes/90/887590/1 && git format-patch -1 --stdout FETCH_HEAD,['tox.ini'],1,26a2266cf4e2395b0146902de2eb0a6966a037ec,bug/2025740, stestr run --slowest --exclude-regex (.*MySQL\.|.*PostgreSQL\.|.*test_get_all_devices|.*TestMetadataAgent\.|.*BaseOVSTestCase\.) {posargs} stestr run --slowest --combine --concurrency 1 (.*MySQL\.|.*PostgreSQL\.|.*test_get_all_devices|.*TestMetadataAgent\.|.*BaseOVSTestCase\.) {posargs}, stestr run --slowest --exclude-regex (.*MySQL\.|.*PostgreSQL\.|.*test_get_all_devices|.*TestMetadataAgent\.) {posargs} stestr run --slowest --combine --concurrency 1 (.*MySQL\.|.*PostgreSQL\.|.*test_get_all_devices|.*TestMetadataAgent\.) {posargs},2,2
openstack%2Fbifrost~stable%2Fwallaby~I208182e65884d63548d78c68f676b899c562a2dc,openstack/bifrost,stable/wallaby,I208182e65884d63548d78c68f676b899c562a2dc,CI: Update cached cirros image to 0.5.3,MERGED,2023-06-12 13:29:32.000000000,2023-07-04 19:37:25.000000000,2023-07-04 19:34:32.000000000,"[{'_account_id': 15519}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-12 13:29:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/5aa7f13a68cc10c23fad7e5da3bf2f0133dfaa4f', 'message': ""CI: Update cached cirros image to 0.5.3\n\nBifrost CI is currently failing to fetch the cirros image from cache:\n\n    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'\n\nThis is caused by the removal of cirros-0.5.1 images from cache in\nchange Ibada405e0c1183559f428c749d0e54d0a45a2223.\n\nSwitch to cirros 0.5.3 instead.\n\nChange-Id: I208182e65884d63548d78c68f676b899c562a2dc\n""}, {'number': 2, 'created': '2023-07-04 17:31:19.000000000', 'files': ['playbooks/test-bifrost.yaml', 'zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/2ffa20c1cf5e89622548048f80fa46685837b481', 'message': ""CI: Update cached cirros image to 0.5.3\n\nBifrost CI is currently failing to fetch the cirros image from cache:\n\n    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'\n\nThis is caused by the removal of cirros-0.5.1 images from cache in\nchange Ibada405e0c1183559f428c749d0e54d0a45a2223.\n\nSwitch to cirros 0.5.3 instead.\n\nAlso remove the upgrade job since Wallaby is in EM.\n\nChange-Id: I208182e65884d63548d78c68f676b899c562a2dc\n""}]",0,885876,2ffa20c1cf5e89622548048f80fa46685837b481,9,2,2,15197,,,0,"CI: Update cached cirros image to 0.5.3

Bifrost CI is currently failing to fetch the cirros image from cache:

    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'

This is caused by the removal of cirros-0.5.1 images from cache in
change Ibada405e0c1183559f428c749d0e54d0a45a2223.

Switch to cirros 0.5.3 instead.

Also remove the upgrade job since Wallaby is in EM.

Change-Id: I208182e65884d63548d78c68f676b899c562a2dc
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/76/885876/2 && git format-patch -1 --stdout FETCH_HEAD,['playbooks/test-bifrost.yaml'],1,5aa7f13a68cc10c23fad7e5da3bf2f0133dfaa4f,, cirros_deploy_image_upstream_url: file:///opt/cache/files/cirros-0.5.3-x86_64-disk.img, cirros_deploy_image_upstream_url: file:///opt/cache/files/cirros-0.5.1-x86_64-disk.img,1,1
openstack%2Fbifrost~stable%2Fxena~I208182e65884d63548d78c68f676b899c562a2dc,openstack/bifrost,stable/xena,I208182e65884d63548d78c68f676b899c562a2dc,CI: Update cached cirros image to 0.5.3,MERGED,2023-07-04 17:32:25.000000000,2023-07-04 19:37:23.000000000,2023-07-04 19:34:31.000000000,"[{'_account_id': 15519}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 17:32:25.000000000', 'files': ['playbooks/test-bifrost.yaml', 'zuul.d/project.yaml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/e74d747385ead4975dc26bda5fc18773e8eb7807', 'message': ""CI: Update cached cirros image to 0.5.3\n\nBifrost CI is currently failing to fetch the cirros image from cache:\n\n    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'\n\nThis is caused by the removal of cirros-0.5.1 images from cache in\nchange Ibada405e0c1183559f428c749d0e54d0a45a2223.\n\nSwitch to cirros 0.5.3 instead.\n\nAlso remove the upgrade jobs since Xena is in EM.\n\nChange-Id: I208182e65884d63548d78c68f676b899c562a2dc\n""}]",0,887639,e74d747385ead4975dc26bda5fc18773e8eb7807,7,2,1,10239,,,0,"CI: Update cached cirros image to 0.5.3

Bifrost CI is currently failing to fetch the cirros image from cache:

    No such file or directory: '/opt/cache/files/cirros-0.5.1-x86_64-disk.img'

This is caused by the removal of cirros-0.5.1 images from cache in
change Ibada405e0c1183559f428c749d0e54d0a45a2223.

Switch to cirros 0.5.3 instead.

Also remove the upgrade jobs since Xena is in EM.

Change-Id: I208182e65884d63548d78c68f676b899c562a2dc
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/39/887639/1 && git format-patch -1 --stdout FETCH_HEAD,"['playbooks/test-bifrost.yaml', 'zuul.d/project.yaml']",2,e74d747385ead4975dc26bda5fc18773e8eb7807,,, - bifrost-upgrade-ubuntu-focal - bifrost-upgrade-keystone-centos8 - bifrost-upgrade-ubuntu-focal - bifrost-upgrade-keystone-centos8,1,5
openstack%2Fcharm-cinder-lvm~stable%2Fwallaby~I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc,openstack/charm-cinder-lvm,stable/wallaby,I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc,Fix target_helper config for >= Wallaby,NEW,2022-10-14 13:30:11.000000000,2023-07-04 19:14:16.000000000,,"[{'_account_id': 10058}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-10-14 13:30:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/dc6833d3ea52e2b06ef7771750efdd4d44c1c058', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit f9ce4b472100dae79c79d0f2b5ce1e0619ec8a0b)\n(cherry picked from commit c6536d63646ac9740a7223990b5e3baebcd38fea)\n(cherry picked from commit c4e0a9d6164b7fdcefc2999fe71a030ee48c0ec1)\n""}, {'number': 2, 'created': '2022-10-21 18:55:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/0dd832b737285f62b25fa9fa49b17693d2992e1f', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit f9ce4b472100dae79c79d0f2b5ce1e0619ec8a0b)\n(cherry picked from commit 0780db3b219ae1b9ae2f92273e578c0f9329ee30)\n(cherry picked from commit f94d6deabcd32b100a8b27d0f9c8064b53d79653)\n(cherry picked from commit 4a17d804fc50a1f9784752fb37af81dcef6665b6)\n""}, {'number': 3, 'created': '2023-03-23 17:56:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/7947e22ccb4b52171ae49fc506d6a561b1f8f349', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit ae8094a0aebd073caa3152a357cd94b47eb77029)\n(cherry picked from commit 059913ff141609648fbbaa5f50d2bdfdd3a3787e)\n(cherry picked from commit 04c180bc49c34142164e1b44fe767c3ca78865e6)\n(cherry picked from commit 846d283c95b313df59380b0f3386f86ce2e26208)\n""}, {'number': 4, 'created': '2023-04-25 21:28:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/150ff90f20b67f9dc68970fe044d8ffef89cc90e', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit 71718d359e32564c3666c84403cbc8399a8deb4d)\n(cherry picked from commit d11eac5a0e448f09f9dcbe3287cabd35639fafa1)\n(cherry picked from commit 55c37d3df0d8cf38d475bf75d324eeac61188d16)\n(cherry picked from commit 3fdc22c63f512bf3c6a6eb57b3362a16b0eb10ea)\n""}, {'number': 5, 'created': '2023-06-30 15:42:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/32b4868f75063743b0ee42df146ed18eeeee5b73', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit 3acbbbcf170d76733456156295866ae3470b14e7)\n(cherry picked from commit ec27742fe6e5b0973b777fbb1e3ad5317389d4ef)\n(cherry picked from commit f32575f42f37db2bcc89da9843cc5b38a0ec11c6)\n(cherry picked from commit 788a0de2e660a5ebedf56f78136a32efacf9524a)\n""}, {'number': 6, 'created': '2023-07-02 15:08:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/e0b4e0c07d5c301e7253e1f405ca563e0f1c4d29', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit 2628cacc1ee688833a4b1e892f854eb5aa09b99d)\n(cherry picked from commit 4cdbbebe8c4912e4be704922d70a84ece504eacb)\n(cherry picked from commit 04eced6ba015ac804e8e78f5bcf0f284a69b6844)\n(cherry picked from commit 2b6806990f0845552c22ea3af315c8bf5de56196)\n""}, {'number': 7, 'created': '2023-07-04 18:22:34.000000000', 'files': ['src/reactive/cinder_lvm_handlers.py', 'src/lib/charm/openstack/cinder_lvm.py', 'unit_tests/test_lib_charm_openstack_cinder_lvm.py'], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/f6740eca0e61ca331f1c124176417710ac6f6b1c', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit bc942d2d42efecdb7bd22ef61e2ba71fe3d93167)\n(cherry picked from commit 08b5eec158088d210c3833eae44c743db231ef48)\n(cherry picked from commit 3f6de9c75c81708bfb1e80e24776c9f88efdb649)\n(cherry picked from commit 0824ad812f2f82e6d2365337765f5bfd0def9f7a)\n""}]",2,861377,f6740eca0e61ca331f1c124176417710ac6f6b1c,28,4,7,10058,,,0,"Fix target_helper config for >= Wallaby

Starting with the release of Wallaby, the 'tgtadm' package is
now the default target helper, which doesn't work for cinder-lvm.
As such, we need to explicitly set the new package to 'lioadm'.

Closes-Bug: #1949074
Change-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc
(cherry picked from commit bc942d2d42efecdb7bd22ef61e2ba71fe3d93167)
(cherry picked from commit 08b5eec158088d210c3833eae44c743db231ef48)
(cherry picked from commit 3f6de9c75c81708bfb1e80e24776c9f88efdb649)
(cherry picked from commit 0824ad812f2f82e6d2365337765f5bfd0def9f7a)
",git fetch https://review.opendev.org/openstack/charm-cinder-lvm refs/changes/77/861377/2 && git format-patch -1 --stdout FETCH_HEAD,"['src/reactive/cinder_lvm_handlers.py', 'src/lib/charm/openstack/cinder_lvm.py', 'unit_tests/test_lib_charm_openstack_cinder_lvm.py']",3,dc6833d3ea52e2b06ef7771750efdd4d44c1c058,fix-target-helper,"import charmhelpers.core.hookenv as ch_hookenv return cinder_lvm.CinderLVMCharmWallaby() cfg_dict = {} _ = [cfg_dict.update({k: v}) for k, v in config] self.assertEqual('3', cfg_dict.get('val')) self.assertIsNone(cfg_dict.get('a')) def test_cinder_wallaby_upgrade(self): charm = self._patch_config_and_charm({}) charm.release = ""wallaby"" config2 = charm.cinder_configuration() self.assertIn(('target_helper', 'lioadm'), config2) self.assertIn(('target_port', 3261), config2) self.patch_object(cinder_lvm, 'juju_log', name='log_mock') charm = self._patch_config_and_charm({ 'config-flags': 'target_helper=tgtadm'}) config3 = charm.cinder_configuration() self.log_mock.assert_any_call( ""Configured target helper '{}' might not be installed as the "" ""default target_helper is lioadm."".format(""tgtadm""), level=ch_hookenv.WARNING) self.assertIn(('target_helper', 'tgtadm'), config3) self.assertIn(('target_port', 3261), config2)"," return cinder_lvm.CinderLVMCharm() self.assertEqual(config[-1][1], '3') self.assertNotIn('a', list(x[0] for x in config))",69,5
openstack%2Fcharm-cinder-lvm~stable%2Fzed~I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc,openstack/charm-cinder-lvm,stable/zed,I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc,Fix target_helper config for >= Wallaby,NEW,2022-10-16 18:59:44.000000000,2023-07-04 19:12:22.000000000,,"[{'_account_id': 10058}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-10-16 18:59:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/829ee8ae6909d625a872b5d1d2decc7f883da92e', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 2, 'created': '2022-10-16 19:00:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/4142d9e6a0319c7533214f18e4e4b286d43d7280', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n""}, {'number': 3, 'created': '2023-03-23 17:56:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/80bd107d4d229bf9da9a66ecfc4562aa6094fe29', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit ae8094a0aebd073caa3152a357cd94b47eb77029)\n""}, {'number': 4, 'created': '2023-04-25 21:27:17.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/d11eac5a0e448f09f9dcbe3287cabd35639fafa1', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit 71718d359e32564c3666c84403cbc8399a8deb4d)\n""}, {'number': 5, 'created': '2023-06-30 15:43:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/ec27742fe6e5b0973b777fbb1e3ad5317389d4ef', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit 3acbbbcf170d76733456156295866ae3470b14e7)\n""}, {'number': 6, 'created': '2023-07-02 15:09:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/4cdbbebe8c4912e4be704922d70a84ece504eacb', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit 2628cacc1ee688833a4b1e892f854eb5aa09b99d)\n""}, {'number': 7, 'created': '2023-07-04 18:23:38.000000000', 'files': ['src/reactive/cinder_lvm_handlers.py', 'src/lib/charm/openstack/cinder_lvm.py', 'unit_tests/test_lib_charm_openstack_cinder_lvm.py'], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/08b5eec158088d210c3833eae44c743db231ef48', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit bc942d2d42efecdb7bd22ef61e2ba71fe3d93167)\n""}]",2,861307,08b5eec158088d210c3833eae44c743db231ef48,24,4,7,20870,,,0,"Fix target_helper config for >= Wallaby

Starting with the release of Wallaby, the 'tgtadm' package is
now the default target helper, which doesn't work for cinder-lvm.
As such, we need to explicitly set the new package to 'lioadm'.

Closes-Bug: #1949074
Change-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc
(cherry picked from commit bc942d2d42efecdb7bd22ef61e2ba71fe3d93167)
",git fetch https://review.opendev.org/openstack/charm-cinder-lvm refs/changes/07/861307/4 && git format-patch -1 --stdout FETCH_HEAD,"['src/reactive/cinder_lvm_handlers.py', 'src/lib/charm/openstack/cinder_lvm.py', 'unit_tests/test_lib_charm_openstack_cinder_lvm.py']",3,829ee8ae6909d625a872b5d1d2decc7f883da92e,fix-target-helper,"import charmhelpers.core.hookenv as ch_hookenv return cinder_lvm.CinderLVMCharmWallaby() cfg_dict = {} _ = [cfg_dict.update({k: v}) for k, v in config] self.assertEqual('3', cfg_dict.get('val')) self.assertIsNone(cfg_dict.get('a')) def test_cinder_wallaby_upgrade(self): charm = self._patch_config_and_charm({}) charm.release = ""wallaby"" config2 = charm.cinder_configuration() self.assertIn(('target_helper', 'lioadm'), config2) self.assertIn(('target_port', 3261), config2) self.patch_object(cinder_lvm, 'juju_log', name='log_mock') charm = self._patch_config_and_charm({ 'config-flags': 'target_helper=tgtadm'}) config3 = charm.cinder_configuration() self.log_mock.assert_any_call( ""Configured target helper '{}' might not be installed as the "" ""default target_helper is lioadm."".format(""tgtadm""), level=ch_hookenv.WARNING) self.assertIn(('target_helper', 'tgtadm'), config3) self.assertIn(('target_port', 3261), config2)"," return cinder_lvm.CinderLVMCharm() self.assertEqual(config[-1][1], '3') self.assertNotIn('a', list(x[0] for x in config))",69,5
openstack%2Fcharm-cinder-lvm~stable%2Fxena~I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc,openstack/charm-cinder-lvm,stable/xena,I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc,Fix target_helper config for >= Wallaby,NEW,2022-10-14 13:30:59.000000000,2023-07-04 19:10:32.000000000,,"[{'_account_id': 10058}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-10-14 13:30:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/c4e0a9d6164b7fdcefc2999fe71a030ee48c0ec1', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit f9ce4b472100dae79c79d0f2b5ce1e0619ec8a0b)\n(cherry picked from commit c6536d63646ac9740a7223990b5e3baebcd38fea)\n""}, {'number': 2, 'created': '2022-10-21 18:55:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/4a17d804fc50a1f9784752fb37af81dcef6665b6', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit f9ce4b472100dae79c79d0f2b5ce1e0619ec8a0b)\n(cherry picked from commit 0780db3b219ae1b9ae2f92273e578c0f9329ee30)\n(cherry picked from commit f94d6deabcd32b100a8b27d0f9c8064b53d79653)\n""}, {'number': 3, 'created': '2023-03-23 17:56:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/5eeeca3df772c2421635201d72d437917b591fc8', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit ae8094a0aebd073caa3152a357cd94b47eb77029)\n(cherry picked from commit 059913ff141609648fbbaa5f50d2bdfdd3a3787e)\n(cherry picked from commit 04c180bc49c34142164e1b44fe767c3ca78865e6)\n""}, {'number': 4, 'created': '2023-04-25 21:27:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/3fdc22c63f512bf3c6a6eb57b3362a16b0eb10ea', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit 71718d359e32564c3666c84403cbc8399a8deb4d)\n(cherry picked from commit d11eac5a0e448f09f9dcbe3287cabd35639fafa1)\n(cherry picked from commit 55c37d3df0d8cf38d475bf75d324eeac61188d16)\n""}, {'number': 5, 'created': '2023-06-30 15:42:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/788a0de2e660a5ebedf56f78136a32efacf9524a', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit 3acbbbcf170d76733456156295866ae3470b14e7)\n(cherry picked from commit ec27742fe6e5b0973b777fbb1e3ad5317389d4ef)\n(cherry picked from commit f32575f42f37db2bcc89da9843cc5b38a0ec11c6)\n""}, {'number': 6, 'created': '2023-07-02 15:08:55.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/2b6806990f0845552c22ea3af315c8bf5de56196', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit 2628cacc1ee688833a4b1e892f854eb5aa09b99d)\n(cherry picked from commit 4cdbbebe8c4912e4be704922d70a84ece504eacb)\n(cherry picked from commit 04eced6ba015ac804e8e78f5bcf0f284a69b6844)\n""}, {'number': 7, 'created': '2023-07-04 18:23:01.000000000', 'files': ['src/reactive/cinder_lvm_handlers.py', 'src/lib/charm/openstack/cinder_lvm.py', 'unit_tests/test_lib_charm_openstack_cinder_lvm.py'], 'web_link': 'https://opendev.org/openstack/charm-cinder-lvm/commit/0824ad812f2f82e6d2365337765f5bfd0def9f7a', 'message': ""Fix target_helper config for >= Wallaby\n\nStarting with the release of Wallaby, the 'tgtadm' package is\nnow the default target helper, which doesn't work for cinder-lvm.\nAs such, we need to explicitly set the new package to 'lioadm'.\n\nCloses-Bug: #1949074\nChange-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc\n(cherry picked from commit bc942d2d42efecdb7bd22ef61e2ba71fe3d93167)\n(cherry picked from commit 08b5eec158088d210c3833eae44c743db231ef48)\n(cherry picked from commit 3f6de9c75c81708bfb1e80e24776c9f88efdb649)\n""}]",2,861378,0824ad812f2f82e6d2365337765f5bfd0def9f7a,28,4,7,10058,,,0,"Fix target_helper config for >= Wallaby

Starting with the release of Wallaby, the 'tgtadm' package is
now the default target helper, which doesn't work for cinder-lvm.
As such, we need to explicitly set the new package to 'lioadm'.

Closes-Bug: #1949074
Change-Id: I4d88d1e30d8b2b133bd575a76d0dc5e566415cbc
(cherry picked from commit bc942d2d42efecdb7bd22ef61e2ba71fe3d93167)
(cherry picked from commit 08b5eec158088d210c3833eae44c743db231ef48)
(cherry picked from commit 3f6de9c75c81708bfb1e80e24776c9f88efdb649)
",git fetch https://review.opendev.org/openstack/charm-cinder-lvm refs/changes/78/861378/6 && git format-patch -1 --stdout FETCH_HEAD,"['src/reactive/cinder_lvm_handlers.py', 'src/lib/charm/openstack/cinder_lvm.py', 'unit_tests/test_lib_charm_openstack_cinder_lvm.py']",3,c4e0a9d6164b7fdcefc2999fe71a030ee48c0ec1,fix-target-helper,"import charmhelpers.core.hookenv as ch_hookenv return cinder_lvm.CinderLVMCharmWallaby() cfg_dict = {} _ = [cfg_dict.update({k: v}) for k, v in config] self.assertEqual('3', cfg_dict.get('val')) self.assertIsNone(cfg_dict.get('a')) def test_cinder_wallaby_upgrade(self): charm = self._patch_config_and_charm({}) charm.release = ""wallaby"" config2 = charm.cinder_configuration() self.assertIn(('target_helper', 'lioadm'), config2) self.assertIn(('target_port', 3261), config2) self.patch_object(cinder_lvm, 'juju_log', name='log_mock') charm = self._patch_config_and_charm({ 'config-flags': 'target_helper=tgtadm'}) config3 = charm.cinder_configuration() self.log_mock.assert_any_call( ""Configured target helper '{}' might not be installed as the "" ""default target_helper is lioadm."".format(""tgtadm""), level=ch_hookenv.WARNING) self.assertIn(('target_helper', 'tgtadm'), config3) self.assertIn(('target_port', 3261), config2)"," return cinder_lvm.CinderLVMCharm() self.assertEqual(config[-1][1], '3') self.assertNotIn('a', list(x[0] for x in config))",69,5
openstack%2Fnova~master~Id4768719abc8bad0dc76789e7a04df53ddebb9d8,openstack/nova,master,Id4768719abc8bad0dc76789e7a04df53ddebb9d8,Allow to set attachment-id for bdm tests,ABANDONED,2023-07-03 13:23:17.000000000,2023-07-04 18:57:51.000000000,,"[{'_account_id': 4393}, {'_account_id': 11604}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 13:23:17.000000000', 'files': ['nova/tests/unit/fake_block_device.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/0bf6dd0c044cc090a24fd7bfbdc102806661d086', 'message': 'Allow to set attachment-id for bdm tests\n\nAdded user to add attachment-id to FakeDbBlockDeviceDict object\n\nChange-Id: Id4768719abc8bad0dc76789e7a04df53ddebb9d8\n'}]",0,887515,0bf6dd0c044cc090a24fd7bfbdc102806661d086,4,3,1,34860,,,0,"Allow to set attachment-id for bdm tests

Added user to add attachment-id to FakeDbBlockDeviceDict object

Change-Id: Id4768719abc8bad0dc76789e7a04df53ddebb9d8
",git fetch https://review.opendev.org/openstack/nova refs/changes/15/887515/1 && git format-patch -1 --stdout FETCH_HEAD,['nova/tests/unit/fake_block_device.py'],1,0bf6dd0c044cc090a24fd7bfbdc102806661d086,dangling-volumes," attachment_id = bdm_dict.pop('attachment_id', uuids.fake_attch_id) fake_db_fields['attachment_id'] = attachment_id", fake_db_fields['attachment_id'] = None,2,1
openstack%2Fpython-glanceclient~master~I76c6ea00523d93bad900776866de6ba22bc516b4,openstack/python-glanceclient,master,I76c6ea00523d93bad900776866de6ba22bc516b4,do_image_import: always pass remote_* to gc.images.image_import,MERGED,2023-04-18 01:31:13.000000000,2023-07-04 18:51:14.000000000,2023-07-04 18:50:19.000000000,"[{'_account_id': 4393}, {'_account_id': 9303}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-04-18 01:31:13.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/python-glanceclient/commit/a0ce3c2a40dee792809d5d0c91cb526ab498cf3b', 'message': 'do_image_import: always pass remote_* to gc.images.image_import\n\nIn do_image_import, gc.images.image_import may be called from two\ndifferent places, depending on the command used (""glance image-import""\nor ""glance image-create-via-import""). Make sure we always pass the\nremote_* arguments to gc.images.import.\n\nChange-Id: I76c6ea00523d93bad900776866de6ba22bc516b4\nPartial-Bug: #2012442\n'}, {'number': 2, 'created': '2023-05-03 13:38:50.000000000', 'files': ['glanceclient/tests/unit/v2/test_shell_v2.py', 'glanceclient/v2/shell.py'], 'web_link': 'https://opendev.org/openstack/python-glanceclient/commit/1ea41f042c2fe4d2184828ef5277d1c119bfbd48', 'message': 'do_image_import: always pass remote_* to gc.images.image_import\n\nIn do_image_import, gc.images.image_import may be called from two\ndifferent places, depending on the command used (""glance image-import""\nor ""glance image-create-via-import""). Make sure we always pass the\nremote_* arguments to gc.images.import.\n\nChange-Id: I76c6ea00523d93bad900776866de6ba22bc516b4\nPartial-Bug: #2012442\n'}]",3,880697,1ea41f042c2fe4d2184828ef5277d1c119bfbd48,16,3,2,8122,,,0,"do_image_import: always pass remote_* to gc.images.image_import

In do_image_import, gc.images.image_import may be called from two
different places, depending on the command used (""glance image-import""
or ""glance image-create-via-import""). Make sure we always pass the
remote_* arguments to gc.images.import.

Change-Id: I76c6ea00523d93bad900776866de6ba22bc516b4
Partial-Bug: #2012442
",git fetch https://review.opendev.org/openstack/python-glanceclient refs/changes/97/880697/1 && git format-patch -1 --stdout FETCH_HEAD,"['glanceclient/tests/unit/v2/test_shell_v2.py', 'glanceclient/v2/shell.py']",2,a0ce3c2a40dee792809d5d0c91cb526ab498cf3b,bug/2012442," remote_region=remote_region, remote_image_id=remote_image_id, remote_service_interface=remote_service_interface,",,6,1
openstack%2Fcinder~master~I352f6e7aaefe08295a92f47864017a332503ad12,openstack/cinder,master,I352f6e7aaefe08295a92f47864017a332503ad12,DNM: Zuul: Adding Job with LVM c-vol and Ceph c-bak,NEW,2023-06-20 17:57:58.000000000,2023-07-04 17:55:09.000000000,,"[{'_account_id': 22348}, {'_account_id': 35075}]","[{'number': 1, 'created': '2023-06-20 17:57:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/2ab3c677179a1209bbb7cc61aab5ae27354b07cc', 'message': 'DNM: Zuul: Adding Job with LVM c-vol and Ceph c-bak\n\nDepends-On: Iff8e1e90ab3c7b519819577ec3aafff838e6934f\nChange-Id: I352f6e7aaefe08295a92f47864017a332503ad12\n'}, {'number': 2, 'created': '2023-06-23 15:18:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/34e8a41e1ad3a0737fe3da852050573db9f5b336', 'message': 'DNM: Zuul: Adding Job with LVM c-vol and Ceph c-bak\n\nDepends-On: Iff8e1e90ab3c7b519819577ec3aafff838e6934f\nChange-Id: I352f6e7aaefe08295a92f47864017a332503ad12\n'}, {'number': 3, 'created': '2023-06-23 15:42:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/e468169d243742fe67a9b0dcd2ba7294dd6470c7', 'message': 'DNM: Zuul: Adding Job with LVM c-vol and Ceph c-bak\n\nDepends-On: Iff8e1e90ab3c7b519819577ec3aafff838e6934f\nChange-Id: I352f6e7aaefe08295a92f47864017a332503ad12\n'}, {'number': 4, 'created': '2023-06-28 10:39:57.000000000', 'files': ['tempest_skiplist.txt', '.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/cinder/commit/28625f7019a2abf30f5ec0efc55053ae58b6738f', 'message': 'DNM: Zuul: Adding Job with LVM c-vol and Ceph c-bak\n\nDepends-On: Iff8e1e90ab3c7b519819577ec3aafff838e6934f\nChange-Id: I352f6e7aaefe08295a92f47864017a332503ad12\n'}]",2,886525,28625f7019a2abf30f5ec0efc55053ae58b6738f,56,2,4,20813,,,0,"DNM: Zuul: Adding Job with LVM c-vol and Ceph c-bak

Depends-On: Iff8e1e90ab3c7b519819577ec3aafff838e6934f
Change-Id: I352f6e7aaefe08295a92f47864017a332503ad12
",git fetch https://review.opendev.org/openstack/cinder refs/changes/25/886525/4 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/tests/functional/api_sample_tests/test_attachments_backup.py', '.zuul.yaml']",2,2ab3c677179a1209bbb7cc61aab5ae27354b07cc,," - cinder-tempest-ceph-multibackend-cbak: irrelevant-files: *gate-irrelevant-files voting: false - job: name: cinder-tempest-ceph-multibackend-cbak parent: cinder-plugin-ceph-tempest description: | Cinder tempest job based on LVM for volume backend and RBD for c-bak. vars: tempest_test_include_list: 'cinder_tempest_plugin.api.volume.admin.test_volume_backup.VolumesBackupsTest.*' tempest_test_exclude_list: '{{ ansible_user_dir }}/{{ zuul.projects[""opendev.org/openstack/cinder""].src_dir }}/tempest_skiplist.txt' devstack_services: c-bak: true devstack_localrc: CINDER_ENABLED_BACKENDS: 'lvm:lvm' CINDER_BACKUP_DRIVER: ceph",,111,0
openstack%2Fmanila-specs~master~I7ecbad25660f10170172fc2a07507000bfafde75,openstack/manila-specs,master,I7ecbad25660f10170172fc2a07507000bfafde75,Add spec-lite NetApp ActiveIQ Weigher for Bobcat,MERGED,2023-04-27 14:58:52.000000000,2023-07-04 17:51:50.000000000,2023-07-04 17:50:52.000000000,"[{'_account_id': 16643}, {'_account_id': 18816}, {'_account_id': 22348}, {'_account_id': 29632}, {'_account_id': 32919}, {'_account_id': 33301}]","[{'number': 1, 'created': '2023-04-27 14:58:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/27899650680ba8f9876236c37696e147e243a9a1', 'message': 'Add spec-lite NetApp ActiveIQ Weigher for Bobcat\n\nAdds the spec-lite for the integration between NetApp ActiveIQ\nsoftware with Manila Scheduler weigher phase.\nIt also adds the bobca release directory specs.\n\nChange-Id: I7ecbad25660f10170172fc2a07507000bfafde75\n'}, {'number': 2, 'created': '2023-06-28 20:24:11.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/a8cede3be7de4318e6542b85ab2d7a342c710d29', 'message': 'Add spec-lite NetApp ActiveIQ Weigher for Bobcat\n\nAdds the spec-lite for the integration between NetApp ActiveIQ\nsoftware with Manila Scheduler weigher phase.\nIt also adds the bobca release directory specs.\n\nChange-Id: I7ecbad25660f10170172fc2a07507000bfafde75\n'}, {'number': 3, 'created': '2023-06-29 20:55:29.000000000', 'files': ['specs/bobcat/spec-lite-netapp-activeiq-weigher.rst', 'doc/source/index.rst'], 'web_link': 'https://opendev.org/openstack/manila-specs/commit/69acbe290ccf2a6c3fa60163bdfaa3b2823d9475', 'message': 'Add spec-lite NetApp ActiveIQ Weigher for Bobcat\n\nAdds the spec-lite for the integration between NetApp ActiveIQ\nsoftware with Manila Scheduler weigher phase.\nIt also adds the bobcat release directory specs.\n\nChange-Id: I7ecbad25660f10170172fc2a07507000bfafde75\n'}]",31,881187,69acbe290ccf2a6c3fa60163bdfaa3b2823d9475,18,6,3,31721,,,0,"Add spec-lite NetApp ActiveIQ Weigher for Bobcat

Adds the spec-lite for the integration between NetApp ActiveIQ
software with Manila Scheduler weigher phase.
It also adds the bobcat release directory specs.

Change-Id: I7ecbad25660f10170172fc2a07507000bfafde75
",git fetch https://review.opendev.org/openstack/manila-specs refs/changes/87/881187/3 && git format-patch -1 --stdout FETCH_HEAD,"['specs/bobcat/spec-lite-add-human-readable-export-locations.rst', 'doc/source/index.rst']",2,27899650680ba8f9876236c37696e147e243a9a1,,Bobcat approved specs ===================== .. toctree:: :glob: :maxdepth: 1 specs/bobcat/* ,,84,0
openstack%2Fcharm-ceph-dashboard~stable%2Fquincy.2~I8edfa2d41d5fea048f14b7b50558a40dc555799e,openstack/charm-ceph-dashboard,stable/quincy.2,I8edfa2d41d5fea048f14b7b50558a40dc555799e,Adds source for application in test bundles,MERGED,2023-07-04 07:23:10.000000000,2023-07-04 17:45:55.000000000,2023-07-04 17:45:55.000000000,"[{'_account_id': 20648}, {'_account_id': 22348}, {'_account_id': 33717}, {'_account_id': 34952}]","[{'number': 1, 'created': '2023-07-04 07:23:10.000000000', 'files': ['tests/bundles/kinetic-zed.yaml', 'tests/bundles/lunar-antelope.yaml', 'tests/bundles/focal-yoga.yaml', 'tests/bundles/jammy-yoga.yaml'], 'web_link': 'https://opendev.org/openstack/charm-ceph-dashboard/commit/66db5575f9335d7e4bee65894dc910071dc5f41d', 'message': 'Adds source for application in test bundles\n\nAlso set channels\n\ncherry pick from I21d7865c41898551f3f1b1859388139b4cff2bd1\n\nChange-Id: I8edfa2d41d5fea048f14b7b50558a40dc555799e\n'}]",1,887549,66db5575f9335d7e4bee65894dc910071dc5f41d,8,4,1,15382,,,0,"Adds source for application in test bundles

Also set channels

cherry pick from I21d7865c41898551f3f1b1859388139b4cff2bd1

Change-Id: I8edfa2d41d5fea048f14b7b50558a40dc555799e
",git fetch https://review.opendev.org/openstack/charm-ceph-dashboard refs/changes/49/887549/1 && git format-patch -1 --stdout FETCH_HEAD,"['tests/bundles/kinetic-zed.yaml', 'tests/bundles/lunar-antelope.yaml', 'tests/bundles/focal-yoga.yaml', 'tests/bundles/jammy-yoga.yaml']",4,66db5575f9335d7e4bee65894dc910071dc5f41d,,variables: openstack-origin: &openstack-origin distro source: *openstack-origin channel: quincy/edge source: *openstack-origin channel: quincy/edge channel: 8.0/edge channel: 8.0/edge channel: quincy/edge options: source: *openstack-origin channel: quincy/edge options: source: *openstack-origin source: *openstack-origin channel: quincy/edge, channel: latest/edge channel: latest/edge channel: latest/edge channel: latest/edge channel: latest/edge channel: latest/edge channel: latest/edge,56,9
openstack%2Fcinder-tempest-plugin~master~I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b,openstack/cinder-tempest-plugin,master,I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b,Test srbac on backups,MERGED,2023-03-26 14:56:52.000000000,2023-07-04 17:18:01.000000000,2023-07-04 17:18:01.000000000,"[{'_account_id': 4523}, {'_account_id': 8556}, {'_account_id': 9600}, {'_account_id': 10459}, {'_account_id': 19262}, {'_account_id': 20813}, {'_account_id': 22348}, {'_account_id': 34510}, {'_account_id': 35621}]","[{'number': 1, 'created': '2023-03-26 14:56:52.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/64593d3121a4af6d8b129a41132aff6aba2ab54b', 'message': 'Test srbac on backups\n\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 2, 'created': '2023-03-27 06:07:06.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/8583c1e05059a70dd748816d6de9202fcef6c68f', 'message': 'Test srbac on backups\n\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 3, 'created': '2023-04-03 09:48:01.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/9c324ef43e5a5bcbf636237f782db26ec7ff0b2a', 'message': 'Test srbac on backups\n\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 4, 'created': '2023-04-13 11:09:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/3481bf20efc2756bb4b01c424fe41b56daecb71a', 'message': 'Test srbac on backups\n\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 5, 'created': '2023-04-13 12:26:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/e3c07347cfd94869a1a18f0c114160b5edb00f17', 'message': 'Test srbac on backups\n\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 6, 'created': '2023-04-16 13:34:47.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/c4900cbdf3d3cf50d63f6c8d297eac2df1b5ae88', 'message': 'Test srbac on backups\n\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 7, 'created': '2023-04-19 19:52:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/7019587037de85b33b9722946693b763bba87cef', 'message': 'Test srbac on backups\n\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 8, 'created': '2023-04-20 08:25:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/4c53de7bdcb19cf53032dd2d6da4bade1625e18c', 'message': 'Test srbac on backups\n\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 9, 'created': '2023-04-24 09:06:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/775aad077d43f67ae3a31c51dacdb1f11d8f8a7e', 'message': 'Test srbac on backups\n\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 10, 'created': '2023-04-24 09:46:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/1bca7430c6ffbf483596563c666aace448e31bc6', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 11, 'created': '2023-04-24 13:50:22.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/1332cb524ee3e46be661af2f26aee9d1c31be0ea', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 12, 'created': '2023-04-24 13:55:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/9dd8c9a2742c3e45223bf4d1f164e856d7666f3c', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 13, 'created': '2023-04-24 14:25:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/b9b4ecb28a742ef70b8eab568cf0a25653f4ab90', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 14, 'created': '2023-05-03 05:53:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/8e6765c12b61c955237d1e56a0dabeaf0183afb3', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 15, 'created': '2023-05-03 09:18:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/1a93d05355b03b407c61d161fc52a9fd3f8d8125', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 16, 'created': '2023-05-03 11:47:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/1a254f508c92feed0c7e22564f1ec73b3d68d8af', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 17, 'created': '2023-05-03 13:08:57.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/0d0a545df534eeed7781efe212a73a760928d7c1', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 18, 'created': '2023-05-03 16:25:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/0e04f3923fab87d6d80d0eb4c6373a869c168c3c', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 19, 'created': '2023-05-04 05:53:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/47c720525730fb823cff0f2f746a3e6e9eb31c42', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 20, 'created': '2023-05-04 06:04:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/3f774d32e912dd620e53e4e4b3f34f74bf3dc2b8', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 21, 'created': '2023-05-07 15:16:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/f946dc58fbb9d9277ff2bdd5867ebf7f2943dd1c', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 22, 'created': '2023-05-08 17:10:25.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/dfd30a69b8fcab990bd45adf3cf7975196b9ee7b', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\n'}, {'number': 23, 'created': '2023-05-10 15:12:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/aaf2762ea154748d1720dcd04ca8b3dc781bb7a6', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 24, 'created': '2023-05-16 08:52:08.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/65af6aa4ba5850595be08510cb922d8433539a3a', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 25, 'created': '2023-05-22 06:42:03.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/7821ed6b05fd987fcb3b31e1f8e7b1abbde24a16', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 26, 'created': '2023-05-22 09:54:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/d7fe5f2c66d5ab8f3b0ae5c0a8006517f4045f35', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 27, 'created': '2023-05-30 13:21:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/b91e0c24deb69f42bb192f837e1f8dfe49357702', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 28, 'created': '2023-05-31 14:04:09.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/a3150b526728f497a5e1deacfa23d4ac859be623', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}, {'number': 29, 'created': '2023-06-07 08:31:14.000000000', 'files': ['cinder_tempest_plugin/rbac/v3/base.py', 'cinder_tempest_plugin/rbac/v3/test_backups.py'], 'web_link': 'https://opendev.org/openstack/cinder-tempest-plugin/commit/bbb3cd612418ec5df17944b00f4e82c172aff821', 'message': 'Test srbac on backups\n\nTesting reader, member and admin roles.\nThis file contains test 2 classes for API version 3.3 and 3.9\n\nSigned-off-by: Yosi Ben Shimon <ybenshim@redhat.com>\nChange-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b\n'}]",165,878587,bbb3cd612418ec5df17944b00f4e82c172aff821,206,9,29,35834,,,0,"Test srbac on backups

Testing reader, member and admin roles.
This file contains test 2 classes for API version 3.3 and 3.9

Signed-off-by: Yosi Ben Shimon <ybenshim@redhat.com>
Change-Id: I9e6ea838fa7a93fcfc3d4ef7f807224aadee187b
",git fetch https://review.opendev.org/openstack/cinder-tempest-plugin refs/changes/87/878587/20 && git format-patch -1 --stdout FETCH_HEAD,['cinder_tempest_plugin/rbac/v3/test_backups.py'],1,64593d3121a4af6d8b129a41132aff6aba2ab54b,secure-rbac,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. from oslo_serialization import base64 from oslo_serialization import jsonutils as json from tempest.common import waiters from tempest.config import CONF from tempest.lib import decorators from tempest.lib import exceptions from tempest.lib.common.utils import data_utils from cinder_tempest_plugin.api.volume import base from cinder_tempest_plugin.rbac.v3 import base as rbac_base class RbacV3BackupsTests(rbac_base.VolumeV3RbacBaseTests): @classmethod def setup_clients(cls): super().setup_clients() cls.persona = getattr(cls, 'os_%s' % cls.credentials[0]) cls.client = cls.persona.backups_client_latest admin_client = cls.os_project_admin cls.admin_backups_client = admin_client.backups_client_latest cls.admin_volumes_client = admin_client.volumes_client_latest @classmethod def setup_credentials(cls): super().setup_credentials() cls.os_primary = getattr(cls, 'os_%s' % cls.credentials[0]) @classmethod def resource_setup(self): super(RbacV3BackupsTests, self).resource_setup() # create a volume self.volume = self.admin_volumes_client.create_volume( size=CONF.volume.volume_size )['volume'] waiters.wait_for_volume_resource_status( self.admin_volumes_client, self.volume['id'], 'available' ) # create backup self.backup = self.admin_backups_client.create_backup( volume_id=self.volume['id'] )['backup'] waiters.wait_for_volume_resource_status( self.admin_backups_client, self.backup['id'], 'available' ) # cleanup self.addClassResourceCleanup( self.admin_backups_client.delete_backup, self.backup['id'] ) self.addClassResourceCleanup( self.admin_volumes_client.delete_volume, self.volume['id'] ) def _encode_backup(self, backup): retval = json.dumps(backup) return base64.encode_as_text(retval) def _decode_url(self, backup_url): return json.loads(base64.decode_as_text(backup_url)) def _modify_backup_url(self, backup_url, changes): backup = self._decode_url(backup_url) backup.update(changes) return self._encode_backup(backup) class BackupReaderTests(RbacV3BackupsTests, base.BaseVolumeTest): """"""Test API with microversion greater than 3.3"""""" min_microversion = '3.3' credentials = ['project_reader', 'project_admin'] @decorators.idempotent_id('5287f280-c328-11ed-a7ea-84c5a6290b8f') def test_list_backups(self): self.do_request(method='list_backups', expected_status=200) @decorators.idempotent_id('4a3722ec-c34e-11ed-a7ea-84c5a6290b8f') def test_list_project_backups(self): self.do_request( method='list_backups', expected_status=200, project_id=self.client.project_id ) @decorators.idempotent_id('5bc8c6d0-c350-11ed-a7ea-84c5a6290b8f') def test_show_backup(self): self.do_request( method='show_backup', expected_status=200, backup_id=self.backup['id'] ) @decorators.idempotent_id('6e6b479e-c351-11ed-a7ea-84c5a6290b8f') def test_delete_backup(self): self.do_request( method='delete_backup', expected_status=exceptions.Forbidden, backup_id=self.backup['id'] ) @decorators.idempotent_id('6b93e070-c352-11ed-a7ea-84c5a6290b8f') def test_restore_backup(self): self.do_request( method='restore_backup', expected_status=exceptions.Forbidden, backup_id=self.backup['id'], name='new-backup-vol' ) @decorators.idempotent_id('8d951774-c353-11ed-a7ea-84c5a6290b8f') def test_create_backup(self): self.do_request( method='create_backup', expected_status=exceptions.Forbidden, volume_id=self.volume['id'] ) @decorators.idempotent_id('8f8a0252-c366-11ed-a7ea-84c5a6290b8f') def test_export_backup(self): self.do_request( method='export_backup', expected_status=exceptions.Forbidden, backup_id=self.backup['id'] ) @decorators.idempotent_id('c7546736-c366-11ed-a7ea-84c5a6290b8f') def test_import_backup(self): export_backup = ( self.admin_backups_client.export_backup( self.backup['id'] )['backup-record'] ) self.assertTrue( export_backup['backup_service'].startswith('cinder.backup.drivers') ) new_id = data_utils.rand_uuid() new_url = self._modify_backup_url( export_backup['backup_url'], {'id': new_id}) self.do_request( method='import_backup', expected_status=exceptions.Forbidden, backup_service=export_backup['backup_service'], backup_url=new_url ) @decorators.idempotent_id('04391302-c369-11ed-a7ea-84c5a6290b8f') def test_reset_backup_status(self): self.do_request( method='reset_backup_status', expected_status=exceptions.Forbidden, backup_id=self.backup['id'], status=""error"" ) class BackupReaderTests39(RbacV3BackupsTests, base.BaseVolumeTest): """"""Test API with microversion greater than 3.3"""""" min_microversion = '3.9' credentials = ['project_reader', 'project_admin'] @decorators.idempotent_id('505a0bf2-c354-11ed-a7ea-84c5a6290b8f') def test_update_backup(self): update_kwargs = {""description"": ""Updated backup description""} self.do_request( method='update_backup', expected_status=exceptions.Forbidden, backup_id=self.backup['id'], **update_kwargs ) ",,185,0
openstack%2Fnetworking-generic-switch~master~If4ca9c58d7f30b40992d0f1aee7e915c6978e0ca,openstack/networking-generic-switch,master,If4ca9c58d7f30b40992d0f1aee7e915c6978e0ca,Do not make actual device changes in bind_port(),NEW,2022-06-24 16:03:24.000000000,2023-07-04 17:10:33.000000000,,"[{'_account_id': 14826}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-06-24 16:03:24.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/networking-generic-switch/commit/c225b79af4129bc53d2c98c4bfde3e8236f8e1de', 'message': 'Do not make actual device changes in bind_port()\n\nAccording to the bind_port specification (as summed up in its docstring),\nwe should not make any change when bind_port() is called, because binding\nresults might end up not getting committed.\n\nTo satisfy this specification, this patch moves actual device\nreconfiguration to update_port_postcommit().\n\nIn addition, this will likely make it easier to improve performance of\nbulk port creation in the future, see https://bugs.launchpad.net/neutron/+bug/1976270\n\nNote that this change introduces a different retry behaviour when we fail\nto configure a device.  Since bind_port() is retried several times by\nNeutron, we indirectly benefited from these retries, but this is no longer\nthe case.  However, NGS already has an internal retry mechanism for SSH\nconnection errors, which should cover most network-related issues.\n\nTo sum up, errors that are no longer covered by a retry are the ones that\nhappen after we successfully connect to the device.  For instance, the\nswitch port may not exist on the device, or the device could be in an\nunexpected state such as a port being currently part of an unexpected\nVLAN.  This kind of errors are unlikely to be solved by retrying, so the\nnew behaviour should be fine and will even allow to return the error much\nfaster to the end-user.\n\nChange-Id: If4ca9c58d7f30b40992d0f1aee7e915c6978e0ca\n'}, {'number': 2, 'created': '2023-07-04 15:23:11.000000000', 'files': ['networking_generic_switch/tests/unit/test_generic_switch_mech.py', 'networking_generic_switch/generic_switch_mech.py'], 'web_link': 'https://opendev.org/openstack/networking-generic-switch/commit/0088fde1debfd80ab2508bfea9f2cff2a1602e80', 'message': 'Do not make actual device changes in bind_port()\n\nAccording to the bind_port specification (as summed up in its docstring),\nwe should not make any change when bind_port() is called, because binding\nresults might end up not getting committed.\n\nTo satisfy this specification, this patch moves actual device\nreconfiguration to update_port_postcommit().\n\nThis makes the behaviour symmetrical for port creation (handled in\nupdate_port_postcommit) and port deletion (handled in delete_port_postcommit).\n\nIn addition, this will make it easier to improve performance of port\ncreation, see https://bugs.launchpad.net/networking-generic-switch/+bug/2024385\n\nNote that this change introduces a different retry behaviour when we fail\nto configure a device.  Since bind_port() is retried several times by\nNeutron, we indirectly benefited from these retries, but this is no longer\nthe case.  However, NGS already has an internal retry mechanism for SSH\nconnection errors, which should cover most network-related issues.\n\nTo sum up, errors that are no longer covered by a retry are the ones that\nhappen after we successfully connect to the device.  For instance, the\nswitch port may not exist on the device, or the device could be in an\nunexpected state such as a port being currently part of an unexpected\nVLAN.  This kind of errors are unlikely to be solved by retrying, so the\nnew behaviour should be fine and will even allow to return the error much\nfaster to the end-user.\n\nRelated-Bug: #2024385\n\nChange-Id: If4ca9c58d7f30b40992d0f1aee7e915c6978e0ca\n'}]",4,847592,0088fde1debfd80ab2508bfea9f2cff2a1602e80,9,2,2,34936,,,0,"Do not make actual device changes in bind_port()

According to the bind_port specification (as summed up in its docstring),
we should not make any change when bind_port() is called, because binding
results might end up not getting committed.

To satisfy this specification, this patch moves actual device
reconfiguration to update_port_postcommit().

This makes the behaviour symmetrical for port creation (handled in
update_port_postcommit) and port deletion (handled in delete_port_postcommit).

In addition, this will make it easier to improve performance of port
creation, see https://bugs.launchpad.net/networking-generic-switch/+bug/2024385

Note that this change introduces a different retry behaviour when we fail
to configure a device.  Since bind_port() is retried several times by
Neutron, we indirectly benefited from these retries, but this is no longer
the case.  However, NGS already has an internal retry mechanism for SSH
connection errors, which should cover most network-related issues.

To sum up, errors that are no longer covered by a retry are the ones that
happen after we successfully connect to the device.  For instance, the
switch port may not exist on the device, or the device could be in an
unexpected state such as a port being currently part of an unexpected
VLAN.  This kind of errors are unlikely to be solved by retrying, so the
new behaviour should be fine and will even allow to return the error much
faster to the end-user.

Related-Bug: #2024385

Change-Id: If4ca9c58d7f30b40992d0f1aee7e915c6978e0ca
",git fetch https://review.opendev.org/openstack/networking-generic-switch refs/changes/92/847592/2 && git format-patch -1 --stdout FETCH_HEAD,"['networking_generic_switch/tests/unit/test_generic_switch_mech.py', 'networking_generic_switch/generic_switch_mech.py']",2,c225b79af4129bc53d2c98c4bfde3e8236f8e1de,g5k_agent_upstream_1,"from neutron_lib import constants as const network = context.network.current # Necessary because the ""provisioning_complete"" event triggers # an additional call to update_port_postcommit(). We don't # want to configure the port a second time. if port.get('status') == const.PORT_STATUS_ACTIVE: LOG.debug(""Port %(port_id)s is already active, "" ""not doing anything"", {'port_id': port['id']}) return # If binding has already succeeded, we should have valid links # at this point, but check just in case. if not self._is_link_valid(port, network): return for link in local_link_information: port_id = link.get('port_id') # If segmentation ID is None, set vlan 1 segmentation_id = network.get('provider:segmentation_id') or 1 LOG.debug(""Putting switch port %(switch_port)s on "" ""%(switch_info)s in vlan %(segmentation_id)s"", {'switch_port': port_id, 'switch_info': switch_info, 'segmentation_id': segmentation_id}) # Move port to network switch.plug_port_to_network(port_id, segmentation_id) LOG.info(""Successfully plugged port %(port_id)s in segment "" ""%(segment_id)s on device %(device)s"", {'port_id': port['id'], 'device': switch_info, 'segment_id': segmentation_id}) segments = context.segments_to_bind"," for link in local_link_information: if not switch: return else: for link in local_link_information: port_id = link.get('port_id') switch_info = link.get('switch_info') switch_id = link.get('switch_id') switch = device_utils.get_switch_device( self.switches, switch_info=switch_info, ngs_mac_address=switch_id) segments = context.segments_to_bind # If segmentation ID is None, set vlan 1 segmentation_id = segments[0].get('segmentation_id') or 1 LOG.debug(""Putting port %(port_id)s on %(switch_info)s "" ""to vlan: %(segmentation_id)s"", {'port_id': port_id, 'switch_info': switch_info, 'segmentation_id': segmentation_id}) # Move port to network switch.plug_port_to_network(port_id, segmentation_id) LOG.info(""Successfully bound port %(port_id)s in segment "" ""%(segment_id)s on device %(device)s"", {'port_id': port['id'], 'device': switch_info, 'segment_id': segmentation_id}) ",45,40
openstack%2Fnetworking-bagpipe~master~I2fec732dac93f972ce1ef1bef059c1086d6258c7,openstack/networking-bagpipe,master,I2fec732dac93f972ce1ef1bef059c1086d6258c7,[sqlalchemy-20]: remove subtransactions=True,MERGED,2023-06-27 08:52:43.000000000,2023-07-04 17:06:29.000000000,2023-07-04 17:05:31.000000000,"[{'_account_id': 11975}, {'_account_id': 13861}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-27 08:52:43.000000000', 'files': ['networking_bagpipe/db/sfc_db.py'], 'web_link': 'https://opendev.org/openstack/networking-bagpipe/commit/c8a974c435ea48219862cf9e6a34f9a95fe24675', 'message': '[sqlalchemy-20]: remove subtransactions=True\n\nChange-Id: I2fec732dac93f972ce1ef1bef059c1086d6258c7\nCloses-Bug: #2025127\n'}]",2,887024,c8a974c435ea48219862cf9e6a34f9a95fe24675,12,4,1,8313,,,0,"[sqlalchemy-20]: remove subtransactions=True

Change-Id: I2fec732dac93f972ce1ef1bef059c1086d6258c7
Closes-Bug: #2025127
",git fetch https://review.opendev.org/openstack/networking-bagpipe refs/changes/24/887024/1 && git format-patch -1 --stdout FETCH_HEAD,['networking_bagpipe/db/sfc_db.py'],1,c8a974c435ea48219862cf9e6a34f9a95fe24675,bug/2025127," ctx = n_context.get_admin_context() with db_api.CONTEXT_READER.using(ctx): query = ctx.session.query( BaGPipePpgRTAssoc).order_by( BaGPipePpgRTAssoc.rtnn) allocated_rtnns = {obj.rtnn for obj in query.all()} with db_api.CONTEXT_WRITER.using(ctx): ctx.session.add(ppg_rtnn) ctx = n_context.get_admin_context() with db_api.CONTEXT_READER.using(ctx): ppg_rts = ctx.session.query( BaGPipePpgRTAssoc).filter_by(ppg_id=ppg_id).all() if not ppg_rts: return None ctx = n_context.get_admin_context() with db_api.CONTEXT_READER.using(ctx): ppg_redirect_rt = ctx.session.query( BaGPipePpgRTAssoc).filter_by(ppg_id=ppg_id, is_redirect=True, reverse=reverse).one() return self._get_rt_from_rtnn(ppg_redirect_rt.rtnn)"," self.session = n_context.get_admin_context().session query = self.session.query( BaGPipePpgRTAssoc).order_by( BaGPipePpgRTAssoc.rtnn) allocated_rtnns = {obj.rtnn for obj in query.all()} with self.session.begin(subtransactions=True): self.session.add(ppg_rtnn) ppg_rts = self.session.query( BaGPipePpgRTAssoc).filter_by(ppg_id=ppg_id).all() if not ppg_rts: return None ppg_redirect_rt = self.session.query( BaGPipePpgRTAssoc).filter_by(ppg_id=ppg_id, is_redirect=True, reverse=reverse).one() return self._get_rt_from_rtnn(ppg_redirect_rt.rtnn)",22,17
openstack%2Fneutron~master~I89db15dd1b629bc963f3b63926391a4a02cbedf7,openstack/neutron,master,I89db15dd1b629bc963f3b63926391a4a02cbedf7,Ensure traffic is not centralized if DVR is enabled,MERGED,2023-06-28 10:28:54.000000000,2023-07-04 16:03:35.000000000,2023-07-04 16:01:41.000000000,"[{'_account_id': 6773}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 16688}, {'_account_id': 22348}, {'_account_id': 23804}]","[{'number': 1, 'created': '2023-06-28 10:28:54.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/081ca4dfd6ef5a5be390bd52b9c927531b33da32', 'message': 'Ensure traffic is not centralized if DVR is enabled\n\nThere is no need to clear the external_mac if DVR is enabled, not\neven when the port is down. This patch ensures the external_mac is\nonly deleted when DVR is not enabled.\n\nWithout this patch, if a VM with a floating IP gets deleted, and\nDVR is enabled, during some time the traffic gets (wrongly)\ncentralized while it should not. And it is also generating more\nload on the OVN side unnecesarily.\n\nChange-Id: I89db15dd1b629bc963f3b63926391a4a02cbedf7\n'}, {'number': 2, 'created': '2023-06-28 14:08:12.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/0ff3648858f1395357060eae078577fb40b608ff', 'message': 'Ensure traffic is not centralized if DVR is enabled\n\nThere is no need to clear the external_mac if DVR is enabled, not\neven when the port is down. This patch ensures the external_mac is\nonly deleted when DVR is not enabled.\n\nWithout this patch, if a VM with a floating IP gets deleted, and\nDVR is enabled, during some time the traffic gets (wrongly)\ncentralized while it should not. And it is also generating more\nload on the OVN side unnecesarily.\n\nCloses-Bug: #2025264\n\nChange-Id: I89db15dd1b629bc963f3b63926391a4a02cbedf7\n'}, {'number': 3, 'created': '2023-06-28 15:05:35.000000000', 'files': ['neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/test_mech_driver.py', 'releasenotes/notes/dvr-external-mac-934409413e515eb2.yaml', 'neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/0090572b93d96348778c724194c26a976a9f8757', 'message': 'Ensure traffic is not centralized if DVR is enabled\n\nThere is no need to clear the external_mac if DVR is enabled, not\neven when the port is down. This patch ensures the external_mac is\nonly deleted when DVR is not enabled.\n\nWithout this patch, if a VM with a floating IP gets deleted, and\nDVR is enabled, during some time the traffic gets (wrongly)\ncentralized while it should not. And it is also generating more\nload on the OVN side unnecesarily.\n\nCloses-Bug: #2025264\n\nChange-Id: I89db15dd1b629bc963f3b63926391a4a02cbedf7\n'}]",10,886626,0090572b93d96348778c724194c26a976a9f8757,29,6,3,23567,,,0,"Ensure traffic is not centralized if DVR is enabled

There is no need to clear the external_mac if DVR is enabled, not
even when the port is down. This patch ensures the external_mac is
only deleted when DVR is not enabled.

Without this patch, if a VM with a floating IP gets deleted, and
DVR is enabled, during some time the traffic gets (wrongly)
centralized while it should not. And it is also generating more
load on the OVN side unnecesarily.

Closes-Bug: #2025264

Change-Id: I89db15dd1b629bc963f3b63926391a4a02cbedf7
",git fetch https://review.opendev.org/openstack/neutron refs/changes/26/886626/1 && git format-patch -1 --stdout FETCH_HEAD,['neutron/plugins/ml2/drivers/ovn/mech_driver/mech_driver.py'],1,081ca4dfd6ef5a5be390bd52b9c927531b33da32,," if ovn_conf.is_ovn_distributed_floating_ip(): if up: mac = nat['external_ids'][ovn_const.OVN_FIP_EXT_MAC_KEY] if nat['external_mac'] != mac: LOG.debug(""Setting external_mac of port %s to %s"", port_id, mac) self.nb_ovn.db_set( 'NAT', nat['_uuid'], ('external_mac', mac)).execute( check_error=True)"," if up and ovn_conf.is_ovn_distributed_floating_ip(): mac = nat['external_ids'][ovn_const.OVN_FIP_EXT_MAC_KEY] if nat['external_mac'] != mac: LOG.debug(""Setting external_mac of port %s to %s"", port_id, mac) self.nb_ovn.db_set( 'NAT', nat['_uuid'], ('external_mac', mac)).execute( check_error=True)",9,8
openstack%2Fneutron~master~I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd,openstack/neutron,master,I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd,[OVN] Prevent Trunk creation/deletion with parent port bound,MERGED,2023-06-02 16:54:49.000000000,2023-07-04 16:02:46.000000000,2023-07-04 16:01:37.000000000,"[{'_account_id': 6773}, {'_account_id': 9845}, {'_account_id': 11975}, {'_account_id': 22348}, {'_account_id': 32029}, {'_account_id': 34271}]","[{'number': 1, 'created': '2023-06-02 16:54:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/77200d9ba91b7ed0020214c970f5b938a2e8ee54', 'message': 'WIP == [OVN] Prevent Trunk creation/deletion with parent port bound\n\nTODO: testing\n      documentation\n      reno\n\nCloses-Bug: #2022059\nChange-Id: I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd\n'}, {'number': 2, 'created': '2023-06-05 16:14:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/89709c64e92566569308feb3f8c8f7f65cb24cd6', 'message': '[OVN] Prevent Trunk creation/deletion with parent port bound\n\nThis patch imitates the ML2/OVS Trunk driver behaviour. When the\ntrunk parent port is bound:\n* A new trunk cannot be created using this parent port.\n* If the port is assigned as parent port of a trunk, this\n  trunk cannot be deleted.\n\nCloses-Bug: #2022059\nChange-Id: I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd\n'}, {'number': 3, 'created': '2023-06-06 10:57:53.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/99b27e5a1cf28dba046cf5d1b52c2bb8864711e4', 'message': '[OVN] Prevent Trunk creation/deletion with parent port bound\n\nThis patch imitates the ML2/OVS Trunk driver behaviour. When the\ntrunk parent port is bound:\n* A new trunk cannot be created using this parent port.\n* If the port is assigned as parent port of a trunk, this\n  trunk cannot be deleted.\n\nCloses-Bug: #2022059\nChange-Id: I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd\n'}, {'number': 4, 'created': '2023-06-22 11:39:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/eda441e95f1889eb2adbe9568759a13b0268faa4', 'message': '[OVN] Prevent Trunk creation/deletion with parent port bound\n\nThis patch imitates the ML2/OVS Trunk driver behaviour. When the\ntrunk parent port is bound:\n* A new trunk cannot be created using this parent port.\n* If the port is assigned as parent port of a trunk, this\n  trunk cannot be deleted.\n\nCloses-Bug: #2022059\nChange-Id: I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd\n'}, {'number': 5, 'created': '2023-06-27 08:32:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/f837f24a3c00633e740e8c127088882b581702a8', 'message': '[OVN] Prevent Trunk creation/deletion with parent port bound\n\nThis patch imitates the ML2/OVS Trunk driver behaviour. When the\ntrunk parent port is bound:\n* A new trunk cannot be created using this parent port.\n* If the port is assigned as parent port of a trunk, this\n  trunk cannot be deleted.\n\nCloses-Bug: #2022059\nChange-Id: I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd\n'}, {'number': 6, 'created': '2023-06-30 14:51:27.000000000', 'files': ['neutron/tests/unit/db/test_l3_dvr_db.py', 'neutron/tests/unit/services/trunk/test_plugin.py', 'neutron/db/l3_dvr_db.py', 'releasenotes/notes/ovn-trunk-check-parent-port-eeca2eceaca9d158.yaml', 'neutron/services/trunk/drivers/ovn/trunk_driver.py', 'neutron/common/utils.py', 'neutron/services/trunk/plugin.py', 'neutron/tests/functional/services/trunk/drivers/ovn/test_trunk_driver.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/833a6d82cd705548130cdac73a88d388f52c7824', 'message': '[OVN] Prevent Trunk creation/deletion with parent port bound\n\nThis patch imitates the ML2/OVS Trunk driver behaviour. When the\ntrunk parent port is bound:\n* A new trunk cannot be created using this parent port.\n* If the port is assigned as parent port of a trunk, this\n  trunk cannot be deleted.\n\nCloses-Bug: #2022059\nChange-Id: I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd\n'}]",11,885154,833a6d82cd705548130cdac73a88d388f52c7824,49,6,6,16688,,,0,"[OVN] Prevent Trunk creation/deletion with parent port bound

This patch imitates the ML2/OVS Trunk driver behaviour. When the
trunk parent port is bound:
* A new trunk cannot be created using this parent port.
* If the port is assigned as parent port of a trunk, this
  trunk cannot be deleted.

Closes-Bug: #2022059
Change-Id: I8cfa7e67524a42224cbb4b3c3cec3cfa49b795fd
",git fetch https://review.opendev.org/openstack/neutron refs/changes/54/885154/4 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/db/l3_dvr_db.py', 'neutron/services/trunk/drivers/ovn/trunk_driver.py', 'neutron/common/utils.py']",3,77200d9ba91b7ed0020214c970f5b938a2e8ee54,bug/2022059,"from neutron_lib.api.definitions import portbindings from neutron_lib.api.definitions import portbindings_extendedfrom neutron_lib.plugins import utils as plugin_utils # TODO(slaweq): this should be moved to neutron_lib.plugins.utils module def is_port_bound(port, log_message=True): active_binding = plugin_utils.get_port_binding_by_status_and_host( port.get('port_bindings', []), n_const.ACTIVE) if not active_binding: if log_message: LOG.warning('Binding for port %s was not found.', port) return False return active_binding[portbindings_extended.VIF_TYPE] not in ( portbindings.VIF_TYPE_UNBOUND, portbindings.VIF_TYPE_BINDING_FAILED)",,38,16
openstack%2Ftripleo-docs~master~I423cef504cc6490236ba960f6cd0adca01be2aa8,openstack/tripleo-docs,master,I423cef504cc6490236ba960f6cd0adca01be2aa8,Document configuring tld during deployed ceph,MERGED,2023-06-13 07:20:21.000000000,2023-07-04 15:12:55.000000000,2023-06-22 12:28:12.000000000,"[{'_account_id': 18002}, {'_account_id': 22348}, {'_account_id': 25402}]","[{'number': 1, 'created': '2023-06-13 07:20:21.000000000', 'files': ['deploy-guide/source/features/deployed_ceph.rst'], 'web_link': 'https://opendev.org/openstack/tripleo-docs/commit/d6b21c01abdc4a38f942e98220ef43e057cbf68c', 'message': 'Document configuring tld during deployed ceph\n\nChange-Id: I423cef504cc6490236ba960f6cd0adca01be2aa8\n'}]",1,885789,d6b21c01abdc4a38f942e98220ef43e057cbf68c,9,3,1,34598,,,0,"Document configuring tld during deployed ceph

Change-Id: I423cef504cc6490236ba960f6cd0adca01be2aa8
",git fetch https://review.opendev.org/openstack/tripleo-docs refs/changes/89/885789/1 && git format-patch -1 --stdout FETCH_HEAD,['deploy-guide/source/features/deployed_ceph.rst'],1,d6b21c01abdc4a38f942e98220ef43e057cbf68c,support_tld," [--tld] --tld Top Level Domain suffix to be added to the short hostname to represent the fully qualified domain name. --tld Top Level Domain suffix to be added to the short hostname to represent the fully qualified domain name.TLD option ---------- During ceph spec generation, if ``--tld`` is passed to `ceph_spec_bootstrap`_ ansible module, generated spec will have the hostnames appended with tld. This ``--tld`` option is available in `openstack overcloud ceph deploy` and `openstack overcloud ceph spec` commands. for example:: openstack overcloud ceph deploy \ --tld ""redhat.local"" During `openstack overcloud ceph deploy` , even the hostnames of all overcloud nodes are appended with ``--tld`` option, which makes it a Fully qualified Domain name (canonical name) suitable for TLS-e configuration.",,23,0
openstack%2Fbifrost~master~I43dd4a2f7cf47934bc9f5cdee85acf53a40dd468,openstack/bifrost,master,I43dd4a2f7cf47934bc9f5cdee85acf53a40dd468,Fix TFTP read access for enforcing SELinux,MERGED,2022-07-11 00:15:41.000000000,2023-07-04 14:38:44.000000000,2022-09-02 19:23:29.000000000,"[{'_account_id': 10239}, {'_account_id': 11655}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-07-11 00:15:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/668b2043ea29cb6deed8ab8f0e329544b4dc5c43', 'message': 'Fix TFTP read access for enforcing SELinux\n\nThis change adds ironic_tftp_master_path to the list of paths to get\nthe tftpdir_t context, so that images copied from here to\ntftp_boot_folder will have a context which allows the file to be read.\n\nThis change also applies the context changes when SELinux is in\nPermissive mode, as well as Enforcing.\n\nChange-Id: I43dd4a2f7cf47934bc9f5cdee85acf53a40dd468\n'}, {'number': 2, 'created': '2022-07-11 23:38:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/bifrost/commit/bf62fadc8a5323bb68570df860169f96ef4480fe', 'message': 'Fix TFTP read access for enforcing SELinux\n\nThis change adds ironic_tftp_master_path to the list of paths to get\nthe tftpdir_t context, so that images copied from here to\ntftp_boot_folder will have a context which allows the file to be read.\n\nThis change also applies the context changes when SELinux is in\nPermissive mode, as well as Enforcing.\n\nChange-Id: I43dd4a2f7cf47934bc9f5cdee85acf53a40dd468\n'}, {'number': 3, 'created': '2022-08-01 00:54:42.000000000', 'files': ['playbooks/roles/bifrost-ironic-install/tasks/bootstrap.yml', 'releasenotes/notes/selinux-tftp-c37e34311238f8fd.yaml'], 'web_link': 'https://opendev.org/openstack/bifrost/commit/0baff1e12445624772e2436d04840316ee09763b', 'message': 'Fix TFTP read access for enforcing SELinux\n\nThis change adds ironic_tftp_master_path to the list of paths to get\nthe tftpdir_t context, so that images copied from here to\ntftp_boot_folder will have a context which allows the file to be read.\n\nThis change also applies the context changes when SELinux is in\nPermissive mode, as well as Enforcing.\n\nChange-Id: I43dd4a2f7cf47934bc9f5cdee85acf53a40dd468\n'}]",0,849246,0baff1e12445624772e2436d04840316ee09763b,14,3,3,4571,,,0,"Fix TFTP read access for enforcing SELinux

This change adds ironic_tftp_master_path to the list of paths to get
the tftpdir_t context, so that images copied from here to
tftp_boot_folder will have a context which allows the file to be read.

This change also applies the context changes when SELinux is in
Permissive mode, as well as Enforcing.

Change-Id: I43dd4a2f7cf47934bc9f5cdee85acf53a40dd468
",git fetch https://review.opendev.org/openstack/bifrost refs/changes/46/849246/2 && git format-patch -1 --stdout FETCH_HEAD,['playbooks/roles/bifrost-ironic-install/tasks/bootstrap.yml'],1,668b2043ea29cb6deed8ab8f0e329544b4dc5c43,grub-network," - ""{{ ironic_tftp_master_path }}"" - ""{{ ironic_tftp_master_path }}"" ansible_selinux.status == 'enabled'"," ansible_selinux.status == 'enabled' and ansible_selinux.mode == ""enforcing""",3,1
openstack%2Ftripleo-ansible~stable%2Ftrain~I86fd1286caf32b8b0871ba7977168653f81d3d9a,openstack/tripleo-ansible,stable/train,I86fd1286caf32b8b0871ba7977168653f81d3d9a,Render extravars as json,ABANDONED,2023-07-01 06:22:46.000000000,2023-07-04 14:34:40.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-01 06:22:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/59af7d243483770f5d6802e8f7f4b337b4fdb56c', 'message': 'Render extravars as json\n\nChange-Id: I86fd1286caf32b8b0871ba7977168653f81d3d9a\n'}, {'number': 2, 'created': '2023-07-03 09:26:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/58bfa60e4e07c7344ee28eae9be71e51a0563797', 'message': 'Render extravars as json\n\nChange-Id: I86fd1286caf32b8b0871ba7977168653f81d3d9a\n'}, {'number': 3, 'created': '2023-07-03 14:05:02.000000000', 'files': ['tripleo_ansible/roles/tripleo-ceph-work-dir/tasks/prepare.yml', 'tripleo_ansible/roles/tripleo-ceph-run-ansible/tasks/main.yml'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/e12433b87aa010740d921c6366c37617b8183e25', 'message': 'Render extravars as json\n\nChange-Id: I86fd1286caf32b8b0871ba7977168653f81d3d9a\n'}]",0,887441,e12433b87aa010740d921c6366c37617b8183e25,6,1,3,25402,,,0,"Render extravars as json

Change-Id: I86fd1286caf32b8b0871ba7977168653f81d3d9a
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/41/887441/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_ansible/roles/tripleo-ceph-work-dir/tasks/prepare.yml'],1,59af7d243483770f5d6802e8f7f4b337b4fdb56c,fp_patch," content: ""{{ ceph_ansible_extra_vars | to_nice_json }}"""," content: ""{{ ceph_ansible_extra_vars | to_nice_yaml }}""",1,1
openstack%2Fcharm-openstack-dashboard~master~I6bbf7e5f81a772ffc6ea859c9ab7c05f2eb9fdc5,openstack/charm-openstack-dashboard,master,I6bbf7e5f81a772ffc6ea859c9ab7c05f2eb9fdc5,Make LocalSettingsContext more robust to priority,MERGED,2023-07-04 11:06:54.000000000,2023-07-04 14:11:49.000000000,2023-07-04 14:11:49.000000000,"[{'_account_id': 8992}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 11:06:54.000000000', 'files': ['hooks/horizon_contexts.py', 'unit_tests/test_horizon_contexts.py'], 'web_link': 'https://opendev.org/openstack/charm-openstack-dashboard/commit/e8d0ca39a1e02abb075e283e4438464b90223b91', 'message': ""Make LocalSettingsContext more robust to priority\n\nThe relation data for for the LocalSettings context could cause the\npriority sorting to break if the priority key wasn't cmpable (e.g. using\n<, > or ==).  This patch fixes the associated bug, by making the sorting\nextra robust and ensuring that un-cmp-able values are 'greater' (e.g.\nfurther down the list) that cmp-able values, and equal to each other.\nE.g. a partially ordered set.\n\nChange-Id: I6bbf7e5f81a772ffc6ea859c9ab7c05f2eb9fdc5\nCloses-bug: #2023404\n""}]",1,887594,e8d0ca39a1e02abb075e283e4438464b90223b91,7,3,1,20870,,,0,"Make LocalSettingsContext more robust to priority

The relation data for for the LocalSettings context could cause the
priority sorting to break if the priority key wasn't cmpable (e.g. using
<, > or ==).  This patch fixes the associated bug, by making the sorting
extra robust and ensuring that un-cmp-able values are 'greater' (e.g.
further down the list) that cmp-able values, and equal to each other.
E.g. a partially ordered set.

Change-Id: I6bbf7e5f81a772ffc6ea859c9ab7c05f2eb9fdc5
Closes-bug: #2023404
",git fetch https://review.opendev.org/openstack/charm-openstack-dashboard refs/changes/94/887594/1 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/horizon_contexts.py', 'unit_tests/test_horizon_contexts.py']",2,e8d0ca39a1e02abb075e283e4438464b90223b91,bug/2023404," def test_LocalSettingsContext_unusual_priority(self): # First, left priority missing. self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] self.relation_get.side_effect = [{'local-settings': 'FOO = True'}, {'priority': 60, 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': []}) # First, right priority missing. self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] self.relation_get.side_effect = [{'priority': 99, 'local-settings': 'FOO = True'}, {'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': []}) # Left priority is None. self.relation_ids.return_value = ['plugin:0', 'plugin-too:0'] self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] self.relation_get.side_effect = [{'priority': None, 'local-settings': 'FOO = True'}, {'priority': 60, 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': ['# horizon-plugin-too/0\n' 'BAR = False', '# horizon-plugin/0\n' 'FOO = True']}) # Right priority is None. self.relation_ids.return_value = ['plugin:0', 'plugin-too:0'] self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] self.relation_get.side_effect = [{'priority': 99, 'local-settings': 'FOO = True'}, {'priority': None, 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': ['# horizon-plugin/0\n' 'FOO = True', '# horizon-plugin-too/0\n' 'BAR = False']}) # Left priority is stringy number. self.relation_ids.return_value = ['plugin:0', 'plugin-too:0'] self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] self.relation_get.side_effect = [{'priority': ""99"", 'local-settings': 'FOO = True'}, {'priority': 60, 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': ['# horizon-plugin-too/0\n' 'BAR = False', '# horizon-plugin/0\n' 'FOO = True']}) # Right priority is stringy number. self.relation_ids.return_value = ['plugin:0', 'plugin-too:0'] self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] self.relation_get.side_effect = [{'priority': 99, 'local-settings': 'FOO = True'}, {'priority': ""60"", 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': ['# horizon-plugin-too/0\n' 'BAR = False', '# horizon-plugin/0\n' 'FOO = True']}) # Both priorities are strings self.relation_ids.return_value = ['plugin:0', 'plugin-too:0'] self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] self.relation_get.side_effect = [{'priority': ""99"", 'local-settings': 'FOO = True'}, {'priority': ""60"", 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': ['# horizon-plugin-too/0\n' 'BAR = False', '# horizon-plugin/0\n' 'FOO = True']}) # Left priority is weired json object self.relation_ids.return_value = ['plugin:0', 'plugin-too:0'] self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] self.relation_get.side_effect = [{'priority': ""{'a': 1}"", 'local-settings': 'FOO = True'}, {'priority': ""60"", 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': ['# horizon-plugin-too/0\n' 'BAR = False', '# horizon-plugin/0\n' 'FOO = True']}) # right priority is weired json object self.relation_ids.return_value = ['plugin:0', 'plugin-too:0'] self.related_units.side_effect = [['horizon-plugin/0'], ['horizon-plugin-too/0']] self.relation_get.side_effect = [{'priority': ""99"", 'local-settings': 'FOO = True'}, {'priority': ""[1,2,3]"", 'local-settings': 'BAR = False'}] self.assertEqual(horizon_contexts.LocalSettingsContext()(), {'settings': ['# horizon-plugin/0\n' 'FOO = True', '# horizon-plugin-too/0\n' 'BAR = False']}) ",,180,2
openstack%2Fcharm-neutron-api-plugin-ovn~master~Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e,openstack/charm-neutron-api-plugin-ovn,master,Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e,Enable e2e functional tests with DPDK in gate,NEW,2022-04-11 15:43:23.000000000,2023-07-04 13:47:08.000000000,,"[{'_account_id': 13686}, {'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-04-11 15:43:23.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/b07f1ee4b5b891cf1b329fb2daaecd9f8b079bff', 'message': 'WIP: Enable functional tests with DPDK in gate\n\nFunc-Test-Pr: https://github.com/openstack-charmers/zaza/pull/498\nFunc-Test-Pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/746\nChange-Id: Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e\n'}, {'number': 2, 'created': '2022-04-12 15:53:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/effb4b9b22745c176da5c9e17eaf699c1dd1ca80', 'message': 'WIP: Enable functional tests with DPDK in gate\n\nUpdate bundles to request more RAM for the nova-compute units to\naccomodate for the DPDK test.  We need to reserve at least 2G RAM\nfor hugepages for it to succeed.\n\nFunc-Test-Pr: https://github.com/openstack-charmers/zaza/pull/498\nFunc-Test-Pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/746\nChange-Id: Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e\n'}, {'number': 3, 'created': '2022-04-12 19:25:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/2aba9445d6d7de4cff9156caf138b232b7a94820', 'message': 'WIP: Enable functional tests with DPDK in gate\n\nUpdate bundles to request more RAM for the nova-compute units to\naccomodate for the DPDK test.  We need to reserve at least 2G RAM\nfor hugepages for it to succeed.\n\nFunc-Test-Pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/746\nChange-Id: Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e\n'}, {'number': 4, 'created': '2022-04-13 05:12:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/5eb1f49194df4641305622a4d68c6d9c267046a5', 'message': 'WIP: Enable functional tests with DPDK in gate\n\nUpdate bundles to request more RAM for the nova-compute units to\naccomodate for the DPDK test.  We need to reserve at least 2G RAM\nfor hugepages for it to succeed.\n\n__NOTE__\n\nDo not merge until temporary charm branch links are removed\nfrom bundles.\n\n__NOTE__\n\nFunc-Test-Pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/746\nChange-Id: Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e\n'}, {'number': 5, 'created': '2022-04-13 05:56:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/9856b95a67e6d4cec4edb4bddb81ccce6cbc9a23', 'message': 'WIP: Enable functional tests with DPDK in gate\n\nUpdate bundles to request more RAM for the nova-compute units to\naccomodate for the DPDK test.  We need to reserve at least 2G RAM\nfor hugepages for it to succeed.\n\n__NOTE__\n\nDo not merge until temporary charm branch links are removed\nfrom bundles.\n\n__NOTE__\n\nFunc-Test-Pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/746\nChange-Id: Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e\n'}, {'number': 6, 'created': '2022-04-21 07:56:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/cc8d884420601d903dc1ef5b7cc0d46d4e256652', 'message': 'WIP: Enable functional tests with DPDK in gate\n\nUpdate bundles to request more RAM for the nova-compute units to\naccomodate for the DPDK test.  We need to reserve at least 2G RAM\nfor hugepages for it to succeed.\n\n__NOTE__\n\nDo not merge until temporary charm branch links are removed\nfrom bundles.\n\n__NOTE__\n\nFunc-Test-Pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/746\nChange-Id: Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e\n'}, {'number': 7, 'created': '2022-04-21 15:36:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/71c5c17bb9b5a497e26e276f0d961ea3bcba6146', 'message': 'WIP: Enable functional tests with DPDK in gate\n\nUpdate bundles to request more RAM for the nova-compute units to\naccomodate for the DPDK test.  We need to reserve at least 2G RAM\nfor hugepages for it to succeed.\n\n__NOTE__\n\nDo not merge until temporary charm branch links are removed\nfrom bundles.\n\n__NOTE__\n\nFunc-Test-Pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/746\nChange-Id: Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e\n'}, {'number': 8, 'created': '2022-04-26 09:54:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/2bf6eb740f54b39526ee3a94058bb1f49175c12a', 'message': 'WIP: Enable functional tests with DPDK in gate\n\nUpdate bundles to request more RAM for the nova-compute units to\naccomodate for the DPDK test.  We need to reserve at least 2G RAM\nfor hugepages for it to succeed.\n\n__NOTE__\n\nDo not merge until temporary charm branch links are removed\nfrom bundles.\n\n__NOTE__\n\nFunc-Test-Pr: https://github.com/openstack-charmers/zaza-openstack-tests/pull/746\nChange-Id: Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e\n'}, {'number': 9, 'created': '2022-05-12 15:03:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/15d8d7846e970b83d17ffa37f88c1d538f432323', 'message': 'Enable e2e functional tests with DPDK in gate\n\nUpdate bundles to request a flavor with appropriate NUMA topology\nto accomodate for the DPDK test.\n\nChange-Id: Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e\n'}, {'number': 10, 'created': '2023-07-03 13:32:44.000000000', 'files': ['src/tests/bundles/lunar-antelope.yaml', 'src/tests/tests.yaml', 'src/tests/bundles/jammy-zed.yaml', 'src/tests/bundles/jammy-antelope.yaml'], 'web_link': 'https://opendev.org/openstack/charm-neutron-api-plugin-ovn/commit/46be6478c23705cc3c6997d2953aaa6e2be5ff43', 'message': 'Enable e2e functional tests with DPDK in gate\n\nUpdate bundles to request a flavor with appropriate NUMA topology\nto accomodate for the DPDK test.\n\nChange-Id: Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e\n'}]",26,837363,46be6478c23705cc3c6997d2953aaa6e2be5ff43,81,3,10,13686,,,0,"Enable e2e functional tests with DPDK in gate

Update bundles to request a flavor with appropriate NUMA topology
to accomodate for the DPDK test.

Change-Id: Ied2bd8bd66a0e0fa050b4f6f7a43e1375029ba1e
",git fetch https://review.opendev.org/openstack/charm-neutron-api-plugin-ovn refs/changes/63/837363/6 && git format-patch -1 --stdout FETCH_HEAD,['src/tests/tests.yaml'],1,b07f1ee4b5b891cf1b329fb2daaecd9f8b079bff,dpdk-tests,# This needs to be the first thing we do to avoid memory being # fragmented on the machines used as hypervisors - zaza.charm_tests.machine.setup.request_hugepages_on_hvs_in_vmsconfigure_options: hypervisor_application: 'nova-compute' tests: #- zaza.openstack.charm_tests.neutron.tests.NeutronNetworkingTest - zaza.openstack.charm_tests.neutron.tests.DPDKNeutronNetworkingTest,tests: - zaza.openstack.charm_tests.neutron.tests.NeutronNetworkingTest,8,1
openstack%2Ftripleo-heat-templates~stable%2Fwallaby~I1578deadbfeb46f5ace818f11a503e0b2f697b41,openstack/tripleo-heat-templates,stable/wallaby,I1578deadbfeb46f5ace818f11a503e0b2f697b41,Merge dict values no overwriting them,MERGED,2023-06-30 07:35:14.000000000,2023-07-04 13:47:07.000000000,2023-07-04 13:47:07.000000000,"[{'_account_id': 20775}, {'_account_id': 22348}, {'_account_id': 31245}, {'_account_id': 34419}]","[{'number': 1, 'created': '2023-06-30 07:35:14.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/d2cace52f9c8a697473aacf5a6436a9691c10f64', 'message': 'Merge dict values no overwriting them\n\nSome parameters were missing when the parameters were defined\nin several environment files.\n\nAlso remove unused function\ni\nResolves: rhbz#2218519\nChange-Id: I555b9d2a276cfa1feb7b610f5768bbecaae666f2\n\nChange-Id: I1578deadbfeb46f5ace818f11a503e0b2f697b41\n'}, {'number': 2, 'created': '2023-06-30 07:36:26.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/af694bf72d4dd9c77a5b238a63d5b75c250eacf5', 'message': 'Merge dict values no overwriting them\n\nSome parameters were missing when the parameters were defined\nin several environment files.\n\nAlso remove unused function\n\nResolves: rhbz#2218519\nChange-Id: I1578deadbfeb46f5ace818f11a503e0b2f697b41\n'}, {'number': 3, 'created': '2023-06-30 08:54:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/65f77f32adf79029e84722a34000d993f785dbde', 'message': 'Merge dict values no overwriting them\n\nSome parameters were missing when the parameters were defined\nin several environment files.\n\nAlso remove unused function\n\nResolves: rhbz#2218519\nChange-Id: I1578deadbfeb46f5ace818f11a503e0b2f697b41\n'}, {'number': 4, 'created': '2023-06-30 08:56:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/7b3d2ccea4c2ff228efb0e8abe2cba4067d825d4', 'message': 'Merge dict values no overwriting them\n\nSome parameters were missing when the parameters were defined\nin several environment files.\n\nAlso remove unused function\n\nResolves: rhbz#2218519\nChange-Id: I1578deadbfeb46f5ace818f11a503e0b2f697b41\n'}, {'number': 5, 'created': '2023-06-30 14:34:39.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/1099fbd24d89a748cdc373d0358f4b598e62f595', 'message': 'Merge dict values no overwriting them\n\nSome parameters were missing when the parameters were defined\nin several environment files.\n\nAlso remove unused function\n\nResolves: rhbz#2218519\nChange-Id: I1578deadbfeb46f5ace818f11a503e0b2f697b41\n'}, {'number': 6, 'created': '2023-07-03 08:35:39.000000000', 'files': ['tools/copy_role_params.py'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/8600b057e6d9352fdcad6a3cda4025729f6fe957', 'message': 'Merge dict values no overwriting them\n\nSome parameters were missing when the parameters were defined\nin several environment files.\n\nAlso remove unused function\n\nResolves: rhbz#2218519\nChange-Id: I1578deadbfeb46f5ace818f11a503e0b2f697b41\n'}]",10,887358,8600b057e6d9352fdcad6a3cda4025729f6fe957,17,4,6,22954,,,0,"Merge dict values no overwriting them

Some parameters were missing when the parameters were defined
in several environment files.

Also remove unused function

Resolves: rhbz#2218519
Change-Id: I1578deadbfeb46f5ace818f11a503e0b2f697b41
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/58/887358/3 && git format-patch -1 --stdout FETCH_HEAD,['tools/copy_role_params.py'],1,d2cace52f9c8a697473aacf5a6436a9691c10f64,rhbz_2218519,"def special_merge(dict_1, dict_2): dict_result = {**dict_1, **dict_2} for key, value in dict_result.items(): if key in dict_1 and key in dict_2: if (type(dict_1[key]) is list): dict_result[key] = dict_1[key] + [value] else: dict_result[key] = [value , dict_1[key]] return dict_result pd = {} rr = {} data = yaml.safe_load(env_file) par = data.get('parameter_defaults', {}) res = data.get('resource_registry', {}) rr = special_merge(rr, res) pd = special_merge(pd, par) if pd: for key, value in pd.items(): if key.startswith(rolename): n_role_params_def[key] = value if rr: for key, value in rr.items(): if key.startswith(""OS::TripleO::{}:"".format(rolename)): n_role_res_registry[key] = value","def copy_role_parameters(env_files, rolename, new_role_env_file): n_role_params_def = {} n_role_res_registry = {} for file in env_files: if os.path.exists(file): with open(file, 'r') as env_file: contents = yaml.safe_load(env_file) pd = contents.get('parameter_defaults', {}) if pd: for key, value in pd.items(): if key.startswith(rolename): n_role_params_def[key] = value rr = contents.get('resource_registry', {}) if rr: for key, value in rr.items(): if key.startswith(""OS::TripleO::{}:"".format(rolename)): n_role_res_registry[key] = value # We can have ComputeCount Compute1Count which # means we have picked on multiple roles and as we # cannot parse here for all role_specific tagged params # we simply pick on Count: to figure out if we don't happen # to have colision role_count = [] filter_out = [] role_count = dict(filter(filter_count, n_role_params_def.items())) if len(role_count) > 1: for key in role_count: if key != ""{}Count"".format(rolename): filter_out.append(key[:-5]) for role in filter_out: n_role_params_def = dict(filter( lambda seq: filter_out_f(seq, rolename=role), n_role_params_def.items())) for word in ['Count', 'HostnameFormat', 'NetworkConfigTemplate', 'ContainerImagePrepare', 'UpgradeInitCommand']: n_role_params_def = dict(filter( lambda seq: filter_out_f_2(seq, removeme=word), n_role_params_def.items())) n_role_res_registry = dict(filter( lambda seq: filter_out_reg(seq), n_role_res_registry.items())) with open(new_role_env_file, 'w') as new_file: # I don't think we can blindly dump here, if one is empty we skip dumping dump_var = {} if (n_role_params_def != {}): dump_var['parameter_defaults'] = n_role_params_def if (n_role_res_registry != {}): dump_var['resource_registry'] = n_role_res_registry if (dump_var != {}): yaml.dump(dump_var, new_file, default_flow_style=False) contents = yaml.safe_load(env_file) pd = contents.get('parameter_defaults', {}) if pd: for key, value in pd.items(): if key.startswith(rolename): n_role_params_def[key] = value rr = contents.get('resource_registry', {}) if rr: for key, value in rr.items(): if key.startswith(""OS::TripleO::{}:"".format(rolename)): n_role_res_registry[key] = value",24,72
openstack%2Ftripleo-heat-templates~stable%2Fwallaby~I8972b16254b141e7102ea87cb6c0d489d8426751,openstack/tripleo-heat-templates,stable/wallaby,I8972b16254b141e7102ea87cb6c0d489d8426751,Enable CAP_AUDIT_WRITE for some containers/steps,MERGED,2023-06-27 15:24:58.000000000,2023-07-04 13:47:03.000000000,2023-07-04 13:47:03.000000000,"[{'_account_id': 8833}, {'_account_id': 9816}, {'_account_id': 11166}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-27 15:24:58.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/921c735e68b3421e1fb84c0640e8803a1b086856', 'message': 'Enable CAP_AUDIT_WRITE for some containers/steps\n\nUsually, db_sync involves call to ""sudo"". Such call are now logging a\nwarning/error in the host log due to a recently removed capability in\npodman, the CAP_AUDIT_WRITE. This capability allows containers to write\nin the audit log whenever there\'s a security related thing.\n\nSudo isn\'t the only one needing this access - sshd also writes in the\naudit. Since the nova-migration-target runs sshd, enabling the\ncapability in there will ensure we\'re keeping clean track of the\naccesses.\n\nChange-Id: I8972b16254b141e7102ea87cb6c0d489d8426751\nCloses-Bug: #1991219\n(cherry picked from commit ae5fa916f7ea0d4f3215738327eb05819bcb5bae)\n'}, {'number': 2, 'created': '2023-07-03 09:13:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/8ce8cb58d7b0a17265f18c997569630d5297eaf2', 'message': 'Enable CAP_AUDIT_WRITE for some containers/steps\n\nUsually, db_sync involves call to ""sudo"". Such call are now logging a\nwarning/error in the host log due to a recently removed capability in\npodman, the CAP_AUDIT_WRITE. This capability allows containers to write\nin the audit log whenever there\'s a security related thing.\n\nSudo isn\'t the only one needing this access - sshd also writes in the\naudit. Since the nova-migration-target runs sshd, enabling the\ncapability in there will ensure we\'re keeping clean track of the\naccesses.\n\nBackport note:\npodman in CentOS 8 does not allow using both of privilege and cap add,\nso this removes the update in nova_migration_target which runs with\nprivilege mode.\n\nChange-Id: I8972b16254b141e7102ea87cb6c0d489d8426751\nCloses-Bug: #1991219\n(cherry picked from commit ae5fa916f7ea0d4f3215738327eb05819bcb5bae)\n'}, {'number': 3, 'created': '2023-07-03 09:14:27.000000000', 'files': ['deployment/ironic/ironic-inspector-container-puppet.yaml', 'deployment/nova/nova-api-container-puppet.yaml', 'deployment/cinder/cinder-api-container-puppet.yaml', 'deployment/glance/glance-api-container-puppet.yaml', 'deployment/barbican/barbican-api-container-puppet.yaml', 'deployment/ironic/ironic-api-container-puppet.yaml', 'deployment/nova/nova-conductor-container-puppet.yaml', 'deployment/octavia/octavia-api-container-puppet.yaml', 'deployment/designate/designate-central-container-puppet.yaml', 'deployment/gnocchi/gnocchi-api-container-puppet.yaml', 'deployment/neutron/neutron-api-container-puppet.yaml', 'deployment/heat/heat-engine-container-puppet.yaml', 'deployment/placement/placement-api-container-puppet.yaml', 'deployment/manila/manila-api-container-puppet.yaml', 'deployment/aodh/aodh-api-container-puppet.yaml'], 'web_link': 'https://opendev.org/openstack/tripleo-heat-templates/commit/5b2a64df0bc17836d72c0f171b19aa1726e5c7e1', 'message': 'Enable CAP_AUDIT_WRITE for some containers/steps\n\nUsually, db_sync involves call to ""sudo"". Such call are now logging a\nwarning/error in the host log due to a recently removed capability in\npodman, the CAP_AUDIT_WRITE. This capability allows containers to write\nin the audit log whenever there\'s a security related thing.\n\nSudo isn\'t the only one needing this access - sshd also writes in the\naudit. Since the nova-migration-target runs sshd, enabling the\ncapability in there will ensure we\'re keeping clean track of the\naccesses.\n\nBackport note:\npodman in CentOS 8 (3.0.1) does not allow using both of privilege and\ncap add, because it does not include [1], so this removes the update in\nnova_migration_target which runs with privilege mode.\n\n[1] https://github.com/containers/podman/pull/13744/commits/1cd529b22d40205c1f3246ed49f07e3615cf8292\n\nChange-Id: I8972b16254b141e7102ea87cb6c0d489d8426751\nCloses-Bug: #1991219\n(cherry picked from commit ae5fa916f7ea0d4f3215738327eb05819bcb5bae)\n'}]",5,887061,5b2a64df0bc17836d72c0f171b19aa1726e5c7e1,18,4,3,28223,,,0,"Enable CAP_AUDIT_WRITE for some containers/steps

Usually, db_sync involves call to ""sudo"". Such call are now logging a
warning/error in the host log due to a recently removed capability in
podman, the CAP_AUDIT_WRITE. This capability allows containers to write
in the audit log whenever there's a security related thing.

Sudo isn't the only one needing this access - sshd also writes in the
audit. Since the nova-migration-target runs sshd, enabling the
capability in there will ensure we're keeping clean track of the
accesses.

Backport note:
podman in CentOS 8 (3.0.1) does not allow using both of privilege and
cap add, because it does not include [1], so this removes the update in
nova_migration_target which runs with privilege mode.

[1] https://github.com/containers/podman/pull/13744/commits/1cd529b22d40205c1f3246ed49f07e3615cf8292

Change-Id: I8972b16254b141e7102ea87cb6c0d489d8426751
Closes-Bug: #1991219
(cherry picked from commit ae5fa916f7ea0d4f3215738327eb05819bcb5bae)
",git fetch https://review.opendev.org/openstack/tripleo-heat-templates refs/changes/61/887061/2 && git format-patch -1 --stdout FETCH_HEAD,"['deployment/ironic/ironic-inspector-container-puppet.yaml', 'deployment/nova/nova-api-container-puppet.yaml', 'deployment/cinder/cinder-api-container-puppet.yaml', 'deployment/glance/glance-api-container-puppet.yaml', 'deployment/barbican/barbican-api-container-puppet.yaml', 'deployment/ironic/ironic-api-container-puppet.yaml', 'deployment/nova/nova-conductor-container-puppet.yaml', 'deployment/octavia/octavia-api-container-puppet.yaml', 'deployment/designate/designate-central-container-puppet.yaml', 'deployment/gnocchi/gnocchi-api-container-puppet.yaml', 'deployment/neutron/neutron-api-container-puppet.yaml', 'deployment/nova/nova-migration-target-container-puppet.yaml', 'deployment/heat/heat-engine-container-puppet.yaml', 'deployment/placement/placement-api-container-puppet.yaml', 'deployment/manila/manila-api-container-puppet.yaml', 'deployment/aodh/aodh-api-container-puppet.yaml']",16,921c735e68b3421e1fb84c0640e8803a1b086856,containers/cap_audit_write-stable/wallaby, cap_add: - AUDIT_WRITE,,32,0
openstack%2Ftripleo-ansible~stable%2Fwallaby~Ifc985333ac377f9eb7b77f1037e282a8ecd16535,openstack/tripleo-ansible,stable/wallaby,Ifc985333ac377f9eb7b77f1037e282a8ecd16535,Set RuntimeDirectory in ceph-nfs systemd unit,MERGED,2023-06-29 20:23:07.000000000,2023-07-04 13:47:00.000000000,2023-07-04 13:47:00.000000000,"[{'_account_id': 22348}, {'_account_id': 25402}, {'_account_id': 26787}, {'_account_id': 34598}]","[{'number': 1, 'created': '2023-06-29 20:23:07.000000000', 'files': ['tripleo_ansible/roles/tripleo_cephadm/templates/ceph-nfs.service.j2'], 'web_link': 'https://opendev.org/openstack/tripleo-ansible/commit/7863ff6d4a7a77c79b71eeb81b61f247b0efcef0', 'message': ""Set RuntimeDirectory in ceph-nfs systemd unit\n\nThe runtime directory uses emphemeral tempfs storage;\nceph-nfs process IDs are stored here, and this directory\nneeds to be re-created by systemd if the node's rebooted.\n\nResolves: rhbz#2180542\nChange-Id: Ifc985333ac377f9eb7b77f1037e282a8ecd16535\n""}]",4,887340,7863ff6d4a7a77c79b71eeb81b61f247b0efcef0,12,4,1,16643,,,0,"Set RuntimeDirectory in ceph-nfs systemd unit

The runtime directory uses emphemeral tempfs storage;
ceph-nfs process IDs are stored here, and this directory
needs to be re-created by systemd if the node's rebooted.

Resolves: rhbz#2180542
Change-Id: Ifc985333ac377f9eb7b77f1037e282a8ecd16535
",git fetch https://review.opendev.org/openstack/tripleo-ansible refs/changes/40/887340/1 && git format-patch -1 --stdout FETCH_HEAD,['tripleo_ansible/roles/tripleo_cephadm/templates/ceph-nfs.service.j2'],1,7863ff6d4a7a77c79b71eeb81b61f247b0efcef0,,RuntimeDirectory=ceph,,1,0
openstack%2Fcinder~master~Ia685e398e5c41ae36e1a37530f0a742f79533e9f,openstack/cinder,master,Ia685e398e5c41ae36e1a37530f0a742f79533e9f,Fix NetApp NFS storage migration between backends,NEW,2022-05-23 19:25:28.000000000,2023-07-04 13:33:59.000000000,,"[{'_account_id': 22348}, {'_account_id': 29122}, {'_account_id': 33301}, {'_account_id': 33493}, {'_account_id': 33648}, {'_account_id': 35586}]","[{'number': 1, 'created': '2022-05-23 19:25:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/57216a092b067a06e84e9cfab57872c9953ed779', 'message': 'Fix NetApp NFS storage migration between backends\n\nMigrating a Cinder volume from a NetApp NFS backend to another\n(different SVM, but same Cluster) was failing in case those\nbackends have same FlexVol name.\n\nThis fix added SVM source and destination parameters to NetApp\nClient to be used as part of API arguments in order to\ndifferentiate the FlexVol volume in the same cluster in case they\nhave the same name.\n\nCloses-Bug: #1969531\nChange-Id: Ia685e398e5c41ae36e1a37530f0a742f79533e9f\n'}, {'number': 2, 'created': '2022-05-24 12:14:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/fbcb41a2785ea1217095b9609fa00b3f8a10b470', 'message': 'Fix NetApp NFS storage migration between backends\n\nMigrating a Cinder volume from a NetApp NFS backend to another\n(different SVM, but same Cluster) was failing in case those\nbackends have same FlexVol name.\n\nThis fix added SVM source and destination parameters to NetApp\nClient to be used as part of API arguments in order to\ndifferentiate the FlexVol volume in the same cluster in case they\nhave the same name.\n\nCloses-Bug: #1969531\nChange-Id: Ia685e398e5c41ae36e1a37530f0a742f79533e9f\n'}, {'number': 3, 'created': '2022-05-26 14:14:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/cinder/commit/48dc3cee37f268bb0b050c7912b91d2ad0d54f0d', 'message': 'Fix NetApp NFS storage migration between backends\n\nMigrating a Cinder volume from a NetApp NFS backend to another\n(different SVM, but same Cluster) was failing in case those\nbackends have same FlexVol name.\n\nThis fix added SVM source and destination parameters to NetApp\nClient to be used as part of API arguments in order to\ndifferentiate the FlexVol volume in the same cluster in case they\nhave the same name.\n\nCloses-Bug: #1969531\nChange-Id: Ia685e398e5c41ae36e1a37530f0a742f79533e9f\n'}, {'number': 4, 'created': '2022-05-27 12:56:23.000000000', 'files': ['cinder/volume/drivers/netapp/dataontap/nfs_cmode.py', 'cinder/tests/unit/volume/drivers/netapp/dataontap/test_nfs_cmode.py', 'cinder/tests/unit/volume/drivers/netapp/dataontap/client/test_client_cmode.py', 'cinder/volume/drivers/netapp/dataontap/client/client_cmode.py'], 'web_link': 'https://opendev.org/openstack/cinder/commit/3cf4ad1a23d12558230fae7dc1bc628b61057a2b', 'message': 'Fix NetApp NFS storage migration between backends\n\nMigrating a Cinder volume from a NetApp NFS backend to another\n(different SVM, but same Cluster) was failing in case those\nbackends have same FlexVol name.\n\nThis fix added SVM source and destination parameters to NetApp\nClient to be used as part of API arguments in order to\ndifferentiate the FlexVol volume in the same cluster in case they\nhave the same name.\n\nCloses-Bug: #1969531\nChange-Id: Ia685e398e5c41ae36e1a37530f0a742f79533e9f\n'}]",11,843018,3cf4ad1a23d12558230fae7dc1bc628b61057a2b,80,6,4,34489,,,0,"Fix NetApp NFS storage migration between backends

Migrating a Cinder volume from a NetApp NFS backend to another
(different SVM, but same Cluster) was failing in case those
backends have same FlexVol name.

This fix added SVM source and destination parameters to NetApp
Client to be used as part of API arguments in order to
differentiate the FlexVol volume in the same cluster in case they
have the same name.

Closes-Bug: #1969531
Change-Id: Ia685e398e5c41ae36e1a37530f0a742f79533e9f
",git fetch https://review.opendev.org/openstack/cinder refs/changes/18/843018/4 && git format-patch -1 --stdout FETCH_HEAD,"['cinder/volume/drivers/netapp/dataontap/nfs_cmode.py', 'cinder/tests/unit/volume/drivers/netapp/dataontap/test_nfs_cmode.py', 'cinder/tests/unit/volume/drivers/netapp/dataontap/client/test_client_cmode.py', 'cinder/volume/drivers/netapp/dataontap/client/client_cmode.py']",4,57216a092b067a06e84e9cfab57872c9953ed779,bug/1969531," dest_file_name=None, src_vserver=None, dst_vserver=None): if dst_vserver is None: dst_vserver = src_vserver 'sfod-operation-path': '%s:%s/%s' % (src_vserver, src_ontap_volume, file_name) 'sfod-operation-path': '%s:%s/%s' % (dst_vserver, dest_ontap_volume, dest_file_name),"," dest_file_name=None): 'sfod-operation-path': '%s/%s' % (src_ontap_volume, file_name) 'sfod-operation-path': '%s/%s' % (dest_ontap_volume, dest_file_name),",39,20
openstack%2Fnova~stable%2Fxena~Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed,openstack/nova,stable/xena,Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed,Fix get_segments_id with subnets without segment_id,MERGED,2023-05-22 15:41:05.000000000,2023-07-04 13:15:44.000000000,2023-07-04 13:14:09.000000000,"[{'_account_id': 7166}, {'_account_id': 9708}, {'_account_id': 11604}, {'_account_id': 17685}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-05-22 15:41:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/dd6c75b06963dbd80f1f13f9ba12d4eb34da58e3', 'message': 'Fix get_segments_id with subnets without segment_id\n\nUnfortunatly when we merged Ie166f3b51fddeaf916cda7c5ac34bbcdda0fd17a we\nforgot that subnets can have no segment_id field.\n\nChange-Id: Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed\nCloses-Bug: #2018375\n(cherry picked from commit 6d7bd6a03446d5227d515b2b4c0da632ef4aa4a1)\n(cherry picked from commit 6b8d9d419170fb0ec2c6df561a0874e6362382c1)\n'}, {'number': 2, 'created': '2023-05-30 17:48:35.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/175aac045173fba0f2cd6879d57bb0975e1fba80', 'message': 'Fix get_segments_id with subnets without segment_id\n\nUnfortunatly when we merged Ie166f3b51fddeaf916cda7c5ac34bbcdda0fd17a we\nforgot that subnets can have no segment_id field.\n\nChange-Id: Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed\nCloses-Bug: #2018375\n(cherry picked from commit 6d7bd6a03446d5227d515b2b4c0da632ef4aa4a1)\n(cherry picked from commit 6b8d9d419170fb0ec2c6df561a0874e6362382c1)\n'}, {'number': 3, 'created': '2023-05-30 17:50:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/c72e8fb1f9401276a644d43d2d48d3255d036eb3', 'message': 'Fix get_segments_id with subnets without segment_id\n\nUnfortunatly when we merged Ie166f3b51fddeaf916cda7c5ac34bbcdda0fd17a we\nforgot that subnets can have no segment_id field.\n\nConflicts:\n    nova/tests/unit/network/test_neutron.py due to I8058902df167239fa455396d3595a56bcf472b2b\n\nChange-Id: Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed\nCloses-Bug: #2018375\n(cherry picked from commit 6d7bd6a03446d5227d515b2b4c0da632ef4aa4a1)\n(cherry picked from commit 6b8d9d419170fb0ec2c6df561a0874e6362382c1)\n'}, {'number': 4, 'created': '2023-05-30 17:55:04.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/bbc357d30a3b010670be8ba3920242c1ee956b5c', 'message': 'Fix get_segments_id with subnets without segment_id\n\nUnfortunatly when we merged Ie166f3b51fddeaf916cda7c5ac34bbcdda0fd17a we\nforgot that subnets can have no segment_id field.\n\nConflicts:\n    nova/tests/unit/network/test_neutron.py due to I8058902df167239fa455396d3595a56bcf472b2b\n\nChange-Id: Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed\nCloses-Bug: #2018375\n(cherry picked from commit 6d7bd6a03446d5227d515b2b4c0da632ef4aa4a1)\n(cherry picked from commit 6b8d9d419170fb0ec2c6df561a0874e6362382c1)\n(cherry picked from commit 77db64237b23050d94df113a38412c5333d23357)\n(cherry picked from commit 9a6a421c045a8031ff0923cca9aa7195fe987896)\n'}, {'number': 5, 'created': '2023-05-31 07:47:48.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/90889cfb2bd5773e02b4a862d82202f5d67aabb1', 'message': 'Fix get_segments_id with subnets without segment_id\n\nUnfortunatly when we merged Ie166f3b51fddeaf916cda7c5ac34bbcdda0fd17a we\nforgot that subnets can have no segment_id field.\n\nConflicts:\n    nova/tests/unit/network/test_neutron.py due to I8058902df167239fa455396d3595a56bcf472b2b\n\nChange-Id: Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed\nCloses-Bug: #2018375\n(cherry picked from commit 6d7bd6a03446d5227d515b2b4c0da632ef4aa4a1)\n(cherry picked from commit 6b8d9d419170fb0ec2c6df561a0874e6362382c1)\n(cherry picked from commit 77db64237b23050d94df113a38412c5333d23357)\n(cherry picked from commit 9a6a421c045a8031ff0923cca9aa7195fe987896)\n'}, {'number': 6, 'created': '2023-05-31 08:17:07.000000000', 'files': ['nova/tests/unit/network/test_neutron.py', 'nova/network/neutron.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/d3f44cd0e77ddacc4c9c9ab57469e5c02fbce1ee', 'message': 'Fix get_segments_id with subnets without segment_id\n\nUnfortunatly when we merged Ie166f3b51fddeaf916cda7c5ac34bbcdda0fd17a we\nforgot that subnets can have no segment_id field.\n\nConflicts:\n    nova/tests/unit/network/test_neutron.py due to I8058902df167239fa455396d3595a56bcf472b2b\n\nChange-Id: Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed\nCloses-Bug: #2018375\n(cherry picked from commit 6d7bd6a03446d5227d515b2b4c0da632ef4aa4a1)\n(cherry picked from commit 6b8d9d419170fb0ec2c6df561a0874e6362382c1)\n(cherry picked from commit 77db64237b23050d94df113a38412c5333d23357)\n(cherry picked from commit 9a6a421c045a8031ff0923cca9aa7195fe987896)\n'}]",4,883725,d3f44cd0e77ddacc4c9c9ab57469e5c02fbce1ee,26,5,6,7166,,,0,"Fix get_segments_id with subnets without segment_id

Unfortunatly when we merged Ie166f3b51fddeaf916cda7c5ac34bbcdda0fd17a we
forgot that subnets can have no segment_id field.

Conflicts:
    nova/tests/unit/network/test_neutron.py due to I8058902df167239fa455396d3595a56bcf472b2b

Change-Id: Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed
Closes-Bug: #2018375
(cherry picked from commit 6d7bd6a03446d5227d515b2b4c0da632ef4aa4a1)
(cherry picked from commit 6b8d9d419170fb0ec2c6df561a0874e6362382c1)
(cherry picked from commit 77db64237b23050d94df113a38412c5333d23357)
(cherry picked from commit 9a6a421c045a8031ff0923cca9aa7195fe987896)
",git fetch https://review.opendev.org/openstack/nova refs/changes/25/883725/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/network/test_neutron.py', 'nova/network/neutron.py']",2,dd6c75b06963dbd80f1f13f9ba12d4eb34da58e3,bug/2018375, if subnet.get('segment_id') is not None], if subnet['segment_id'] is not None],18,2
openstack%2Fnova~stable%2Fxena~Ic0f25e4d2395560fc2b68f3b469e266ac59abaa2,openstack/nova,stable/xena,Ic0f25e4d2395560fc2b68f3b469e266ac59abaa2,Fix segment-aware scheduling permissions error,MERGED,2023-05-31 07:40:00.000000000,2023-07-04 12:57:31.000000000,2023-07-04 12:56:17.000000000,"[{'_account_id': 7166}, {'_account_id': 9708}, {'_account_id': 11604}, {'_account_id': 17685}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-05-31 07:40:00.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/b833c79bf6870e70dc2fcc859cd5cbf76e84d7ee', 'message': ""Fix segment-aware scheduling permissions error\n\nResolves a bug encountered when setting the Nova scheduler to\nbe aware of Neutron routed provider network segments, by using\n'query_placement_for_routed_network_aggregates'.\n\nNon-admin users attempting to access the 'segment_id' attribute\nof a subnet caused a traceback, resulting in instance creation\nfailure.\n\nThis patch ensures the Neutron client is initialised with an\nadministrative context no matter what the requesting user's\npermissions are.\n\nChange-Id: Ic0f25e4d2395560fc2b68f3b469e266ac59abaa2\nCloses-Bug: #1970383\n(cherry picked from commit ee32934f34afd8e6df467361e9d71788cd36f6ee)\n(cherry picked from commit 60548e804219d91d8c68ab3d74dd0ae956cd33f3)\n""}, {'number': 2, 'created': '2023-05-31 07:44:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f0ad263056005a70ae90042cbd0ffe75873a6e46', 'message': ""Fix segment-aware scheduling permissions error\n\nResolves a bug encountered when setting the Nova scheduler to\nbe aware of Neutron routed provider network segments, by using\n'query_placement_for_routed_network_aggregates'.\n\nNon-admin users attempting to access the 'segment_id' attribute\nof a subnet caused a traceback, resulting in instance creation\nfailure.\n\nThis patch ensures the Neutron client is initialised with an\nadministrative context no matter what the requesting user's\npermissions are.\n\nConflicts:\n   nova/network/neutron.py : I8058902df167239fa455396d3595a56bcf472b2b\n     introduced some changes.\n\nChange-Id: Ic0f25e4d2395560fc2b68f3b469e266ac59abaa2\nCloses-Bug: #1970383\n(cherry picked from commit ee32934f34afd8e6df467361e9d71788cd36f6ee)\n(cherry picked from commit 60548e804219d91d8c68ab3d74dd0ae956cd33f3)\n""}, {'number': 3, 'created': '2023-05-31 08:17:07.000000000', 'files': ['nova/tests/unit/network/test_neutron.py', 'nova/network/neutron.py', 'releasenotes/notes/bug-1970383-segment-scheduling-permissions-92ba907b10a9eb1c.yaml'], 'web_link': 'https://opendev.org/openstack/nova/commit/28f94eb69a9b8267275460d0dbc18b3d7309cebd', 'message': ""Fix segment-aware scheduling permissions error\n\nResolves a bug encountered when setting the Nova scheduler to\nbe aware of Neutron routed provider network segments, by using\n'query_placement_for_routed_network_aggregates'.\n\nNon-admin users attempting to access the 'segment_id' attribute\nof a subnet caused a traceback, resulting in instance creation\nfailure.\n\nThis patch ensures the Neutron client is initialised with an\nadministrative context no matter what the requesting user's\npermissions are.\n\nConflicts:\n   nova/network/neutron.py\n   nova/tests/unit/network/test_neutron.py\nBoth conflicts are due to I8058902df167239fa455396d3595a56bcf472b2b\nWe just picked the needed modifications then.\n\nChange-Id: Ic0f25e4d2395560fc2b68f3b469e266ac59abaa2\nCloses-Bug: #1970383\n(cherry picked from commit ee32934f34afd8e6df467361e9d71788cd36f6ee)\n(cherry picked from commit 60548e804219d91d8c68ab3d74dd0ae956cd33f3)\n""}]",3,884808,28f94eb69a9b8267275460d0dbc18b3d7309cebd,14,5,3,7166,,,0,"Fix segment-aware scheduling permissions error

Resolves a bug encountered when setting the Nova scheduler to
be aware of Neutron routed provider network segments, by using
'query_placement_for_routed_network_aggregates'.

Non-admin users attempting to access the 'segment_id' attribute
of a subnet caused a traceback, resulting in instance creation
failure.

This patch ensures the Neutron client is initialised with an
administrative context no matter what the requesting user's
permissions are.

Conflicts:
   nova/network/neutron.py
   nova/tests/unit/network/test_neutron.py
Both conflicts are due to I8058902df167239fa455396d3595a56bcf472b2b
We just picked the needed modifications then.

Change-Id: Ic0f25e4d2395560fc2b68f3b469e266ac59abaa2
Closes-Bug: #1970383
(cherry picked from commit ee32934f34afd8e6df467361e9d71788cd36f6ee)
(cherry picked from commit 60548e804219d91d8c68ab3d74dd0ae956cd33f3)
",git fetch https://review.opendev.org/openstack/nova refs/changes/08/884808/3 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/network/test_neutron.py', 'nova/network/neutron.py', 'releasenotes/notes/bug-1970383-segment-scheduling-permissions-92ba907b10a9eb1c.yaml']",3,b833c79bf6870e70dc2fcc859cd5cbf76e84d7ee,bug/2018375,"--- fixes: - | `Bug #1970383 <https://bugs.launchpad.net/nova/+bug/1970383>`_: Fixes a permissions error when using the 'query_placement_for_routed_network_aggregates' scheduler variable, which caused a traceback on instance creation for non-admin users. ",,25,4
openstack%2Fneutron~master~I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265,openstack/neutron,master,I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265,Don't allow deletion of the router ports without IP addresses,MERGED,2023-06-26 12:37:57.000000000,2023-07-04 12:55:10.000000000,2023-07-04 12:53:39.000000000,"[{'_account_id': 1131}, {'_account_id': 8313}, {'_account_id': 9845}, {'_account_id': 13861}, {'_account_id': 22348}, {'_account_id': 32238}]","[{'number': 1, 'created': '2023-06-26 12:37:57.000000000', 'files': ['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/32d589f03ed0d2744fe15173ebacafd18fced8a9', 'message': ""Don't allow deletion of the router ports without IP addresses\n\nThis patch effectively reverts old patch [1]. From now on it will be not\nallowed to directly remove router ports which don't have fixed IPs\nassigned. Such ports will be treated as any other ports connected to the\nrouters.\nOriginally [1] was introduced to allow cleanup of the router ports for\nwhich subnets were deleted. But now it's not needed anymore as we\nprevent deletion of subnet if there are any ports with IP allocated from\nthat subnet.\n\nCloses-bug: #2025056\n\n[1] https://review.opendev.org/c/openstack/neutron/+/20424\n\nChange-Id: I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265\n""}]",10,886983,32d589f03ed0d2744fe15173ebacafd18fced8a9,43,6,1,11975,,,0,"Don't allow deletion of the router ports without IP addresses

This patch effectively reverts old patch [1]. From now on it will be not
allowed to directly remove router ports which don't have fixed IPs
assigned. Such ports will be treated as any other ports connected to the
routers.
Originally [1] was introduced to allow cleanup of the router ports for
which subnets were deleted. But now it's not needed anymore as we
prevent deletion of subnet if there are any ports with IP allocated from
that subnet.

Closes-bug: #2025056

[1] https://review.opendev.org/c/openstack/neutron/+/20424

Change-Id: I1a3723ae999fefb5dcbe3a60cf1a4902da9f0265
",git fetch https://review.opendev.org/openstack/neutron refs/changes/83/886983/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/db/l3_db.py', 'neutron/tests/unit/db/test_l3_db.py']",2,32d589f03ed0d2744fe15173ebacafd18fced8a9,bug/2025056," 'device_id': '44', 'id': 'f', } with testtools.ExpectedException(n_exc.ServicePortInUse): self.db.prevent_l3_port_deletion(mock.Mock(), None)"," 'id': 'f' } self.db.prevent_l3_port_deletion(None, None)",3,11
openstack%2Fneutron~stable%2Fzed~I0c623a101357226ec0b4e3e42b829303846b6f6b,openstack/neutron,stable/zed,I0c623a101357226ec0b4e3e42b829303846b6f6b,"[OVN] Migrate ""reside-on-redirect-chassis"" for distributed FIP",ABANDONED,2023-07-04 12:39:16.000000000,2023-07-04 12:52:53.000000000,,[],"[{'number': 1, 'created': '2023-07-04 12:39:16.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/5fe4e6ff21447a65babfa5747a125acc900f6912', 'message': '[OVN] Migrate ""reside-on-redirect-chassis"" for distributed FIP\n\nThis patch complement the changes made at\n828765412789a15a5f375060cfab7628c65e9d5d by creating a maintenance task\nresposible for migrating the ""reside-on-redirect-chassis"" option value\nfor existing router ports when the deployment is updated.\n\nChange-Id: I0c623a101357226ec0b4e3e42b829303846b6f6b\nRelated-Bug: #1920976\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n(cherry picked from commit 5bef86847790f60bf2cb87903db8b9ef88051623)\n'}, {'number': 2, 'created': '2023-07-04 12:40:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/64f1824281d6b70e667ce6164f736f1ee3af1591', 'message': '[OVN] Migrate ""reside-on-redirect-chassis"" for distributed FIP\n\nThis patch complement the changes made at\n828765412789a15a5f375060cfab7628c65e9d5d by creating a maintenance task\nresposible for migrating the ""reside-on-redirect-chassis"" option value\nfor existing router ports when the deployment is updated.\n\nChange-Id: I0c623a101357226ec0b4e3e42b829303846b6f6b\nRelated-Bug: #1920976\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n(cherry picked from commit 5bef86847790f60bf2cb87903db8b9ef88051623)\n'}, {'number': 3, 'created': '2023-07-04 12:43:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/neutron/commit/2f14dafe91e54c057d83d6c091158ab0e2357fd8', 'message': '[OVN] Migrate ""reside-on-redirect-chassis"" for distributed FIP\n\nThis patch complement the changes made at\n828765412789a15a5f375060cfab7628c65e9d5d by creating a maintenance task\nresposible for migrating the ""reside-on-redirect-chassis"" option value\nfor existing router ports when the deployment is updated.\n\nChange-Id: I0c623a101357226ec0b4e3e42b829303846b6f6b\nRelated-Bug: #1920976\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n(cherry picked from commit 5bef86847790f60bf2cb87903db8b9ef88051623)\n'}, {'number': 4, 'created': '2023-07-04 12:48:17.000000000', 'files': ['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_maintenance.py', 'neutron/common/ovn/constants.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/maintenance.py'], 'web_link': 'https://opendev.org/openstack/neutron/commit/d348322f09769f08e0137acea0f1a25ad103576c', 'message': '[OVN] Migrate ""reside-on-redirect-chassis"" for distributed FIP\n\nThis patch complement the changes made at\n828765412789a15a5f375060cfab7628c65e9d5d by creating a maintenance task\nresposible for migrating the ""reside-on-redirect-chassis"" option value\nfor existing router ports when the deployment is updated.\n\nChange-Id: I0c623a101357226ec0b4e3e42b829303846b6f6b\nRelated-Bug: #1920976\nSigned-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>\n(cherry picked from commit 5bef86847790f60bf2cb87903db8b9ef88051623)\n'}]",1,887469,d348322f09769f08e0137acea0f1a25ad103576c,6,0,4,35976,,,0,"[OVN] Migrate ""reside-on-redirect-chassis"" for distributed FIP

This patch complement the changes made at
828765412789a15a5f375060cfab7628c65e9d5d by creating a maintenance task
resposible for migrating the ""reside-on-redirect-chassis"" option value
for existing router ports when the deployment is updated.

Change-Id: I0c623a101357226ec0b4e3e42b829303846b6f6b
Related-Bug: #1920976
Signed-off-by: Lucas Alvares Gomes <lucasagomes@gmail.com>
(cherry picked from commit 5bef86847790f60bf2cb87903db8b9ef88051623)
",git fetch https://review.opendev.org/openstack/neutron refs/changes/69/887469/1 && git format-patch -1 --stdout FETCH_HEAD,"['neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_client.py', 'neutron/tests/unit/plugins/ml2/drivers/ovn/mech_driver/ovsdb/test_maintenance.py', 'neutron/common/ovn/constants.py', 'neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/maintenance.py']",4,5fe4e6ff21447a65babfa5747a125acc900f6912,unset-reside-cr-dvr-stable/zed,"<<<<<<< HEAD (b4f7c9 [OVN] Improve Hash Ring logs)======= from neutron_lib.api.definitions import external_net from neutron_lib.api.definitions import provider_net as pnet from neutron_lib.api.definitions import segment as segment_def >>>>>>> CHANGE (5bef86 [OVN] Migrate ""reside-on-redirect-chassis"" for distributed F)<<<<<<< HEAD (b4f7c9 [OVN] Improve Hash Ring logs)======= def check_vlan_distributed_ports(self): """"""Check VLAN distributed ports Check for the option ""reside-on-redirect-chassis"" value for distributed VLAN ports. """""" if not self.has_lock: return context = n_context.get_admin_context() cmds = [] # Get router ports belonging to VLAN networks vlan_nets = self._ovn_client._plugin.get_networks( context, {pnet.NETWORK_TYPE: [n_const.TYPE_VLAN]}) vlan_net_ids = [vn['id'] for vn in vlan_nets] router_ports = self._ovn_client._plugin.get_ports( context, {'network_id': vlan_net_ids, 'device_owner': n_const.ROUTER_PORT_OWNERS}) expected_value = ('false' if ovn_conf.is_ovn_distributed_floating_ip() else 'true') for rp in router_ports: lrp_name = utils.ovn_lrouter_port_name(rp['id']) lrp = self._nb_idl.get_lrouter_port(lrp_name) if lrp.options.get( ovn_const.LRP_OPTIONS_RESIDE_REDIR_CH) != expected_value: opt = {ovn_const.LRP_OPTIONS_RESIDE_REDIR_CH: expected_value} cmds.append(self._nb_idl.db_set( 'Logical_Router_Port', lrp_name, ('options', opt))) if cmds: with self._nb_idl.transaction(check_error=True) as txn: for cmd in cmds: txn.add(cmd) >>>>>>> CHANGE (5bef86 [OVN] Migrate ""reside-on-redirect-chassis"" for distributed F)",,96,0
openstack%2Fbarbican~master~I4abb46dba51a00628c58eeb516074e1a149b8f35,openstack/barbican,master,I4abb46dba51a00628c58eeb516074e1a149b8f35,Add support for Vault Namespaces,MERGED,2021-10-12 10:27:02.000000000,2023-07-04 12:45:18.000000000,2023-07-04 12:44:09.000000000,"[{'_account_id': 7973}, {'_account_id': 9914}, {'_account_id': 10342}, {'_account_id': 14250}, {'_account_id': 22348}]","[{'number': 1, 'created': '2021-10-12 10:27:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/c73442da0b50b377ba33c4d197a9543a060b3f43', 'message': 'Add support for Vault Namespaces\n\nChange https://review.opendev.org/c/openstack/castellan/+/810124\nadded support for Vault namespaces to castellan.\n\nIn order to be able to use that functionality in Barbican, we need\nto register and pass a corresponding config option in Barbican as well.\n\nChange-Id: I4abb46dba51a00628c58eeb516074e1a149b8f35\n'}, {'number': 2, 'created': '2022-02-21 16:55:56.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/dd161daed4424f0e0e984390d4699fa54899a293', 'message': 'Add support for Vault Namespaces\n\nChange https://review.opendev.org/c/openstack/castellan/+/810124\nadded support for Vault namespaces to castellan.\n\nIn order to be able to use that functionality in Barbican, we need\nto register and pass a corresponding config option in Barbican as well.\n\nChange-Id: I4abb46dba51a00628c58eeb516074e1a149b8f35\n'}, {'number': 3, 'created': '2022-06-10 18:22:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/44d0778c98c07ddebcf273e6a7d819b0092da1af', 'message': 'Add support for Vault Namespaces\n\nChange https://review.opendev.org/c/openstack/castellan/+/810124\nadded support for Vault namespaces to castellan.\n\nIn order to be able to use that functionality in Barbican, we need\nto register and pass a corresponding config option in Barbican as well.\n\nChange-Id: I4abb46dba51a00628c58eeb516074e1a149b8f35\n'}, {'number': 4, 'created': '2022-07-15 16:25:50.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/e72c798b50bda6bf776245fd3582231ed2098884', 'message': 'Add support for Vault Namespaces\n\nChange https://review.opendev.org/c/openstack/castellan/+/810124\nadded support for Vault namespaces to castellan.\n\nIn order to be able to use that functionality in Barbican, we need\nto register and pass a corresponding config option in Barbican as well.\n\nChange-Id: I4abb46dba51a00628c58eeb516074e1a149b8f35\n'}, {'number': 5, 'created': '2022-12-20 09:42:09.000000000', 'files': ['barbican/plugin/vault_secret_store.py'], 'web_link': 'https://opendev.org/openstack/barbican/commit/2a23260a345c995cece4707e6302debef21826ad', 'message': 'Add support for Vault Namespaces\n\nChange https://review.opendev.org/c/openstack/castellan/+/810124\nadded support for Vault namespaces to castellan.\n\nIn order to be able to use that functionality in Barbican, we need\nto register and pass a corresponding config option in Barbican as well.\n\nChange-Id: I4abb46dba51a00628c58eeb516074e1a149b8f35\n'}]",0,813606,2a23260a345c995cece4707e6302debef21826ad,18,5,5,9542,,,0,"Add support for Vault Namespaces

Change https://review.opendev.org/c/openstack/castellan/+/810124
added support for Vault namespaces to castellan.

In order to be able to use that functionality in Barbican, we need
to register and pass a corresponding config option in Barbican as well.

Change-Id: I4abb46dba51a00628c58eeb516074e1a149b8f35
",git fetch https://review.opendev.org/openstack/barbican refs/changes/06/813606/5 && git format-patch -1 --stdout FETCH_HEAD,['barbican/plugin/vault_secret_store.py'],1,c73442da0b50b377ba33c4d197a9543a060b3f43,vault-namespaces," cfg.StrOpt(""namespace"", help=_(""Vault Namespace to use for all requests. "" ""Namespaces is a feature available in HasiCorp Vault "" ""Enterprise only."")), vault_use_ssl=conf.vault_plugin.use_ssl, vault_namespace=conf.vault_plugin.namespace", vault_use_ssl=conf.vault_plugin.use_ssl,6,1
openstack%2Fbarbican~master~I86ca68286f7650a99680e6a5db9d4d9c9550910b,openstack/barbican,master,I86ca68286f7650a99680e6a5db9d4d9c9550910b,Add support for Vault namespaces,ABANDONED,2023-01-05 15:56:10.000000000,2023-07-04 11:57:48.000000000,,"[{'_account_id': 10342}, {'_account_id': 14250}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-01-05 15:56:10.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/barbican/commit/0d310af4dd6a83fcb775d14b23b59b905e844d31', 'message': 'Add support for Vault namespaces\n\nAllows an operator to configure a Vault namespace if they\nare using Vault enterprise.\n\nhttps: //www.vaultproject.io/docs/enterprise/namespaces\n\nChange-Id: I86ca68286f7650a99680e6a5db9d4d9c9550910b\n'}, {'number': 2, 'created': '2023-01-05 16:24:04.000000000', 'files': ['barbican/plugin/vault_secret_store.py'], 'web_link': 'https://opendev.org/openstack/barbican/commit/db4a0bee0bea926793bbb3a91b28cc34b2dfef99', 'message': 'Add support for Vault namespaces\n\nAllows an operator to configure a Vault namespace if they\nare using Vault enterprise.\n\nhttps://www.vaultproject.io/docs/enterprise/namespaces\n\nChange-Id: I86ca68286f7650a99680e6a5db9d4d9c9550910b\n'}]",4,869381,db4a0bee0bea926793bbb3a91b28cc34b2dfef99,11,4,2,29543,,,0,"Add support for Vault namespaces

Allows an operator to configure a Vault namespace if they
are using Vault enterprise.

https://www.vaultproject.io/docs/enterprise/namespaces

Change-Id: I86ca68286f7650a99680e6a5db9d4d9c9550910b
",git fetch https://review.opendev.org/openstack/barbican refs/changes/81/869381/2 && git format-patch -1 --stdout FETCH_HEAD,['barbican/plugin/vault_secret_store.py'],1,0d310af4dd6a83fcb775d14b23b59b905e844d31,hash-vault-namespace," cfg.StrOpt('namespace', help='Optional namespace to use for authenticating with vault'), vault_namespace=conf.vault_plugin.namespace,",,3,0
openstack%2Fnova~stable%2Fyoga~Ia873bcc6b07121c9bd0b94c593567d537b4c1112,openstack/nova,stable/yoga,Ia873bcc6b07121c9bd0b94c593567d537b4c1112,Fix fair internal lock used from eventlet.spawn_n,NEW,2022-09-03 15:31:20.000000000,2023-07-04 11:03:21.000000000,,"[{'_account_id': 17685}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-09-03 15:31:20.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/613c2e7de9dc9230e3dc100f04f98e796cb4586c', 'message': ""Fix fair internal lock used from eventlet.spawn_n\n\nThe fasteners lib in version 0.15.0 removed the\nthreading.current_thread workaround for eventlet[1] because eventlet\nseemed to fixed the current_thread issues tracked in [2]. However the\nfix for [2] only fixed half of the problem. The threading.current_thread\ncall works if it is called from thread created by eventlet.spawn.\nHowever if the thread is created with eventlet.spawn_n then\nthreading.current_thread is still broken and returns the ID of the\npython native thread.\n\nThe fasteners' ReaderWriterLock depends heavily on\nthreading.current_thread to decide which thread holds a lock and to\nallow re-entry of that thread. This leads to the situation that\nmultiple threads created from spawn_n could take the same\nReaderWriterLock at the same time.\n\nThe fair internal lock in oslo.concurrency uses ReaderWriterLock and\nas a result such lock is broken for threads created with spawn_n.\n\nNote that this issue was raised with eventlet in [3] when the we\ndetected it via a direct usage of ReaderWriterLock in the nova test\ncode. As [3] did not lead to a solution in eventlet we implemented a\nnova local fix for the test code in [4].\n\nHowever now we detected that nova is effected outside of the test code\nas well. Nova uses oslo.concurrency's fair lock in the ResourceTracker.\nThe oslo.concurrency fair lock is affected by this issue as it uses\nReaderWriterLock in the background.\n\nThere is an oslo.concurrency fix proposed in [5]. But until that is\nmerged, released and the global requirements bumped this patch\nimplement a nova local fix for the issue.\n\n[1] https://github.com/harlowja/fasteners/commit/467ed75ee1e9465ebff8b5edf452770befb93913\n[2] https://github.com/eventlet/eventlet/issues/172\n[3] https://github.com/eventlet/eventlet/issues/731\n[4] https://review.opendev.org/c/openstack/nova/+/813114\n[5] https://review.opendev.org/c/openstack/oslo.concurrency/+/855714\n\nCloses-Bug: #1988311\nChange-Id: Ia873bcc6b07121c9bd0b94c593567d537b4c1112\n(cherry picked from commit d6e6d55dcb2b9289112bff213bbdabb8d245818f)\n""}, {'number': 2, 'created': '2022-09-03 16:08:29.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/6e3722c6047d257fbd6bb71cce3575a3033ce20a', 'message': ""Fix fair internal lock used from eventlet.spawn_n\n\nThe fasteners lib in version 0.15.0 removed the\nthreading.current_thread workaround for eventlet[1] because eventlet\nseemed to fixed the current_thread issues tracked in [2]. However the\nfix for [2] only fixed half of the problem. The threading.current_thread\ncall works if it is called from thread created by eventlet.spawn.\nHowever if the thread is created with eventlet.spawn_n then\nthreading.current_thread is still broken and returns the ID of the\npython native thread.\n\nThe fasteners' ReaderWriterLock depends heavily on\nthreading.current_thread to decide which thread holds a lock and to\nallow re-entry of that thread. This leads to the situation that\nmultiple threads created from spawn_n could take the same\nReaderWriterLock at the same time.\n\nThe fair internal lock in oslo.concurrency uses ReaderWriterLock and\nas a result such lock is broken for threads created with spawn_n.\n\nNote that this issue was raised with eventlet in [3] when the we\ndetected it via a direct usage of ReaderWriterLock in the nova test\ncode. As [3] did not lead to a solution in eventlet we implemented a\nnova local fix for the test code in [4].\n\nHowever now we detected that nova is effected outside of the test code\nas well. Nova uses oslo.concurrency's fair lock in the ResourceTracker.\nThe oslo.concurrency fair lock is affected by this issue as it uses\nReaderWriterLock in the background.\n\nThere is an oslo.concurrency fix proposed in [5]. But until that is\nmerged, released and the global requirements bumped this patch\nimplement a nova local fix for the issue.\n\n[1] https://github.com/harlowja/fasteners/commit/467ed75ee1e9465ebff8b5edf452770befb93913\n[2] https://github.com/eventlet/eventlet/issues/172\n[3] https://github.com/eventlet/eventlet/issues/731\n[4] https://review.opendev.org/c/openstack/nova/+/813114\n[5] https://review.opendev.org/c/openstack/oslo.concurrency/+/855714\n\nCloses-Bug: #1988311\nChange-Id: Ia873bcc6b07121c9bd0b94c593567d537b4c1112\n(cherry picked from commit 66d80ba622687c8878499aa7f26384b0e8b2bdb6)\n""}, {'number': 3, 'created': '2022-09-05 11:33:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/f41a8aca9e9a3ef65b4db451fb16b511fc5a329c', 'message': ""Fix fair internal lock used from eventlet.spawn_n\n\nThe fasteners lib in version 0.15.0 removed the\nthreading.current_thread workaround for eventlet[1] because eventlet\nseemed to fixed the current_thread issues tracked in [2]. However the\nfix for [2] only fixed half of the problem. The threading.current_thread\ncall works if it is called from thread created by eventlet.spawn.\nHowever if the thread is created with eventlet.spawn_n then\nthreading.current_thread is still broken and returns the ID of the\npython native thread.\n\nThe fasteners' ReaderWriterLock depends heavily on\nthreading.current_thread to decide which thread holds a lock and to\nallow re-entry of that thread. This leads to the situation that\nmultiple threads created from spawn_n could take the same\nReaderWriterLock at the same time.\n\nThe fair internal lock in oslo.concurrency uses ReaderWriterLock and\nas a result such lock is broken for threads created with spawn_n.\n\nNote that this issue was raised with eventlet in [3] when the we\ndetected it via a direct usage of ReaderWriterLock in the nova test\ncode. As [3] did not lead to a solution in eventlet we implemented a\nnova local fix for the test code in [4].\n\nHowever now we detected that nova is effected outside of the test code\nas well. Nova uses oslo.concurrency's fair lock in the ResourceTracker.\nThe oslo.concurrency fair lock is affected by this issue as it uses\nReaderWriterLock in the background.\n\nThere is an oslo.concurrency fix proposed in [5]. But until that is\nmerged, released and the global requirements bumped this patch\nimplement a nova local fix for the issue.\n\nNote that an issue[6] has been opened on fasteners to restore the\nworkaround[1].\n\n[1] https://github.com/harlowja/fasteners/commit/467ed75ee1e9465ebff8b5edf452770befb93913\n[2] https://github.com/eventlet/eventlet/issues/172\n[3] https://github.com/eventlet/eventlet/issues/731\n[4] https://review.opendev.org/c/openstack/nova/+/813114\n[5] https://review.opendev.org/c/openstack/oslo.concurrency/+/855714\n[6] https://github.com/harlowja/fasteners/issues/96\n\nCloses-Bug: #1988311\nChange-Id: Ia873bcc6b07121c9bd0b94c593567d537b4c1112\n(cherry picked from commit 3956053dc77d2dfd83513ae0c59a5be521bb9c65)\n""}, {'number': 4, 'created': '2022-09-05 14:36:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/897fee3ac9ac736bde4f8522c4741c6b9ee20c49', 'message': ""Fix fair internal lock used from eventlet.spawn_n\n\nThe fasteners lib in version 0.15.0 removed the\nthreading.current_thread workaround for eventlet[1] because eventlet\nseemed to fixed the current_thread issues tracked in [2]. However the\nfix for [2] only fixed half of the problem. The threading.current_thread\ncall works if it is called from thread created by eventlet.spawn.\nHowever if the thread is created with eventlet.spawn_n then\nthreading.current_thread is still broken and returns the ID of the\npython native thread.\n\nThe fasteners' ReaderWriterLock depends heavily on\nthreading.current_thread to decide which thread holds a lock and to\nallow re-entry of that thread. This leads to the situation that\nmultiple threads created from spawn_n could take the same\nReaderWriterLock at the same time.\n\nThe fair internal lock in oslo.concurrency uses ReaderWriterLock and\nas a result such lock is broken for threads created with spawn_n.\n\nNote that this issue was raised with eventlet in [3] when the we\ndetected it via a direct usage of ReaderWriterLock in the nova test\ncode. As [3] did not lead to a solution in eventlet we implemented a\nnova local fix for the test code in [4].\n\nHowever now we detected that nova is effected outside of the test code\nas well. Nova uses oslo.concurrency's fair lock in the ResourceTracker.\nThe oslo.concurrency fair lock is affected by this issue as it uses\nReaderWriterLock in the background.\n\nThere is an oslo.concurrency fix proposed in [5]. But until that is\nmerged, released and the global requirements bumped this patch\nimplement a nova local fix for the issue.\n\nNote that an issue[6] has been opened on fasteners to restore the\nworkaround[1].\n\n[1] https://github.com/harlowja/fasteners/commit/467ed75ee1e9465ebff8b5edf452770befb93913\n[2] https://github.com/eventlet/eventlet/issues/172\n[3] https://github.com/eventlet/eventlet/issues/731\n[4] https://review.opendev.org/c/openstack/nova/+/813114\n[5] https://review.opendev.org/c/openstack/oslo.concurrency/+/855714\n[6] https://github.com/harlowja/fasteners/issues/96\n\nCloses-Bug: #1988311\nChange-Id: Ia873bcc6b07121c9bd0b94c593567d537b4c1112\n(cherry picked from commit 55d0c180bda80108744e99f5fd16ee3cbbceb9e0)\n""}, {'number': 5, 'created': '2022-09-05 16:31:15.000000000', 'files': ['requirements.txt', 'nova/tests/unit/test_utils.py', 'nova/utils.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/bb039c6a9aec5a02c03691e94055a4448baabd2e', 'message': ""Fix fair internal lock used from eventlet.spawn_n\n\nThe fasteners lib in version 0.15.0 removed the\nthreading.current_thread workaround for eventlet[1] because eventlet\nseemed to fixed the current_thread issues tracked in [2]. However the\nfix for [2] only fixed half of the problem. The threading.current_thread\ncall works if it is called from thread created by eventlet.spawn.\nHowever if the thread is created with eventlet.spawn_n then\nthreading.current_thread is still broken and returns the ID of the\npython native thread.\n\nThe fasteners' ReaderWriterLock depends heavily on\nthreading.current_thread to decide which thread holds a lock and to\nallow re-entry of that thread. This leads to the situation that\nmultiple threads created from spawn_n could take the same\nReaderWriterLock at the same time.\n\nThe fair internal lock in oslo.concurrency uses ReaderWriterLock and\nas a result such lock is broken for threads created with spawn_n.\n\nNote that this issue was raised with eventlet in [3] when the we\ndetected it via a direct usage of ReaderWriterLock in the nova test\ncode. As [3] did not lead to a solution in eventlet we implemented a\nnova local fix for the test code in [4].\n\nHowever now we detected that nova is effected outside of the test code\nas well. Nova uses oslo.concurrency's fair lock in the ResourceTracker.\nThe oslo.concurrency fair lock is affected by this issue as it uses\nReaderWriterLock in the background.\n\nThere is an oslo.concurrency fix proposed in [5]. But until that is\nmerged, released and the global requirements bumped this patch\nimplement a nova local fix for the issue.\n\nNote that an issue[6] has been opened on fasteners to restore the\nworkaround[1].\n\n[1] https://github.com/harlowja/fasteners/commit/467ed75ee1e9465ebff8b5edf452770befb93913\n[2] https://github.com/eventlet/eventlet/issues/172\n[3] https://github.com/eventlet/eventlet/issues/731\n[4] https://review.opendev.org/c/openstack/nova/+/813114\n[5] https://review.opendev.org/c/openstack/oslo.concurrency/+/855714\n[6] https://github.com/harlowja/fasteners/issues/96\n\nCloses-Bug: #1988311\nChange-Id: Ia873bcc6b07121c9bd0b94c593567d537b4c1112\n(cherry picked from commit 298cc8c3ff7e93a6e07c8e5851984cf4e84e4617)\n""}]",1,855718,bb039c6a9aec5a02c03691e94055a4448baabd2e,17,2,5,9708,,,0,"Fix fair internal lock used from eventlet.spawn_n

The fasteners lib in version 0.15.0 removed the
threading.current_thread workaround for eventlet[1] because eventlet
seemed to fixed the current_thread issues tracked in [2]. However the
fix for [2] only fixed half of the problem. The threading.current_thread
call works if it is called from thread created by eventlet.spawn.
However if the thread is created with eventlet.spawn_n then
threading.current_thread is still broken and returns the ID of the
python native thread.

The fasteners' ReaderWriterLock depends heavily on
threading.current_thread to decide which thread holds a lock and to
allow re-entry of that thread. This leads to the situation that
multiple threads created from spawn_n could take the same
ReaderWriterLock at the same time.

The fair internal lock in oslo.concurrency uses ReaderWriterLock and
as a result such lock is broken for threads created with spawn_n.

Note that this issue was raised with eventlet in [3] when the we
detected it via a direct usage of ReaderWriterLock in the nova test
code. As [3] did not lead to a solution in eventlet we implemented a
nova local fix for the test code in [4].

However now we detected that nova is effected outside of the test code
as well. Nova uses oslo.concurrency's fair lock in the ResourceTracker.
The oslo.concurrency fair lock is affected by this issue as it uses
ReaderWriterLock in the background.

There is an oslo.concurrency fix proposed in [5]. But until that is
merged, released and the global requirements bumped this patch
implement a nova local fix for the issue.

Note that an issue[6] has been opened on fasteners to restore the
workaround[1].

[1] https://github.com/harlowja/fasteners/commit/467ed75ee1e9465ebff8b5edf452770befb93913
[2] https://github.com/eventlet/eventlet/issues/172
[3] https://github.com/eventlet/eventlet/issues/731
[4] https://review.opendev.org/c/openstack/nova/+/813114
[5] https://review.opendev.org/c/openstack/oslo.concurrency/+/855714
[6] https://github.com/harlowja/fasteners/issues/96

Closes-Bug: #1988311
Change-Id: Ia873bcc6b07121c9bd0b94c593567d537b4c1112
(cherry picked from commit 298cc8c3ff7e93a6e07c8e5851984cf4e84e4617)
",git fetch https://review.opendev.org/openstack/nova refs/changes/18/855718/1 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/test_utils.py', 'nova/utils.py']",2,613c2e7de9dc9230e3dc100f04f98e796cb4586c,bug/1988311,"from unittest import mock_synchronized = lockutils.synchronized_with_prefix('nova-') def synchronized(*args, **kwargs): """"""The lockutils.synchronized decorator but patched to use the proper threading.current_thread call in an eventlet monkey patched environment. This hack can be removed once oslo.concurrency, or fasteners, or eventlet fixes this issues. See: * https://github.com/eventlet/eventlet/issues/731 * https://bugs.launchpad.net/oslo.concurrency/+bug/1988311 """""" def wrapper(f): # Apply oslo.concurrency's lock decorator first f = _synchronized(*args, **kwargs)(f) # Then make another wrapper around the result to make sure that the # lock is fixed before the function enters through the original # synchronized decorator taking the lock. # While it is tempting just to replace threading.current_thread with # eventlet.getcurrent in fixer() that replacement will be in place for # the whole run of ``f`` which has unexpected side effects. E.g. the # logging library calls threading.current_thread().name which is not a # valid thing for a greenlet.greenlet object returned by # eventlet.getcurrent if called from a thread created by # eventlet.spawn_n(). # So we need a more surgical change. Fortunately fasteners' # ReaderWriterLock caches the value of threading.current_thread when # the lock is initialized. So we only need to replace that cached # value to make lock local change. @functools.wraps(f) def fixer(*args, **kwargs): patched = eventlet.patcher.is_monkey_patched(""thread"") if not patched: # we are not eventlet monkey patched, so we don't need to do # anything return f(*args, **kwargs) import fasteners orig_lock_init = fasteners.lock.ReaderWriterLock.__init__ def wrapped_lock_init(_self, *args, **kwargs): orig_lock_init(_self, *args, **kwargs) _self._current_thread = eventlet.getcurrent with mock.patch( ""fasteners.lock.ReaderWriterLock.__init__"", new=wrapped_lock_init, ): return f(*args, **kwargs) return fixer return wrapper ",synchronized = lockutils.synchronized_with_prefix('nova-'),137,1
openstack%2Frequirements~master~I035f1c2325b2cdf4b561129f01ad90a640516005,openstack/requirements,master,I035f1c2325b2cdf4b561129f01ad90a640516005,update constraint for neutron-lib to new release 3.7.0,MERGED,2023-06-29 12:38:21.000000000,2023-07-04 10:59:27.000000000,2023-07-04 10:57:33.000000000,"[{'_account_id': 11904}, {'_account_id': 14288}, {'_account_id': 16688}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 12:38:21.000000000', 'files': ['upper-constraints.txt'], 'web_link': 'https://opendev.org/openstack/requirements/commit/d1ea9bcb95612bbde92d1a85b31074c142278e5e', 'message': 'update constraint for neutron-lib to new release 3.7.0\n\nmeta: version: 3.7.0\nmeta: diff-start: -\nmeta: series: bobcat\nmeta: branch: master\nmeta: release-type: release\nmeta: pypi: yes\nmeta: first: no\nmeta: release:Author: Rodolfo Alonso Hernandez <ralonsoh@redhat.com>\nmeta: release:Commit: Rodolfo Alonso <ralonsoh@redhat.com>\nmeta: release:Change-Id: I765db5b9ebbcb685a8f4e3804105da64c3b8bf84\nmeta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>\nmeta: release:Code-Review+1: Bernard Cafarelli <bcafarel@redhat.com>\nmeta: release:Workflow+1: Thierry Carrez <thierry@openstack.org>\nmeta: release:Code-Review+1: Brian Haley <haleyb.dev@gmail.com>\nmeta: release:Code-Review+2: Thierry Carrez <thierry@openstack.org>\nmeta: release:Code-Review+1: Lajos Katona <katonalala@gmail.com>\nChange-Id: I035f1c2325b2cdf4b561129f01ad90a640516005\n'}]",9,887270,d1ea9bcb95612bbde92d1a85b31074c142278e5e,28,4,1,11131,,,0,"update constraint for neutron-lib to new release 3.7.0

meta: version: 3.7.0
meta: diff-start: -
meta: series: bobcat
meta: branch: master
meta: release-type: release
meta: pypi: yes
meta: first: no
meta: release:Author: Rodolfo Alonso Hernandez <ralonsoh@redhat.com>
meta: release:Commit: Rodolfo Alonso <ralonsoh@redhat.com>
meta: release:Change-Id: I765db5b9ebbcb685a8f4e3804105da64c3b8bf84
meta: release:Code-Review+2: Elod Illes <elod.illes@est.tech>
meta: release:Code-Review+1: Bernard Cafarelli <bcafarel@redhat.com>
meta: release:Workflow+1: Thierry Carrez <thierry@openstack.org>
meta: release:Code-Review+1: Brian Haley <haleyb.dev@gmail.com>
meta: release:Code-Review+2: Thierry Carrez <thierry@openstack.org>
meta: release:Code-Review+1: Lajos Katona <katonalala@gmail.com>
Change-Id: I035f1c2325b2cdf4b561129f01ad90a640516005
",git fetch https://review.opendev.org/openstack/requirements refs/changes/70/887270/1 && git format-patch -1 --stdout FETCH_HEAD,['upper-constraints.txt'],1,d1ea9bcb95612bbde92d1a85b31074c142278e5e,new-release,neutron-lib===3.7.0,neutron-lib===3.6.1,1,1
openstack%2Fnova~stable%2Fyoga~I3ad2cc340b66c8bf6f4d37be28bdf7a5e479e755,openstack/nova,stable/yoga,I3ad2cc340b66c8bf6f4d37be28bdf7a5e479e755,DNM test openstacksdk project cleanup,ABANDONED,2022-09-20 10:14:39.000000000,2023-07-04 10:58:56.000000000,,"[{'_account_id': 11604}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-09-20 10:14:39.000000000', 'files': ['.zuul.yaml'], 'web_link': 'https://opendev.org/openstack/nova/commit/d8543e869863c833087d845df29a25ef08501f22', 'message': 'DNM test openstacksdk project cleanup\n\ndummy change to test stability of the project cleanup in cinder on older\nbranches.\n\nDepends-On: https://review.opendev.org/c/openstack/openstacksdk/+/858268\nChange-Id: I3ad2cc340b66c8bf6f4d37be28bdf7a5e479e755\n'}]",0,858512,d8543e869863c833087d845df29a25ef08501f22,5,2,1,27900,,,0,"DNM test openstacksdk project cleanup

dummy change to test stability of the project cleanup in cinder on older
branches.

Depends-On: https://review.opendev.org/c/openstack/openstacksdk/+/858268
Change-Id: I3ad2cc340b66c8bf6f4d37be28bdf7a5e479e755
",git fetch https://review.opendev.org/openstack/nova refs/changes/12/858512/1 && git format-patch -1 --stdout FETCH_HEAD,['.zuul.yaml'],1,d8543e869863c833087d845df29a25ef08501f22,,# Dummy change,,1,0
openstack%2Fnova~stable%2Fyoga~Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed,openstack/nova,stable/yoga,Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed,Fix get_segments_id with subnets without segment_id,MERGED,2023-05-22 15:39:59.000000000,2023-07-04 10:58:35.000000000,2023-07-04 10:57:28.000000000,"[{'_account_id': 7634}, {'_account_id': 9708}, {'_account_id': 17685}, {'_account_id': 22348}, {'_account_id': 34860}]","[{'number': 1, 'created': '2023-05-22 15:39:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/nova/commit/e9e308e433f48e6c98e52924523ee00a8db86124', 'message': 'Fix get_segments_id with subnets without segment_id\n\nUnfortunatly when we merged Ie166f3b51fddeaf916cda7c5ac34bbcdda0fd17a we\nforgot that subnets can have no segment_id field.\n\nChange-Id: Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed\nCloses-Bug: #2018375\n(cherry picked from commit 6d7bd6a03446d5227d515b2b4c0da632ef4aa4a1)\n(cherry picked from commit 6b8d9d419170fb0ec2c6df561a0874e6362382c1)\n'}, {'number': 2, 'created': '2023-05-30 17:53:20.000000000', 'files': ['nova/tests/unit/network/test_neutron.py', 'nova/network/neutron.py'], 'web_link': 'https://opendev.org/openstack/nova/commit/9a6a421c045a8031ff0923cca9aa7195fe987896', 'message': 'Fix get_segments_id with subnets without segment_id\n\nUnfortunatly when we merged Ie166f3b51fddeaf916cda7c5ac34bbcdda0fd17a we\nforgot that subnets can have no segment_id field.\n\nChange-Id: Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed\nCloses-Bug: #2018375\n(cherry picked from commit 6d7bd6a03446d5227d515b2b4c0da632ef4aa4a1)\n(cherry picked from commit 6b8d9d419170fb0ec2c6df561a0874e6362382c1)\n(cherry picked from commit 77db64237b23050d94df113a38412c5333d23357)\n'}]",5,883724,9a6a421c045a8031ff0923cca9aa7195fe987896,25,5,2,7166,,,0,"Fix get_segments_id with subnets without segment_id

Unfortunatly when we merged Ie166f3b51fddeaf916cda7c5ac34bbcdda0fd17a we
forgot that subnets can have no segment_id field.

Change-Id: Idb35b7e3c69fe8efe498abe4ebcc6cad8918c4ed
Closes-Bug: #2018375
(cherry picked from commit 6d7bd6a03446d5227d515b2b4c0da632ef4aa4a1)
(cherry picked from commit 6b8d9d419170fb0ec2c6df561a0874e6362382c1)
(cherry picked from commit 77db64237b23050d94df113a38412c5333d23357)
",git fetch https://review.opendev.org/openstack/nova refs/changes/24/883724/2 && git format-patch -1 --stdout FETCH_HEAD,"['nova/tests/unit/network/test_neutron.py', 'nova/network/neutron.py']",2,e9e308e433f48e6c98e52924523ee00a8db86124,bug/2018375, if subnet.get('segment_id') is not None], if subnet['segment_id'] is not None],18,2
openstack%2Fkeystone~master~I64252086f994901a5ebe05afec37a6afd3a192ee,openstack/keystone,master,I64252086f994901a5ebe05afec37a6afd3a192ee,sql: Remove duplicate constraints,MERGED,2023-04-05 18:01:36.000000000,2023-07-04 10:55:11.000000000,2023-07-04 10:53:58.000000000,"[{'_account_id': 7414}, {'_account_id': 14250}, {'_account_id': 22348}, {'_account_id': 32238}, {'_account_id': 32761}]","[{'number': 1, 'created': '2023-04-05 18:01:36.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/9c6a4841c3aca27660cb5b8d6f8f4751838b75a3', 'message': 'sql: Remove duplicate constraints\n\nA primary key is automatically unique, therefore if one or columns is\nincluded in a primary key constraint there is no need to add a separate\nunique constraint for these columns. Remove it.\n\nNote that this only affects MySQL. Both SQLite and PostgreSQL appear to\nignore the duplicate unique constraint. As a result, it was necessary to\nrun auto-generation against MySQL instead of the default SQLite. The\nactual command used was similar to what we normally do, however.\n\n  $ python keystone/common/sql/migrations/manage.py revision \\\n      --autogenerate --message \'Remove duplicate constraints\'\n\nAs always, the resulting schema migrations then needed some manual\ntweaks to remove ""please adjust!"" comments and unnecessary imports but\nthey are correct.\n\nChange-Id: I64252086f994901a5ebe05afec37a6afd3a192ee\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}, {'number': 2, 'created': '2023-04-06 10:48:29.000000000', 'files': ['keystone/common/sql/migrations/env.py', 'keystone/common/sql/migrations/versions/CONTRACT_HEAD', 'keystone/resource/backends/sql_model.py', 'keystone/tests/unit/common/sql/test_upgrades.py', 'keystone/common/sql/migrations/versions/bobcat/contract/c88cdce8f248_remove_duplicate_constraints.py', 'keystone/tests/unit/test_sql_banned_operations.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/535bc8e22ecb67eaa17a7a0a61037aac6f602b21', 'message': 'sql: Remove duplicate constraints\n\nA primary key is automatically unique, therefore if one or columns is\nincluded in a primary key constraint there is no need to add a separate\nunique constraint for these columns. Remove it.\n\nNote that this only affects MySQL. Both SQLite and PostgreSQL appear to\nignore the duplicate unique constraint. As a result, it was necessary to\nrun auto-generation against MySQL instead of the default SQLite. The\nactual command used was similar to what we normally do, however.\n\n  $ python keystone/common/sql/migrations/manage.py revision \\\n      --autogenerate --message \'Remove duplicate constraints\'\n\nAs always, the resulting schema migrations then needed some manual\ntweaks to remove ""please adjust!"" comments and unnecessary imports but\nthey are correct.\n\nChange-Id: I64252086f994901a5ebe05afec37a6afd3a192ee\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}]",0,879678,535bc8e22ecb67eaa17a7a0a61037aac6f602b21,15,5,2,15334,,,0,"sql: Remove duplicate constraints

A primary key is automatically unique, therefore if one or columns is
included in a primary key constraint there is no need to add a separate
unique constraint for these columns. Remove it.

Note that this only affects MySQL. Both SQLite and PostgreSQL appear to
ignore the duplicate unique constraint. As a result, it was necessary to
run auto-generation against MySQL instead of the default SQLite. The
actual command used was similar to what we normally do, however.

  $ python keystone/common/sql/migrations/manage.py revision \
      --autogenerate --message 'Remove duplicate constraints'

As always, the resulting schema migrations then needed some manual
tweaks to remove ""please adjust!"" comments and unnecessary imports but
they are correct.

Change-Id: I64252086f994901a5ebe05afec37a6afd3a192ee
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/keystone refs/changes/78/879678/2 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/common/sql/migrations/env.py', 'keystone/common/sql/migrations/versions/CONTRACT_HEAD', 'keystone/resource/backends/sql_model.py', 'keystone/tests/unit/common/sql/test_upgrades.py', 'keystone/common/sql/migrations/versions/bobcat/contract/c88cdce8f248_remove_duplicate_constraints.py', 'keystone/tests/unit/test_sql_banned_operations.py']",6,9c6a4841c3aca27660cb5b8d6f8f4751838b75a3,sqlalchemy-20," def _pre_upgrade_c88cdce8f248(self, connection): if connection.engine.name != 'mysql': return # NOTE(stephenfin): Even though the migration is written to handle # names generated by both alembic and sqlalchemy-migrate, we only check # for the former here since we don't apply sqlalchemy-migrate # migrations anymore inspector = sqlalchemy.inspect(connection) indexes = inspector.get_indexes('project_tag') self.assertIn('project_id', {x['name'] for x in indexes}) def _check_c88cdce8f248(self, connection): # This migration only applies to MySQL if connection.engine.name != 'mysql': return inspector = sqlalchemy.inspect(connection) indexes = inspector.get_indexes('project_tag') self.assertNotIn('project_id', {x['name'] for x in indexes}) ",,78,16
openstack%2Fkeystone~master~Ie1be3df78189f4165079a43d0a9050fcece6989b,openstack/keystone,master,Ie1be3df78189f4165079a43d0a9050fcece6989b,sql: Fix incorrect constraints,MERGED,2022-08-02 11:37:15.000000000,2023-07-04 10:31:36.000000000,2023-07-04 10:30:11.000000000,"[{'_account_id': 7414}, {'_account_id': 14250}, {'_account_id': 15334}, {'_account_id': 22348}]","[{'number': 1, 'created': '2022-08-02 11:37:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/9f2311be936bf4d38816c1efd25db3f134edd0de', 'message': 'sql: Fix incorrect constraints\n\nThis is our first tests of the autogeneration tooling integrated in\nchange I17c9ff9508c5e2bd9521c18973af093d7550ab5a. To generate this, we\nsimply removed the list of ""skipped"" constraints defined in the \'env.py\'\nfile and then ran the following command within a virtualenv:\n\n  $ python keystone/common/sql/migrations/manage.py revision \\\n      --autogenerate --message \'Fix incorrect constraints\'\n\nThe resulting schema migrations then needed some manual tweaks to remove\n""please adjust!"" comments (don\'t worry, the commands were correct.\n\nChange-Id: Ie1be3df78189f4165079a43d0a9050fcece6989b\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}, {'number': 2, 'created': '2022-08-02 11:49:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/9e42474172c1345e8a3f61adf945f3d7047494b4', 'message': 'sql: Fix incorrect constraints\n\nThis is our first test of the autogeneration tooling integrated in\nchange I17c9ff9508c5e2bd9521c18973af093d7550ab5a. To generate this, we\nsimply removed the list of ""skipped"" constraints defined in the \'env.py\'\nfile and then ran the following command within a virtualenv:\n\n  $ python keystone/common/sql/migrations/manage.py revision \\\n      --autogenerate --message \'Fix incorrect constraints\'\n\nThe resulting schema migrations then needed some manual tweaks to remove\n""please adjust!"" comments (don\'t worry, the commands were correct).\n\nChange-Id: Ie1be3df78189f4165079a43d0a9050fcece6989b\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}, {'number': 3, 'created': '2022-08-02 16:19:30.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/2a3c7cc7411568001c50786f7a18facd92cea9ea', 'message': 'sql: Fix incorrect constraints\n\nThis is our first test of the autogeneration tooling integrated in\nchange I17c9ff9508c5e2bd9521c18973af093d7550ab5a. To generate this, we\nsimply removed the list of ""skipped"" constraints defined in the \'env.py\'\nfile and then ran the following command within a virtualenv:\n\n  $ python keystone/common/sql/migrations/manage.py revision \\\n      --autogenerate --message \'Fix incorrect constraints\'\n\nThe resulting schema migrations then needed some manual tweaks to remove\n""please adjust!"" comments (don\'t worry, the commands were correct).\n\nChange-Id: Ie1be3df78189f4165079a43d0a9050fcece6989b\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}, {'number': 4, 'created': '2023-02-28 17:26:46.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/d4765d95ebba91fcc373a7bb52620c72055f50f9', 'message': 'sql: Fix incorrect constraints\n\nThis is our first test of the autogeneration tooling integrated in\nchange I17c9ff9508c5e2bd9521c18973af093d7550ab5a. To generate this, we\nsimply removed the list of ""skipped"" constraints defined in the \'env.py\'\nfile and then ran the following command within a virtualenv:\n\n  $ python keystone/common/sql/migrations/manage.py revision \\\n      --autogenerate --message \'Fix incorrect constraints\'\n\nThe resulting schema migrations then needed some manual tweaks to remove\n""please adjust!"" comments (don\'t worry, the commands were correct).\n\nChange-Id: Ie1be3df78189f4165079a43d0a9050fcece6989b\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}, {'number': 5, 'created': '2023-02-28 17:55:18.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/keystone/commit/709d54c9d3fcc97e4cc590d2edb52babbb29d759', 'message': 'sql: Fix incorrect constraints\n\nThis is our first test of the autogeneration tooling integrated in\nchange I17c9ff9508c5e2bd9521c18973af093d7550ab5a. To generate this, we\nsimply removed the list of ""skipped"" constraints defined in the \'env.py\'\nfile and then ran the following command within a virtualenv:\n\n  $ python keystone/common/sql/migrations/manage.py revision \\\n      --autogenerate --message \'Fix incorrect constraints\'\n\nThe resulting schema migrations then needed some manual tweaks to remove\n""please adjust!"" comments (don\'t worry, the commands were correct).\n\nChange-Id: Ie1be3df78189f4165079a43d0a9050fcece6989b\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}, {'number': 6, 'created': '2023-04-05 18:01:36.000000000', 'files': ['keystone/common/sql/migrations/env.py', 'keystone/common/sql/migrations/versions/EXPAND_HEAD', 'keystone/common/sql/migrations/versions/bobcat/expand/b4f8b3f584e0_fix_incorrect_constraints.py', 'keystone/common/sql/migrations/versions/CONTRACT_HEAD', 'keystone/tests/unit/common/sql/test_upgrades.py', 'keystone/common/sql/migrations/versions/bobcat/contract/99de3849d860_fix_incorrect_constraints.py', 'keystone/tests/unit/test_sql_banned_operations.py'], 'web_link': 'https://opendev.org/openstack/keystone/commit/56c47d0a39e5ea4af4269e8a7196a293fa9b9cfe', 'message': 'sql: Fix incorrect constraints\n\nThis is our first test of the autogeneration tooling integrated in\nchange I17c9ff9508c5e2bd9521c18973af093d7550ab5a. To generate this, we\nsimply removed all but one of the ""skipped"" constraints defined in the\n\'env.py\' file and then ran the following command within a virtualenv:\n\n  $ python keystone/common/sql/migrations/manage.py revision \\\n      --autogenerate --message \'Fix incorrect constraints\'\n\nThe resulting schema migrations then needed some manual tweaks to remove\n""please adjust!"" comments (don\'t worry, the commands were correct).\n\nChange-Id: Ie1be3df78189f4165079a43d0a9050fcece6989b\nSigned-off-by: Stephen Finucane <sfinucan@redhat.com>\n'}]",3,851845,56c47d0a39e5ea4af4269e8a7196a293fa9b9cfe,24,4,6,15334,,,0,"sql: Fix incorrect constraints

This is our first test of the autogeneration tooling integrated in
change I17c9ff9508c5e2bd9521c18973af093d7550ab5a. To generate this, we
simply removed all but one of the ""skipped"" constraints defined in the
'env.py' file and then ran the following command within a virtualenv:

  $ python keystone/common/sql/migrations/manage.py revision \
      --autogenerate --message 'Fix incorrect constraints'

The resulting schema migrations then needed some manual tweaks to remove
""please adjust!"" comments (don't worry, the commands were correct).

Change-Id: Ie1be3df78189f4165079a43d0a9050fcece6989b
Signed-off-by: Stephen Finucane <sfinucan@redhat.com>
",git fetch https://review.opendev.org/openstack/keystone refs/changes/45/851845/5 && git format-patch -1 --stdout FETCH_HEAD,"['keystone/common/sql/migrations/env.py', 'keystone/common/sql/migrations/versions/zed/expand/b4f8b3f584e0_fix_incorrect_constraints.py', 'keystone/common/sql/migrations/versions/zed/contract/99de3849d860_fix_incorrect_constraints.py']",3,9f2311be936bf4d38816c1efd25db3f134edd0de,sqlalchemy-20,"# Licensed under the Apache License, Version 2.0 (the ""License""); you may # not use this file except in compliance with the License. You may obtain # a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations # under the License. """"""Fix incorrect constraints Revision ID: 99de3849d860 Revises: e25ffa003242 Create Date: 2022-08-02 12:23:25.525035 """""" from alembic import op # revision identifiers, used by Alembic. revision = '99de3849d860' down_revision = 'e25ffa003242' branch_labels = None depends_on = None def upgrade(): with op.batch_alter_table('access_rule', schema=None) as batch_op: batch_op.drop_constraint('access_rule_external_id_key', type_='unique') with op.batch_alter_table('trust', schema=None) as batch_op: batch_op.drop_constraint( 'duplicate_trust_constraint_expanded', type_='unique' ) ",,76,25
openstack%2Fcharm-rabbitmq-server~stable%2Ffocal~Id4e3dbd85748e41bb4b1c8db282495cfffaa823d,openstack/charm-rabbitmq-server,stable/focal,Id4e3dbd85748e41bb4b1c8db282495cfffaa823d,Refactor methods which query rabbit,NEW,2023-03-13 14:44:59.000000000,2023-07-04 09:49:33.000000000,,"[{'_account_id': 20648}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-03-13 14:44:59.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-rabbitmq-server/commit/5b240bcc3639b474e6004b52a40bde1924d4d686', 'message': 'Refactor methods which query rabbit\n\nRefactor methods which query rabbit to remove the duplication\naround checking if json output is supported.\n\nChange-Id: Id4e3dbd85748e41bb4b1c8db282495cfffaa823d\n(cherry picked from commit dbab66b0c56b926cef5053aae77f5ff0f0b3b627)\n'}, {'number': 2, 'created': '2023-05-19 00:57:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-rabbitmq-server/commit/61152a7a154955c106425dac06c177f5b718cc39', 'message': 'Refactor methods which query rabbit\n\nRefactor methods which query rabbit to remove the duplication\naround checking if json output is supported.\n\nChange-Id: Id4e3dbd85748e41bb4b1c8db282495cfffaa823d\n(cherry picked from commit dbab66b0c56b926cef5053aae77f5ff0f0b3b627)\n'}, {'number': 3, 'created': '2023-06-05 09:49:15.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-rabbitmq-server/commit/97c4df1dabbadb7d096c35f48baba772e0a5bcc6', 'message': 'Refactor methods which query rabbit\n\nRefactor methods which query rabbit to remove the duplication\naround checking if json output is supported.\n\nChange-Id: Id4e3dbd85748e41bb4b1c8db282495cfffaa823d\n(cherry picked from commit dbab66b0c56b926cef5053aae77f5ff0f0b3b627)\n'}, {'number': 4, 'created': '2023-06-05 09:57:45.000000000', 'files': ['unit_tests/test_rabbit_utils.py', 'hooks/rabbit_utils.py'], 'web_link': 'https://opendev.org/openstack/charm-rabbitmq-server/commit/091cd02ee264a6c2e18ce5361e4535116942b44a', 'message': 'Refactor methods which query rabbit\n\nRefactor methods which query rabbit to remove the duplication\naround checking if json output is supported.\n\nChange-Id: Id4e3dbd85748e41bb4b1c8db282495cfffaa823d\n(cherry picked from commit dbab66b0c56b926cef5053aae77f5ff0f0b3b627)\n'}]",1,877289,091cd02ee264a6c2e18ce5361e4535116942b44a,12,2,4,9247,,,0,"Refactor methods which query rabbit

Refactor methods which query rabbit to remove the duplication
around checking if json output is supported.

Change-Id: Id4e3dbd85748e41bb4b1c8db282495cfffaa823d
(cherry picked from commit dbab66b0c56b926cef5053aae77f5ff0f0b3b627)
",git fetch https://review.opendev.org/openstack/charm-rabbitmq-server refs/changes/89/877289/4 && git format-patch -1 --stdout FETCH_HEAD,"['unit_tests/test_rabbitmq_server_relations.py', 'unit_tests/test_rabbit_utils.py', 'hooks/rabbit_utils.py']",3,5b240bcc3639b474e6004b52a40bde1924d4d686,bug/1899183-stable/focal,"def run_cmd(cmd): """"""Run provided command and decode the output. :param cmd: Command to run :type cmd: List[str] :returns: output from command :rtype: str """""" output = subprocess.check_output(cmd) output = output.decode('utf-8') return output def rabbit_supports_json(): """"""Check if version of rabbit supports json formatted output. :returns: If json output is supported. :rtype: bool """""" return caching_cmp_pkgrevno('rabbitmq-server', '3.8.2') >= 0 @cached def caching_cmp_pkgrevno(package, revno, pkgcache=None): """"""Compare supplied revno with the revno of the installed package. * 1 => Installed revno is greater than supplied arg * 0 => Installed revno is the same as supplied arg * -1 => Installed revno is less than supplied arg :param package: Package to check revno of :type package: str :param revno: Revision number to compare against :type revno: str :param pkgcache: Version obj from pkgcache :type pkgcache: ubuntu_apt_pkg.Version :returns: Whether versions match :rtype: int """""" return cmp_pkgrevno(package, revno, pkgcache) def query_rabbit(cmd, raw_processor=None, json_processor=None, binary=RABBITMQ_CTL): """"""Run query against rabbit. Run query against rabbit and then run post-query processor on the output. If the version of rabbit that is installed supports formatting the output in json format then the '--formatter=json' flag is added. :param cmd: Query to run :type cmd: List[str] :param raw_processor: Function to call with command output as the only argument. :type raw_processor: Callable :param json_processor: Function to call with json loaded output as the only argument. :type json_processor: Callable :returns: Return processed output from query :rtype: ANY """""" cmd.insert(0, binary) if rabbit_supports_json(): cmd.append('--formatter=json') output = json.loads(run_cmd(cmd)) if json_processor: return json_processor(output) else: # A processor may not be needed for loaded json. return output else: if raw_processor: return raw_processor(run_cmd(cmd)) else: raise NotImplementedError def list_vhosts(): """"""Returns a list of all the available vhosts :returns: List of vhosts :rtype: [str] """""" def _json_processor(output): return [ll['name'] for ll in output] def _raw_processor(output): if '...done' in output: try: return query_rabbit( ['list_vhosts'], raw_processor=_raw_processor, json_processor=_json_processor)def list_vhost_queue_info(vhost): """"""Provide a list of queue info objects for the given vhost. :returns: List of dictionaries of queue information eg [{'name': 'queue name', 'messages': 0, 'consumers': 1}, ...] :rtype: List[Dict[str, Union[str, int]]] :raises: CalledProcessError def _raw_processor(output): queue_info = [] if '...done' in output: queues = output.split('\n')[1:-2] else: queues = output.split('\n')[1:-1] for queue in queues: [qname, qmsgs, qconsumers] = queue.split() queue_info.append({ 'name': qname, 'messages': int(qmsgs), 'consumers': int(qconsumers) }) return queue_info cmd = ['-p', vhost, 'list_queues', 'name', 'messages', 'consumers'] return query_rabbit( cmd, raw_processor=_raw_processor) def list_users(): """"""Returns a list of users. :returns: List of users :rtype: [str] """""" def _json_processor(output): return [ll['user'] for ll in output] def _raw_processor(output): lines = output.split('\n')[1:] return [line.split('\t')[0] for line in lines] return query_rabbit( ['list_users'], raw_processor=_raw_processor, json_processor=_json_processor) def vhost_queue_info(vhost): return list_vhost_queue_info(vhost) return user in list_users() """"""Grant all permissions on a vhost to a user. :param user: Name of user to give permissions to. :type user: str :param vhost: Name of vhost to give permissions on :type vhost: str """""" log( ""Granting permissions for user {} on vhost {}"".format(user, vhost), level='DEBUG')def get_plugin_manager(): """"""Find the path to the executable for managing plugins. :returns: Path to rabbitmq-plugins executable :rtype: str """""" manager = glob.glob( '/usr/lib/rabbitmq/lib/rabbitmq_server-*/sbin/rabbitmq-plugins')[0] return manager plugin_manager = get_plugin_manager() subprocess.check_call([plugin_manager, action, plugin])","def list_vhosts(): """""" Returns a list of all the available vhosts """""" try: cmd = [RABBITMQ_CTL, 'list_vhosts'] # NOTE(ajkavanagh): In focal and above, rabbitmq-server now has a # --formatter option. if caching_cmp_pkgrevno('rabbitmq-server', '3.8.2') >= 0: cmd.append('--formatter=json') output = subprocess.check_output(cmd) output = output.decode('utf-8') if caching_cmp_pkgrevno('rabbitmq-server', '3.8.2') >= 0: decoded = json.loads(output) return [ll['name'] for ll in decoded] # NOTE(jamespage): Earlier rabbitmqctl versions append ""...done"" # to the output of list_vhosts elif '...done' in output:def vhost_queue_info(vhost): """"""Provide a list of queue info objects for the given vhost in RabbitMQ Each object provides name (str), messages (int), and consumers (int) @raises CalledProcessError on failure to list_queues of the vhost cmd = [RABBITMQ_CTL, '-p', vhost, 'list_queues', 'name', 'messages', 'consumers'] # NOTE(ajkavanagh): In focal and above, rabbitmq-server now has a # --formatter option. if caching_cmp_pkgrevno('rabbitmq-server', '3.8.2') >= 0: cmd.append('--formatter=json') output = subprocess.check_output(cmd).decode('utf-8') queue_info = [] if caching_cmp_pkgrevno('rabbitmq-server', '3.8.2') >= 0: decoded = json.loads(output) # note that the json is already in the desired output of queue_info # below return decoded # NOTE(jamespage): Earlier rabbitmqctl versions append ""...done"" # to the output of list_queues elif '...done' in output: queues = output.split('\n')[1:-2] else: queues = output.split('\n')[1:-1] for queue in queues: [qname, qmsgs, qconsumers] = queue.split() queue_info.append({ 'name': qname, 'messages': int(qmsgs), 'consumers': int(qconsumers) }) return queue_info cmd = [RABBITMQ_CTL, 'list_users'] # NOTE(ajkavanagh): In focal and above, rabbitmq-server now has a # --formatter option. if caching_cmp_pkgrevno('rabbitmq-server', '3.8.2') >= 0: cmd.append('--formatter=json') out = subprocess.check_output(cmd).decode('utf-8') decoded = json.loads(out) users = [ll['user'] for ll in decoded] return user in users # NOTE(ajkavanagh): pre 3.8.2 the code needs to deal with just a text # output out = subprocess.check_output(cmd).decode('utf-8') lines = out.split('\n')[1:] for line in lines: _user = line.split('\t')[0] if _user == user: return True return False@cached def caching_cmp_pkgrevno(package, revno, pkgcache=None): return cmp_pkgrevno(package, revno, pkgcache) _rabbitmq_plugins = \ glob.glob('/usr/lib/rabbitmq/lib/rabbitmq_server-*' '/sbin/rabbitmq-plugins')[0] subprocess.check_call([_rabbitmq_plugins, action, plugin])",174,79
openstack%2Fmagnum~master~I137f549c4eb281fbb5cfa62d58d714f59418872a,openstack/magnum,master,I137f549c4eb281fbb5cfa62d58d714f59418872a,[WIP][DNM]test patch for constraints,ABANDONED,2023-07-03 04:41:21.000000000,2023-07-04 08:37:45.000000000,,"[{'_account_id': 12404}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 04:41:21.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/ab1ee89bec36c61262f5067b0eb6859eb21bf914', 'message': '[WIP][DNM]test patch for constraints\n\nChange-Id: I137f549c4eb281fbb5cfa62d58d714f59418872a\n'}, {'number': 2, 'created': '2023-07-03 05:02:37.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/b474fd3a0e4951ccbff78de9989609bd40218858', 'message': '[WIP][DNM]test patch for constraints\n\nChange-Id: I137f549c4eb281fbb5cfa62d58d714f59418872a\n'}, {'number': 3, 'created': '2023-07-03 08:14:28.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/fbc5610bf2f545bb75b163afbaab99be9470886f', 'message': '[WIP][DNM]test patch for constraints\n\nChange-Id: I137f549c4eb281fbb5cfa62d58d714f59418872a\n'}, {'number': 4, 'created': '2023-07-03 09:21:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/433133da9cd7e6d4c9f3056924dff4795c7eb87a', 'message': '[WIP][DNM]test patch for constraints\n\nChange-Id: I137f549c4eb281fbb5cfa62d58d714f59418872a\n'}, {'number': 5, 'created': '2023-07-03 10:01:26.000000000', 'files': ['magnum/tests/unit/api/controllers/v1/test_certificate.py', 'magnum/tests/unit/api/test_hooks.py', 'devstack/lib/magnum', 'magnum/common/policies/cluster.py', 'magnum/tests/unit/api/controllers/v1/test_stats.py', 'devstack/settings', 'magnum/tests/fakes.py', 'requirements.txt', 'magnum/common/context.py', 'magnum/tests/unit/api/base.py', 'magnum/tests/unit/api/controllers/v1/test_cluster_actions.py', 'magnum/tests/unit/api/controllers/test_root.py', 'magnum/api/hooks.py', 'magnum/common/policies/stats.py', 'releasenotes/notes/allow_admin_perform_acitons-cc988655bb72b3f3.yaml', 'releasenotes/notes/enable-enforce-scope-and-new-defaults-7e6e503f74283071.yaml', 'magnum/common/policies/certificate.py', 'magnum/common/policies/nodegroup.py', 'magnum/common/policies/federation.py', 'magnum/tests/unit/api/controllers/v1/test_quota.py', 'magnum/tests/unit/common/test_context.py', 'magnum/common/policies/cluster_template.py', 'magnum/common/policies/base.py', 'magnum/tests/unit/api/controllers/v1/test_cluster.py', 'magnum/tests/unit/api/controllers/v1/test_nodegroup.py', 'magnum/common/policies/quota.py', 'magnum/common/policy.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/846d028b84fc18d49227f9c9361394875b360b71', 'message': '[WIP][DNM]test patch for constraints\n\nChange-Id: I137f549c4eb281fbb5cfa62d58d714f59418872a\n'}]",0,887472,846d028b84fc18d49227f9c9361394875b360b71,11,2,5,12404,,,0,"[WIP][DNM]test patch for constraints

Change-Id: I137f549c4eb281fbb5cfa62d58d714f59418872a
",git fetch https://review.opendev.org/openstack/magnum refs/changes/72/887472/4 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/tests/unit/api/controllers/v1/test_certificate.py', 'magnum/tests/unit/api/test_hooks.py', 'devstack/lib/magnum', 'magnum/common/policies/cluster.py', 'magnum/tests/unit/api/controllers/v1/test_stats.py', 'devstack/settings', 'magnum/tests/fakes.py', 'requirements.txt', 'magnum/common/context.py', 'magnum/tests/unit/api/base.py', 'magnum/tests/unit/api/controllers/v1/test_cluster_actions.py', 'magnum/tests/unit/api/controllers/test_root.py', 'magnum/api/hooks.py', 'magnum/common/policies/stats.py', 'releasenotes/notes/allow_admin_perform_acitons-cc988655bb72b3f3.yaml', 'releasenotes/notes/enable-enforce-scope-and-new-defaults-7e6e503f74283071.yaml', 'magnum/common/policies/certificate.py', 'magnum/common/policies/nodegroup.py', 'magnum/common/policies/federation.py', 'magnum/tests/unit/api/controllers/v1/test_quota.py', 'magnum/tests/unit/common/test_context.py', 'magnum/common/policies/cluster_template.py', 'magnum/common/policies/base.py', 'magnum/tests/unit/api/controllers/v1/test_cluster.py', 'magnum/tests/unit/api/controllers/v1/test_nodegroup.py', 'magnum/common/policies/quota.py', 'magnum/common/policy.py']",27,ab1ee89bec36c61262f5067b0eb6859eb21bf914,,"from oslo_log import log as loggingLOG = logging.getLogger(__name__) try: result = enforcer.enforce(rule, target, credentials, do_raise=do_raise, exc=exc, *args, **kwargs) except policy.InvalidScope as ex: LOG.debug(f""Invalide scope while enforce policy :{str(ex)}"") raise exc(action=rule) return result"," return enforcer.enforce(rule, target, credentials, do_raise=do_raise, exc=exc, *args, **kwargs)",424,120
openstack%2Fmagnum~stable%2Fzed~I137f549c4eb281fbb5cfa62d58d714f59418872a,openstack/magnum,stable/zed,I137f549c4eb281fbb5cfa62d58d714f59418872a,[DNM]test patch for constraints,ABANDONED,2023-07-03 16:13:41.000000000,2023-07-04 08:37:40.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-07-03 16:13:41.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/magnum/commit/4333f53b69386de00bbb0c6dd0acb0963065892e', 'message': '[DNM]test patch for constraints\n\nChange-Id: I137f549c4eb281fbb5cfa62d58d714f59418872a\n'}, {'number': 2, 'created': '2023-07-03 16:14:53.000000000', 'files': ['magnum/tests/unit/api/controllers/v1/test_certificate.py', 'magnum/tests/unit/api/test_hooks.py', 'devstack/lib/magnum', 'magnum/common/policies/cluster.py', 'magnum/tests/unit/api/controllers/v1/test_stats.py', 'devstack/settings', 'magnum/tests/fakes.py', 'requirements.txt', 'magnum/common/context.py', 'magnum/tests/unit/api/base.py', 'magnum/tests/unit/api/controllers/v1/test_cluster_actions.py', 'magnum/tests/unit/api/controllers/test_root.py', 'magnum/api/hooks.py', 'magnum/common/policies/stats.py', 'releasenotes/notes/allow_admin_perform_acitons-cc988655bb72b3f3.yaml', 'releasenotes/notes/enable-enforce-scope-and-new-defaults-7e6e503f74283071.yaml', 'magnum/common/policies/certificate.py', 'magnum/common/policies/nodegroup.py', 'magnum/common/policies/federation.py', 'magnum/tests/unit/api/controllers/v1/test_quota.py', 'magnum/tests/unit/common/test_context.py', 'magnum/common/policies/cluster_template.py', 'magnum/common/policies/base.py', 'magnum/tests/unit/api/controllers/v1/test_cluster.py', 'magnum/tests/unit/api/controllers/v1/test_nodegroup.py', 'magnum/common/policies/quota.py', 'magnum/common/policy.py'], 'web_link': 'https://opendev.org/openstack/magnum/commit/e05bd3c8eb59cbb9be0fd1f54de818742d75c959', 'message': '[DNM]test patch for constraints\n\nChange-Id: I137f549c4eb281fbb5cfa62d58d714f59418872a\n'}]",0,887458,e05bd3c8eb59cbb9be0fd1f54de818742d75c959,4,1,2,12404,,,0,"[DNM]test patch for constraints

Change-Id: I137f549c4eb281fbb5cfa62d58d714f59418872a
",git fetch https://review.opendev.org/openstack/magnum refs/changes/58/887458/1 && git format-patch -1 --stdout FETCH_HEAD,"['magnum/tests/unit/api/controllers/v1/test_certificate.py', 'magnum/tests/unit/api/test_hooks.py', 'devstack/lib/magnum', 'magnum/common/policies/cluster.py', 'magnum/tests/unit/api/controllers/v1/test_stats.py', 'devstack/settings', 'magnum/tests/fakes.py', 'requirements.txt', 'magnum/common/context.py', 'magnum/tests/unit/api/base.py', 'magnum/tests/unit/api/controllers/v1/test_cluster_actions.py', 'magnum/tests/unit/api/controllers/test_root.py', 'magnum/api/hooks.py', 'magnum/common/policies/stats.py', 'releasenotes/notes/allow_admin_perform_acitons-cc988655bb72b3f3.yaml', 'releasenotes/notes/enable-enforce-scope-and-new-defaults-7e6e503f74283071.yaml', 'magnum/common/policies/certificate.py', 'magnum/common/policies/nodegroup.py', 'magnum/common/policies/federation.py', 'magnum/tests/unit/api/controllers/v1/test_quota.py', 'magnum/tests/unit/common/test_context.py', 'magnum/common/policies/cluster_template.py', 'magnum/common/policies/base.py', 'magnum/tests/unit/api/controllers/v1/test_cluster.py', 'magnum/tests/unit/api/controllers/v1/test_nodegroup.py', 'magnum/common/policies/quota.py', 'magnum/common/policy.py']",27,4333f53b69386de00bbb0c6dd0acb0963065892e,,"from oslo_log import log as loggingLOG = logging.getLogger(__name__) try: result = enforcer.enforce(rule, target, credentials, do_raise=do_raise, exc=exc, *args, **kwargs) except policy.InvalidScope as ex: LOG.debug(f""Invalide scope while enforce policy :{str(ex)}"") raise exc(action=rule) return result"," return enforcer.enforce(rule, target, credentials, do_raise=do_raise, exc=exc, *args, **kwargs)",432,115
openstack%2Fcharm-cinder~stable%2Fxena~Iaecae3c22db1f4f2309f73f8c6836e6c072b848b,openstack/charm-cinder,stable/xena,Iaecae3c22db1f4f2309f73f8c6836e6c072b848b,Render [service_user] only for identity-service relation,MERGED,2023-06-29 19:33:46.000000000,2023-07-04 08:34:16.000000000,2023-07-04 08:34:16.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:33:46.000000000', 'files': ['templates/ussuri/cinder.conf'], 'web_link': 'https://opendev.org/openstack/charm-cinder/commit/128acd4d703589bbc66a706b563c8bc90b9434b7', 'message': 'Render [service_user] only for identity-service relation\n\nThe service token section [service_user] is not required when\ncinder-volume is deployed as a separate service. In other words\nit is not required for the identity-credentials relation.\n\nThe [service_user] section is nearly the same as the\n[keystone_authtoken] section, and the keystone_authtoken data\nis only produced for the IdentityServiceContext, therefore this\nchange will not render [service_user] for the\nIdentityCredentialsContext.\n\nCloses-Bug: #2024676\nChange-Id: Iaecae3c22db1f4f2309f73f8c6836e6c072b848b\n(cherry picked from commit ebbedcbf58660ce13823152d6943fee036af7e11)\n'}]",0,887334,128acd4d703589bbc66a706b563c8bc90b9434b7,8,4,1,11805,,,0,"Render [service_user] only for identity-service relation

The service token section [service_user] is not required when
cinder-volume is deployed as a separate service. In other words
it is not required for the identity-credentials relation.

The [service_user] section is nearly the same as the
[keystone_authtoken] section, and the keystone_authtoken data
is only produced for the IdentityServiceContext, therefore this
change will not render [service_user] for the
IdentityCredentialsContext.

Closes-Bug: #2024676
Change-Id: Iaecae3c22db1f4f2309f73f8c6836e6c072b848b
(cherry picked from commit ebbedcbf58660ce13823152d6943fee036af7e11)
",git fetch https://review.opendev.org/openstack/charm-cinder refs/changes/34/887334/1 && git format-patch -1 --stdout FETCH_HEAD,['templates/ussuri/cinder.conf'],1,128acd4d703589bbc66a706b563c8bc90b9434b7,bug/1992840,{% if keystone_authtoken -%}{% endif -%},,2,0
openstack%2Fcharm-cinder~stable%2Fxena~I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9,openstack/charm-cinder,stable/xena,I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9,Add support for using service tokens,MERGED,2023-06-29 19:11:05.000000000,2023-07-04 08:34:15.000000000,2023-07-04 08:34:15.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:11:05.000000000', 'files': ['templates/ussuri/cinder.conf'], 'web_link': 'https://opendev.org/openstack/charm-cinder/commit/2c5908b3009f0de1a294276dc02d9445da6cae52', 'message': 'Add support for using service tokens\n\nThis patch configures Cinder to send a service token along with the\nreceived user token on requests to other services. This can allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Cinder. Also with this patch Cinder will\naccept request from other services with invalid user tokens but valid\nservice tokens. Service tokens exist since Openstack Queens.\n\nCloses-Bug: #1992840\nChange-Id: I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9\n(cherry picked from commit 81c330b5d87a64a7a9ec601f4dd263b836ee9c01)\n(cherry picked from commit b96c85f5a6b8e3d173a4f810fd4d5fd82737795e)\n'}]",3,887319,2c5908b3009f0de1a294276dc02d9445da6cae52,11,4,1,11805,,,0,"Add support for using service tokens

This patch configures Cinder to send a service token along with the
received user token on requests to other services. This can allow those
other services to accept the request even if the user token has been
invalidated since received by Cinder. Also with this patch Cinder will
accept request from other services with invalid user tokens but valid
service tokens. Service tokens exist since Openstack Queens.

Closes-Bug: #1992840
Change-Id: I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9
(cherry picked from commit 81c330b5d87a64a7a9ec601f4dd263b836ee9c01)
(cherry picked from commit b96c85f5a6b8e3d173a4f810fd4d5fd82737795e)
",git fetch https://review.opendev.org/openstack/charm-cinder refs/changes/19/887319/1 && git format-patch -1 --stdout FETCH_HEAD,['templates/ussuri/cinder.conf'],1,2c5908b3009f0de1a294276dc02d9445da6cae52,bug/1992840,"{% include ""section-service-user"" %} ",,2,0
openstack%2Fcharm-cinder~stable%2Fxena~I408caab40e78ff72c139b13e6f45d42fdccbe1a2,openstack/charm-cinder,stable/xena,I408caab40e78ff72c139b13e6f45d42fdccbe1a2,charm-helpers sync,MERGED,2023-06-29 18:36:13.000000000,2023-07-04 08:34:14.000000000,2023-07-04 08:34:14.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:36:13.000000000', 'files': ['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/storage/linux/ceph.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'charmhelpers/contrib/openstack/templates/haproxy.cfg', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken'], 'web_link': 'https://opendev.org/openstack/charm-cinder/commit/e1977bed58d3a41a990cf39911e0e01a131df3f8', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: I408caab40e78ff72c139b13e6f45d42fdccbe1a2\n'}]",0,887303,e1977bed58d3a41a990cf39911e0e01a131df3f8,9,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: I408caab40e78ff72c139b13e6f45d42fdccbe1a2
",git fetch https://review.opendev.org/openstack/charm-cinder refs/changes/03/887303/1 && git format-patch -1 --stdout FETCH_HEAD,"['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/storage/linux/ceph.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'charmhelpers/contrib/openstack/templates/haproxy.cfg', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken']",17,e1977bed58d3a41a990cf39911e0e01a131df3f8,bug/1992840,{% if service_type -%} service_type = {{ service_type }} {% endif -%} service_token_roles = {{ admin_role }} service_token_roles_required = True,,95,12
openstack%2Fopenstack-ansible-haproxy_server~master~I9e53f1fa0b4ce1e276bfc5c792d976216c30f471,openstack/openstack-ansible-haproxy_server,master,I9e53f1fa0b4ce1e276bfc5c792d976216c30f471,Allow using domain name as internal_lb_vip_address,ABANDONED,2023-05-30 08:28:44.000000000,2023-07-04 08:26:30.000000000,,[{'_account_id': 22348}],"[{'number': 1, 'created': '2023-05-30 08:28:44.000000000', 'files': ['handlers/main.yml', 'vars/main.yml'], 'web_link': 'https://opendev.org/openstack/openstack-ansible-haproxy_server/commit/549112af9cd90cc07607852b1ac032ea4b6f0adb', 'message': 'Allow using domain name as internal_lb_vip_address\n\nChange-Id: I9e53f1fa0b4ce1e276bfc5c792d976216c30f471\n'}]",2,884660,549112af9cd90cc07607852b1ac032ea4b6f0adb,5,1,1,34653,,,0,"Allow using domain name as internal_lb_vip_address

Change-Id: I9e53f1fa0b4ce1e276bfc5c792d976216c30f471
",git fetch https://review.opendev.org/openstack/openstack-ansible-haproxy_server refs/changes/60/884660/1 && git format-patch -1 --stdout FETCH_HEAD,"['handlers/main.yml', 'vars/main.yml']",2,549112af9cd90cc07607852b1ac032ea4b6f0adb,multiceph," {% for vip in _haproxy_tls_vip_binds %} {% set _vip_interface = vip['interface'] | default('') %} {% set san = 'DNS:' ~ ansible_facts['hostname'] ~ ',DNS:' ~ ansible_facts['fqdn'] ~ ',' ~ (vip['address'] | ansible.utils.ipaddr) | ternary('IP:', 'DNS:') ~ vip['address'] %} {% if vip['address'] == haproxy_bind_internal_lb_vip_address %} {% set san = san ~ (internal_lb_vip_address | ansible.utils.ipaddr) | ternary('', ',DNS:' ~ internal_lb_vip_address) %} {% endif %} {% if vip['address'] == haproxy_bind_external_lb_vip_address %} {% set san = san ~ (external_lb_vip_address | ansible.utils.ipaddr) | ternary('', ',DNS:' ~ external_lb_vip_address) %} {% endif %} {% set _ = _pki_certs.append( { 'name': 'haproxy_' ~ ansible_facts['hostname'] ~ '-' ~ (_vip_interface is truthy) | ternary(vip['address'] ~ '-' ~ _vip_interface, vip['address']), 'provider': 'ownca', 'cn': ansible_facts['hostname'], 'san': san, 'signed_by': haproxy_pki_intermediate_cert_name, } ) %}"," {% for vip in haproxy_tls_vip_binds %} {% set _vip_interface = vip['interface'] | default('') %} {% set _ = _pki_certs.append( { 'name': 'haproxy_' ~ ansible_facts['hostname'] ~ '-' ~ (_vip_interface is truthy) | ternary(vip['address'] ~ '-' ~ _vip_interface, vip['address']), 'provider': 'ownca', 'cn': ansible_facts['hostname'], 'san': 'DNS:' ~ ansible_facts['hostname'] ~ ',DNS:' ~ ansible_facts['fqdn'] ~ ',' ~ (vip['address'] | ansible.utils.ipaddr) | ternary('IP:', 'DNS:') ~ vip['address'], 'signed_by': haproxy_pki_intermediate_cert_name, } ) %}",19,12
openstack%2Fcharm-cinder~stable%2Fwallaby~Iaecae3c22db1f4f2309f73f8c6836e6c072b848b,openstack/charm-cinder,stable/wallaby,Iaecae3c22db1f4f2309f73f8c6836e6c072b848b,Render [service_user] only for identity-service relation,MERGED,2023-06-29 19:33:58.000000000,2023-07-04 08:24:07.000000000,2023-07-04 08:24:07.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:33:58.000000000', 'files': ['templates/ussuri/cinder.conf'], 'web_link': 'https://opendev.org/openstack/charm-cinder/commit/3e7d3efa13d28c277061030979013a52bab33c11', 'message': 'Render [service_user] only for identity-service relation\n\nThe service token section [service_user] is not required when\ncinder-volume is deployed as a separate service. In other words\nit is not required for the identity-credentials relation.\n\nThe [service_user] section is nearly the same as the\n[keystone_authtoken] section, and the keystone_authtoken data\nis only produced for the IdentityServiceContext, therefore this\nchange will not render [service_user] for the\nIdentityCredentialsContext.\n\nCloses-Bug: #2024676\nChange-Id: Iaecae3c22db1f4f2309f73f8c6836e6c072b848b\n(cherry picked from commit ebbedcbf58660ce13823152d6943fee036af7e11)\n'}]",0,887335,3e7d3efa13d28c277061030979013a52bab33c11,8,4,1,11805,,,0,"Render [service_user] only for identity-service relation

The service token section [service_user] is not required when
cinder-volume is deployed as a separate service. In other words
it is not required for the identity-credentials relation.

The [service_user] section is nearly the same as the
[keystone_authtoken] section, and the keystone_authtoken data
is only produced for the IdentityServiceContext, therefore this
change will not render [service_user] for the
IdentityCredentialsContext.

Closes-Bug: #2024676
Change-Id: Iaecae3c22db1f4f2309f73f8c6836e6c072b848b
(cherry picked from commit ebbedcbf58660ce13823152d6943fee036af7e11)
",git fetch https://review.opendev.org/openstack/charm-cinder refs/changes/35/887335/1 && git format-patch -1 --stdout FETCH_HEAD,['templates/ussuri/cinder.conf'],1,3e7d3efa13d28c277061030979013a52bab33c11,bug/1992840,{% if keystone_authtoken -%}{% endif -%},,2,0
openstack%2Fcharm-cinder~stable%2Fwallaby~I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9,openstack/charm-cinder,stable/wallaby,I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9,Add support for using service tokens,MERGED,2023-06-29 19:11:31.000000000,2023-07-04 08:24:05.000000000,2023-07-04 08:24:05.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:11:31.000000000', 'files': ['templates/ussuri/cinder.conf'], 'web_link': 'https://opendev.org/openstack/charm-cinder/commit/3ad66cac1e5f1cfd0879e529a0199c0e9911f9e5', 'message': 'Add support for using service tokens\n\nThis patch configures Cinder to send a service token along with the\nreceived user token on requests to other services. This can allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Cinder. Also with this patch Cinder will\naccept request from other services with invalid user tokens but valid\nservice tokens. Service tokens exist since Openstack Queens.\n\nCloses-Bug: #1992840\nChange-Id: I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9\n(cherry picked from commit 81c330b5d87a64a7a9ec601f4dd263b836ee9c01)\n(cherry picked from commit b96c85f5a6b8e3d173a4f810fd4d5fd82737795e)\n'}]",0,887320,3ad66cac1e5f1cfd0879e529a0199c0e9911f9e5,8,4,1,11805,,,0,"Add support for using service tokens

This patch configures Cinder to send a service token along with the
received user token on requests to other services. This can allow those
other services to accept the request even if the user token has been
invalidated since received by Cinder. Also with this patch Cinder will
accept request from other services with invalid user tokens but valid
service tokens. Service tokens exist since Openstack Queens.

Closes-Bug: #1992840
Change-Id: I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9
(cherry picked from commit 81c330b5d87a64a7a9ec601f4dd263b836ee9c01)
(cherry picked from commit b96c85f5a6b8e3d173a4f810fd4d5fd82737795e)
",git fetch https://review.opendev.org/openstack/charm-cinder refs/changes/20/887320/1 && git format-patch -1 --stdout FETCH_HEAD,['templates/ussuri/cinder.conf'],1,3ad66cac1e5f1cfd0879e529a0199c0e9911f9e5,bug/1992840,"{% include ""section-service-user"" %} ",,2,0
openstack%2Fcharm-cinder~stable%2Fwallaby~Ib080c878070cfe5e8217b45f43ebb5c6ee13fd79,openstack/charm-cinder,stable/wallaby,Ib080c878070cfe5e8217b45f43ebb5c6ee13fd79,charm-helpers sync,MERGED,2023-06-29 18:43:21.000000000,2023-07-04 08:24:04.000000000,2023-07-04 08:24:04.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:43:21.000000000', 'files': ['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/storage/linux/ceph.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'charmhelpers/contrib/openstack/templates/haproxy.cfg', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken'], 'web_link': 'https://opendev.org/openstack/charm-cinder/commit/c7fce2abecf06caa8f3f176b34a7054a8d158130', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: Ib080c878070cfe5e8217b45f43ebb5c6ee13fd79\n'}]",0,887310,c7fce2abecf06caa8f3f176b34a7054a8d158130,8,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: Ib080c878070cfe5e8217b45f43ebb5c6ee13fd79
",git fetch https://review.opendev.org/openstack/charm-cinder refs/changes/10/887310/1 && git format-patch -1 --stdout FETCH_HEAD,"['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/storage/linux/ceph.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'charmhelpers/contrib/openstack/templates/haproxy.cfg', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken']",17,c7fce2abecf06caa8f3f176b34a7054a8d158130,bug/1992840,{% if service_type -%} service_type = {{ service_type }} {% endif -%} service_token_roles = {{ admin_role }} service_token_roles_required = True,,101,13
openstack%2Fcharm-cinder~stable%2Fvictoria~Iaecae3c22db1f4f2309f73f8c6836e6c072b848b,openstack/charm-cinder,stable/victoria,Iaecae3c22db1f4f2309f73f8c6836e6c072b848b,Render [service_user] only for identity-service relation,MERGED,2023-06-29 19:34:09.000000000,2023-07-04 08:24:03.000000000,2023-07-04 08:24:03.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:34:09.000000000', 'files': ['templates/ussuri/cinder.conf'], 'web_link': 'https://opendev.org/openstack/charm-cinder/commit/7d8a79fcb6893f0face057f481566cceb4ea9e01', 'message': 'Render [service_user] only for identity-service relation\n\nThe service token section [service_user] is not required when\ncinder-volume is deployed as a separate service. In other words\nit is not required for the identity-credentials relation.\n\nThe [service_user] section is nearly the same as the\n[keystone_authtoken] section, and the keystone_authtoken data\nis only produced for the IdentityServiceContext, therefore this\nchange will not render [service_user] for the\nIdentityCredentialsContext.\n\nCloses-Bug: #2024676\nChange-Id: Iaecae3c22db1f4f2309f73f8c6836e6c072b848b\n(cherry picked from commit ebbedcbf58660ce13823152d6943fee036af7e11)\n'}]",0,887336,7d8a79fcb6893f0face057f481566cceb4ea9e01,8,4,1,11805,,,0,"Render [service_user] only for identity-service relation

The service token section [service_user] is not required when
cinder-volume is deployed as a separate service. In other words
it is not required for the identity-credentials relation.

The [service_user] section is nearly the same as the
[keystone_authtoken] section, and the keystone_authtoken data
is only produced for the IdentityServiceContext, therefore this
change will not render [service_user] for the
IdentityCredentialsContext.

Closes-Bug: #2024676
Change-Id: Iaecae3c22db1f4f2309f73f8c6836e6c072b848b
(cherry picked from commit ebbedcbf58660ce13823152d6943fee036af7e11)
",git fetch https://review.opendev.org/openstack/charm-cinder refs/changes/36/887336/1 && git format-patch -1 --stdout FETCH_HEAD,['templates/ussuri/cinder.conf'],1,7d8a79fcb6893f0face057f481566cceb4ea9e01,bug/1992840,{% if keystone_authtoken -%}{% endif -%},,2,0
openstack%2Fopenstack-helm~master~I6fba78163f193dbe053f2e33befe95e6b4d03a03,openstack/openstack-helm,master,I6fba78163f193dbe053f2e33befe95e6b4d03a03,Run compute-kit umbrella job with Zed release,MERGED,2023-07-03 18:14:05.000000000,2023-07-04 08:16:59.000000000,2023-07-04 08:14:44.000000000,"[{'_account_id': 22348}, {'_account_id': 33330}]","[{'number': 1, 'created': '2023-07-03 18:14:05.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/ddc8b56bb5452851b77bbd3e279c91b1c3073e2c', 'message': 'Check zed umbrella\n\nChange-Id: I6fba78163f193dbe053f2e33befe95e6b4d03a03\n'}, {'number': 2, 'created': '2023-07-03 18:16:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/1458f84ecb2eef7ebb8fcbf9d46910e825235bfe', 'message': 'Check zed umbrella\n\nChange-Id: I6fba78163f193dbe053f2e33befe95e6b4d03a03\n'}, {'number': 3, 'created': '2023-07-03 19:17:19.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/c301171df25e5e1ddaabcc35e8a3f0571dcb1c04', 'message': 'Add zed umbrella job\n\nChange-Id: I6fba78163f193dbe053f2e33befe95e6b4d03a03\n'}, {'number': 4, 'created': '2023-07-03 20:57:51.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/5771e5dd0b3e4ae7037def0571faaafa4c34e9e5', 'message': 'Add zed umbrella job\n\nChange-Id: I6fba78163f193dbe053f2e33befe95e6b4d03a03\n'}, {'number': 5, 'created': '2023-07-03 21:35:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/e367df11597bdceb97bdeb3ebd90abf90333c4bd', 'message': 'Add zed umbrella job\n\nChange-Id: I6fba78163f193dbe053f2e33befe95e6b4d03a03\n'}, {'number': 6, 'created': '2023-07-04 00:32:56.000000000', 'files': ['openstack/values_overrides/nova/zed-ubuntu_focal.yaml', 'releasenotes/notes/openstack.yaml', 'openstack/values_overrides/keystone/zed-ubuntu_focal.yaml', 'openstack/values_overrides/libvirt/zed-ubuntu_focal.yaml', 'openstack/values_overrides/glance/zed-ubuntu_focal.yaml', 'zuul.d/jobs-openstack-helm.yaml', 'openstack/values_overrides/horizon/zed-ubuntu_focal.yaml', 'openstack/values_overrides/placement/zed-ubuntu_focal.yaml', 'openstack/values_overrides/neutron/zed-ubuntu_focal.yaml', 'zuul.d/project.yaml', 'openstack/values_overrides/heat/zed-ubuntu_focal.yaml', 'openstack/Chart.yaml'], 'web_link': 'https://opendev.org/openstack/openstack-helm/commit/277ade05066affc731842675b648bebd0b0ed347', 'message': 'Run compute-kit umbrella job with Zed release\n\nChange-Id: I6fba78163f193dbe053f2e33befe95e6b4d03a03\n'}]",3,887532,277ade05066affc731842675b648bebd0b0ed347,16,2,6,3009,,,0,"Run compute-kit umbrella job with Zed release

Change-Id: I6fba78163f193dbe053f2e33befe95e6b4d03a03
",git fetch https://review.opendev.org/openstack/openstack-helm refs/changes/32/887532/4 && git format-patch -1 --stdout FETCH_HEAD,['zuul.d/project.yaml'],1,ddc8b56bb5452851b77bbd3e279c91b1c3073e2c,zed_umbrella, # - openstack-helm-lint # - openstack-helm-bandit # - openstack-helm-cinder-victoria-ubuntu_focal # - openstack-helm-cinder-wallaby-ubuntu_focal # - openstack-helm-cinder-xena-ubuntu_focal # - openstack-helm-cinder-yoga-ubuntu_focal # - openstack-helm-cinder-zed-ubuntu_focal # - openstack-helm-compute-kit-victoria-ubuntu_focal # - openstack-helm-compute-kit-victoria-ubuntu_focal-umbrella # - openstack-helm-compute-kit-wallaby-ubuntu_focal # - openstack-helm-compute-kit-xena-ubuntu_focal # - openstack-helm-compute-kit-yoga-ubuntu_focal # - openstack-helm-compute-kit-zed-ubuntu_focal # - openstack-helm-horizon-victoria-ubuntu_focal # - openstack-helm-horizon-wallaby-ubuntu_focal # - openstack-helm-keystone-ldap - openstack-helm-compute-kit-zed-ubuntu_focal-umbrella, - openstack-helm-lint - openstack-helm-bandit - openstack-helm-cinder-victoria-ubuntu_focal - openstack-helm-cinder-wallaby-ubuntu_focal - openstack-helm-cinder-xena-ubuntu_focal - openstack-helm-cinder-yoga-ubuntu_focal - openstack-helm-cinder-zed-ubuntu_focal - openstack-helm-compute-kit-victoria-ubuntu_focal - openstack-helm-compute-kit-victoria-ubuntu_focal-umbrella - openstack-helm-compute-kit-wallaby-ubuntu_focal - openstack-helm-compute-kit-xena-ubuntu_focal - openstack-helm-compute-kit-yoga-ubuntu_focal - openstack-helm-compute-kit-zed-ubuntu_focal - openstack-helm-horizon-victoria-ubuntu_focal - openstack-helm-horizon-wallaby-ubuntu_focal - openstack-helm-keystone-ldap,17,16
openstack%2Fcharm-nova-cloud-controller~stable%2Fwallaby~I95021600da8af12cb75ef5681fb5af8780ade4f8,openstack/charm-nova-cloud-controller,stable/wallaby,I95021600da8af12cb75ef5681fb5af8780ade4f8,Add support for using service tokens,MERGED,2023-06-29 19:14:06.000000000,2023-07-04 08:15:55.000000000,2023-07-04 08:15:55.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-06-29 19:14:06.000000000', 'files': ['templates/train/nova.conf', 'templates/pike/nova.conf', 'templates/rocky/nova.conf'], 'web_link': 'https://opendev.org/openstack/charm-nova-cloud-controller/commit/fab2bc29f3b246724a3260ea755abb365ef5aefb', 'message': 'Add support for using service tokens\n\nThis patch configures nova-cloud-controller to send a service token\nalong with the received user token on requests sent to other services.\nThis allows those other services to accept the request even if the user\ntoken has been invalidated since received by the nova services running\nin nova-cloud-controller units, the same applies for incoming requests\nfrom other services. Service tokens exist since Openstack Queens.\n\nChange-Id: I95021600da8af12cb75ef5681fb5af8780ade4f8\nCloses-Bug: #1992840\n(cherry picked from commit fd810f9afd92904cd66544c00610f830fd337299)\n(cherry picked from commit 98b637d8e9d9df19cd001e582f846e2046fba1bd)\n'}]",0,887328,fab2bc29f3b246724a3260ea755abb365ef5aefb,9,5,1,11805,,,0,"Add support for using service tokens

This patch configures nova-cloud-controller to send a service token
along with the received user token on requests sent to other services.
This allows those other services to accept the request even if the user
token has been invalidated since received by the nova services running
in nova-cloud-controller units, the same applies for incoming requests
from other services. Service tokens exist since Openstack Queens.

Change-Id: I95021600da8af12cb75ef5681fb5af8780ade4f8
Closes-Bug: #1992840
(cherry picked from commit fd810f9afd92904cd66544c00610f830fd337299)
(cherry picked from commit 98b637d8e9d9df19cd001e582f846e2046fba1bd)
",git fetch https://review.opendev.org/openstack/charm-nova-cloud-controller refs/changes/28/887328/1 && git format-patch -1 --stdout FETCH_HEAD,"['templates/train/nova.conf', 'templates/pike/nova.conf', 'templates/rocky/nova.conf']",3,fab2bc29f3b246724a3260ea755abb365ef5aefb,bug/1992840,"{% include ""section-service-user"" %} ",,6,0
openstack%2Fcharm-nova-cloud-controller~stable%2Fwallaby~Ifd1aa100a8d742ea8c0a5cf0473de078a14790ed,openstack/charm-nova-cloud-controller,stable/wallaby,Ifd1aa100a8d742ea8c0a5cf0473de078a14790ed,charm-helpers sync,MERGED,2023-06-29 18:42:41.000000000,2023-07-04 08:15:54.000000000,2023-07-04 08:15:54.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:42:41.000000000', 'files': ['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken'], 'web_link': 'https://opendev.org/openstack/charm-nova-cloud-controller/commit/88f8a3742794ce04ca3673d38dc91a4a088f0692', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: Ifd1aa100a8d742ea8c0a5cf0473de078a14790ed\n'}]",0,887308,88f8a3742794ce04ca3673d38dc91a4a088f0692,8,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: Ifd1aa100a8d742ea8c0a5cf0473de078a14790ed
",git fetch https://review.opendev.org/openstack/charm-nova-cloud-controller refs/changes/08/887308/1 && git format-patch -1 --stdout FETCH_HEAD,"['charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/contrib/openstack/utils.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken']",11,88f8a3742794ce04ca3673d38dc91a4a088f0692,bug/1992840,service_token_roles = {{ admin_role }} service_token_roles_required = True,,56,10
openstack%2Fcharm-keystone~stable%2Fwallaby~Id7e84d38a9f774179808137548307c9174a87f87,openstack/charm-keystone,stable/wallaby,Id7e84d38a9f774179808137548307c9174a87f87,Add admin-role parameter value to identity relation,MERGED,2023-06-29 19:12:57.000000000,2023-07-04 08:15:37.000000000,2023-07-04 08:15:37.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:12:57.000000000', 'files': ['unit_tests/test_keystone_utils.py', 'hooks/keystone_utils.py'], 'web_link': 'https://opendev.org/openstack/charm-keystone/commit/725fb321da1adc3cca29a3bb3baa356cfb25c406', 'message': 'Add admin-role parameter value to identity relation\n\nThis parameter is added to the relation in order to configure service\ntokens on related services. The role of the service user is required for\nservice token validation.\n\nCloses-Bug: #1992840\nChange-Id: Id7e84d38a9f774179808137548307c9174a87f87\n(cherry picked from commit 55bd7022242857fd8d8c1cc823411021e61bcba4)\n(cherry picked from commit d3246b4c6e3e6274c67f49b282f1077373c3247a)\n'}]",0,887324,725fb321da1adc3cca29a3bb3baa356cfb25c406,9,4,1,11805,,,0,"Add admin-role parameter value to identity relation

This parameter is added to the relation in order to configure service
tokens on related services. The role of the service user is required for
service token validation.

Closes-Bug: #1992840
Change-Id: Id7e84d38a9f774179808137548307c9174a87f87
(cherry picked from commit 55bd7022242857fd8d8c1cc823411021e61bcba4)
(cherry picked from commit d3246b4c6e3e6274c67f49b282f1077373c3247a)
",git fetch https://review.opendev.org/openstack/charm-keystone refs/changes/24/887324/1 && git format-patch -1 --stdout FETCH_HEAD,"['unit_tests/test_keystone_utils.py', 'hooks/keystone_utils.py']",2,725fb321da1adc3cca29a3bb3baa356cfb25c406,bug/1992840," ""admin_role"": config(""admin-role""),",,2,0
openstack%2Fovn-bgp-agent~master~I5627a5d546740e8797edb5c2c3a6fc6b6598699c,openstack/ovn-bgp-agent,master,I5627a5d546740e8797edb5c2c3a6fc6b6598699c,Make debug log less chatty,MERGED,2023-07-04 05:57:28.000000000,2023-07-04 08:14:24.000000000,2023-07-04 08:13:27.000000000,"[{'_account_id': 6773}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-04 05:57:28.000000000', 'files': ['ovn_bgp_agent/config.py'], 'web_link': 'https://opendev.org/openstack/ovn-bgp-agent/commit/61848561e52178b74586e471ac0d3aae7122f65c', 'message': 'Make debug log less chatty\n\nThere is a lot of logs from pyroute2 through privsep. This change\nensures the DEBUG option does not change them to DEBUG too so that\nthe ovn-bgp-agent logs are not that chatty and do not include\nuneeded exceptions at the lower levels that are part of the normal\nexecution path (such as adding an IP already added)\n\nChange-Id: I5627a5d546740e8797edb5c2c3a6fc6b6598699c\n'}]",0,887550,61848561e52178b74586e471ac0d3aae7122f65c,7,2,1,23567,,,0,"Make debug log less chatty

There is a lot of logs from pyroute2 through privsep. This change
ensures the DEBUG option does not change them to DEBUG too so that
the ovn-bgp-agent logs are not that chatty and do not include
uneeded exceptions at the lower levels that are part of the normal
execution path (such as adding an IP already added)

Change-Id: I5627a5d546740e8797edb5c2c3a6fc6b6598699c
",git fetch https://review.opendev.org/openstack/ovn-bgp-agent refs/changes/50/887550/1 && git format-patch -1 --stdout FETCH_HEAD,['ovn_bgp_agent/config.py'],1,61848561e52178b74586e471ac0d3aae7122f65c,,EXTRA_LOG_LEVEL_DEFAULTS = [ 'oslo.privsep.daemon=INFO' ] logging.set_defaults(default_log_levels=logging.get_default_log_levels() + EXTRA_LOG_LEVEL_DEFAULTS), logging.set_defaults(default_log_levels=logging.get_default_log_levels()),5,1
openstack%2Freleases~master~I8ba141e2eb4df647b8a04d1ee95dea0ea47996f9,openstack/releases,master,I8ba141e2eb4df647b8a04d1ee95dea0ea47996f9,[glance] Transition Rocky to End of Life,MERGED,2023-04-26 18:17:47.000000000,2023-07-04 08:11:47.000000000,2023-07-04 08:11:47.000000000,"[{'_account_id': 17685}, {'_account_id': 19138}, {'_account_id': 22348}, {'_account_id': 28522}]","[{'number': 1, 'created': '2023-04-26 18:17:47.000000000', 'files': ['deliverables/rocky/python-glanceclient.yaml', 'deliverables/rocky/glance-store.yaml', 'deliverables/rocky/glance.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/c1920afeb628d86e00edc5724e9e6ef4051a14a5', 'message': ""[glance] Transition Rocky to End of Life\n\nAs discussed in the mail thread [1][2], this patch transition the Rocky\nbranch to End of Life. The last patch of the branch will be tagged with\nrocky-eol tag. stable/rocky branch cannot be used anymore and will be\ndeleted if this patch merges.\n\nThis is needed as stable/rocky is not actively maintained in recent\nperiod, thus gates are mostly broken due to job failures. Besides,\nby removing these branches, infra resources will be freed up, too.\n\nPlease try to identify any zuul job, that is defined outside of the\nrepositories in this patch (for example in openstack-zuul-jobs, etc.)\nand won't be used anymore if stable/rocky is deleted. Propose a job\nremoval patch for them.\n\nPlease +1 if the team is ready for us to proceed with this transition,\nor -1 if there are still some activity on the branch and the team wants\nto continue to maintain it.\n\n[1] https://lists.openstack.org/pipermail/openstack-discuss/2023-January/031922.html\n[2] https://lists.openstack.org/pipermail/openstack-discuss/2023-April/033386.html\n\nChange-Id: I8ba141e2eb4df647b8a04d1ee95dea0ea47996f9\n""}]",8,881624,c1920afeb628d86e00edc5724e9e6ef4051a14a5,11,4,1,17685,,,0,"[glance] Transition Rocky to End of Life

As discussed in the mail thread [1][2], this patch transition the Rocky
branch to End of Life. The last patch of the branch will be tagged with
rocky-eol tag. stable/rocky branch cannot be used anymore and will be
deleted if this patch merges.

This is needed as stable/rocky is not actively maintained in recent
period, thus gates are mostly broken due to job failures. Besides,
by removing these branches, infra resources will be freed up, too.

Please try to identify any zuul job, that is defined outside of the
repositories in this patch (for example in openstack-zuul-jobs, etc.)
and won't be used anymore if stable/rocky is deleted. Propose a job
removal patch for them.

Please +1 if the team is ready for us to proceed with this transition,
or -1 if there are still some activity on the branch and the team wants
to continue to maintain it.

[1] https://lists.openstack.org/pipermail/openstack-discuss/2023-January/031922.html
[2] https://lists.openstack.org/pipermail/openstack-discuss/2023-April/033386.html

Change-Id: I8ba141e2eb4df647b8a04d1ee95dea0ea47996f9
",git fetch https://review.opendev.org/openstack/releases refs/changes/24/881624/1 && git format-patch -1 --stdout FETCH_HEAD,"['deliverables/rocky/python-glanceclient.yaml', 'deliverables/rocky/glance-store.yaml', 'deliverables/rocky/glance.yaml']",3,c1920afeb628d86e00edc5724e9e6ef4051a14a5,rocky-eol, - version: rocky-eol projects: - repo: openstack/glance hash: ad39c12c64c8ff017918a8790d69d5278ac379da,,12,0
openstack%2Freleases~master~I89c90f174c61ecc45e20b26cf3f82b65adc6a096,openstack/releases,master,I89c90f174c61ecc45e20b26cf3f82b65adc6a096,Deprecate Tripleo independent deliverables,NEW,2023-07-03 11:02:40.000000000,2023-07-04 08:07:10.000000000,,"[{'_account_id': 7144}, {'_account_id': 8449}, {'_account_id': 9816}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 11:02:40.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/releases/commit/f1baf61f88d6107f91f8eb087bf7a97fd7507617', 'message': 'Deprecate Tripleo independent deliverables\n\nTripleo deliverables have been deprecated, including those using\nindependent release model. This should no longer be handled by release\nmanagement and therefore be marked release-model: abandoned.\n\nAlso reaffect os-apply-config os-collect-config and os-refresh-config to\nHeat team to match current governance.\n\nChange-Id: I89c90f174c61ecc45e20b26cf3f82b65adc6a096\n'}, {'number': 2, 'created': '2023-07-04 07:52:46.000000000', 'files': ['deliverables/_independent/tripleo-common.yaml', 'deliverables/_independent/validations-common.yaml', 'deliverables/_independent/ansible-role-redhat-subscription.yaml', 'deliverables/_independent/os-collect-config.yaml', 'deliverables/_independent/ansible-role-chrony.yaml', 'deliverables/_independent/tripleo-ansible.yaml', 'deliverables/_independent/puppet-pacemaker.yaml', 'deliverables/_independent/tripleo-heat-templates.yaml', 'deliverables/_independent/ansible-role-tripleo-modify-image.yaml', 'deliverables/_independent/os-net-config.yaml', 'deliverables/_independent/tripleo-quickstart.yaml', 'deliverables/_independent/ansible-role-openstack-operations.yaml', 'deliverables/_independent/tripleo-ipsec.yaml', 'deliverables/_independent/validations-libs.yaml', 'deliverables/_independent/os-apply-config.yaml', 'deliverables/_independent/tripleo-image-elements.yaml', 'deliverables/_independent/puppet-tripleo.yaml', 'deliverables/_independent/tripleo-operator-ansible.yaml', 'deliverables/_independent/tripleo-validations.yaml', 'deliverables/_independent/os-refresh-config.yaml', 'deliverables/_independent/tripleo-upgrade.yaml', 'deliverables/_independent/openstack-virtual-baremetal.yaml', 'deliverables/_independent/python-tripleoclient.yaml', 'deliverables/_independent/tripleo-puppet-elements.yaml', 'deliverables/_independent/ansible-role-collect-logs.yaml', 'deliverables/_independent/tripleo-docs.yaml', 'deliverables/_independent/ansible-role-container-registry.yaml'], 'web_link': 'https://opendev.org/openstack/releases/commit/8fa2a3a0bdf64f2f4343dc0c4d7c594901500e23', 'message': 'Deprecate Tripleo independent deliverables\n\nTripleo deliverables have been deprecated, including those using\nindependent release model. This should no longer be handled by release\nmanagement and therefore be marked release-model: abandoned.\n\nAlso reaffect os-apply-config os-collect-config and os-refresh-config to\nHeat team to match current governance.\n\nChange-Id: I89c90f174c61ecc45e20b26cf3f82b65adc6a096\n'}]",1,887505,8fa2a3a0bdf64f2f4343dc0c4d7c594901500e23,4,4,2,308,,,0,"Deprecate Tripleo independent deliverables

Tripleo deliverables have been deprecated, including those using
independent release model. This should no longer be handled by release
management and therefore be marked release-model: abandoned.

Also reaffect os-apply-config os-collect-config and os-refresh-config to
Heat team to match current governance.

Change-Id: I89c90f174c61ecc45e20b26cf3f82b65adc6a096
",git fetch https://review.opendev.org/openstack/releases refs/changes/05/887505/2 && git format-patch -1 --stdout FETCH_HEAD,"['deliverables/_independent/tripleo-common.yaml', 'deliverables/_independent/validations-common.yaml', 'deliverables/_independent/ansible-role-redhat-subscription.yaml', 'deliverables/_independent/os-collect-config.yaml', 'deliverables/_independent/tripleo-repos.yaml', 'deliverables/_independent/ansible-role-chrony.yaml', 'deliverables/_independent/tripleo-ansible.yaml', 'deliverables/_independent/puppet-pacemaker.yaml', 'deliverables/_independent/tripleo-heat-templates.yaml', 'deliverables/_independent/ansible-role-tripleo-modify-image.yaml', 'deliverables/_independent/os-net-config.yaml', 'deliverables/_independent/tripleo-quickstart.yaml', 'deliverables/_independent/ansible-role-openstack-operations.yaml', 'deliverables/_independent/tripleo-ipsec.yaml', 'deliverables/_independent/validations-libs.yaml', 'deliverables/_independent/os-apply-config.yaml', 'deliverables/_independent/tripleo-image-elements.yaml', 'deliverables/_independent/puppet-tripleo.yaml', 'deliverables/_independent/tripleo-operator-ansible.yaml', 'deliverables/_independent/tripleo-validations.yaml', 'deliverables/_independent/os-refresh-config.yaml', 'deliverables/_independent/tripleo-upgrade.yaml', 'deliverables/_independent/openstack-virtual-baremetal.yaml', 'deliverables/_independent/python-tripleoclient.yaml', 'deliverables/_independent/tripleo-puppet-elements.yaml', 'deliverables/_independent/ansible-role-collect-logs.yaml', 'deliverables/_independent/tripleo-docs.yaml', 'deliverables/_independent/ansible-role-container-registry.yaml']",28,f1baf61f88d6107f91f8eb087bf7a97fd7507617,tripleo-independent-deliverables,release-model: abandoned,,28,3
openstack%2Fcharm-cinder~stable%2Fvictoria~I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9,openstack/charm-cinder,stable/victoria,I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9,Add support for using service tokens,MERGED,2023-06-29 19:11:48.000000000,2023-07-04 08:06:50.000000000,2023-07-04 08:06:50.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:11:48.000000000', 'files': ['templates/ussuri/cinder.conf'], 'web_link': 'https://opendev.org/openstack/charm-cinder/commit/1f66274e4956dc33e1720769814c41301ff94a94', 'message': 'Add support for using service tokens\n\nThis patch configures Cinder to send a service token along with the\nreceived user token on requests to other services. This can allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Cinder. Also with this patch Cinder will\naccept request from other services with invalid user tokens but valid\nservice tokens. Service tokens exist since Openstack Queens.\n\nCloses-Bug: #1992840\nChange-Id: I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9\n(cherry picked from commit 81c330b5d87a64a7a9ec601f4dd263b836ee9c01)\n(cherry picked from commit b96c85f5a6b8e3d173a4f810fd4d5fd82737795e)\n'}]",0,887321,1f66274e4956dc33e1720769814c41301ff94a94,8,4,1,11805,,,0,"Add support for using service tokens

This patch configures Cinder to send a service token along with the
received user token on requests to other services. This can allow those
other services to accept the request even if the user token has been
invalidated since received by Cinder. Also with this patch Cinder will
accept request from other services with invalid user tokens but valid
service tokens. Service tokens exist since Openstack Queens.

Closes-Bug: #1992840
Change-Id: I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9
(cherry picked from commit 81c330b5d87a64a7a9ec601f4dd263b836ee9c01)
(cherry picked from commit b96c85f5a6b8e3d173a4f810fd4d5fd82737795e)
",git fetch https://review.opendev.org/openstack/charm-cinder refs/changes/21/887321/1 && git format-patch -1 --stdout FETCH_HEAD,['templates/ussuri/cinder.conf'],1,1f66274e4956dc33e1720769814c41301ff94a94,bug/1992840,"{% include ""section-service-user"" %} ",,2,0
openstack%2Fcharm-cinder~stable%2Fvictoria~Iddd7e5c59512d579f9fa32e006a1ae69e0becfa6,openstack/charm-cinder,stable/victoria,Iddd7e5c59512d579f9fa32e006a1ae69e0becfa6,charm-helpers sync,MERGED,2023-06-29 18:54:16.000000000,2023-07-04 08:06:49.000000000,2023-07-04 08:06:49.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:54:16.000000000', 'files': ['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/storage/linux/ceph.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'charmhelpers/contrib/openstack/templates/haproxy.cfg', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken'], 'web_link': 'https://opendev.org/openstack/charm-cinder/commit/92283321e7ec7162895dd02946fc9267a01474c3', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: Iddd7e5c59512d579f9fa32e006a1ae69e0becfa6\n'}]",0,887311,92283321e7ec7162895dd02946fc9267a01474c3,8,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: Iddd7e5c59512d579f9fa32e006a1ae69e0becfa6
",git fetch https://review.opendev.org/openstack/charm-cinder refs/changes/11/887311/1 && git format-patch -1 --stdout FETCH_HEAD,"['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/storage/linux/ceph.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'charmhelpers/contrib/openstack/templates/haproxy.cfg', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken']",17,92283321e7ec7162895dd02946fc9267a01474c3,bug/1992840,{% if service_type -%} service_type = {{ service_type }} {% endif -%} service_token_roles = {{ admin_role }} service_token_roles_required = True,,101,13
openstack%2Fcharm-cinder~stable%2Fussuri~Iaecae3c22db1f4f2309f73f8c6836e6c072b848b,openstack/charm-cinder,stable/ussuri,Iaecae3c22db1f4f2309f73f8c6836e6c072b848b,Render [service_user] only for identity-service relation,MERGED,2023-06-29 19:34:18.000000000,2023-07-04 08:06:48.000000000,2023-07-04 08:06:48.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:34:18.000000000', 'files': ['templates/ussuri/cinder.conf'], 'web_link': 'https://opendev.org/openstack/charm-cinder/commit/487221a52de17f6a85f4159175d73a434e85d467', 'message': 'Render [service_user] only for identity-service relation\n\nThe service token section [service_user] is not required when\ncinder-volume is deployed as a separate service. In other words\nit is not required for the identity-credentials relation.\n\nThe [service_user] section is nearly the same as the\n[keystone_authtoken] section, and the keystone_authtoken data\nis only produced for the IdentityServiceContext, therefore this\nchange will not render [service_user] for the\nIdentityCredentialsContext.\n\nCloses-Bug: #2024676\nChange-Id: Iaecae3c22db1f4f2309f73f8c6836e6c072b848b\n(cherry picked from commit ebbedcbf58660ce13823152d6943fee036af7e11)\n'}]",1,887337,487221a52de17f6a85f4159175d73a434e85d467,10,4,1,11805,,,0,"Render [service_user] only for identity-service relation

The service token section [service_user] is not required when
cinder-volume is deployed as a separate service. In other words
it is not required for the identity-credentials relation.

The [service_user] section is nearly the same as the
[keystone_authtoken] section, and the keystone_authtoken data
is only produced for the IdentityServiceContext, therefore this
change will not render [service_user] for the
IdentityCredentialsContext.

Closes-Bug: #2024676
Change-Id: Iaecae3c22db1f4f2309f73f8c6836e6c072b848b
(cherry picked from commit ebbedcbf58660ce13823152d6943fee036af7e11)
",git fetch https://review.opendev.org/openstack/charm-cinder refs/changes/37/887337/1 && git format-patch -1 --stdout FETCH_HEAD,['templates/ussuri/cinder.conf'],1,487221a52de17f6a85f4159175d73a434e85d467,bug/1992840,{% if keystone_authtoken -%}{% endif -%},,2,0
openstack%2Fcharm-nova-compute~stable%2Fwallaby~I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1,openstack/charm-nova-compute,stable/wallaby,I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1,Add support for using service tokens,MERGED,2023-02-15 16:07:49.000000000,2023-07-04 08:05:44.000000000,2023-07-04 08:05:44.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-02-15 16:07:49.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-nova-compute/commit/8f63e1b93a2f99e99707db6149b18024f83c9e34', 'message': 'Add support for using service tokens\n\nThis patch configures Nova to send a service token along with the\nreceived user token on requests to other services. This can allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Nova. Also with this patch Nova will\naccept request from other services with invalid user tokens but valid\nservice tokens. Service tokens exist since Openstack Queens.\n\nCloses-Bug: #1992840\nChange-Id: I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1\n'}, {'number': 2, 'created': '2023-06-29 19:17:21.000000000', 'files': ['hooks/nova_compute_context.py', 'templates/stein/nova.conf', 'templates/train/nova.conf', 'templates/queens/nova.conf', 'templates/rocky/nova.conf', 'unit_tests/test_nova_compute_contexts.py'], 'web_link': 'https://opendev.org/openstack/charm-nova-compute/commit/7e427a9257fbdf5f41df97a6bc016ec8bd260ada', 'message': 'Add support for using service tokens\n\nThis patch configures Nova to send a service token along with the\nreceived user token on requests to other services. This can allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Nova. Also with this patch Nova will\naccept request from other services with invalid user tokens but valid\nservice tokens. Service tokens exist since Openstack Queens.\n\nCloses-Bug: #1992840\nChange-Id: I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1\n(cherry picked from commit 3c53110282b97c42a00cee9ee344f32dc8cf29c5)\n(cherry picked from commit b54f6701d6cc408990add5fdb81bac1c74aa20db)\n'}]",0,873922,7e427a9257fbdf5f41df97a6bc016ec8bd260ada,12,4,2,34275,,,0,"Add support for using service tokens

This patch configures Nova to send a service token along with the
received user token on requests to other services. This can allow those
other services to accept the request even if the user token has been
invalidated since received by Nova. Also with this patch Nova will
accept request from other services with invalid user tokens but valid
service tokens. Service tokens exist since Openstack Queens.

Closes-Bug: #1992840
Change-Id: I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1
(cherry picked from commit 3c53110282b97c42a00cee9ee344f32dc8cf29c5)
(cherry picked from commit b54f6701d6cc408990add5fdb81bac1c74aa20db)
",git fetch https://review.opendev.org/openstack/charm-nova-compute refs/changes/22/873922/1 && git format-patch -1 --stdout FETCH_HEAD,"['templates/yoga/nova.conf', 'hooks/nova_compute_context.py', 'templates/stein/nova.conf', 'templates/train/nova.conf', 'templates/queens/nova.conf', 'templates/rocky/nova.conf', 'unit_tests/test_nova_compute_contexts.py']",7,8f63e1b93a2f99e99707db6149b18024f83c9e34,bug/1992840," 'admin_role': 'Admin', 'admin_role': 'Admin', 'admin_role': 'Admin', 'admin_role': 'Admin', 'admin_role': 'Admin',",,414,4
openstack%2Fcharm-nova-compute~stable%2Fwallaby~I2c7a1441aac4c1a869714c48525d233f65bec53e,openstack/charm-nova-compute,stable/wallaby,I2c7a1441aac4c1a869714c48525d233f65bec53e,charm-helpers sync,MERGED,2023-06-29 18:42:21.000000000,2023-07-04 08:05:43.000000000,2023-07-04 08:05:43.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:42:21.000000000', 'files': ['hooks/charmhelpers/contrib/hahelpers/cluster.py', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'hooks/charmhelpers/contrib/openstack/templates/haproxy.cfg', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken', 'hooks/charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'hooks/charmhelpers/contrib/network/ovs/__init__.py', 'hooks/charmhelpers/contrib/openstack/ssh_migrations.py', 'hooks/charmhelpers/contrib/openstack/context.py', 'hooks/charmhelpers/fetch/ubuntu.py', 'hooks/charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'hooks/charmhelpers/contrib/network/ovs/ovsdb.py', 'hooks/charmhelpers/contrib/openstack/templates/section-service-user', 'hooks/charmhelpers/contrib/openstack/cert_utils.py', 'hooks/charmhelpers/contrib/openstack/utils.py', 'hooks/charmhelpers/contrib/storage/linux/ceph.py', 'hooks/charmhelpers/core/host.py', 'hooks/charmhelpers/contrib/network/ip.py', 'hooks/charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'hooks/charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf'], 'web_link': 'https://opendev.org/openstack/charm-nova-compute/commit/e0490a596ea56fae25b719d5700b4e53f8382340', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: I2c7a1441aac4c1a869714c48525d233f65bec53e\n'}]",0,887307,e0490a596ea56fae25b719d5700b4e53f8382340,8,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: I2c7a1441aac4c1a869714c48525d233f65bec53e
",git fetch https://review.opendev.org/openstack/charm-nova-compute refs/changes/07/887307/1 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/charmhelpers/contrib/hahelpers/cluster.py', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'hooks/charmhelpers/contrib/openstack/templates/haproxy.cfg', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken', 'hooks/charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'hooks/charmhelpers/contrib/network/ovs/__init__.py', 'hooks/charmhelpers/contrib/openstack/ssh_migrations.py', 'hooks/charmhelpers/contrib/openstack/context.py', 'hooks/charmhelpers/fetch/ubuntu.py', 'hooks/charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'hooks/charmhelpers/contrib/network/ovs/ovsdb.py', 'hooks/charmhelpers/contrib/openstack/templates/section-service-user', 'hooks/charmhelpers/contrib/openstack/cert_utils.py', 'hooks/charmhelpers/contrib/openstack/utils.py', 'hooks/charmhelpers/contrib/storage/linux/ceph.py', 'hooks/charmhelpers/core/host.py', 'hooks/charmhelpers/contrib/network/ip.py', 'hooks/charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'hooks/charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf']",19,e0490a596ea56fae25b719d5700b4e53f8382340,bug/1992840, KeepAliveTimeout 75 MaxKeepAliveRequests 1000 KeepAliveTimeout 75 MaxKeepAliveRequests 1000 KeepAliveTimeout 75 MaxKeepAliveRequests 1000,,103,15
openstack%2Fcharm-keystone~stable%2Fwallaby~I20719e8f816114a797d362a3601fd9d0acb6f3d7,openstack/charm-keystone,stable/wallaby,I20719e8f816114a797d362a3601fd9d0acb6f3d7,charm-helpers sync,MERGED,2023-06-29 18:42:58.000000000,2023-07-04 08:05:29.000000000,2023-07-04 08:05:29.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:42:58.000000000', 'files': ['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken'], 'web_link': 'https://opendev.org/openstack/charm-keystone/commit/342f1174a74740a112f5c51c1362823a444b409b', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: I20719e8f816114a797d362a3601fd9d0acb6f3d7\n'}]",0,887309,342f1174a74740a112f5c51c1362823a444b409b,8,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: I20719e8f816114a797d362a3601fd9d0acb6f3d7
",git fetch https://review.opendev.org/openstack/charm-keystone refs/changes/09/887309/1 && git format-patch -1 --stdout FETCH_HEAD,"['charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/contrib/openstack/utils.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken']",11,342f1174a74740a112f5c51c1362823a444b409b,bug/1992840,service_token_roles = {{ admin_role }} service_token_roles_required = True,,49,10
openstack%2Fcharm-cinder~stable%2Fussuri~I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9,openstack/charm-cinder,stable/ussuri,I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9,Add support for using service tokens,MERGED,2023-06-29 19:12:00.000000000,2023-07-04 08:04:37.000000000,2023-07-04 08:04:37.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:12:00.000000000', 'files': ['templates/ussuri/cinder.conf'], 'web_link': 'https://opendev.org/openstack/charm-cinder/commit/d901f7f56e20763a1bed28bfadea28d975c6ad6d', 'message': 'Add support for using service tokens\n\nThis patch configures Cinder to send a service token along with the\nreceived user token on requests to other services. This can allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Cinder. Also with this patch Cinder will\naccept request from other services with invalid user tokens but valid\nservice tokens. Service tokens exist since Openstack Queens.\n\nCloses-Bug: #1992840\nChange-Id: I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9\n(cherry picked from commit 81c330b5d87a64a7a9ec601f4dd263b836ee9c01)\n(cherry picked from commit b96c85f5a6b8e3d173a4f810fd4d5fd82737795e)\n'}]",0,887322,d901f7f56e20763a1bed28bfadea28d975c6ad6d,8,4,1,11805,,,0,"Add support for using service tokens

This patch configures Cinder to send a service token along with the
received user token on requests to other services. This can allow those
other services to accept the request even if the user token has been
invalidated since received by Cinder. Also with this patch Cinder will
accept request from other services with invalid user tokens but valid
service tokens. Service tokens exist since Openstack Queens.

Closes-Bug: #1992840
Change-Id: I6cb9b1cb257db0b57bd7984c795b8caa1e3b74d9
(cherry picked from commit 81c330b5d87a64a7a9ec601f4dd263b836ee9c01)
(cherry picked from commit b96c85f5a6b8e3d173a4f810fd4d5fd82737795e)
",git fetch https://review.opendev.org/openstack/charm-cinder refs/changes/22/887322/1 && git format-patch -1 --stdout FETCH_HEAD,['templates/ussuri/cinder.conf'],1,d901f7f56e20763a1bed28bfadea28d975c6ad6d,bug/1992840,"{% include ""section-service-user"" %} ",,2,0
openstack%2Fcharm-keystone~stable%2Fvictoria~Id7e84d38a9f774179808137548307c9174a87f87,openstack/charm-keystone,stable/victoria,Id7e84d38a9f774179808137548307c9174a87f87,Add admin-role parameter value to identity relation,MERGED,2023-06-29 19:13:08.000000000,2023-07-04 08:04:21.000000000,2023-07-04 08:04:21.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:13:08.000000000', 'files': ['unit_tests/test_keystone_utils.py', 'hooks/keystone_utils.py'], 'web_link': 'https://opendev.org/openstack/charm-keystone/commit/41ac71d4c2e2645f51919b4656f700dc53b191cd', 'message': 'Add admin-role parameter value to identity relation\n\nThis parameter is added to the relation in order to configure service\ntokens on related services. The role of the service user is required for\nservice token validation.\n\nCloses-Bug: #1992840\nChange-Id: Id7e84d38a9f774179808137548307c9174a87f87\n(cherry picked from commit 55bd7022242857fd8d8c1cc823411021e61bcba4)\n(cherry picked from commit d3246b4c6e3e6274c67f49b282f1077373c3247a)\n'}]",0,887325,41ac71d4c2e2645f51919b4656f700dc53b191cd,8,4,1,11805,,,0,"Add admin-role parameter value to identity relation

This parameter is added to the relation in order to configure service
tokens on related services. The role of the service user is required for
service token validation.

Closes-Bug: #1992840
Change-Id: Id7e84d38a9f774179808137548307c9174a87f87
(cherry picked from commit 55bd7022242857fd8d8c1cc823411021e61bcba4)
(cherry picked from commit d3246b4c6e3e6274c67f49b282f1077373c3247a)
",git fetch https://review.opendev.org/openstack/charm-keystone refs/changes/25/887325/1 && git format-patch -1 --stdout FETCH_HEAD,"['unit_tests/test_keystone_utils.py', 'hooks/keystone_utils.py']",2,41ac71d4c2e2645f51919b4656f700dc53b191cd,bug/1992840," ""admin_role"": config(""admin-role""),",,2,0
openstack%2Fcharm-keystone~stable%2Fvictoria~Ia48011707d0f43245335f4b26f984dc244ebd921,openstack/charm-keystone,stable/victoria,Ia48011707d0f43245335f4b26f984dc244ebd921,charm-helpers sync,MERGED,2023-06-29 18:54:39.000000000,2023-07-04 08:04:20.000000000,2023-07-04 08:04:20.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:54:39.000000000', 'files': ['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken'], 'web_link': 'https://opendev.org/openstack/charm-keystone/commit/cbdbe67af987915f8853ef46fd2832e9a1393941', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: Ia48011707d0f43245335f4b26f984dc244ebd921\n'}]",0,887312,cbdbe67af987915f8853ef46fd2832e9a1393941,8,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: Ia48011707d0f43245335f4b26f984dc244ebd921
",git fetch https://review.opendev.org/openstack/charm-keystone refs/changes/12/887312/1 && git format-patch -1 --stdout FETCH_HEAD,"['charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/contrib/openstack/utils.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken']",11,cbdbe67af987915f8853ef46fd2832e9a1393941,bug/1992840,service_token_roles = {{ admin_role }} service_token_roles_required = True,,49,10
openstack%2Fopenstack-helm-images~master~I7d2f5176493b99f0878df2cedb9c88a466357880,openstack/openstack-helm-images,master,I7d2f5176493b99f0878df2cedb9c88a466357880,Add ovn jammy,MERGED,2023-07-03 15:21:36.000000000,2023-07-04 08:04:18.000000000,2023-07-04 07:59:13.000000000,"[{'_account_id': 3009}, {'_account_id': 8898}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-07-03 15:21:36.000000000', 'files': ['zuul.d/ovn.yaml', 'ovn/build.sh', 'ovn/Dockerfile.ubuntu'], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/817b2d727bcac38587419c6b5b8892c1e7128e35', 'message': 'Add ovn jammy\n\nChange-Id: I7d2f5176493b99f0878df2cedb9c88a466357880\n'}]",0,887521,817b2d727bcac38587419c6b5b8892c1e7128e35,8,3,1,35691,,,0,"Add ovn jammy

Change-Id: I7d2f5176493b99f0878df2cedb9c88a466357880
",git fetch https://review.opendev.org/openstack/openstack-helm-images refs/changes/21/887521/1 && git format-patch -1 --stdout FETCH_HEAD,"['ovn/build.sh', 'zuul.d/ovn.yaml', 'ovn/Dockerfile.ubuntu']",3,817b2d727bcac38587419c6b5b8892c1e7128e35,ovn-jammy,ARG FROM FROM ${FROM},FROM docker.io/ubuntu:focal,28,3
openstack%2Fopenstack-helm-images~master~I0c657d4040521312d340664dfe7d5c494e5c70b9,openstack/openstack-helm-images,master,I0c657d4040521312d340664dfe7d5c494e5c70b9,Add libvirt version antelope,MERGED,2023-07-02 14:42:38.000000000,2023-07-04 08:03:23.000000000,2023-07-04 07:59:11.000000000,"[{'_account_id': 3009}, {'_account_id': 8898}, {'_account_id': 22348}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-07-02 14:42:38.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/e747d6904111ace696c1d013b592e71cac37c9ea', 'message': 'Add libvirt version antelope\n\nChange-Id: I0c657d4040521312d340664dfe7d5c494e5c70b9\n'}, {'number': 2, 'created': '2023-07-02 16:30:32.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/cf7ccd3487c6f2cfaa32580655ea7905d18717e0', 'message': 'Add libvirt version antelope\n\nChange-Id: I0c657d4040521312d340664dfe7d5c494e5c70b9\n'}, {'number': 3, 'created': '2023-07-03 14:10:49.000000000', 'files': ['zuul.d/libvirt.yaml', 'libvirt/Dockerfile'], 'web_link': 'https://opendev.org/openstack/openstack-helm-images/commit/14ab7ee572703119e9a51baf9fdd31ee6a4a943a', 'message': 'Add libvirt version antelope\n\nChange-Id: I0c657d4040521312d340664dfe7d5c494e5c70b9\n'}]",3,887470,14ab7ee572703119e9a51baf9fdd31ee6a4a943a,17,4,3,35691,,,0,"Add libvirt version antelope

Change-Id: I0c657d4040521312d340664dfe7d5c494e5c70b9
",git fetch https://review.opendev.org/openstack/openstack-helm-images refs/changes/70/887470/1 && git format-patch -1 --stdout FETCH_HEAD,"['zuul.d/libvirt.yaml', 'libvirt/Dockerfile']",2,e747d6904111ace696c1d013b592e71cac37c9ea,libvirt-antelope," elif [[ ""${RELEASE}"" = ""zed"" || ""${RELEASE}"" = ""antelope"" ]]; then qemu-system \ qemu-kvm \ swtpm \ trousers"," elif [[ ""${RELEASE}"" = ""zed"" ]]; then qemu-kvm",17,2
openstack%2Fcharm-keystone~stable%2Fussuri~Id7e84d38a9f774179808137548307c9174a87f87,openstack/charm-keystone,stable/ussuri,Id7e84d38a9f774179808137548307c9174a87f87,Add admin-role parameter value to identity relation,MERGED,2023-06-29 19:13:20.000000000,2023-07-04 08:01:38.000000000,2023-07-04 08:01:38.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:13:20.000000000', 'files': ['unit_tests/test_keystone_utils.py', 'hooks/keystone_utils.py'], 'web_link': 'https://opendev.org/openstack/charm-keystone/commit/72b45b2709b30f60ddff5eb6ae402634b25221f0', 'message': 'Add admin-role parameter value to identity relation\n\nThis parameter is added to the relation in order to configure service\ntokens on related services. The role of the service user is required for\nservice token validation.\n\nCloses-Bug: #1992840\nChange-Id: Id7e84d38a9f774179808137548307c9174a87f87\n(cherry picked from commit 55bd7022242857fd8d8c1cc823411021e61bcba4)\n(cherry picked from commit d3246b4c6e3e6274c67f49b282f1077373c3247a)\n'}]",0,887326,72b45b2709b30f60ddff5eb6ae402634b25221f0,8,4,1,11805,,,0,"Add admin-role parameter value to identity relation

This parameter is added to the relation in order to configure service
tokens on related services. The role of the service user is required for
service token validation.

Closes-Bug: #1992840
Change-Id: Id7e84d38a9f774179808137548307c9174a87f87
(cherry picked from commit 55bd7022242857fd8d8c1cc823411021e61bcba4)
(cherry picked from commit d3246b4c6e3e6274c67f49b282f1077373c3247a)
",git fetch https://review.opendev.org/openstack/charm-keystone refs/changes/26/887326/1 && git format-patch -1 --stdout FETCH_HEAD,"['unit_tests/test_keystone_utils.py', 'hooks/keystone_utils.py']",2,72b45b2709b30f60ddff5eb6ae402634b25221f0,bug/1992840," ""admin_role"": config(""admin-role""),",,2,0
openstack%2Fcharm-keystone~stable%2Fussuri~I94955242ab9275c9ba25614b57447954b9787e98,openstack/charm-keystone,stable/ussuri,I94955242ab9275c9ba25614b57447954b9787e98,charm-helpers sync,MERGED,2023-06-29 18:59:13.000000000,2023-07-04 08:01:37.000000000,2023-07-04 08:01:37.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:59:13.000000000', 'files': ['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken'], 'web_link': 'https://opendev.org/openstack/charm-keystone/commit/089f0e8ab0be8f0f689127d02ef7879eadffe0b8', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: I94955242ab9275c9ba25614b57447954b9787e98\n'}]",0,887317,089f0e8ab0be8f0f689127d02ef7879eadffe0b8,8,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: I94955242ab9275c9ba25614b57447954b9787e98
",git fetch https://review.opendev.org/openstack/charm-keystone refs/changes/17/887317/1 && git format-patch -1 --stdout FETCH_HEAD,"['charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/contrib/openstack/utils.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken']",11,089f0e8ab0be8f0f689127d02ef7879eadffe0b8,bug/1992840,service_token_roles = {{ admin_role }} service_token_roles_required = True,,49,10
openstack%2Fcharm-nova-compute~stable%2Fvictoria~I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1,openstack/charm-nova-compute,stable/victoria,I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1,Add support for using service tokens,MERGED,2023-06-29 19:18:15.000000000,2023-07-04 08:01:09.000000000,2023-07-04 08:01:09.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-06-29 19:18:15.000000000', 'files': ['hooks/nova_compute_context.py', 'templates/stein/nova.conf', 'templates/train/nova.conf', 'templates/queens/nova.conf', 'templates/rocky/nova.conf', 'unit_tests/test_nova_compute_contexts.py'], 'web_link': 'https://opendev.org/openstack/charm-nova-compute/commit/0a5bc68fe170e09dc15caee671883006b1a0dfe9', 'message': 'Add support for using service tokens\n\nThis patch configures Nova to send a service token along with the\nreceived user token on requests to other services. This can allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Nova. Also with this patch Nova will\naccept request from other services with invalid user tokens but valid\nservice tokens. Service tokens exist since Openstack Queens.\n\nCloses-Bug: #1992840\nChange-Id: I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1\n(cherry picked from commit 3c53110282b97c42a00cee9ee344f32dc8cf29c5)\n(cherry picked from commit b54f6701d6cc408990add5fdb81bac1c74aa20db)\n'}]",0,887331,0a5bc68fe170e09dc15caee671883006b1a0dfe9,9,5,1,11805,,,0,"Add support for using service tokens

This patch configures Nova to send a service token along with the
received user token on requests to other services. This can allow those
other services to accept the request even if the user token has been
invalidated since received by Nova. Also with this patch Nova will
accept request from other services with invalid user tokens but valid
service tokens. Service tokens exist since Openstack Queens.

Closes-Bug: #1992840
Change-Id: I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1
(cherry picked from commit 3c53110282b97c42a00cee9ee344f32dc8cf29c5)
(cherry picked from commit b54f6701d6cc408990add5fdb81bac1c74aa20db)
",git fetch https://review.opendev.org/openstack/charm-nova-compute refs/changes/31/887331/1 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/nova_compute_context.py', 'templates/stein/nova.conf', 'templates/train/nova.conf', 'templates/queens/nova.conf', 'templates/rocky/nova.conf', 'unit_tests/test_nova_compute_contexts.py']",6,0a5bc68fe170e09dc15caee671883006b1a0dfe9,bug/1992840," 'admin_role': 'Admin', 'admin_role': 'Admin', 'admin_role': 'Admin', 'admin_role': 'Admin', 'admin_role': 'Admin',",,15,0
openstack%2Fcharm-nova-compute~stable%2Fvictoria~Iae9b23ffccac41cb4c801e06edff27c8284523b1,openstack/charm-nova-compute,stable/victoria,Iae9b23ffccac41cb4c801e06edff27c8284523b1,charm-helpers sync,MERGED,2023-06-29 18:55:27.000000000,2023-07-04 08:01:08.000000000,2023-07-04 08:01:08.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:55:27.000000000', 'files': ['hooks/charmhelpers/contrib/hahelpers/cluster.py', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'hooks/charmhelpers/contrib/openstack/templates/haproxy.cfg', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken', 'hooks/charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'hooks/charmhelpers/contrib/network/ovs/__init__.py', 'hooks/charmhelpers/contrib/openstack/ssh_migrations.py', 'hooks/charmhelpers/contrib/openstack/context.py', 'hooks/charmhelpers/fetch/ubuntu.py', 'hooks/charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'hooks/charmhelpers/contrib/network/ovs/ovsdb.py', 'hooks/charmhelpers/contrib/openstack/templates/section-service-user', 'hooks/charmhelpers/contrib/openstack/cert_utils.py', 'hooks/charmhelpers/contrib/openstack/utils.py', 'hooks/charmhelpers/contrib/storage/linux/ceph.py', 'hooks/charmhelpers/core/host.py', 'hooks/charmhelpers/contrib/network/ip.py', 'hooks/charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'hooks/charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf'], 'web_link': 'https://opendev.org/openstack/charm-nova-compute/commit/b8c31d16f235cfe6c1d3f115701f310b00ebc147', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: Iae9b23ffccac41cb4c801e06edff27c8284523b1\n'}]",0,887314,b8c31d16f235cfe6c1d3f115701f310b00ebc147,8,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: Iae9b23ffccac41cb4c801e06edff27c8284523b1
",git fetch https://review.opendev.org/openstack/charm-nova-compute refs/changes/14/887314/1 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/charmhelpers/contrib/hahelpers/cluster.py', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'hooks/charmhelpers/contrib/openstack/templates/haproxy.cfg', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken', 'hooks/charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'hooks/charmhelpers/contrib/network/ovs/__init__.py', 'hooks/charmhelpers/contrib/openstack/ssh_migrations.py', 'hooks/charmhelpers/contrib/openstack/context.py', 'hooks/charmhelpers/fetch/ubuntu.py', 'hooks/charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'hooks/charmhelpers/contrib/network/ovs/ovsdb.py', 'hooks/charmhelpers/contrib/openstack/templates/section-service-user', 'hooks/charmhelpers/contrib/openstack/cert_utils.py', 'hooks/charmhelpers/contrib/openstack/utils.py', 'hooks/charmhelpers/contrib/storage/linux/ceph.py', 'hooks/charmhelpers/core/host.py', 'hooks/charmhelpers/contrib/network/ip.py', 'hooks/charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'hooks/charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf']",19,b8c31d16f235cfe6c1d3f115701f310b00ebc147,bug/1992840, KeepAliveTimeout 75 MaxKeepAliveRequests 1000 KeepAliveTimeout 75 MaxKeepAliveRequests 1000 KeepAliveTimeout 75 MaxKeepAliveRequests 1000,,103,15
openstack%2Fcharm-nova-compute~stable%2Fussuri~I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1,openstack/charm-nova-compute,stable/ussuri,I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1,Add support for using service tokens,MERGED,2023-06-29 19:19:04.000000000,2023-07-04 08:01:07.000000000,2023-07-04 08:01:07.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:19:04.000000000', 'files': ['hooks/nova_compute_context.py', 'templates/stein/nova.conf', 'templates/train/nova.conf', 'templates/queens/nova.conf', 'templates/rocky/nova.conf', 'unit_tests/test_nova_compute_contexts.py'], 'web_link': 'https://opendev.org/openstack/charm-nova-compute/commit/73c4fddf348cf983d5fd9d75aa9afd264648cd99', 'message': 'Add support for using service tokens\n\nThis patch configures Nova to send a service token along with the\nreceived user token on requests to other services. This can allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Nova. Also with this patch Nova will\naccept request from other services with invalid user tokens but valid\nservice tokens. Service tokens exist since Openstack Queens.\n\nCloses-Bug: #1992840\nChange-Id: I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1\n(cherry picked from commit 3c53110282b97c42a00cee9ee344f32dc8cf29c5)\n(cherry picked from commit b54f6701d6cc408990add5fdb81bac1c74aa20db)\n'}]",1,887332,73c4fddf348cf983d5fd9d75aa9afd264648cd99,10,4,1,11805,,,0,"Add support for using service tokens

This patch configures Nova to send a service token along with the
received user token on requests to other services. This can allow those
other services to accept the request even if the user token has been
invalidated since received by Nova. Also with this patch Nova will
accept request from other services with invalid user tokens but valid
service tokens. Service tokens exist since Openstack Queens.

Closes-Bug: #1992840
Change-Id: I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1
(cherry picked from commit 3c53110282b97c42a00cee9ee344f32dc8cf29c5)
(cherry picked from commit b54f6701d6cc408990add5fdb81bac1c74aa20db)
",git fetch https://review.opendev.org/openstack/charm-nova-compute refs/changes/32/887332/1 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/nova_compute_context.py', 'templates/stein/nova.conf', 'templates/train/nova.conf', 'templates/queens/nova.conf', 'templates/rocky/nova.conf', 'unit_tests/test_nova_compute_contexts.py']",6,73c4fddf348cf983d5fd9d75aa9afd264648cd99,bug/1992840," 'admin_role': 'Admin', 'admin_role': 'Admin', 'admin_role': 'Admin', 'admin_role': 'Admin', 'admin_role': 'Admin',",,15,0
openstack%2Fcharm-nova-compute~stable%2Fussuri~Icb77ff079e9924cba6681bb919b84d6a51d5fe6b,openstack/charm-nova-compute,stable/ussuri,Icb77ff079e9924cba6681bb919b84d6a51d5fe6b,charm-helpers sync,MERGED,2023-06-29 18:59:10.000000000,2023-07-04 08:01:05.000000000,2023-07-04 08:01:05.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:59:10.000000000', 'files': ['hooks/charmhelpers/contrib/hahelpers/cluster.py', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'hooks/charmhelpers/contrib/openstack/templates/haproxy.cfg', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken', 'hooks/charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'hooks/charmhelpers/contrib/network/ovs/__init__.py', 'hooks/charmhelpers/contrib/openstack/ssh_migrations.py', 'hooks/charmhelpers/contrib/openstack/context.py', 'hooks/charmhelpers/fetch/ubuntu.py', 'hooks/charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'hooks/charmhelpers/contrib/network/ovs/ovsdb.py', 'hooks/charmhelpers/contrib/openstack/templates/section-service-user', 'hooks/charmhelpers/contrib/openstack/cert_utils.py', 'hooks/charmhelpers/contrib/openstack/utils.py', 'hooks/charmhelpers/contrib/storage/linux/ceph.py', 'hooks/charmhelpers/core/host.py', 'hooks/charmhelpers/contrib/network/ip.py', 'hooks/charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'hooks/charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf'], 'web_link': 'https://opendev.org/openstack/charm-nova-compute/commit/c05eb8d904093d32787b3ef603df5d2535a67371', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: Icb77ff079e9924cba6681bb919b84d6a51d5fe6b\n'}]",0,887315,c05eb8d904093d32787b3ef603df5d2535a67371,8,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: Icb77ff079e9924cba6681bb919b84d6a51d5fe6b
",git fetch https://review.opendev.org/openstack/charm-nova-compute refs/changes/15/887315/1 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/charmhelpers/contrib/hahelpers/cluster.py', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'hooks/charmhelpers/contrib/openstack/templates/haproxy.cfg', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken', 'hooks/charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'hooks/charmhelpers/contrib/network/ovs/__init__.py', 'hooks/charmhelpers/contrib/openstack/ssh_migrations.py', 'hooks/charmhelpers/contrib/openstack/context.py', 'hooks/charmhelpers/fetch/ubuntu.py', 'hooks/charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'hooks/charmhelpers/contrib/network/ovs/ovsdb.py', 'hooks/charmhelpers/contrib/openstack/templates/section-service-user', 'hooks/charmhelpers/contrib/openstack/cert_utils.py', 'hooks/charmhelpers/contrib/openstack/utils.py', 'hooks/charmhelpers/contrib/storage/linux/ceph.py', 'hooks/charmhelpers/core/host.py', 'hooks/charmhelpers/contrib/network/ip.py', 'hooks/charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'hooks/charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf']",19,c05eb8d904093d32787b3ef603df5d2535a67371,bug/1992840, KeepAliveTimeout 75 MaxKeepAliveRequests 1000 KeepAliveTimeout 75 MaxKeepAliveRequests 1000 KeepAliveTimeout 75 MaxKeepAliveRequests 1000,,103,15
openstack%2Fcharm-nova-compute~stable%2Fxena~I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1,openstack/charm-nova-compute,stable/xena,I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1,Add support for using service tokens,MERGED,2023-02-15 16:09:34.000000000,2023-07-04 07:56:30.000000000,2023-07-04 07:56:30.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-02-15 16:09:34.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-nova-compute/commit/eb7cc6b68177c9e6b99dc4171f7702f4ed867dab', 'message': 'Add support for using service tokens\n\nThis patch configures Nova to send a service token along with the\nreceived user token on requests to other services. This can allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Nova. Also with this patch Nova will\naccept request from other services with invalid user tokens but valid\nservice tokens. Service tokens exist since Openstack Queens.\n\nCloses-Bug: #1992840\nChange-Id: I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1\n'}, {'number': 2, 'created': '2023-06-29 19:16:22.000000000', 'files': ['hooks/nova_compute_context.py', 'templates/stein/nova.conf', 'templates/train/nova.conf', 'templates/queens/nova.conf', 'templates/rocky/nova.conf', 'unit_tests/test_nova_compute_contexts.py'], 'web_link': 'https://opendev.org/openstack/charm-nova-compute/commit/e82f5304ef0061b4cee4049b29a45c60194dd6bd', 'message': 'Add support for using service tokens\n\nThis patch configures Nova to send a service token along with the\nreceived user token on requests to other services. This can allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Nova. Also with this patch Nova will\naccept request from other services with invalid user tokens but valid\nservice tokens. Service tokens exist since Openstack Queens.\n\nCloses-Bug: #1992840\nChange-Id: I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1\n(cherry picked from commit 3c53110282b97c42a00cee9ee344f32dc8cf29c5)\n(cherry picked from commit b54f6701d6cc408990add5fdb81bac1c74aa20db)\n'}]",1,873923,e82f5304ef0061b4cee4049b29a45c60194dd6bd,14,4,2,34275,,,0,"Add support for using service tokens

This patch configures Nova to send a service token along with the
received user token on requests to other services. This can allow those
other services to accept the request even if the user token has been
invalidated since received by Nova. Also with this patch Nova will
accept request from other services with invalid user tokens but valid
service tokens. Service tokens exist since Openstack Queens.

Closes-Bug: #1992840
Change-Id: I78b43ef77dc1d7b5976ec81ecddf63c9e6c8b6c1
(cherry picked from commit 3c53110282b97c42a00cee9ee344f32dc8cf29c5)
(cherry picked from commit b54f6701d6cc408990add5fdb81bac1c74aa20db)
",git fetch https://review.opendev.org/openstack/charm-nova-compute refs/changes/23/873923/2 && git format-patch -1 --stdout FETCH_HEAD,"['templates/yoga/nova.conf', 'hooks/nova_compute_context.py', 'templates/stein/nova.conf', 'templates/train/nova.conf', 'templates/queens/nova.conf', 'templates/rocky/nova.conf', 'unit_tests/test_nova_compute_contexts.py']",7,eb7cc6b68177c9e6b99dc4171f7702f4ed867dab,bug/1992840," 'admin_role': 'Admin', 'admin_role': 'Admin', 'admin_role': 'Admin', 'admin_role': 'Admin', 'admin_role': 'Admin',",,414,4
openstack%2Fcharm-nova-compute~stable%2Fxena~Ic4cde39600150785027c2d3ba1235eb6020d63ad,openstack/charm-nova-compute,stable/xena,Ic4cde39600150785027c2d3ba1235eb6020d63ad,charm-helpers sync,MERGED,2023-06-29 18:36:33.000000000,2023-07-04 07:56:28.000000000,2023-07-04 07:56:28.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:36:33.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-nova-compute/commit/fd4fec093fed994df12d83298446ea5b291a8185', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: Ic4cde39600150785027c2d3ba1235eb6020d63ad\n'}, {'number': 2, 'created': '2023-06-29 19:07:53.000000000', 'files': ['hooks/charmhelpers/contrib/hahelpers/cluster.py', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken', 'hooks/charmhelpers/contrib/network/ovs/__init__.py', 'hooks/charmhelpers/contrib/openstack/ssh_migrations.py', 'hooks/charmhelpers/contrib/openstack/context.py', 'hooks/charmhelpers/fetch/ubuntu.py', 'hooks/charmhelpers/contrib/network/ovs/ovsdb.py', 'hooks/charmhelpers/contrib/openstack/templates/section-service-user', 'hooks/charmhelpers/contrib/openstack/cert_utils.py', 'hooks/charmhelpers/contrib/openstack/utils.py', 'hooks/charmhelpers/core/host.py', 'hooks/charmhelpers/contrib/network/ip.py'], 'web_link': 'https://opendev.org/openstack/charm-nova-compute/commit/f3d18c7a715019c9d2f1d571eace6c22f44b3399', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: Ic4cde39600150785027c2d3ba1235eb6020d63ad\n'}]",0,887305,f3d18c7a715019c9d2f1d571eace6c22f44b3399,12,4,2,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: Ic4cde39600150785027c2d3ba1235eb6020d63ad
",git fetch https://review.opendev.org/openstack/charm-nova-compute refs/changes/05/887305/2 && git format-patch -1 --stdout FETCH_HEAD,"['hooks/charmhelpers/contrib/hahelpers/cluster.py', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'hooks/charmhelpers/contrib/openstack/templates/section-keystone-authtoken', 'hooks/charmhelpers/contrib/network/ovs/__init__.py', 'hooks/charmhelpers/contrib/openstack/ssh_migrations.py', 'hooks/charmhelpers/contrib/openstack/context.py', 'hooks/charmhelpers/fetch/ubuntu.py', 'hooks/charmhelpers/contrib/network/ovs/ovsdb.py', 'hooks/charmhelpers/contrib/openstack/templates/section-service-user', 'hooks/charmhelpers/contrib/openstack/cert_utils.py', 'hooks/charmhelpers/contrib/openstack/utils.py', 'hooks/charmhelpers/core/host.py', 'hooks/charmhelpers/contrib/network/ip.py']",13,fd4fec093fed994df12d83298446ea5b291a8185,bug/1992840," except (dns.resolver.NXDOMAIN, dns.resolver.NoNameservers): return not (bool(result))", except dns.resolver.NXDOMAIN: return not(bool(result)),59,30
openstack%2Fcharm-cinder~stable%2Fussuri~Ie1baf68249c55c3b3a9f026ebd54edbc033fda5f,openstack/charm-cinder,stable/ussuri,Ie1baf68249c55c3b3a9f026ebd54edbc033fda5f,charm-helpers sync,MERGED,2023-06-29 18:59:14.000000000,2023-07-04 07:54:54.000000000,2023-07-04 07:54:54.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:59:14.000000000', 'files': ['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/storage/linux/ceph.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'charmhelpers/contrib/openstack/templates/haproxy.cfg', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken'], 'web_link': 'https://opendev.org/openstack/charm-cinder/commit/93ec74a5af32640c31dc911b32e4f0350704f227', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: Ie1baf68249c55c3b3a9f026ebd54edbc033fda5f\n'}]",0,887318,93ec74a5af32640c31dc911b32e4f0350704f227,8,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: Ie1baf68249c55c3b3a9f026ebd54edbc033fda5f
",git fetch https://review.opendev.org/openstack/charm-cinder refs/changes/18/887318/1 && git format-patch -1 --stdout FETCH_HEAD,"['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/storage/linux/ceph.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-api.conf', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/templates/wsgi-openstack-metadata.conf', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend', 'charmhelpers/contrib/openstack/templates/openstack_https_frontend.conf', 'charmhelpers/contrib/openstack/templates/haproxy.cfg', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken']",17,93ec74a5af32640c31dc911b32e4f0350704f227,bug/1992840,{% if service_type -%} service_type = {{ service_type }} {% endif -%} service_token_roles = {{ admin_role }} service_token_roles_required = True,,101,13
openstack%2Fcharm-nova-cloud-controller~stable%2Fxena~I95021600da8af12cb75ef5681fb5af8780ade4f8,openstack/charm-nova-cloud-controller,stable/xena,I95021600da8af12cb75ef5681fb5af8780ade4f8,Add support for using service tokens,MERGED,2023-06-29 19:13:54.000000000,2023-07-04 07:47:14.000000000,2023-07-04 07:47:14.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:13:54.000000000', 'files': ['templates/train/nova.conf', 'templates/pike/nova.conf', 'templates/rocky/nova.conf'], 'web_link': 'https://opendev.org/openstack/charm-nova-cloud-controller/commit/fdfb8331d25e80f952de67515af342183c7b6daf', 'message': 'Add support for using service tokens\n\nThis patch configures nova-cloud-controller to send a service token\nalong with the received user token on requests sent to other services.\nThis allows those other services to accept the request even if the user\ntoken has been invalidated since received by the nova services running\nin nova-cloud-controller units, the same applies for incoming requests\nfrom other services. Service tokens exist since Openstack Queens.\n\nChange-Id: I95021600da8af12cb75ef5681fb5af8780ade4f8\nCloses-Bug: #1992840\n(cherry picked from commit fd810f9afd92904cd66544c00610f830fd337299)\n(cherry picked from commit 98b637d8e9d9df19cd001e582f846e2046fba1bd)\n'}]",0,887327,fdfb8331d25e80f952de67515af342183c7b6daf,8,4,1,11805,,,0,"Add support for using service tokens

This patch configures nova-cloud-controller to send a service token
along with the received user token on requests sent to other services.
This allows those other services to accept the request even if the user
token has been invalidated since received by the nova services running
in nova-cloud-controller units, the same applies for incoming requests
from other services. Service tokens exist since Openstack Queens.

Change-Id: I95021600da8af12cb75ef5681fb5af8780ade4f8
Closes-Bug: #1992840
(cherry picked from commit fd810f9afd92904cd66544c00610f830fd337299)
(cherry picked from commit 98b637d8e9d9df19cd001e582f846e2046fba1bd)
",git fetch https://review.opendev.org/openstack/charm-nova-cloud-controller refs/changes/27/887327/1 && git format-patch -1 --stdout FETCH_HEAD,"['templates/train/nova.conf', 'templates/pike/nova.conf', 'templates/rocky/nova.conf']",3,fdfb8331d25e80f952de67515af342183c7b6daf,bug/1992840,"{% include ""section-service-user"" %} ",,6,0
openstack%2Fcharm-nova-cloud-controller~stable%2Fxena~Ib07b4584181db0c925f0700ff6a5ebf104d06db3,openstack/charm-nova-cloud-controller,stable/xena,Ib07b4584181db0c925f0700ff6a5ebf104d06db3,charm-helpers sync,MERGED,2023-06-29 18:36:21.000000000,2023-07-04 07:47:13.000000000,2023-07-04 07:47:13.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:36:21.000000000', 'files': ['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken'], 'web_link': 'https://opendev.org/openstack/charm-nova-cloud-controller/commit/b81ab8a900f39a3ee9f3f4e8c4f8b124ea7f20f8', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: Ib07b4584181db0c925f0700ff6a5ebf104d06db3\n'}]",1,887304,b81ab8a900f39a3ee9f3f4e8c4f8b124ea7f20f8,11,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: Ib07b4584181db0c925f0700ff6a5ebf104d06db3
",git fetch https://review.opendev.org/openstack/charm-nova-cloud-controller refs/changes/04/887304/1 && git format-patch -1 --stdout FETCH_HEAD,"['charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/contrib/openstack/utils.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken']",11,b81ab8a900f39a3ee9f3f4e8c4f8b124ea7f20f8,bug/1992840,service_token_roles = {{ admin_role }} service_token_roles_required = True,,56,10
openstack%2Fcharm-ironic-conductor~stable%2Fwallaby~Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9,openstack/charm-ironic-conductor,stable/wallaby,Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9,Add support for using service tokens,MERGED,2023-06-30 17:15:42.000000000,2023-07-04 07:46:06.000000000,2023-07-04 07:46:06.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-06-30 17:15:42.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ironic-conductor/commit/ac9a1621616018375ab9cf88df1f33f7d992a013', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-conductor to send a service token along\nwith the received user token on requests to other services. This allow\nthose other services to accept the request even if the user token has\nbeen invalidated since received by Ironic. Also with this patch Ironic\nwill accept request from other services with invalid user tokens but\nvalid service tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/zed\n\nCloses-Bug: #1992840\nChange-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9\n(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)\n(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)\n'}, {'number': 2, 'created': '2023-06-30 17:24:45.000000000', 'files': ['src/templates/train/ironic.conf', 'src/templates/parts/keystone-authtoken', 'src/build.lock'], 'web_link': 'https://opendev.org/openstack/charm-ironic-conductor/commit/9679a7945cf89e014cc458d804195ffadae52445', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-conductor to send a service token along\nwith the received user token on requests to other services. This allow\nthose other services to accept the request even if the user token has\nbeen invalidated since received by Ironic. Also with this patch Ironic\nwill accept request from other services with invalid user tokens but\nvalid service tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/wallaby\n\nCloses-Bug: #1992840\nChange-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9\n(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)\n(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)\n'}]",0,887415,9679a7945cf89e014cc458d804195ffadae52445,10,5,2,11805,,,0,"Add support for using service tokens

This patch configures ironic-conductor to send a service token along
with the received user token on requests to other services. This allow
those other services to accept the request even if the user token has
been invalidated since received by Ironic. Also with this patch Ironic
will accept request from other services with invalid user tokens but
valid service tokens.

Update src/build.lock to get backported patches into
charm-helpers@stable/wallaby

Closes-Bug: #1992840
Change-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9
(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)
(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)
",git fetch https://review.opendev.org/openstack/charm-ironic-conductor refs/changes/15/887415/2 && git format-patch -1 --stdout FETCH_HEAD,"['src/templates/train/ironic.conf', 'src/templates/parts/keystone-authtoken', 'src/build.lock']",3,ac9a1621616018375ab9cf88df1f33f7d992a013,bug/1992840," ""version"": ""77f25c49dfd08ac16c7a1cc018b008aa704bd313"",} "," ""version"": ""f8f822d5e908b1ac5694815c2d5522b6533e04f8"",}",6,2
openstack%2Fcharm-keystone~stable%2Fxena~Id7e84d38a9f774179808137548307c9174a87f87,openstack/charm-keystone,stable/xena,Id7e84d38a9f774179808137548307c9174a87f87,Add admin-role parameter value to identity relation,MERGED,2023-06-29 19:12:45.000000000,2023-07-04 07:45:11.000000000,2023-07-04 07:45:11.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 19:12:45.000000000', 'files': ['unit_tests/test_keystone_utils.py', 'hooks/keystone_utils.py'], 'web_link': 'https://opendev.org/openstack/charm-keystone/commit/5e1da17387a7549e94afa71606619f0ed89163e1', 'message': 'Add admin-role parameter value to identity relation\n\nThis parameter is added to the relation in order to configure service\ntokens on related services. The role of the service user is required for\nservice token validation.\n\nCloses-Bug: #1992840\nChange-Id: Id7e84d38a9f774179808137548307c9174a87f87\n(cherry picked from commit 55bd7022242857fd8d8c1cc823411021e61bcba4)\n(cherry picked from commit d3246b4c6e3e6274c67f49b282f1077373c3247a)\n'}]",0,887323,5e1da17387a7549e94afa71606619f0ed89163e1,8,4,1,11805,,,0,"Add admin-role parameter value to identity relation

This parameter is added to the relation in order to configure service
tokens on related services. The role of the service user is required for
service token validation.

Closes-Bug: #1992840
Change-Id: Id7e84d38a9f774179808137548307c9174a87f87
(cherry picked from commit 55bd7022242857fd8d8c1cc823411021e61bcba4)
(cherry picked from commit d3246b4c6e3e6274c67f49b282f1077373c3247a)
",git fetch https://review.opendev.org/openstack/charm-keystone refs/changes/23/887323/1 && git format-patch -1 --stdout FETCH_HEAD,"['unit_tests/test_keystone_utils.py', 'hooks/keystone_utils.py']",2,5e1da17387a7549e94afa71606619f0ed89163e1,bug/1992840," ""admin_role"": config(""admin-role""),",,2,0
openstack%2Fcharm-keystone~stable%2Fxena~Ic51ddbb22f4ccc2fcc2e4cec6655c9e5a4a90d3a,openstack/charm-keystone,stable/xena,Ic51ddbb22f4ccc2fcc2e4cec6655c9e5a4a90d3a,charm-helpers sync,MERGED,2023-06-29 18:36:11.000000000,2023-07-04 07:45:10.000000000,2023-07-04 07:45:10.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:36:11.000000000', 'files': ['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken'], 'web_link': 'https://opendev.org/openstack/charm-keystone/commit/bfd4a1716ef2965e3f0994692071fc81a6bd0061', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: Ic51ddbb22f4ccc2fcc2e4cec6655c9e5a4a90d3a\n'}]",1,887302,bfd4a1716ef2965e3f0994692071fc81a6bd0061,11,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: Ic51ddbb22f4ccc2fcc2e4cec6655c9e5a4a90d3a
",git fetch https://review.opendev.org/openstack/charm-keystone refs/changes/02/887302/1 && git format-patch -1 --stdout FETCH_HEAD,"['charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/contrib/openstack/utils.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken']",11,bfd4a1716ef2965e3f0994692071fc81a6bd0061,bug/1992840,service_token_roles = {{ admin_role }} service_token_roles_required = True,,49,10
openstack%2Fcharm-ironic-conductor~stable%2Fxena~Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9,openstack/charm-ironic-conductor,stable/xena,Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9,Add support for using service tokens,MERGED,2023-06-30 17:14:02.000000000,2023-07-04 07:42:22.000000000,2023-07-04 07:42:22.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-06-30 17:14:02.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ironic-conductor/commit/24b7c690fe03edaf21b85df0784ffffe3849a44b', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-conductor to send a service token along\nwith the received user token on requests to other services. This allow\nthose other services to accept the request even if the user token has\nbeen invalidated since received by Ironic. Also with this patch Ironic\nwill accept request from other services with invalid user tokens but\nvalid service tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/zed\n\nCloses-Bug: #1992840\nChange-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9\n(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)\n(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)\n'}, {'number': 2, 'created': '2023-06-30 17:25:06.000000000', 'files': ['src/templates/train/ironic.conf', 'src/templates/parts/keystone-authtoken', 'src/build.lock'], 'web_link': 'https://opendev.org/openstack/charm-ironic-conductor/commit/f5a89b29246185826cf855d977ce1747e538aa5b', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-conductor to send a service token along\nwith the received user token on requests to other services. This allow\nthose other services to accept the request even if the user token has\nbeen invalidated since received by Ironic. Also with this patch Ironic\nwill accept request from other services with invalid user tokens but\nvalid service tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/xena\n\nCloses-Bug: #1992840\nChange-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9\n(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)\n(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)\n'}]",0,887414,f5a89b29246185826cf855d977ce1747e538aa5b,11,5,2,11805,,,0,"Add support for using service tokens

This patch configures ironic-conductor to send a service token along
with the received user token on requests to other services. This allow
those other services to accept the request even if the user token has
been invalidated since received by Ironic. Also with this patch Ironic
will accept request from other services with invalid user tokens but
valid service tokens.

Update src/build.lock to get backported patches into
charm-helpers@stable/xena

Closes-Bug: #1992840
Change-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9
(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)
(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)
",git fetch https://review.opendev.org/openstack/charm-ironic-conductor refs/changes/14/887414/1 && git format-patch -1 --stdout FETCH_HEAD,"['src/templates/train/ironic.conf', 'src/templates/parts/keystone-authtoken', 'src/build.lock']",3,24b7c690fe03edaf21b85df0784ffffe3849a44b,bug/1992840," ""version"": ""5f8dacee1a7583d3927d39887bd5460ff2e2f812"",} "," ""version"": ""63229d2ba725e773d9a0dfc97f10643b89a7f7ac"",}",6,2
openstack%2Fcharm-ironic-conductor~stable%2Fvictoria~Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9,openstack/charm-ironic-conductor,stable/victoria,Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9,Add support for using service tokens,MERGED,2023-06-30 17:16:43.000000000,2023-07-04 07:42:21.000000000,2023-07-04 07:42:21.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-06-30 17:16:43.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ironic-conductor/commit/1b78d2cce28765dc6af8ae8560dcdd5ff7c8fc64', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-conductor to send a service token along\nwith the received user token on requests to other services. This allow\nthose other services to accept the request even if the user token has\nbeen invalidated since received by Ironic. Also with this patch Ironic\nwill accept request from other services with invalid user tokens but\nvalid service tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/zed\n\nCloses-Bug: #1992840\nChange-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9\n(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)\n(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)\n'}, {'number': 2, 'created': '2023-06-30 17:24:31.000000000', 'files': ['src/templates/train/ironic.conf', 'src/templates/parts/keystone-authtoken', 'src/build.lock'], 'web_link': 'https://opendev.org/openstack/charm-ironic-conductor/commit/2540cd90e5646d3f324b4657c7b60f7b305505b2', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-conductor to send a service token along\nwith the received user token on requests to other services. This allow\nthose other services to accept the request even if the user token has\nbeen invalidated since received by Ironic. Also with this patch Ironic\nwill accept request from other services with invalid user tokens but\nvalid service tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/victoria\n\nCloses-Bug: #1992840\nChange-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9\n(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)\n(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)\n'}]",0,887416,2540cd90e5646d3f324b4657c7b60f7b305505b2,10,5,2,11805,,,0,"Add support for using service tokens

This patch configures ironic-conductor to send a service token along
with the received user token on requests to other services. This allow
those other services to accept the request even if the user token has
been invalidated since received by Ironic. Also with this patch Ironic
will accept request from other services with invalid user tokens but
valid service tokens.

Update src/build.lock to get backported patches into
charm-helpers@stable/victoria

Closes-Bug: #1992840
Change-Id: Ie94b5ce9ba9d015a31a78bb71ce7ca786377d6d9
(cherry picked from commit c7dda3f3a8c4b3e5445c727590eb44e4a6482cc3)
(cherry picked from commit aa73b57b9c4bffd376ca65869df964bc205accef)
",git fetch https://review.opendev.org/openstack/charm-ironic-conductor refs/changes/16/887416/2 && git format-patch -1 --stdout FETCH_HEAD,"['src/templates/train/ironic.conf', 'src/templates/parts/keystone-authtoken', 'src/build.lock']",3,1b78d2cce28765dc6af8ae8560dcdd5ff7c8fc64,bug/1992840," ""version"": ""7d2e709dc443b24177db524eb6b0c44639532479"",} "," ""version"": ""df2e33ed634200de1d02d7e5f1baaf80f22a6315"",}",6,2
openstack%2Fcharm-nova-cloud-controller~stable%2Fvictoria~I95021600da8af12cb75ef5681fb5af8780ade4f8,openstack/charm-nova-cloud-controller,stable/victoria,I95021600da8af12cb75ef5681fb5af8780ade4f8,Add support for using service tokens,MERGED,2023-06-29 19:14:19.000000000,2023-07-04 07:42:02.000000000,2023-07-04 07:42:02.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-06-29 19:14:19.000000000', 'files': ['templates/train/nova.conf', 'templates/pike/nova.conf', 'templates/rocky/nova.conf'], 'web_link': 'https://opendev.org/openstack/charm-nova-cloud-controller/commit/f3357d4254f6e0d7128b068e4fb1ef6c1c35d96a', 'message': 'Add support for using service tokens\n\nThis patch configures nova-cloud-controller to send a service token\nalong with the received user token on requests sent to other services.\nThis allows those other services to accept the request even if the user\ntoken has been invalidated since received by the nova services running\nin nova-cloud-controller units, the same applies for incoming requests\nfrom other services. Service tokens exist since Openstack Queens.\n\nChange-Id: I95021600da8af12cb75ef5681fb5af8780ade4f8\nCloses-Bug: #1992840\n(cherry picked from commit fd810f9afd92904cd66544c00610f830fd337299)\n(cherry picked from commit 98b637d8e9d9df19cd001e582f846e2046fba1bd)\n'}]",0,887329,f3357d4254f6e0d7128b068e4fb1ef6c1c35d96a,9,5,1,11805,,,0,"Add support for using service tokens

This patch configures nova-cloud-controller to send a service token
along with the received user token on requests sent to other services.
This allows those other services to accept the request even if the user
token has been invalidated since received by the nova services running
in nova-cloud-controller units, the same applies for incoming requests
from other services. Service tokens exist since Openstack Queens.

Change-Id: I95021600da8af12cb75ef5681fb5af8780ade4f8
Closes-Bug: #1992840
(cherry picked from commit fd810f9afd92904cd66544c00610f830fd337299)
(cherry picked from commit 98b637d8e9d9df19cd001e582f846e2046fba1bd)
",git fetch https://review.opendev.org/openstack/charm-nova-cloud-controller refs/changes/29/887329/1 && git format-patch -1 --stdout FETCH_HEAD,"['templates/train/nova.conf', 'templates/pike/nova.conf', 'templates/rocky/nova.conf']",3,f3357d4254f6e0d7128b068e4fb1ef6c1c35d96a,bug/1992840,"{% include ""section-service-user"" %} ",,6,0
openstack%2Fcharm-nova-cloud-controller~stable%2Fvictoria~I0fd5b2e614fd02b132a33a2a9c09443f853d119f,openstack/charm-nova-cloud-controller,stable/victoria,I0fd5b2e614fd02b132a33a2a9c09443f853d119f,charm-helpers sync,MERGED,2023-06-29 18:55:01.000000000,2023-07-04 07:41:56.000000000,2023-07-04 07:41:56.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-29 18:55:01.000000000', 'files': ['charmhelpers/contrib/openstack/utils.py', 'charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken'], 'web_link': 'https://opendev.org/openstack/charm-nova-cloud-controller/commit/28f7708aa4b4a54fec97b819b5c8ca99bde7b3ee', 'message': 'charm-helpers sync\n\nSynchronize charm-helpers to get service token related patches.\n\nRelated-Bug: #1992840\nChange-Id: I0fd5b2e614fd02b132a33a2a9c09443f853d119f\n'}]",2,887313,28f7708aa4b4a54fec97b819b5c8ca99bde7b3ee,14,4,1,11805,,,0,"charm-helpers sync

Synchronize charm-helpers to get service token related patches.

Related-Bug: #1992840
Change-Id: I0fd5b2e614fd02b132a33a2a9c09443f853d119f
",git fetch https://review.opendev.org/openstack/charm-nova-cloud-controller refs/changes/13/887313/1 && git format-patch -1 --stdout FETCH_HEAD,"['charmhelpers/contrib/hahelpers/cluster.py', 'charmhelpers/contrib/openstack/utils.py', 'charmhelpers/core/host.py', 'charmhelpers/contrib/openstack/ssh_migrations.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken-mitaka', 'charmhelpers/contrib/openstack/templates/section-service-user', 'charmhelpers/contrib/openstack/context.py', 'charmhelpers/contrib/network/ip.py', 'charmhelpers/fetch/ubuntu.py', 'charmhelpers/contrib/openstack/cert_utils.py', 'charmhelpers/contrib/openstack/templates/section-keystone-authtoken']",11,28f7708aa4b4a54fec97b819b5c8ca99bde7b3ee,bug/1992840,service_token_roles = {{ admin_role }} service_token_roles_required = True,,56,10
openstack%2Fcharm-ironic-conductor~stable%2Fyoga~Id6fb1d5dddfa0bc0f7eabfe18ca1300d7fdc934e,openstack/charm-ironic-conductor,stable/yoga,Id6fb1d5dddfa0bc0f7eabfe18ca1300d7fdc934e,Update build.lock for charm-helper service token support,MERGED,2023-06-29 19:26:22.000000000,2023-07-04 07:41:44.000000000,2023-07-04 07:41:44.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}, {'_account_id': 32029}]","[{'number': 1, 'created': '2023-06-29 19:26:22.000000000', 'files': ['src/build.lock'], 'web_link': 'https://opendev.org/openstack/charm-ironic-conductor/commit/ba2bf0ffa5804620b29078638f7f3d0acedd4f34', 'message': 'Update build.lock for charm-helper service token support\n\nRelated-Bug: #1992840\nChange-Id: Id6fb1d5dddfa0bc0f7eabfe18ca1300d7fdc934e\n'}]",0,887333,ba2bf0ffa5804620b29078638f7f3d0acedd4f34,9,5,1,11805,,,0,"Update build.lock for charm-helper service token support

Related-Bug: #1992840
Change-Id: Id6fb1d5dddfa0bc0f7eabfe18ca1300d7fdc934e
",git fetch https://review.opendev.org/openstack/charm-ironic-conductor refs/changes/33/887333/1 && git format-patch -1 --stdout FETCH_HEAD,['src/build.lock'],1,ba2bf0ffa5804620b29078638f7f3d0acedd4f34,bug/1992840," ""version"": ""32772ff502e179027f46daaa04729a2f5d49f5e5"",} "," ""version"": ""971ac5117c96bd1cb4a6a520787e4509564da582"",}",2,2
openstack%2Fcharm-ironic-api~stable%2Fwallaby~Ic84e4706b93c38916e89b91dfc30bf32396e5213,openstack/charm-ironic-api,stable/wallaby,Ic84e4706b93c38916e89b91dfc30bf32396e5213,Add support for using service tokens,MERGED,2023-06-30 17:20:44.000000000,2023-07-04 07:41:38.000000000,2023-07-04 07:41:38.000000000,"[{'_account_id': 935}, {'_account_id': 20648}, {'_account_id': 20870}, {'_account_id': 22348}]","[{'number': 1, 'created': '2023-06-30 17:20:44.000000000', 'files': [], 'web_link': 'https://opendev.org/openstack/charm-ironic-api/commit/816366f727f0700d47ea0122cafb07c8da794ccc', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-api to send a service token along with the\nreceived user token on requests to other services. This allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Ironic. Also with this patch Ironic will\naccept request from other services with invalid user tokens but valid\nservice tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/yoga\n\nCloses-Bug: #1992840\nChange-Id: Ic84e4706b93c38916e89b91dfc30bf32396e5213\n(cherry picked from commit 02b7180a6b9cd9c22c4b672ae8510ea14af5e0df)\n(cherry picked from commit fca337274407d8c145c33fe5ce4805497878eb04)\n'}, {'number': 2, 'created': '2023-06-30 17:23:34.000000000', 'files': ['src/templates/train/ironic.conf', 'src/templates/parts/keystone-authtoken', 'src/build.lock'], 'web_link': 'https://opendev.org/openstack/charm-ironic-api/commit/caaf97333d36bf834827ff96349b2f699259eb41', 'message': 'Add support for using service tokens\n\nThis patch configures ironic-api to send a service token along with the\nreceived user token on requests to other services. This allow those\nother services to accept the request even if the user token has been\ninvalidated since received by Ironic. Also with this patch Ironic will\naccept request from other services with invalid user tokens but valid\nservice tokens.\n\nUpdate src/build.lock to get backported patches into\ncharm-helpers@stable/wallaby\n\nCloses-Bug: #1992840\nChange-Id: Ic84e4706b93c38916e89b91dfc30bf32396e5213\n(cherry picked from commit 02b7180a6b9cd9c22c4b672ae8510ea14af5e0df)\n(cherry picked from commit fca337274407d8c145c33fe5ce4805497878eb04)\n'}]",0,887419,caaf97333d36bf834827ff96349b2f699259eb41,9,4,2,11805,,,0,"Add support for using service tokens

This patch configures ironic-api to send a service token along with the
received user token on requests to other services. This allow those
other services to accept the request even if the user token has been
invalidated since received by Ironic. Also with this patch Ironic will
accept request from other services with invalid user tokens but valid
service tokens.

Update src/build.lock to get backported patches into
charm-helpers@stable/wallaby

Closes-Bug: #1992840
Change-Id: Ic84e4706b93c38916e89b91dfc30bf32396e5213
(cherry picked from commit 02b7180a6b9cd9c22c4b672ae8510ea14af5e0df)
(cherry picked from commit fca337274407d8c145c33fe5ce4805497878eb04)
",git fetch https://review.opendev.org/openstack/charm-ironic-api refs/changes/19/887419/2 && git format-patch -1 --stdout FETCH_HEAD,"['src/templates/train/ironic.conf', 'src/templates/parts/keystone-authtoken', 'src/build.lock']",3,816366f727f0700d47ea0122cafb07c8da794ccc,bug/1992840," ""version"": ""77f25c49dfd08ac16c7a1cc018b008aa704bd313"",} "," ""version"": ""f8f822d5e908b1ac5694815c2d5522b6533e04f8"",}",6,2
